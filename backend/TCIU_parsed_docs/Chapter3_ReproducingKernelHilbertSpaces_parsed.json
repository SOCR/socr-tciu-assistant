{
  "metadata": {
    "created_at": "2025-05-15T17:01:01.232773",
    "total_sections": 7,
    "total_code_chunks": 70,
    "total_tables": 1,
    "r_libraries": [
      "DT",
      "GPBayes",
      "MASS",
      "animation",
      "circular",
      "doParallel",
      "dtw",
      "foreach",
      "forecast",
      "furrr",
      "glmnet",
      "kableExtra",
      "kernlab",
      "mgcv",
      "mlbench",
      "patchwork",
      "plotly",
      "tidyverse"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"Spacekime Analytics (Time Complexity and Inferential Uncertainty)\"\nsubtitle: \"Reproducing Kernel Hilbert Spaces and Temporal Distribution Dynamics (TDD)\"\nauthor: \"SOCR Team (Yueyang Shen, Jun Chen, Ivo Dinov)\"\ndate: \"`r format(Sys.time(),'%m/%d/%Y')`\"\noutput:\n  bookdown::html_document2:\n    theme: spacelab\n    highlight: tango\n    includes:\n     before_body: TCIU_header.html\n    toc: yes\n    number_sections: yes\n    toc_depth: 3\n    toc_float:\n      collapsed: no\n      smooth_scroll: yes\n    code_folding: hide\n  word_document:\n    toc: yes\n\n*Hilbert space embeddings of distributions*, or *kernel mean embeddings*, map distributions into *reproducing kernel Hilbert spaces* (RKHS's) \nwhere kernel methods are extended to probability measures. Kernel mean embedding methods have many applications in probabilistic modeling of complex phenomena, statistical inference based on high-dimensional data, and artificial intelligence. In this [TCIU Reproducing Kernel Hilbert Spaces and Temporal Distribution Dynamics (TDD) Section](https://tciu.predictive.space/) we explore the relations between RKHS, kime-phase distributions, and temporal distribution dynamics.\n\nSpecifically, in this section we explore *repeated spatiotemporal sampling*, where $\\forall\\ n$, we observe $K$ (longitudinal or spatiotemporal) sample \nmeans $\\{\\bar{X}_{n1},\\bar{X}_{n2},\\cdots, \\bar{X}_{nK}\\}$.\nIn this case, $\\forall\\ 1\\leq k_o\\leq K$ each IID sample $\\{X_{1k_o},X_{2k_o},\\cdots ,X_{nk_o}\\}\\sim F_X$ (across the kime-phase space) allows us to compute the $k_o^{th}$ sample mean $\\bar{X}_{nk_o}$. \n\nComputing $\\bar{X}_{nk_o}$ from the $k_o^{th}$ sample $\\{X_{1k_o},\\cdots ,X_{nk_o}\\}$\nis identical to directly sampling $\\bar{X}_{nk_o}$ from the \n*sampling distribution of the mean* $F_{\\bar{X}_n}$. *Kime-phase sampling*, at a\nfixed *spatiotemporal index* $k=k_o$ samples from the kime-phase distribution, $\\{X_{1k_o},\\cdots , X_{nk_o}\\}$. The complementary *spatiotemporal sampling* reflects the varying $k$ index, where the kime-phase variability is annihilated by using a *kime-phase aggregator*, in this case sample-averages, to generate a sample $\\{\\bar{X}_{n1},\\cdots , \\bar{X}_{nK} \\}$.\n\nPrior to data acquisition, $\\bar{X}_{nk_o}$ is a random variable, once the observed data values are plugged in, it's a constant. Hence, the *sample mean random variable* (a *sum* of $n$ $\\sim F_X$ variables) $\\bar{X}_{nk_o}$ \nbased on $\\{X_{1k_o},\\cdots , X_{nk_o}\\}\\sim F_X$, \nand the (*single*!) random variable $Y\\sim F_{\\bar{X}_{nk_o}}$ \nrepresent exactly the same random variable. \n\nThere is a kime-phase connection to quantum fluctuations, statistical \ninference, and artificial intelligence. This is related to random drawing of a second-order tensor $X_{n\\times K}$ (a *random matrix*) representing \n$K$ (*spatiotemporal* or *longitudinal*) samples of $n$ *IID observations* \n$\\{X_{1k_o},\\cdots , X_{nk_o}\\}_{k_o}\\sim F_X$. Computing the $K$ \ncorresponding *means* $\\bar{X}_{nk_o}=\\frac{1}{n}\\sum_{i=1}^n{X_{ik_o}}$\nis *equivalent* to randomly drawing $K$ samples (a *random vector*) directly from the *sampling distribution of the mean* $F_{\\bar{X}_{nk_o}}$.",
      "word_count": 366
    },
    {
      "title": "Kernels",
      "content": "Kernel functions are positive definite operators that arise as a way to perform *inner products* $\\langle \\cdot, \\cdot \\rangle$ in high-dimensional feature spaces $\\mathcal{F}$ given observed data points $x, y \\in \\mathcal{X}$. These kernel functions guarantee \nthe existence of mappings from the *native state spaces of observed data* $\\mathcal{X}$ to \n*dot product feature spaces* $\\mathcal{F}$, $\\phi: \\mathcal{X} \\to \\mathcal{F}$, such that\nthe kernel inner product $\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle$ can be computed\nwithout the need for explicit calculation of the enigmatic *feature mapping* $\\phi$.\n\nKernel methods are used by machine learning (ML) algorithms and artificial intelligence (AI) techniques to replace the native space inner products $\\langle x, y\\rangle_{\\mathcal{X}}$ with \nother, possibly nonlinear, *distance* or *similarity measures* computed in the higher dimensional *feature space* representing the range (output) of nonlinear transformations\n$\\phi: \\mathcal{X} \\to \\mathcal{F}$, which map the native space (domain, $\\mathcal{X}$)\ninto a higher-dimensional *feature space* $\\mathcal{F}$. Each *feature map* $\\phi$\ntransforms the native space inner product distance calculations \n$\\langle x, y\\rangle_{\\mathcal{X}}: \\mathcal{X}\\times \\mathcal{X}\\to \\mathbb{R}$ with *kernel function* calculations \n$\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_{\\mathcal{F}}: \\mathcal{F}\\times \\mathcal{F}\\to \\mathbb{R}$ on the *feature space* $\\mathcal{F}$. As most of the time the elements of the inner product are clearly identifiable, we can skip the space-designating subscript\nand write $\\langle \\cdot, \\cdot\\rangle$ for $\\langle \\cdot, \\cdot\\rangle_{\\mathcal{X}}$ and $\\langle \\cdot, \\cdot\\rangle_{\\mathcal{F}}$.\n\nIn other words, we interpret $\\kappa(x, y)$ as a nonlinear *similarity measure* \nbetween the sets $x$ and $y$. Many ML/AI algorithms that depend only on the\ninner product (distances) between state space data points, i.e., on $\\langle x, y\\rangle$.\nHence, this *kernel-trick* allows us to obtain nonlinear extensions of \nsuch algorithms by simply substituting the kernel distance measures \n$\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_{\\mathcal{F}}$ for\nthe corresponding native space distances, $\\langle x, y\\rangle$ without\naltering the algorithm learning, training, and validation steps. \nFor instance, consider a *polynomial feature map* $\\phi:\\mathbb{R}^2\\to \\mathbb{R}^3$ \n\n$$\\phi(x) = (x^2_1,\\ x^2_2,\\ \\sqrt{2}\\ x_1 x_2),\\ \\forall\\ x=(x_1,x_2)\\in\\mathbb{R}^2\\ ,$$\n\nwhich transforms the calculations of the *native-space inner product* to another *feature-space inner product* as follows\n\n$$\\underbrace{\\langle x, y\\rangle_{\\mathcal{X}} \\equiv x_1 y_1 + x_2 y_2}_{\\text{native space similarity, }\\mathcal{X}} \\longrightarrow \n\\underbrace{\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_{\\mathcal{F}}\\equiv\nx_1^2 y_1^2 + x_2^2 y_2^2 + 2x_1 x_2 y_1 y_2}_{\\text{(corresponding) RKHS space similarity, }\\mathcal{F}}\\equiv \\langle x, y\\rangle_{\\mathcal{X}}^2 \\ .$$\n\nIn this case, the *kernel trick* suggests that the $\\phi$-transformed similarity measure, $\\langle \\phi(x), \\phi(y)\\rangle_{\\mathcal{F}}$, is just the square of the inner product in the native space $\\mathcal{X}$,i.e., $\\langle x, y\\rangle ^2$. \n\nThe more general $d$-degree polynomial kernel relies on a map \n$\\phi(x):\\mathcal{X}\\equiv\\mathbb{R}^n \\to \\mathcal{F}$, where $\\phi(x)$ represent\nall possible $d^{th}$ degree ordered products of the observable data $x,y\\in\\mathbb{R}^n$\n\n$$\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle_{\\mathcal{F}} = \n\\langle x, y\\rangle_{\\mathcal{X}} ^d\\ .$$ \n\nIn [DSPA Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html) we show several types of kernels commonly used in support vector machines (SVM) and other ML methods. Here are some kernel examples.\n\n - The *linear kernel* represents the simplest kernel, which is just the dot product of the features.\n\n$$\\kappa(x, y)=\\langle x,\\ y \\rangle\\ .$$\n\n - The *polynomial kernel* of degree *d* transform the data by adding a simple non-linear transformation of the data.\n\n$$\\kappa(x, y) = \\langle x, y\\rangle ^d=(x\\cdot y+1)^d \\ .$$\n\n - The *sigmoid kernel* is used in neural networks utilizing the sigmoid activation function with *scale* $s$ and *offset* $o$ parameters.\n\n$$\\kappa(x, y)=\\tanh(s\\ \\langle x, y\\rangle + o).$$\n\n - The *Gaussian radial basis function (RBF) kernel*.\n\n$$\\kappa(x, y)=e^{- \\left (\\frac{\\lVert x-y\\rVert^2}{2\\sigma^2}\\right )} .$$\n\n - The *hyperbolic tangent* kernel, for some given *scale* $s$ and *offset* $o$ parameters is the same as the *sigmoid kernel*, since the $\\tanh(\\cdot)$ function is a scaled and shifted version of the *sigmoid function*  $\\sigma(x)=\\frac{1}{1+e^{-x}}\\equiv 1-\\sigma(-x)\\in [0,1)$ and \n the *hyperbolic tangent*\n\n$$\\tanh(x)\\equiv\\frac{e^x-e^{-x}}{e^x+e^{-x}} = \\frac{1-e^{-2x}}{1+e^{-2x}}=\\frac{2-(1+e^{-2x})}{1+e^{-2x}} =\n\\frac{2}{1+e^{-2x}}-1 = 2\\sigma(2x)-1\\in(-1,1)$$\n\n$$\\kappa(x, y)=\\tanh(s\\langle x, y\\rangle + o)\\ .$$\n\n - The *Laplacian* kernel with *width* parameter $\\sigma$\n\n$$\\kappa(x, y) = e^{-\\left (\\frac{\\lVert x-y\\rVert}{\\sigma}\\right )} \\ . $$\n\n - The [*Matern* kernel](https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function) provides an interpolation between *Laplacian kernel* and *Gaussian kernel* \nparameterized by the smoothness parameter $\\nu$\n \n$$\\kappa_{\\nu} (x,y)=\\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left \n (\\sqrt{2\\nu}\\frac{\\|x-y\\|}{\\rho}\\right )^{\\nu}\n K_{\\nu}\\left (\\sqrt{2\\nu}\\frac{\\|x-y\\|}{\\rho} \\right )\\ ,$$\n\nwhere $\\sigma^2$ is the variance parameter and $\\rho$ controls the \nkernel scale (width).\n\nWhen $\\nu=\\frac{1}{2}$, \n$$\\kappa_{\\frac{1}{2}}(x,y)=\\sigma^2\\sqrt{\\frac{2}{\\pi}\\frac{\\|x-y\\|}{\\rho}}\n\\underbrace{K_{\\frac{1}{2}}\\left (\\frac{\\|x-y\\|}{\\rho}\\right )}_{\\left (2\\frac{\\|x-y\\|}{\\rho\\pi}\\right )^{-1/2}\n\\exp\\left (-\\frac{\\|x-y\\|}{\\rho}\\right )}=\n\\sigma^2 \\exp\\left (-\\frac{\\|x-y\\|}{\\rho}\\right )\\ .$$\n\nAs $\\nu\\to\\infty$, the *Matern kernel* transforms into the *Gaussian kernel*, \n\n$$\\lim_{\\nu\\to\\infty}\\kappa_{\\nu}(x,y)=\\sigma^2\\exp\n\\left (-\\frac{\\|x-y\\|^2}{\\rho}\\right )$$\n\nwhere the Laplace method may be used to [approximate the asymptotic convergence of the kernel as$\\nu\\to\\infty$](https://mathoverflow.net/questions/158598/asymptotic-expansion-of-modified-bessel-function-k-alpha)\n\n$$K_{\\nu}(x)\\to \\sqrt{\\frac{\\pi}{2\\nu}}e^{-\\nu}(2\\nu/x)^{\\nu},\\ \\Gamma(\\nu)\\to \\sqrt{2\\pi}\\nu^{\\nu-1/2}e^{-\\nu}\\ ,$$\n\n$$K_{\\nu}(\\sqrt{v}x)\\to \\sqrt{\\frac{\\pi}{2\\nu}}e^{-\\nu}\\left\n(2\\frac{\\sqrt{\\nu}}{x}\\right )^{\\nu}\\exp(x^2)\\ ,$$\n\nand $\\sqrt{\\nu x^2+\\nu^2}\\sim \\nu+\\frac{x^2}{2},\\ \\nu\\to \\infty$.\n\nThis unique interpolation of the Matern kernel between Gaussian and Laplacian kernels is effectively a *1-parameter kernel homotopy*, just like the [t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) \nis a *1-parameter (probability distribution) homotopy* between \n[Cauchy distribution](https://en.wikipedia.org/wiki/Cauchy_distribution) and \n[Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution).\n\nAlso, [Matern kernels generate classical Sobolev spaces](https://doi.org/10.1007/s00211-011-0391-2), indicating that reproducing-kernel Hilbert spaces (RKHS) associated with the Green function can be embedded into or \nare equivalent to a generalized Sobolev space.\n\nHere are the Matern kernels for a couple of \n$\\nu\\in \\left \\{\\frac{3}{2}, \\frac{5}{2} \\right \\}$ parameters:\n\n$$\\kappa_{\\frac{3}{2}}(x,y)=\\sigma^2\\left (1+\\frac{\\sqrt{3}\\|x-y\\|}{\\rho}\\right )\n\\exp\\left (-\\frac{\\sqrt{3}\\|x-y\\|}{\\rho}\\right)$$\n\n$$\\kappa_{\\frac{5}{2}}(x,y)=\\sigma^2\\left (1+\\frac{\\sqrt{5}\\|x-y\\|}{\\rho}+\\frac{5(x-y)^2}{3\\rho^2}\\right )\n\\exp \\left (-\\frac{\\sqrt{3}\\|x-y\\|}{\\rho}\\right ).$$\n\nBelow are plots of the shapes of several **Matern kernels**.\n \n\n - The *Bessel* kernel uses the [modified Bessel functions](https://en.wikipedia.org/wiki/Bessel_function#Modified_Bessel_functions), $Bessel_{\\nu +1}^{n}(u)$, or the *Bessel function of first kind* is defined by the contour integral, $J_{n}(z)$,\n\n$$J_{n}(z)=\\frac{1}{2\\pi i} \\oint e^{\\frac{z}{2}(t-\\frac{1}{t})} t^{-n-1}dt\\ ,$$\n$$Bessel_{\\nu +1}^{n}(u) = \\frac{1}{(4 \\pi)^{(\\nu +1)/2}\\Gamma ((\\nu +1)/2)} \n     \\int_0^{\\infty} {\\frac{e^{-\\left (\\frac{\\pi \\vert u \\vert^2}{s}+\\frac{s}{4 \\pi}\\right )}}\n     {s^{1 + \\frac{n - (\\nu + 1)}{2}}}\\,\\mathrm{d}s}\\ , \\forall\\ u\\in\\mathbb{R}^n\\setminus\\{0\\} ,$$\n\n$$\\kappa(x, y) = -Bessel_{\\nu +1}^{n} \\left ( \\sigma ||x-y||^2 \\right ) \\equiv\n\\frac{J_{\\nu +1}(\\sigma ||x-y||^2)}{||x-y||^{-n(\\nu + 1)}}\\ .$$\n\n - The *Spline* kernel is defined as a special case of a piecewise cubic polynomial\n\n$$\\kappa(x, y) = \\prod_{i=1}^n\\left (1+ x_i y_i (1+\\min(x_i,y_i))- \\frac{(x_i+y_i)\\min^2(x_i,y_i)}{2} +\\frac{\\min^3(x_i,y_i)}{3}\\right )\\ .$$\n\nThe [R implementations of all `kernlab` kernels are available here](https://github.com/cran/kernlab/blob/master/R/kernels.R).\n\nIn general, for each pair of observations $x,y\\in\\mathcal{X}$, evaluating the \nkernel $\\kappa(x, y)$ involves two computationally expensive steps:\n\n - First, *compute the feature maps* $\\phi(x)$ and $\\phi(y)$. \n - Second, compute the feature space inner-product $\\mathcal{F}$, $\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle _{\\mathcal{F}}$. \n \nNote that there is a simplified alternative way to compute *directly* \nthe kernel inner product (second step)\n$\\langle \\phi(x), \\phi(y)\\rangle _{\\mathcal{F}}$ without explicitly computing\nthe feature maps (step one). This requires an explicit relation, like\nthe (quadratic and $d^{th}$ degree) polynomial kernel relations we saw earlier, e.g., \n$\\kappa(x, y) = \\langle x, y\\rangle_{\\mathcal{F}}^2$ or $\\kappa(x, y) = \\langle x, y\\rangle_{\\mathcal{F}}^d$.\n\nIt turns out that for each positive definite kernel $\\kappa(x, y)$,\nthere always exists a feature map $\\phi: \\mathcal{X} \\to \\mathcal{F}$\nso $\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle _{\\mathcal{F}}$. \nAnd since feature space inner product $\\langle \\cdot, \\cdot\\rangle _{\\mathcal{F}}$ is positive definite, it implies that $\\kappa(x, y) = \\langle \\phi(x), \\phi(y)\\rangle _{\\mathcal{F}}$ is positive definite $\\forall\\ \\phi:\\mathcal{X} \\to \\mathcal{F}$. \n\n**Definition**: A *reproducing kernel Hilbert space (RKHS)* $\\mathcal{H}$ is defined as\nthe space of feature functions $\\phi:\\mathcal{X} \\to \\mathcal{F}$,\ncorresponding to *reproducing positive semi-definite and symmetric kernels $\\kappa$ \n*\n\n$$\\kappa: \\mathcal{X}\\times \\mathcal{X} \\longrightarrow \\mathbb{C}\\ \\ ({\\text{or  }} \\mathbb{R})\\ ,$$\n\n$$\\text{Reproducing}: f(x)=\\langle \\kappa(x,\\cdot),f(\\cdot)\\rangle_{\\mathcal{H}} \\forall f\\in\\mathcal{H}$$\n$$\\text{Positive semi-definiteness}:\\sum_{i,j=1}^n {c_i^*\\ c_j\\ \\kappa(x_i,x_j)}\\geq 0,\\ \\forall\\ n\\in\\mathbb{N},\\ \n\\forall\\ \\{x_i\\}_i \\in \\mathcal{X}, \\forall\\ \\{c_i\\}_i \\in \\mathbb{C} \\ . $$\n\nTwo important RKHS properties are:\n\n - $\\forall\\ x \\in \\mathcal{X}$, the function $\\kappa(x,\\cdot ): \\mathcal{X} \\to \\mathbb{C}$ defined by $\\kappa(x,\\cdot ): y\\in \\mathcal{X} \\to \\kappa(x,y) \\in \\mathbb{C}$ is an element of the RKHS $\\mathcal{H}$. The feature space $\\mathcal{F}$ is the RKHS $\\mathcal{H}$ associated with this kernel $\\kappa$ and the canonical feature map $\\kappa\\left (\\underbrace{\\_}_{arg},\\cdot \\right ): \\mathcal{X} \\to \\mathcal{H} \\subseteq \\mathcal{V}_{\\mathcal{X}\\to \\mathbb{C}}$, $x\\mapsto \\kappa(x,\\cdot )$, where $\\mathcal{V}_{\\mathcal{X}\\to \\mathbb{C}}$ is the vector space of functions $\\mathcal{X}\\to \\mathbb{C}$.\n - $\\forall\\ f \\in \\mathcal{H}$ and $\\forall\\ x \\in \\mathcal{X}$, the kernel $\\kappa(\\cdot ,\\cdot )$ represents *point evaluation*, $f (x) = \\langle f, \\kappa (x, \\cdot )\\rangle_{\\mathcal{H}}$. If $\\exists\\ x_o\\in \\mathcal{X}$ such that $f(x_o) = \\langle f, \\kappa (x_o, \\cdot )\\rangle_{\\mathcal{H}}$, the *reproducing property* is guaranteed\n \n$$\\langle \\kappa (x, \\cdot ),\\ \\kappa (x_o, \\cdot )\\rangle_{\\mathcal{H}}\\equiv \n\\kappa (x, x_o)\\ .$$\n\nThus, we do not need to explicate the feature map $\\phi_x = \\kappa(x, \\cdot )$ as it can be derived directly from the kernel $\\kappa$ by the above reproducing relation.\n\n**Note**: Below we will demonstrate several examples of using kernel/RKHS approaches for\nregularized linear modeling of data (*Ridge* and *LASSO* regression) and show the\nnotion of LASSO *shrinkage on features* (identification of salient variables associated with predicting a specific outcome of interest) and *shrinkage on cases* (selection of observation instances, i.e., participants, that play the most influential role in \nderiving model-based predictions of the outcome).",
      "word_count": 1412
    },
    {
      "title": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "content": "Let's look at some hands-on parameter estimation examples of RKHS including a regularized linear model fitting ([Ridge and LASSO](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html)) and simulation cases.\n\n## Ridge Regression Example\n\nLet's consider the [DSPA amyotrophic lateral sclerosis (ALS) case-study](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html), which consists of $2,223$ observations and $131$ numeric variables. We select *ALSFRS slope* as our outcome variable, as it captures the patientsâ€™ clinical decline over a year. In this simple demonstration, we'll only use $5$ **covariates** $\\{x_i\\}_{i=1}^5$, $x_1=$*Age_mean*, $x_2=$*Albumin_max*, $x_3=$*Albumin_median*, $x_4=$*Albumin_min*, $x_5=$*Albumin_range*, to predict the **outcome** $Y=ALSFRS\\_slope$. This ALS case-study is intentionally chosen as it represents a tough modeling, prediction, and classification AI challenge.\n\n### Data ingestion and preprocessing\n\nWe start by importing the raw data and identifying the design matrix $X$ and the clinical outcome of interest, $Y=ALSFRS\\_slope$.\n\n\n### Matrix inversion function\n\nTo avoid potential computationally singular results, we define a new function `inverse()`, which will be used in place of the more commonly used `base::solve()`.\n\n\n### Reproducing Kernel\n\nDefine a bivariate *reproducing linear kernel* operator, `rk()`, \n$\\kappa(s,t) = rk(s,t)=s'\\cdot t=\\langle s, t \\rangle$, in this case a simple *inner-product*.\n\n\n### Gram matrix\n\nFor a set of vectors $S = \\{x_1,x_2,\\cdots, x_n\\}$ representing an observed sample \nof $n$ multivariable data points $\\{x_i\\}$, the *Gram matrix* is defined as the *kernel-matrix* \n\n$$G_{n\\times n}=\\left (G_{ij} \\equiv \\langle \\phi(x_i), \\phi(x_j) \\rangle\n\\equiv \\underbrace{\\kappa (x_i, x_j )}_{kernel} \\right )\\ ,$$\n\nwhere the kernel function $\\kappa(\\cdot,\\cdot)$ computes the inner products \nin the feature-space based on the feature map $\\phi(\\cdot)$. Note that $G_{ij}\\equiv G_{ji},\\ \\forall\\ 1\\leq i,j\\leq n$ and the symmetric Gram matrix, $G'\\equiv G$,\ncontains all the information needed to compute the pairwise distances within the\ndataset.  Indeed, the Gram matrix is a *lossy representation* of the complete\noriginal dataset (set of features). Despite the fact that the Gram matrix loses some information about the original dataset it has some desirable properties:\n\n - The Gram matrix of inner products is invariant w.r.t. rotations about the origin.\n - The Gram representation loses information about the alignment between the points and the axes, since the Gram matrix is rotationally invariant. Yet, any rotation of the coordinate system will leave the Gram matrix of inner products unchanged.\n\nThe power of the Gram matrix comes from its *preservation of all pairwise distances* between the observed data points, i.e., the vectors $\\{x_i\\}_{i=1}^n$, and their $\\phi$-transformed counterparts, $\\{\\phi(x_i)\\}_{i=1}^n$.\n\nIn our examples, the `gram(X,RK)` function will represent an alternative of the standard (square transposed cross-product matrix) $tcrossprod(X)=tcrossprod(X,Y\\equiv X)= X\\times Y'$ function, which takes the (transposed) cross-product of a matrix with its transpose, i.e., $X\\times X'$.  The `gram()` operator allows us to generalize `tcrossprod()`, so that it can be used with *alternative kernels* (`RK`) provided as arguments to `gram()`.\n\n\n### Generalized Cross-Validation (GCV) Measure\n\nGiven a linear model $Y=X\\beta +\\varepsilon$ and a value of the regularization hyperparameter, $\\lambda$, the *GCV statistic* is\n\n$$V(λ)=\\frac{\\frac{1}{n}||(I-A(\\lambda))y||^2}{\n\\left (\\frac{1}{n}\\cdot Tr(I-A(\\lambda))\\right )^2}\\ ,$$\n\nwhere $n$ is the number of observations, $H(\\lambda)=X(X^TX)^{-1} X^T$\nis the *hat matrix*, and $A(\\lambda)=X(X^TX+n\\lambda I)^{-1} X^T$ \nis a *smoothed hat matrix*, and *fitted values* of the linear least-squares \nregression model are $\\hat{y}=X\\beta = X(X^TX)^{-1} X^T = Hy$.\n\nRecall the two alternative forms of the [generalized cross-validation (GCV)](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline#Generalized_cross_validation) score defined in terms of the *root sum-square (RSS) error* or the *deviance*\n\n$$GCV_{RSS} = \\frac{RSS/n}{(1 -Tr(H(\\lambda))/n)^2}$$\n$$GCV_{Deviance} = -2\\frac{LogLikelihood/n}{(1 -Tr(H(\\lambda))/n)^2} \\ .$$\n\nHere we will define the `ridge(X, y, lambda)` function, which computes the Gram matrix $G_{n\\times n}$, formulates a non-singular design matrix $Q$ (with an intercept first column), \ncomputes the inverse of the *cross-product matrix*, $M$, estimates the *ridge parameters*, and calculates the *GCV measure.*\n\nThe `R` package [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) already provides an efficient method for fitting elastic net regularized linear models.\nWe will define a *de novo* `ridge()` method that demonstrates RKHS approaches using\nthe Gram matrix to fit a Ridge regularized linear model for estimating the parameters (effect sizes).\n\nRecall that if $X$ is the *design matrix* of observed values of \n*predicting-covariates* and $Y$ is the observed *outcome vector*, [the linear model *predictions* (*fitted-values*)](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html) are expressed as\n\n$$\\underbrace{\\hat{Y}}_{fitted\\ values}=X\\hat{\\beta}=X(X'X)^{-1}X'Y\\ , $$\n\nwhere the effect-size estimates $\\hat{\\beta}=\\{\\beta_i\\}_i$ for all covariates \n$\\{x_i\\}_i$ correspond to the specific *objective (cost/loss) function* used in \nthe optimization process\n\n - *Least squares loss* (classical unregularized linear model): $\\hat{\\beta}^{OLS} = (X' X)^{-1} X' Y$.\n - *Ridge loss*: $\\hat{\\beta}_j^{(Ridge)} = (1 + N \\lambda )^{-1}\\hat{\\beta}^{OLS}_j$.\n - *LASSO loss*: $\\hat{\\beta}_j^{(LASSO)} = S_{N \\lambda}(\\hat{\\beta}^\\text{OLS}_j ) = \\hat{\\beta}^\\text{OLS}_j \\max \\left( 0, 1 - \\frac{ N \\lambda }{ |\\hat{\\beta}^{OLS}_j| } \\right)$.\n\nWe implement the kernelized ridge regression as follows (Some additional details See appendix \\@ref(kernelridge)):\n\n### Ridge regression\n\n\n\n### Optimal Hyper-Parameter Estimation\n\nUsing the *ALS dataset*, we will implement a direct search for the GCV optimal *ridge* regularization parameter $\\lambda$ to predict the clinical outcome `ALSFRS_slope`. We will use the `R` package `furrr` and request multiple cores ($workers = 8$) in the call to `furrr::future_map()`. Using multicore processing will significantly expedite this massive search calculation.\n\nWe use [LASSO](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html) (`alpha = 1`) regularization on the same ALS test case with\na *Laplacian reproducing kernel* operator, `rk()` \n\n$$\\kappa(s,t) = rk(s,t)= e^{-\\left (\\frac{\\lVert s - t\\rVert}{\\sigma}\\right )}\\ .$$\n\n\nNext we fit the `Ridge` model and plot the trajectories of the parameter estimates across\n$\\lambda$.\n\n\n\n\nThe plot allows manual zoom in to confirm that *instances (case observations)* \n$4$ and $18$ appear to be most dominating in estimating the case-contribution weights in the linear regularized model, among the first $20$ participants. By zooming in or deselecting these two instances $\\{4,18\\}$, we can explore the canonical regularization path trajectories.\n\n### RSS for different kernels and kernel-parameters \n\nThe following example uses small numbers to reduce the extreme computational burden\nand expedite the computational estimates in compiling the HTML output from the Rmd source.\nIn practice, larger numbers will provide more precise estimates. The resulting RSS\nare reported in tables.\n\n\n\n\n\n\nNext, we will expand the previous standard Ridge model by using a RKHS and\nspecifying alternative kernels in the estimation of the model coefficients.\n\n#### Linear Ridge Kernel \n\nFor $\\lambda\\in \\{0.01, 0.1, 1, 10\\}$.\n\n\n\n<!-- **Linear Kernel:** -->\n\n<!-- |$\\lambda=0.01$| $\\lambda=0.1$| $\\lambda=1$| $\\lambda=10$| -->\n<!-- |:-------|:--------|:---------|:-------| -->\n<!-- |813.27   |813.42|821.07|869.15| -->\n\n#### Matern Ridge Kernel \n\nFor $\\lambda=0.1$ and $\\nu\\in \\{0.01, 0.1, 0.5, 1, 1.5, 2, 2.5, 10\\}$.\n\n\nWe can also use any of the other kernels (e.g., polynomial)\nto fit RKHS Ridge models and compare their performance across different \nkernels, and hyperparameters (e.g., $\\nu$, $d$).\n\n\n\n<!-- **Mat&#233;rn Kernel:** ($\\lambda=0.1$)  -->\n\n<!-- |$\\nu=0.01$| $\\nu=0.5$| $\\nu=1$| $\\nu=1.5$| $\\lambda=2.5$| $\\lambda=10$| -->\n<!-- |:-------|:--------|:---------|:-------|:-------|:-------| -->\n<!-- |130.04   |384.35|652.80|719.08|749.01|762.84| -->\n\n\n\n\n<!-- **Polynomial Kernel:** ($\\lambda=0.1$)  -->\n\n<!-- |$d=0.3$| $d=0.5$| $d=0.7$| $d=0.9$| $d=1$| $d=1.5$| -->\n<!-- |:-------|:--------|:---------|:-------|:-------|:-------| -->\n<!-- |841.69|977.92|917.44|821.92|805.33|770.90| -->\n\n\nFinally, we can select the *best model* corresponding to the optimal regularization parameter $\\lambda=\\lambda_{[which.min(V)]}$ associated with minimization of the *GCV* search and obtain the corresponding *effect size estimates* $\\beta$'s, for example,\nby refitting the *Ridge* model with a linear kernel.\n\n\n### Comparison of *manual RKHS-Ridge* and *traditional GLM-Net* parameter estimates\n\nNext, we will compare our manual ridge-regularized linear model parameter estimation strategy (using RKHS) against the classical `glmnet()` gold standard. [Recall that setting the `glmnet` hyperparameter](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html) `alpha = 0` and `alpha = 1` correspond to *ridge* ($L_2$) and *LASSO* ($L_1$) regression, respectively.\n\n\nThe following table contrasts the *manual RKHS-Ridge* and the automated \n*GLM-Net Ridge* regression effect size estimates for all $5$ covariate \npredictors of the outcome `ALSFRS_slope`.\nTheoretically, the manual estimates are not expected to match \nthe *GLM-Net Ridge* estimates, as $X^TX\\neq I$.\n\n\n## LASSO Regression Example\n\nAgain, we will define a *manual* `lasso()` method *de novo* using RKHS modeling,\nthe Gram matrix, and fitting a LASSO regularized linear model for estimating the parameters (effect sizes). To see the theoretical derivation for kernelized LASSO regression (See appendix \\@ref(kernellasso)). \n\n**There is a distinction of shrinking on observations and shrinking on features**. \nWe run some experiments showing a possible way of implementing kernelized LASSO with shrinkage on the *features* or on the *instances.* Then, we will compare the results with the classical [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) approach for efficient elastic net regularized linear modeling. \n\n### Shrinkage on the instances (cases)\n\nTo demonstrate participant selection, i.e., instance or case shrinkage, we'll use \nthe following linear kernel implementation.\n\n\nThe prediction procedure is essentially\n$$\\hat{y}=K S_{\\lambda}(K^{-1}y)\\ .$$\nNotice that when $\\lambda=0$, this degenerates into a perfect reconstruction. As a benchmark, the resulting RSS and GCV values are $34.57$ and $11.71$, respectively.\n\n\nNext, we run over the grid space of the $\\lambda$ parameter.\n\n\nWe observe that initially, the GCV score decreases, since $GCV(lambda=0)= 11.71$),\nand then alternates between increases and decreases.\n\n\n\nThe large lambda values (on the right) indicate trivial effect size estimates, whereas\nthe smaller lambda values (on the left) signify nonzero effect size estimates for\nthe importance of the observation instance's (cases) as contributors to the LASSO model prediction. Of course, due to the shrinkage nature for LASSO, as lambda increases, \nall observation effect sizes are discounted and thresholded to $0$.\n\n### Feature shrinkage\n\nSelection of salient features (variable selection) is independent from the prior\nsearch for influential cases (observations) that impact the LASSO model prediction.\n\nNow we focus on the traditional *feature selection* using LASSO.\n\n\n\nLet's examine the GCV and RSS performance metrics.\n\n\nFit the LASSO feature-selection model over the $\\lambda$ space partition (grid).\n\n\nPlot the resulting effect-size (beta) estimate trajectories.\n\n\nDirect comparisons between different *RSS measures* across different kernels is\nchallenging, as the corresponding *feature maps* $\\phi(y)$ are different and hence,\nthe RKHS embedding is in different space scales. For instance, the scale of\nthe *Laplace kernel* feature map may be very small compared to\nthe larger scale of the feature map associated with the linear kernel. \nThere is no uniform criterion to interpreting performance metrics such as *RSS*\nacross different RKHS embedding spaces.\n\nAs a sanity check at the end, we can select the *best model* corresponding to the \noptimal regularization parameter $\\lambda=\\lambda_{[which.min(V)]}$ associated \nwith minimization of the *GCV* search and obtain the corresponding effect size \nestimates $\\beta$'s. This will allow us to contrast our implementation of\n*LASSO kernel feature selection* shrinkage against the *linear dot product kernel*,\nwhich roughly matches the `glmnet()` approach. Again, these will not be\nidentical as $X^TX\\neq I$.\n\n\n### Contrasting the *manual RKHS-LASSO* and *traditional GLM-Net* parameter estimates\n\nLet's compare the manual *LASSO-regularized linear model parameter estimation strategy (using RKHS)* against the classical `glmnet()`, as a gold standard. [Recall that setting the `glmnet` hyperparameter](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html) `alpha = 1` corresponds to *LASSO* ($L_1$) regression.\n\n\nThe following table compares the *manual RKHS-LASSO* and the automated \n*GLM-Net LASSO* regression effect size estimates for all $5$ covariate \npredictors of the outcome `ALSFRS_slope`.\n\n\nCompare these *LASSO* (effect-size) $\\beta$ estimates against their *Ridge*\ncounterparts, which we computed earlier for the same *ALS data*.\n\n## 1D Cubic Spline Example\n\n### Data Setup\n\nFor this example we'll simulate a spirals dataset representing 2D point cloud data of two spirals corrupted with Gaussian noise using the functions `kernlab::spirals()`\nand `mlbench::mlbench.spirals()`.\n### Binary Cross-Entropy (BCE) loss function\n\nThe [binary cross-entropy (BCE)](https://en.wikipedia.org/wiki/Cross-entropy) loss function, also called *Log Loss*, or *logistic loss* can be used for binary outcome predictions\n\n$$BCE\\equiv L_{\\log}(y,p) = -\\left ( y\\ \\log(p) + (1-y)\\ \\log(1-p) \\right )\\ , $$\n\nwhere $y$ is the true binary label (e.g., $0$ or $1$) and $p$ is the model-based\nprobability estimate that $0\\leq p=Prob(y=1)\\leq 1$. A more general *Balanced Log Loss* includes weight coefficients $w(y_i)$\n\n$$BalancedBCE\\equiv = -\\frac{1}{n}\\sum_{i=1}^n {\nw(y_i)\\left ( y_i\\ \\log(p(y_i)) + (1-y(p_i))\\ \\log(1-p(y_i)) \\right )}\\ , $$\n\nwhere $n$ is the total number of samples, $y_i$ is the true binary label of the\n$i^{th}$ sample ($0$ or $1$), $p(y_i)$ is the predicted probability of the\n$i^{th}$ sample being of class $1$, and the weights $w(y_i)$\nfor class $y_i$ are appropriately chosen, e.g., the inverse of the class\nfrequencies.\n\n\n### Reproducing Kernel\n\nTo compute the Gram matrix, we need to first define the *spline reproducing kernel function* `rk_spline()` implementing the special case of a piecewise cubic polynomial\n\n$$\\kappa(x, y) = \\prod_{i=1}^n\\left (1+ x_i y_i (1+\\min(x_i,y_i))- \\frac{(x_i+y_i)\\min^2(x_i,y_i)}{2} +\\frac{\\min^3(x_i,y_i)}{3}\\right )\\ .$$\n\n\n### Smoothing Spline\n\nNext we define the `smoothing_spline()` function using the Gram matrix and the\nRKHS `rk_spline()` function.\n\n\n### Hyper-parameter Estimation\n\nNow we can fit the RKHS spline model for multiple $\\lambda$ values and optimize for the GCV metric to select the optimal $\\lambda$ parameter.\n\n\nPlot of GCV across search space of $\\lambda$ values.\n\n\n### Comparison\n\nAgain, we can compare and contrast the *manual spline RKHS model* fit estimation\nagainst a standard model, in this case, a *generalized additive model (GAM)*\nestimated using function `mgcv::gam()`. See the [*gam* model specification format](https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/gam.models.html).\n\nThe graph below shows the raw spirals simulated data (as scatter points color coded by their true arc-type class labels), and three different models - *Spline-RKHS*, *GAM*, and *Binary* classifier. The *shapes* and *colors* of the *data-points* and the *predictions* of the $3$ models (also rendered as scatter to avoid occlusions/overlaps) represent the true class labels (shapes) and the model-fit (colors). Note how well the binary-GAM model\ndiscriminates between the two spiral arches, correctly identifying the majority of the \n*yellow-arch* points (high positive values, *red-triangle symbols*) and *black-arch* points (low negative values, *red-circle symbols*).",
      "word_count": 2149
    },
    {
      "title": "Predicting the Future Behavior of Time-Varying Probability Distributions",
      "content": "In this section we will connect *spacekime analytics* with *Reproducing Kernel Hilbert Spaces* (RKHS). The basic idea is that *repeated measurements of longitudinal processes* can be viewed as *time-varying probability distributions.* The explicit connection\nis that the kime-phase distribution $\\Phi_{[-\\pi,\\pi)}^{(t)}$ represents the distribution of the repeated measurements at a fixed spatiotemporal location $t$. As time (kime-magnitude) and space evolve, these *phase* distributions change and their dynamics can be modeled using RKHS. Recall the following example shown in [TCIU Chapter 3 appendix (Radon-Nikodym Derivatives, Kime Measures, and Kime Operator](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_Radon_NikodymDerivatives.html).\n\n## Simulation of Kime-phase Distribution Dynamics\n\nIn this simulation, the $3$ *spatial dimensions* $(x,y,z)\\in\\mathbb{R}^3$ are compressed \ninto $1D$ along the vertical axis $z\\in\\mathbb{R}^1$, the *radial displacement* represents\nthe time dynamics, $t\\in \\mathbb{R}^+$, and the angular scatters of three different processes, representing repeated measurements from $3$ different circular distributions\nat a fixed spatiotemporal location, are shown in different colors. Mean-dynamics\nacross time of the three different time-varying distributions are shown as smooth curves color-coded to reflect the color of their corresponding *phase distributions*, *samples*, \nand *sampling-distributions.*\n\n\n\nThe *intrinsic process error* $\\varepsilon_m\\sim N(0,\\sigma^2V)$ reflects classical *variability in the underlying process distribution*. The more subtle *error-tensor* $\\varepsilon_g\\sim N(0,R_g)$ depends on the *variance-covariance* matrix of the group-level errors $R_g$ and can be interpreted as *repeated-measures variability due to the kime-phase distribution*. \n\nHence, the kime-phases can be used to model or track *repeated-measure variations*. Recall this kime-phase simulation we showed above demonstrating alternative kime-phase distributions coupled with corresponding *mean-phase-aggregators* (solid radial lines) representing real observable (deterministic, not stochastic) measurable quantities. The latter avoid the intrinsic kime-phase distribution noise due to random sampling through the phase-space distribution $\\Phi_{[-\\pi,\\pi)}^{(t)}$ when taking real measurements depending *de facto* on some phase-aggregating kernels that smooth the *intractable* (unobservable) and *intrinsically-random* kime-phases.\n\nIn applied sciences, *sample statistics*, such as the sample mean, variance, percentiles, range, interquartile range, etc., are always well-defined. Their values are not known at any given time until the actual (finite) sample $\\{x_i\\}_{i=1}^n$ is collected (observations are recorded). For instance the *sample mean statistic* is the *arithmetic average* of all observations, $\\bar{x}_n=\\frac{1}{n}\\sum_{i-1}^n {x_i}$, yet, not knowing the value of the sample-mean prior to collecting the data is simply due to lack of evidence, since we have a closed form expression of how to obtain the *sample average statistic* estimating the population (physical system's) *expected mean response*, $\\mu_X\\ \\underbrace{\\leftarrow}_{{n\\to\\infty}}\\ \\bar{x}_n$. \n\n*Quantum systems* interact in ways that can be explained with superpositions of different discrete base-states, and quantum system measurements yield statistical results corresponding to any one of the possible states appearing at random. \n\nSimilarly, *analytical systems* interact in ways that can be explained with superpositions of different, discrete, and finite samples. Correspondingly, analytical system measurements yield statistics corresponding to any one of the specific sample-statistic outcomes, which vary between samples and experiments. However, the first two fundamental laws of probability theory governing all statistical inference ([CLT](https://doi.org/10.1080/10691898.2008.11889560) and [LLN](https://doi.org/10.1080/10691898.2009.11889499)) yield that this *between sample variability of sample-statistics* is expected to decrease rapidly with the increases of the sample size. For instance, IID samples $\\{x_i\\}_{i=1}^n\\sim \\mathcal{D}(\\mu_X,\\sigma_X^2)$, drawn from most *nice distributions* $\\mathcal{D}$ with mean $\\mu_X$ and variance $\\sigma_X^2$, yield *sample-means* $\\bar{x}_n$ whose [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution) converges (in distribution) to a [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), an [asymptotic limiting distribution](https://en.wikipedia.org/wiki/Asymptotic_distribution) with rapidly decaying variance as the sample size increases\n\n$$\\underbrace{\\bar{x}_n }_{Sample\\\\ average}\\ \\ \\sim\\ \n\\underbrace{\\bar{\\mathcal{D}}(\\overbrace{\\mu_{\\bar{x}_n}}^{mean},\n\\overbrace{\\sigma^2_{\\bar{x}_n}}^{variance}) }_{Sampling\\\\ distribution}\\ \\ \\  \\overbrace{\\underbrace{\\longrightarrow}_{n\\to\\infty}}^{Convergence\\\\ in\\ distribution} \\underbrace{\\mathcal{N}\\left(\\mu_X,\\frac{\\sigma_X^2}{n}\\right)}_{Asymptotic\\\\ distribution}\\ .$$\n\n## Dual Kime-Phase Distribution and Spatiotemporal State Sampling\n\nLet's try to explicate the duality between sampling from the *kime-phase distribution*\nand the complementary sampling of the process state space (*spatiotemporal sampling*).\n\nSuppose the process $X\\sim F_X$, where $F_X$ is an arbitrary distribution with a\nmean $\\mu=\\mu_X$ and variance $\\sigma^2=\\sigma^2_X$. If $\\{X_1,\\cdots ,X_n\\}\\sim F_X$ is an IID sample the CLT suggests that under mild assumptions \n\n$$\\bar{X}\\underset{n\\to\\infty}{\\overset{d}{\\longrightarrow}} N\\left(\\mu,\\frac{\\sigma^2}{n}\\right )\\ .$$\n\nIn *repeated spatiotemporal sampling*, $\\forall\\ n$, we \nobserve $K$ (longitudinal) sample means $\\{\\bar{X}_{n1},\\bar{X}_{n2},\\cdots, \\bar{X}_{nK}\\}$,\nwhere $\\forall\\ 1\\leq k_o\\leq K$ each IID sample $\\{X_{1k_o},X_{2k_o},\\cdots ,X_{nk_o}\\}\\sim F_X$ (across the kime-phase space) allows us to compute the $k_o^{th}$ sample mean $\\bar{X}_{nk_o}$. \n\nObserve that computing $\\bar{X}_{nk_o}$ from the $k_o^{th}$ sample $\\{X_{1k_o},\\cdots ,X_{nk_o}\\}$\nis identical to directly sampling $\\bar{X}_{nk_o}$ from the distribution $F_{\\bar{X}_n}$. Let's reflect on this set up which clearly involves $2$ independent sampling strategies.\n\n - *Kime-phase sampling*, where the *spatiotemporal index* $k=k_o$ is fixed and we sample through the kime-phase distribution, $\\{X_{1k_o},\\cdots , X_{nk_o}\\}$.\n - The complementary *spatiotemporal sampling* reflects the varying $k$ index, where the kime-phase variability is annihilated by using a *kime-phase aggregator*, in this case sample-averages, to generate a sample $\\{\\bar{X}_{n1},\\cdots , \\bar{X}_{nK} \\}$.\n\nNote that the distribution $F_{\\bar{X}_n}$ need not be Normal, it may be, but in general it won't be *exactly* Normal. For instance suppose our spatiotemporal sampling scheme involves exponential distribution, i.e., $\\{X_{1k_o},\\cdots , X_{nk_o}\\}\\sim f_X\\equiv {\\text{Exp}}(\\lambda)\\equiv\\Gamma(1,\\lambda)$ and $\\sum_{i=1}^n{X_{ik_o}}\\sim \\Gamma\\left(n,\\lambda\\right )$,\n\n$$f_{{\\text{Exp}}{(\\lambda)}}(x)=\\lambda e^{-\\lambda x}\\ \\ ; \\ \\mathbb{E}(X_{{\\text{Exp}}{(\\lambda)}})=\\frac{1}{\\lambda} \\ \\ ; \\\nVar(X_{{\\text{Exp}}{(\\lambda)}})= \\frac{1}{\\lambda^2}\\ ; \\\\ \nf_{\\Gamma(shape=\\alpha,scale=\\lambda)}(x)=\\frac{x^{\\alpha-1}\\lambda^{\\alpha}}{\\Gamma(\\alpha)}e^{-\\lambda x}\\ \\ ; \\ \n\\mathbb{E}(X_{\\Gamma(\\alpha,\\lambda)})=\\alpha\\lambda \\ \\ ; \\\nVar(X_{\\Gamma(\\alpha,\\lambda)})= \\alpha\\lambda^2\\ \\ .$$\n\nBased on this sample, the sampling distribution of $\\bar{X}_{nk_o}\\equiv \\frac{1}{n}\\sum_{i=1}^n{X_{ik_o}}\\sim Exp\\left (\\lambda\\right )\\equiv\\Gamma\\left(n,n\\lambda\\right )$, since\na sum of exponential IID $Exp(\\lambda)$ variables is $\\Gamma$-distributed, \n$\\sum_{i=1}^n{X_{ik_o}}\\sim \\Gamma\\left(n,\\lambda\\right )$ and scaling a random variable $Y=cX$ yields $F_Y(x)=\\frac{1}{c}f_X(\\frac{x}{c})$, in our case the \nconstant multiplier $c=\\frac{1}{n}$, and $Y=\\frac{1}{n}\\sum_{i=1}^n{X_i}$.\n\nOf course, as $n\\to\\infty$, $\\Gamma\\left(n,\\frac{\\gamma}{n}\\right )\\to N(0,1)$, yet\nfor any fixed $n$, the (gamma) $\\Gamma$ distribution is similar, but not identical to *normal.* \n\nPrior to data acquisition, $\\bar{X}_{nk_o}$ is a random variable, once the observed data values are plugged in, it's a constant. Hence, the *sample mean random variable* (a *sum* of $n$ $\\sim F_X$\nvariables) $\\bar{X}_{nk_o}$ based on $\\{X_{1k_o},\\cdots , X_{nk_o}\\}\\sim F_X$, \nand the (*single*!) random variable $Y\\sim F_{\\bar{X}_{nk_o}}$ represent exactly\nthe same random variable. \n\n**Corollary**: Random drawing of a second-order tensor $X_{n\\times K}$ \n(a *random matrix*) representing $K$ (*spatiotemporal* or *longitudinal*)\nsamples of $n$ *IID observations* \n$\\{X_{1k_o},\\cdots , X_{nk_o}\\}_{k_o}\\sim F_X$ and then computing the $K$ \ncorresponding means $\\bar{X}_{nk_o}=\\frac{1}{n}\\sum_{i=1}^n{X_{ik_o}}$\nis *equivalent* to randomly drawing $K$ samples (a *random vector*) directly from the sampling distribution of the mean $F_{\\bar{X}_{nk_o}}$. \n\n\n## Stochastic Processes\n\nBelow is an example of a 3D [Brownian motion/Wiener process](https://en.wikipedia.org/wiki/Stochastic_process) as an example of a [stochastic random walk process](https://en.wikipedia.org/wiki/Stochastic_process).\nIn this example, the Wiener process is intentionally disturbed by random Poisson noise, which leads to occasional abrupt disruptions of the already stochastic process.\n\n\nIn our spacekime representation, we effectively have a (repeated measurement) spatiotemporal process including a 3D spatial Gaussian model which is dynamic in time. In other words, the 3D [Gaussian Process](https://socr.umich.edu/DSPA2/DSPA2_notes/13_FunctionOptimization.html#57_Bayesian_Optimization) with a *mean vector* and a *variance-covariance matrix tensor* both dependent on time, $\\mu=\\mu(t)=(\\mu_x(t),\\mu_y(t),\\mu_z(t))'$ and \n\n$$\\Sigma=\\Sigma(t)=\\left ( \\begin{array}{ccc} \\Sigma_{xx}(t) & \\Sigma_{xy}(t) & \\Sigma_{xz}(t)\\\\ \\Sigma_{yx}(t) & \\Sigma_{yy}(t) & \\Sigma_{yz}(t) \\\\ \\Sigma_{zx}(t) & \\Sigma_{zy}(t) & \\Sigma_{zz}(t) \\end{array}\\right )\\ .$$\n\nThe process distribution $\\mathcal{D}(x,y,z,t)$ is specified by $\\mu=\\mu(t)$ and $\\Sigma=\\Sigma(t)$. Given a spatial location, e.g., brain voxel, we the distribution probability density function at $(x,y,z)\\in\\mathbb{R}^3$ depends on the time localization, $t\\in \\mathbb{R}^+$. Actual repeated sample observations will draw phases from the phase distribution, $\\{\\phi_i\\}_i\\in\\Phi_{[-\\pi,\\pi)}^{(t)}$, which are associated with the *fixed spatiotemporal location* $(x,y,z,t)\\in\\mathbb{R}^3\\times \\mathbb{R}^+$. The repeated spatiotemporal samples are \n$$\\left\\{\\left (\\underbrace{x_i}_{x(\\phi_i)}, \\underbrace{y_i}_{y(\\phi_i)}, \\underbrace{z_i}_{z(\\phi_i)},\n\\underbrace{t_i}_{t(\\phi_i)}\\right )\\right\\}_i\\in\\mathbb{R}^3\\times \\mathbb{R}^+\\ .$$\n\nWhen the *mean vector* and the *variance-covariance matrix* vary with time $t$, proper inference may require use of [Wiener processes (Brownian motion)](https://en.wikipedia.org/wiki/Wiener_process), [Ito calculus](https://en.wikipedia.org/wiki/It%C3%B4_calculus), [Heston models](https://en.wikipedia.org/wiki/Heston_model), or\n[stochastic differential equation models](https://en.wikipedia.org/wiki/Stochastic_differential_equation). \n\nMore details about stochastic differential equations, Radon-Nikodym derivatives,\nand kime phase-distribution random sampling are available in [TCIU Chapter 3 Appendix (Radon-Nikodym Derivatives, Kime Measures, and Kime Operator)](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_Radon_NikodymDerivatives.html).\n\n## Hilbert Spaces\n\nA *Hilbert space* $\\mathcal{H}$ is any vector space over the complex field $\\mathbb{C}$ \n(or the reals $\\mathbb{R}$) that satisfies the following conditions:\n\n - There is a well defined *complex inner product* on elements of $\\mathcal{H}$, $\\langle\\cdot,\\cdot\\rangle_{\\mathcal{H}} \\equiv \\langle\\cdot,\\cdot\\rangle:\\mathcal{H}\\times \\mathcal{H}\\to \\mathbb{C}$.\n - $\\mathcal{H}$ is also a complete metric space with respect to the *distance function* induced by the inner product, $d_{\\mathcal{H}}(x,y)=|x-y|_{\\mathcal{H}}={\\sqrt {\\langle x-y,x-y\\rangle }},\\ \\forall\\ x,y\\in \\mathcal{H}$.\n  - The inner product is *conjugate-symmetric*, $\\langle y,x\\rangle ={\\overline {\\langle x,y\\rangle }},\\ \\forall\\ x,y\\in \\mathcal{H}$, and $\\langle x,x\\rangle\\in\\mathbb{R}\\subsetneqq\\mathbb{C}$.\n  - The inner product is *linear* $\\langle a_1 x_{1}+a_2 x_2,y\\rangle =a_1\\langle x_1,y\\rangle +a_2\\langle x_2,y\\rangle$, $\\forall\\ x_1,x_2,y\\in \\mathcal{H}$ (vectors), and $\\forall\\ a_1,a_2\\in \\mathbb{C}$ (scalars).\n  - The self inner product of an element $x\\in \\mathcal{H}$ is positive definite, $\\langle x,x\\rangle\\begin{cases} > 0\\quad {\\text{ , if }}x\\neq 0,\\\\ =0\\quad  {\\text{ , if }}x=0\\,.\\end{cases}$.\n\nThe base scalar field of the Hilbert space could also be the reals, $\\mathbb{R}$. Hilbert spaces are important as for any pair of square-integrable real-valued functions\n$f,g\\in \\mathcal{H}\\equiv L^2[a,b]$, their function *inner product* defined by\n$\\langle f,g\\rangle =\\int _{a}^{b}f(x)g(x)\\,\\mathrm {d} x$ has analogous properties as the classical *Euclidean dot product* and leads to defining function-bases as orthonormal\nfunctions spanning the vector (Hilbert) space $\\mathcal{H}$. This (function) inner product\n$\\langle f,g\\rangle$ induces a [spectral decomposition of an operator](https://en.wikipedia.org/wiki/Spectral_theorem) of the form\n\n$$f(x)\\longrightarrow \\int _{a}^{b}\\kappa(x,y)f(y)\\,\\mathrm {d} y\\ ,$$\n\nwhere the *continuous and symmetric kernel* $\\kappa(x,y)$ permits\n(spectral) *eigenfunction decomposition* as a series \n\n$$\\kappa(x,y)=\\sum _{n}\\lambda_{n}\\varphi _{n}(x)\\phi_{n}(y), $$\n\nin terms of the orthonormal *eigenfunctions* $\\langle \\phi_n,\\phi_k\\rangle=\\begin{cases} 1\\quad {\\text{ , if }}n = k\\\\ 0\\quad  {\\text{ , if }}n\\not= k\\end{cases}$ and the\ncorresponding (scalar) *eigenvalues* $\\{\\lambda_{i}\\}_i\\in\\mathbb{C}$. The *completeness* requirement of Hilbert spaces prevents such eigenfunction expansions from failing\nto converge to square-integrable functions! \n\n\n**Spectral Theorem**: If $A$ is a *bounded self-adjoint operator* on a Hilbert space \n$\\mathcal{H}$. Then, there exists a *measure space* $(X, \\Sigma, \\mu)$, a real-valued bounded *measurable function* $f:X\\to\\mathbb{R}$, and a *unitary operator* \n$U:\\mathcal{H}\\to L^2(X, \\mu)$ such that $A=U^*TU$, where $T$ is the \n*multiplication operator* $(T\\phi )(x)=f(x)\\phi (x)$ and \n$\\|T\\|=\\|f\\|_{\\infty }$.\n\nThe spectral theorem guarantees that **all** *bounded self-adjoint operators* are \nunitarily equivalent to *multiplication operators*, which are separable and\neasier to represent, compute, and interpret. \n\n\n## Temporal Distribution Dynamics (TDD)\n\n**Computational Task**: The goal here is to implement this *time-Varying kime-phase probability distribution modeling* scheme described in this arXiv article [Predicting the Future Behavior of a Time-Varying Probability Distribution](https://arxiv.org/abs/1406.5362) ...\n\n### Protocol Outline\n\nOne approach for modeling the *temporal distribution dynamics (TDD)* requires the following operations:\n\n - Representation of each repeated sample at a fixed spatiotemporal point as a *vector* in a Hilbert space (this connects with RKHS).\n - Deriving a *time-operator* that captures the longitudinal (kime-magnitude, time) dynamics of the repeated-measurement vectors from $t_i$ to $t_{i+1} \\gt t_i$.\n - Applying the time-operator to the repeated-measurement vectors at the final time point $t_T\\gt t_{T-1}\\gt \\cdots\\ \\gt t_2 \\gt t_1$, which will extrapolate and *predict* the longitudinal dynamics one step beyond the final observed time $t_T$. \n - Draw a new repeated sample from the posterior predictive probability distribution, i.e., simulate realistic kime-phases, and forecast the process behavior (by computing an estimate of the corresponding *kime-phase aggregator*, or a *repeated sample statistic*, at the next, yet unobserved, time point, $t_{(T+1)}$.\n\n### Step 1: RKHS Embedding\n\nAt a fixed spatiotemporal point we can build the desired *repeated sample vector representations* in a Hilbert space by using reproducing kernel Hilbert space (RKHS) embedding of the corresponding kime-phase probability distributions.\nDenote by $\\mathcal{P}=\\{\\Phi_{[-\\pi,\\pi)}\\}$ the set of all kime-phase probability distributions on $\\mathcal{X}=[-\\pi,\\pi)$ with respect to a specific $\\sigma$-algebra. Assume $\\kappa: \\mathcal{Z}\\times \\mathcal{Z}\\to \\mathbb{R}$ is a positive definite symmetric *kernel* corresponding to a *RKHS* $\\mathcal{H}$ and a *feature map* $\\phi: \\mathcal{X}\\to \\mathcal{H}$, such that $||\\phi(x)||\\leq 1$, $\\forall\\ x\\in \\mathcal{X}$. The \n*kernel mean embedding* $\\mu :\\mathcal{P}\\to \\mathcal{H}$ associated with $\\kappa$\nis defined by\n\n$$\\mathcal{P}\\ni p \\to \\mu(p) \\equiv \\mathbb{E}_{x\\sim p(x)}(\\phi(x))\\in \\mathcal{H}\\ .$$\n\nEffectively, for a fixed *kernel* $\\kappa$, *feature map* $\\phi$, and a corresponding *RKHS* $\\mathcal{H}$, $\\mu(p)$ as the *RKHS embedding* of $p$. Let $\\mathcal{M}$ be the $\\mu$-image of $\\mathcal{P}$.\n\nThen, $\\mu(\\mathcal{P})\\equiv \\mathcal{M}\\subseteq \\mathcal{H}$ and the image $\\mathcal{M}$ *represents the set of repeated sample vectors* corresponding\nto embedded probability distributions. For special *characteristic kernels*, \nsuch as the Gaussian, the *kernel mean map* is a lossless *bijection* \n$\\mu: \\mathcal{P}\\longleftrightarrow \\mathcal{M}$ where no information\nis lost by the embedding operation. Hence, we will refer to all\nobjects $\\Phi\\in \\mathcal{P}$ and $\\mu\\in\\mathcal{M}$ as **distributions**.\n\nAn important *kernel mean embedding property* allows computing expected values \nvia inner products by\n\n$$\\mathbb{E}_p(\\phi)\\equiv \\mathbb{E}_{x\\sim p(x)}(\\phi(x))\n= \\langle \\mu(p), \\phi\\rangle_{\\mathcal{H}}\\in \\mathcal{M}\\subseteq \\mathcal{H}\\ .$$\n\nThe theoretical distribution *kernel mean embedding* $\\mu :\\mathcal{P}\\to \\mathcal{M}$\nhas a discrete sample counterpart - the empirical *sample kernel mean embedding*\ndefined for any observed IID sample of kime-phases $S=\\{x_1, x_2, \\cdots, x_n\\}$\n\n$$S\\to \\hat{\\mu}_n(S)\\equiv\\frac{1}{n}\\sum_{i=1}^n {\\phi(x_i)} \\ .$$\n\nOf course, the empirical *sample kernel mean embedding* is just one of many *kime-phase aggregators* we can consider, see [Spacekime Analytics/TCIU Book](https://doi.org/10.1515/9783110697827). Other examples of kime-phase \naggregators include *geometric-mean*, *median*, *kurtosis*, *scrambling phase*, etc.\n\nSubject to some RKHS constraints on $\\mathcal{H}$, the *sample kernel mean embedding* converges in probability to the theoretical distribution *kernel mean embedding*\ntrue embedding, see [this article for details](https://link.springer.com/chapter/10.1007/11776420_13).\n\n$$\\underbrace{\\hat{\\mu}_n(S)\\underset{n\\to\\infty}{\\overset{d}{\\longrightarrow}} \\mu(p)}_{\\text {for any fixed time, } t},\\ \n\\ \\ \\hat{\\mu}_n(S)\\approx \\mu(p) + O\\left (\\frac{1}{\\sqrt{n}}\\right )\\ .$$\n\nObserve that the sample kernel mean embedding $\\hat{\\mu}_n(S)$, implicitly depends on\nthe time $t$. The first step of the TDD involves computing the series of sample kernel mean embeddings for all time points,\n\n$$\\{ \\hat{\\mu}_{1}=\\hat{\\mu}_{n,t=1}(S_1), \\hat{\\mu}_{2}=\\hat{\\mu}_{n,t=2}(S_2), \\cdots,\n\\hat{\\mu}_{T}=\\hat{\\mu}_{n,t=T}(S_T)\\}$$\n\ncorresponding to the $T$ longitudinally observed repeated sample sets\n$\\{ S_1, S_2, \\cdots, S_T \\}$. The vectors $\\{\\hat{\\mu}_{t}\\}$ may not\nbe always explicitly computed since the kernel feature map $\\phi$\nis generally unknown. This is why RKHS representations are important\nas kernel methods do not require explicit knowledge of the embedding\nvectors. In many algorithms it is often sufficient to compute the\ncorresponding vector inner products, which can be accomplished by evaluation\nof the kernel function.\n\n### Step 2: Deriving the time dynamics of the repeated-measurement vectors\n\nNext we use the kime-phase distributions of the observed repeated-measurement vectors at fixed spatiotemporal locations to derive the global time dynamics of the system.\n\nVector-valued regression or other strategies can be employed to model the process temporal dynamics represented by the RKHS embedded distributions evolving with time.\n\nWe follow the vector-valued RKHS regression approaches in the following papers: [Two-Sample Test Statistics for Measuring Discrepancies Between Two Multivariate Probability Density Functions Using Kernel-Based Density Estimates](https://doi.org/10.1006/jmva.1994.1033), [A Hilbert Space Embedding for Distributions](https://link.springer.com/chapter/10.1007/978-3-540-75225-7_5), [Predicting the Future Behavior of a Time-Varying Probability Distribution](https://doi.org/10.1109/CVPR.2015.7298696), and [Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces](https://doi.org/10.1002/cjs.5550350410). \n\nNote that learning the RKHS operators requires that both the regression *independent variables* (*inputs*) and *dependent outcome* (*output*) are vectors. Let's first define the state space, $\\mathcal{F}$, of \noperators on the RKHS $\\mathcal{H}$ that we will search over for an optimal regression operator, i.e., $\\mathcal{F}:\\mathcal{H}\\overset{linear}{\\longrightarrow} \\mathcal{H}$. \nSuppose \n$\\mathcal{L}(\\mathcal{H})=\\{\\mathcal{F}:\\mathcal{H}\\longrightarrow \\mathcal{H}\\}$ is the space of all *bounded linear operators* on $\\mathcal{H}$ and \n$L:\\mathcal{H}\\times \\mathcal{H}\\to \\mathcal{L}(\\mathcal{H})$ is the non-negative $\\mathcal{L}(\\mathcal{H})$-valued kernel defined by \n\n$$L(f, g) \\equiv \\underbrace{\\langle f, g\\rangle _{\\mathcal{H}}}_{scalar}\\ \\underbrace{\\mathbb{I}_{\\mathcal{H}}}_{operator}\n:\\mathcal{H}\\times \\mathcal{H}\\to \\mathcal{L}(\\mathcal{H}),$$\n\nwhere $f,g\\in \\mathcal{H}$ and $\\mathbb{I}_{\\mathcal{H}}$ is the *identity*\noperator on $\\mathcal{H}$ such that \n$\\mathbb{I}_{\\mathcal{H}}(h)=h,\\ \\forall\\ h\\in \\mathcal{H}$. \n\n**Definition**: The *rank-1 operators* $A\\in\\mathcal{L}(\\mathcal{H})=\\{\\mathcal{F}:\\mathcal{H}\\longrightarrow \\mathcal{H}\\}$\nare such that $\\forall\\ \\phi\\in\\mathcal{H}$, $A\\phi\\equiv \\langle \\psi, \\phi\\rangle\\nu$, \nwhere $\\nu\\in Rg(A)\\subseteq \\mathcal{H}$ and $\\psi\\in Dom(A)\\subseteq \\mathcal{H}$ \nare bounded linear functionals on $\\mathcal{H}$. Similarly, *finite finite-rank* $n$\n*operators* $A$ have finite dimensional *range* $Rg(\\mathcal{H})$ and \n\n$$\\forall\\ \\phi\\in\\mathcal{H},\\ \\ A\\phi\\equiv \\sum_{i=1}^{n} {\n\\underbrace{\\langle \\psi_i, \\phi\\rangle}_{scalar,\\ in\\ \\mathbb{C}}\n\\overbrace{\\nu_i}^{basis}},$$\n\nwhere $\\nu_i\\in Rg(A)\\subseteq \\mathcal{H}$ and $\\psi_i\\in Dom(A)\\subseteq \\mathcal{H}$ \nare bounded linear functionals on $\\mathcal{H}$. \n\n**Definition**: A *functional RKHS space* $\\mathcal{H}$ is a subset of \n$\\mathcal{F}: \\mathcal{H}\\to \\mathcal{H}$ which is a Hilbert space\nequipped with the inner product $\\langle\\cdot,\\cdot\\rangle_\\mathcal{H}$\nso that $\\forall\\ h\\in \\mathcal{H}$ the *point evaluation operator* \n$L_h : \\mathcal{F} \\to \\mathcal{F}(h)$ is a bounded linear operator. This is a non-constructive definition.\n\nBy the [Riesz representation theorem](https://en.wikipedia.org/wiki/Riesz_representation_theorem) applied to \nthe Hilbert space $\\mathcal{H}$, $\\exists\\ K_h^g\\in \\mathcal{H}$, such that\n$\\langle K_h^g, F \\rangle_{\\mathcal{H}} = \\langle F(h), g\\rangle _{H}$. Since the mapping \n$g\\to  K_h^g$ is linear and the point evaluation is bounded we have that\n\n$$\\langle K_h^g, F \\rangle_{\\mathcal{H}}= \\langle F(h), g \\rangle_H \\leq \nc||F||_{\\mathcal{H}}||g||_{H}\\ .$$\n\nThis suggest that implies that the mapping $g\\to K_h^g$ is also bounded and\n$\\forall\\ h'\\in H$ $g\\to K_h^g(h')=:K(h,h')$ is a bounded linear operator, i.e,\n$K(h,h')(g) := K_h^g(h')$.\n\n**Definition**: The bounded linear operator \n$K(\\cdot,\\cdot): \\mathcal{H}\\times\\mathcal{H}\\to \\mathbb{C}$ defined by \n$g\\to K_h^g(h')=:K(h,h')$ is called the *reproducing kernel associated* with \n$\\mathcal{H}$. \n\n**Definition**: The *kernel reproducing property* is \n$\\langle K(h, \\cdot)(g)\\ ,\\ F \\rangle_{\\mathcal{H}} = \\langle F (h), g\\rangle_H,\\ \\forall\\ h,g \\in H$.\n\n**Reproducing kernel Properties**\n\n - (Anti-symmetry) $K(h, h') = K(h',h)^*$, where $^*$ denotes the adjoint operator. This property is derived by using $\\langle K(h, \\cdot)(g)\\ ,\\ F \\rangle_{\\mathcal{H}} = \\langle F (h), g\\rangle_H$ and expanding the point evaluation of the left hand side for any $f,g\\in H$\n\n$$\\langle K(h, h')g, f \\rangle_H = \\langle K_h^g(h'), f\\rangle_H=\n\\langle K_{h'}^f(h), g\\rangle_{H}=\\langle g, K(h',h)f\\rangle_{H}\\ .$$\n\n - (non-negative definiteness) Since \n\n$$\\sum_{i,j}\\langle K(h_i,h_j)(f_i), f_j\\rangle_H=\n\\sum_{i,j}\\langle K_{h_i}^{f_i}, K_{h_j}^{f_j}\\rangle_{\\mathcal{H}}=\n||\\sum_{i} K_{h_i}^{f_i}||_{\\mathcal{H}}, $$\n\nwe have\n\n$$\\sum_{i,j}\\langle K(h_i,h_j)(f_i), f_j\\rangle_H\\geq 0\\ .$$\n\nThe *reproducing kernel of some operator-valued RKHS* \n$L(\\cdot, \\cdot):\\mathcal{H}\\times \\mathcal{H}\\to \\mathcal{L}(\\mathcal{H})$ is\n$\\mathcal{F}\\subseteq \\mathcal{L}(\\mathcal{H})$, where $\\mathcal{F}$\ncontains at least the span of all *rank-1 operators* whose range is one-dimensional.\n\nLet $f,g\\in \\mathcal{H}$ and \n$$g^*\\equiv \\langle g,\\cdot \\rangle_{\\mathcal{H}}: \\mathcal{H}\\to \n\\mathcal{L}(\\mathcal{H}).$$ \n\nThen $L(f, g) \\equiv \\langle f, g\\rangle _{\\mathcal{H}} \\mathbb{I}_{\\mathcal{H}}$\nis the reproducing kernel of the operator \n$g^*\\equiv \\langle g,\\cdot \\rangle_{\\mathcal{H}}$, since\n$\\forall\\ f_1,f_2,g_1,g_2\\in \\mathcal{H}$ the feature-space ($\\mathcal{F}$) \ninner product is separable in the Hilbert-space ($\\mathcal{H}$), \n\n$$\\langle f_1g_1^*, f_2g_2^* \\rangle_{\\mathcal{F}}=\n\\langle f_1, g_1 \\rangle_{\\mathcal{H}}\n\\langle f_2, g_2 \\rangle_{\\mathcal{H}}\\ ,$$\nwhere\n\n$$fg^*\\in \\mathcal{L}(\\mathcal{H}):\\mathcal{H}\\to \\mathcal{H}, \\ {\\text{s.t.}}\\ \n\\ \\ \\forall\\ f,g\\in \\mathcal{H},\\ \\ g^*\\equiv \\langle g,\\cdot \\rangle_{\\mathcal{H}},\n\\ \\ fg^*= f\\langle g,\\cdot \\rangle_{\\mathcal{H}}\\ .$$\n\nAll bounded operators of rank-1 \n$A\\in\\mathcal{L}(\\mathcal{H})=\\{\\mathcal{F}:\\mathcal{H}\\longrightarrow \\mathcal{H}\\}$\nhave 1D range, $\\dim(Rg(A)\\equiv1$), and $\\forall\\ h\\in \\mathcal{H}$ \n$\\exists!\\ \\phi\\in \\mathcal{H}$ and $\\exists\\ \\psi\\in Rg(\\mathcal{H})\\setminus\\{0\\}$\nso that $Ah=\\langle h, \\phi\\rangle \\psi$. Observe that since \n$\\psi\\in Rg(\\mathcal{H})\\setminus\\{0\\}$, $\\exists\\ y\\in \\mathcal{H}$, such that\n$Ay=\\psi$ and $\\langle y, \\phi\\rangle\\equiv 1$, since \n\n$$Ah=\\langle h, \\phi\\rangle \\psi= \\langle h, \\phi\\rangle \n\\overbrace{\\underbrace{\\langle y, \\phi\\rangle}_{1} Ay}^{\\psi=Ay}\\ .$$\n\nBy *linearity* of the inner product and the *completeness* of the Hilbert space,\nthe inner product of all finite-rank $(n)$ operators in the feature-space \n$\\mathcal{F}\\subseteq \\mathcal{L}(\\mathcal{H})$ can be expressed as \nsuperpositions, or linear combinations, of rank-1 operator inner products \nrelative to some basis.\n\nNext we can define vector-valued regression to learn a predictive model of the \n*dynamics of the kime-phase distribution* and explicate the *time dynamics* of \nthe repeated-measurement vectors. Suppose the changes of the phase\ndistributions from one time point to the next can be approximated by an\n[autoregressive process](https://socr.umich.edu/DSPA2/DSPA2_notes/12_LongitudinalDataAnalysis.html) \n$\\mu_{t+1} = A\\mu_t +\\varepsilon_t,\\ \\forall\\ t\\geq 0$, for some bounded operator\n$A:\\mathcal{H}\\to \\mathcal{H}$ where the residual errors \n$\\{\\varepsilon_t\\}_{t=0}^T \\sim\\mathcal{D}$ are independent zero-mean random \nvariables (IID) from the same distribution $\\mathcal{D}$. To approximate\nthe unknown autoregressive operator $A$ we can use classical least-squares where\nthe *objective (cost) function* is a mixture of a *fidelity* term and a \n*regularization* term with some unknown regularization hyperparameter $\\lambda\\geq 0$:\n\n$$C(A)=\\min_{A\\in \\mathcal{F}} \\left (\n\\underbrace{\\sum_{t=0}^{T-1} {||\\overbrace{\\hat{\\mu}_{t+1}}^{observed}-\n\\overbrace{A\\hat{\\mu}_t}^{model\\ est.}||_{\\mathcal{H}}^2}}_{fidelity} +\n\\lambda \\underbrace{||A||_{\\mathcal{F}}^2}_{regularizer} \\right ) \\ ,$$\n\nThe closed-form *solution minimizing this cost function* $C(A)$ is\n\n$$\\hat{A}= \\sum_{t=0}^{T-1} {\\left ( \\hat{\\mu}_{t+1} \n\\sum_{s=0}^{T-1} {w_{ts}\\hat{\\mu}_s^*}\n\\right )}\\ ,$$\n\nwhere the *weight-coefficient matrix* $W = (w_{ts})=(K + \\lambda I)^{-1}$, \n$I=I_{(T-1)\\times(T-1)}$ is the *identity matrix*, and the *kernel matrix* \n$K=\\left (\\{\\langle \\hat{\\mu}_s , \\hat{\\mu}_t \\rangle_{\\mathcal{H}} \\}_{s,t} \\right )\\in \\mathbb{R}^{(T-1)\\times(T-1)}$, or in $\\mathbb{C}^{(T-1)\\times(T-1)}$.\n\nThis optimal solution is the result of the [following theorem (Lian)](https://www.jstor.org/stable/20445281):\n\n**Theorem**: Given an observed dataset $\\{(x_i, y_i)\\}^n_{i=1}$, the solution to \nthe optimization problem $\\min_A C(A)$ has the following representation\n$\\hat{A}=\\sum_{i=1}^n {K(h_i,\\cdot)\\alpha_i}$, where the functional coefficients\n$\\alpha_i \\in H$.\n\n*Proof*: Let's denote by $\\mathcal{H}_o\\subseteq \\mathcal{H}$ the subspace spanned by the kernel centered at the observed covariates\n$$\\mathcal{H}_o = \\left \\{\\sum_{i=1}^n { K(h_i,\\cdot )\\alpha_i, \\ \\ \\alpha_i \\in H }\\right \\}.$$\n\nAny $A\\in \\mathcal{H}$ can be decomposed to $A = A_o + G$, where $A_o\\in \\mathcal{H}_o$\nand $G\\in \\mathcal{H} \\perp \\mathcal{H}_o$. Then by the reproducing property,\n$\\forall\\ 1\\leq j\\leq n$ and\n$\\forall\\ h \\in H$, $\\langle G(x_i), h\\rangle_H = \\langle K(x_j , \\cdot)(h)\\rangle, G\\rangle_H = 0$. As this holds $\\forall\\ h \\in H$, $G(x_j) = 0$.\nAs $G \\perp A_o$, $\\forall\\ G\\not= 0$\n\n$$||A||_{\\mathcal{H}} = ||A_o||_{\\mathcal{H}} + ||G||_{\\mathcal{H}} > ||A_o||_{\\mathcal{H}}\\ .$$ \n\nTherefore, the minimizer $\\hat{A}\\in \\mathcal{H}_o$ since\n\n$$\\sum_{i=1}^n {||y_i - A(x_i)||_2^2} + \\lambda ||A||_{\\mathcal{H}} \\gt\n\\sum_{i=1}^n {||y_i - A_o(x_i)||_2^2} + \\lambda ||A_o||_{\\mathcal{H}}\\ .$$\n\nHence, when $A\\in \\mathcal{F}$, the estimated operator $\\hat{A}$ will converge \nto the true operator $A$ as the number of sample sets (*replicated samples*) \n$n\\to\\infty$ and the number of samples per set (*time*) $T\\to\\infty$. \n[This is true under some minor requirements (topological separability for embedded distributions in reproducing kernel Hilbert spaces)](https://doi.org/10.48550/arXiv.1411.2066).\n\n\n### Step 3: Applying the time-operator for forward extrapolation and *prediction*\n\nTo model and extrapolate the temporal dynamics of the kime-phase distribution forward we can apply the learned operator $\\hat{A}$ to the last time point $t$ where we observed the kime-phase distribution $\\hat{\\mu}_T$. In reality, the kime-phases\nare random, i.e., they may not be directly observable as sample phases. However, \ntheir distribution $\\Phi_{[-\\pi,\\pi)}$ is generally symmetric and zero-mean, and\nit can be modeled using some priors (e.g., truncated Laplace distribution) or inferred\nvia analytical time-to-kime transformations, e.g., [Laplace Transform](https://www.socr.umich.edu/TCIU/HTMLs/Chapter4_Laplace_Transform_Timeseries_Kimesurfaces.html) \n$\\mathcal{L}:\\mathbb{R}^+ \\to \\mathbb{C}$.\n\nThe forward prediction $\\hat{\\mu}_{T+1}=\\hat{A}\\hat{\\mu}_{T}$ approximates the \nunknown phase distribution $\\mu_{T+1}$ as a mixture (weighted linear combination) of\nthe previously observed kime-phase distributions at times $1\\leq t\\leq T$. Specifically,\n\n$$\\hat{\\mu}_{T+1}= \\sum_{t=2}^T {\\beta_t \\hat{\\mu}_{t}}\\ , \\ \\  {\\text{where }}\\\\ \n\\underbrace{\\beta_{t+1}}_{time\\ effects} =\\ \\sum_{s=1}^{T-1} {w_{ts}\\langle  \n\\hat{\\mu}_{s}, \\hat{\\mu}_{T}\\rangle_{\\mathcal{H}} } \\ , \\ \\forall\\ 1\\leq t\\leq T-1\\ , \\\\\n\\langle  \n\\hat{\\mu}_{s}, \\hat{\\mu}_{t}\\rangle_{\\mathcal{H}}=\n\\frac{1}{n_s\\ n_t} \\sum_{i=1}^{n_s}{\\sum_{j=1}^{n_t}{\n\\kappa\\left(z_i^{(s)}, z_i^{(t)}\\right )}}, \\\\\n\\underbrace{W = (\\{w_{ts}\\}_{ts})}_{weights}=(\\underbrace{K}_{kernel} + \\lambda I)^{-1},\\ \\ \\underbrace{I=I_{(T-1)\\times(T-1)}}_{identity\\ matrix},\\\\\nK=\\left (\\{\\langle \\hat{\\mu}_s , \\hat{\\mu}_t \\rangle_{\\mathcal{H}} \\}_{s,t} \n\\right )\\in \\mathbb{C}^{(T-1)\\times(T-1)}\\ .$$\n\nThus, $\\hat{\\mu}_{T+1}$ can be expressed via the RKHS relation, \n$\\langle \\kappa (x, \\cdot ),\\ \\kappa (x_o, \\cdot )\\rangle_{\\mathcal{H}}\\equiv \\kappa (x, x_o)$,\nin terms of the originally observed repeated sample measurements \n$$\\underbrace{S_t \\equiv \\left \\{z_i^{(t)}\\right \\}}_{\\text{Data}},\\ \n\\underbrace{\\forall\\ \\ 1\\leq i\\leq n_t}_{\\text{repeated phase sampling}}, \n\\ \\underbrace{\\forall\\ \\ 1\\leq t\\leq T}_{\\text{time points}}\\ .$$\n\n#### Extrapolation\n\nEmploying the estimates of the *time-effect* values $\\beta_t\\in \\mathbb{C}$ \nwe can extrapolate the *kime-phase aggregate statistics* $\\hat{\\mu}_{T+1}$ one time-step forward\nbeyond the observed time-points $1\\leq t\\leq T$. That is we can estimate various\nkime-phase population distribution characteristics (e.g., mean phase) outside\nof the *convex hull* of the observed kime-phase distributions. However,\nthe estimate $\\hat{\\mu}_{T+1}$ is guaranteed to be in the subspace spanned \n$\\{\\hat{\\mu}_{2}, \\hat{\\mu}_{3}, \\cdots, \\hat{\\mu}_{T}\\}$ computed using\nthe observed samples $\\{S_t\\}_t$. \n\nSince $\\mathcal{H}$ is the RKHS of the *kernel* $\\kappa(\\cdot,\\cdot)$ corresponding\nto the *feature map* $\\phi(\\cdot)$, \n$\\forall\\ z\\in \\mathcal{Z},\\ {\\text{ and }}\\ \\forall\\ f\\in \\mathcal{H},\\ \\langle \\phi(z), f\\rangle_{\\mathcal{H}}\\equiv f(z)$. Hence, for any element in the (vector) Hilbert space, $\\forall\\ f\\in \\mathcal{H}$, we can estimate the *pseudo expected value* of $f$, $\\widehat{\\mathbb{E}}(f)$, with respect to the forward prediction $\\hat{\\mu}_{T+1}$ as follows\n\n$$\\widehat{\\langle f\\rangle}_{\\hat{\\mu}_{T+1}} \\equiv {\\widehat{\\mathbb{E}}}_{\\hat{\\mu}_{T+1}}(f)=\n\\langle \\hat{\\mu}_{T+1}, f\\rangle_{\\mathcal{H}}=\n\\sum_{t=2}^T {\\beta_t \\langle \\hat{\\mu}_{t}, f\\rangle_{\\mathcal{H}}}=\\\\\n\\sum_{t=2}^T {\\left (\\beta_t \\frac{1}{n_t}\\sum_{i=1}^{n_t}\\langle \n\\phi(z_i^t), f\\rangle_{\\mathcal{H}}\\right )}=\n\\sum_{t=2}^T \\sum_{i=1}^{n_t}{\\left (\\frac{\\beta_t}{n_t}f(z_i^t) \\right )}\\ .$$\n\nRecall that the *kernel mean embedding* from the set $\\mathcal{P}$ of all kime-phase\nprobability distributions on $\\mathcal{Z}$ into the RKHS $\\mathcal{H}$, \n$\\mu: \\mathcal{P}\\to \\mathcal{H}$ associated with the kernel \n$\\kappa:\\mathcal{Z}\\times \\mathcal{Z}\\underset{p\\mapsto \\mu(p)}{\\longrightarrow} \\mathbb{C}$ is defined by $\\mu(p) = \\mathbb{E}_{z\\sim p(z)}(\\phi(z))$.\nThus, the *pseudo expected value* $\\widehat{\\mathbb{E}}(f)$ is just an *estimate* of the\ncorresponding *expected value* ${\\mathbb{E}}(f)$. This is because \n$\\langle \\hat{\\mu}_{T+1}, f\\rangle_{\\mathcal{H}}$ is not guaranteed to have a \npre-image $p\\in \\mathcal{P}$ in the space of kime-phase probability distributions $\\mathcal{P}$. However this lemma shows that $\\hat{\\mu}_{T+1}$ is a reasonable\nproxy measure for the the unknown *kernel mean embedding* at time $T+1$, i.e.,\n$\\hat{\\mu}_{T+1}\\approx {\\mu}_{T+1}$.\n\n**Lemma**: Let $\\mathcal{M}$ represent the space of all *kernel mean embedding* \nfrom the set $\\mathcal{P}$ of all kime-phase probability distributions on \n$\\mathcal{Z}$ over time points $1\\leq t\\leq T$. Suppose \n$\\exists\\ \\mu_T\\in \\mathcal{M}$, $\\exists\\ \\hat{\\mu}_T, \\varepsilon_T\\in \\mathcal{M}$,\nand $A,\\hat{A}\\in \\mathcal{F}$ are bounded linear operators such that $\\hat{\\mu}_{T+1}=\\hat{A}\\hat{\\mu}_T$ and ${\\mu}_{T+1}=A\\mu_T+\\varepsilon_T$.\nThen, $\\forall\\ f\\in \\mathcal{H}$ with $||f||_{\\mathcal{H}}\\lt 1$ we have the following upper bound on the distance between the corresponding exact and pseudo expected values\n\n$$|{\\mathbb{E}}_{{\\mu}_{T+1}}(f) - {\\widehat{\\mathbb{E}}}_{\\hat{\\mu}_{T+1}}(f)|\\leq\n\\underbrace{\\underbrace{||A||_{\\mathcal{F}}}_{constant\\ in\\ time}\\  \n\\underbrace{||\\mu_T - \\hat{\\mu}_T||_{\\mathcal{H}}}_{\\underset{n_T\\to\\infty}\n{\\ \\longrightarrow\\ 0}}}_{\\longrightarrow\\ 0} + \\underbrace{||A - \\hat{A}||_{\\mathcal{F}}}_{\\underset{T \\to \\infty\\\\ n_T \\to \\infty}{\\longrightarrow 0\\ ,}}\n+ \\underbrace{||\\varepsilon_T||_{\\mathcal{F}}}_{{\\text{Autoregressive Model}}\\\\ \n{\\text{Error }}\\ \\sim\\ \\mathcal{D}(mean=0)}\\ .$$\n\n*Proof*: Derive the upper bound by expanding the left hand side and using the \ninner product properties and the RKHS embedding definition.\n\nThis *Lemma* quantifies the approximation $\\hat{\\mu}_{T+1}\\approx {\\mu}_{T+1}$\nof the mean kernel embedding at the future time point $T+1$. The approximation\nis good since  \n\n - $||A||_{\\mathcal{F}}$ is constant in time and as the number of *repeated samples* in $S_T$ increases $n_T\\to\\infty$, then the empirical distribution converges to the true distribution $\\hat{\\mu}_T\\underset{n_T \\to\\infty}{\\longrightarrow} \\mu_T$. \n - As the number of time-points increases ($T\\to\\infty$) and the number of repeated samples increases ($n_t \\to\\infty$) the estimated time-dynamics operator converges to the true time dynamics operator, $\\hat{A}\\to A$. \n - We make assumptions about the *autoregressive distribution model evolution* which requires that the model unaccounted error is small, i.e., $||\\varepsilon_T||_{\\mathcal{F}}$ is controlled.\n \nHence, given sufficiently large and representative data $\\{S_t\\}$, the \n*time distribution dynamic* estimation $\\hat{\\mu}_{T+1}$ of the next distribution \ntime step ($T+1$) is expected to be highly accurate approximation of the unknown\ntrue time distribution ${\\mu}_{T+1}$.\n\n\n### Step 4: Drawing new repeated samples from the posterior predictive kime-phase probability distribution\n\nThe explicit expectation estimate we derived above\n\n$$\\widehat{\\langle f\\rangle}_{\\hat{\\mu}_{T+1}} \\equiv\n{\\widehat{\\mathbb{E}}}_{\\hat{\\mu}_{T+1}}(f) =\n\\sum_{t=2}^T \\sum_{i=1}^{n_t}{\\left (\\frac{\\beta_t}{n_t}f(z_i^t) \\right )}$$\n\nsuggests an explicit scheme to draw new IID (random) samples from the approximate\nmean kernel $\\hat{\\mu}_{T+1}$, as proxy of prospective true random samples from\nthe unknown kime-phase distribution at time $T+1$, ${\\mu}_{T+1}$. The explicit \nrandom sampling scheme representing draws from $\\hat{\\mu}_{T+1}$ is:\n\n$$\\hat{S}_{T+1} = \\cup _{t=2}^{T} \\left \\{\n\\frac{\\beta_t}{n_t}\\cdot z_1^{(t)}\\ , \\ \n\\frac{\\beta_t}{n_t}\\cdot z_2^{(t)}\\ ,\\ \\cdots \\ ,\\ \n\\underbrace{\\overbrace{\\frac{\\beta_t}{n_t}}^{weight}\\cdot \n\\overbrace{z_i^{(t)}}^{sample}}_{{\\text{Each sample } \nz_i^{(t)}\\sim \\mathcal{Z}}\\\\ {\\text{is weighted by }} \\frac{\\beta_t}{n_t}}\n\\ , \\ \\cdots \\ ,\\ \n\\frac{\\beta_t}{n_t}\\cdot z_{n_T}^{(t)}\\right \\}\\ .$$\n\nBoth *weighted sampling* and *uniformly equally-weighted sampling* weighting\nschemes are possible. Above we show the *weighted sampling* approach and a \nuniformly weighted samples can be generated by arithmetically averaging\n\n$$\\bar{S}_{T+1}=\\{\\bar{z}_{1}, \\bar{z}_{2}, \\cdots, \\bar{z}_{m}\\},\\ \\ \n\\hat{\\mu}_{T+1}\\approx\\mu(\\bar{S}_{T+1})=\\frac{1}{m}\\sum_{i=1}^m\n{\\phi(\\bar{z}_i)}\\ .$$\n\n[RKHS kernel herding](https://doi.org/10.48550/arXiv.1203.3472) may be used to \ndraw random samples using an algorithmic approach approximating the unknown \nkime-phase  probability distribution based on observed samples. Each\nembedded phase distribution $\\eta\\in\\mathcal{M}$ can be used to generate\na sequence of *RKHS kernel herding samples* $\\{\\bar{z}_i\\}_i$ as follows\n\n$$\\bar{z}_1=\\arg\\max_{z\\in\\mathcal{Z}} {\\langle \\phi (z), \\eta\\rangle_{\\mathcal{H}}}\\ ,\\\\\n\\bar{z}_2=\\arg\\max_{z\\in\\mathcal{Z}} {\\left \\langle \\phi (z), \\eta -\n\\frac{1}{2}{\\phi(\\bar{z}_1)} \\right \\rangle_{\\mathcal{H}}}\\ ,\\\\\n\\vdots\\\\\n\\bar{z}_n=\\arg\\max_{z\\in\\mathcal{Z}} {\\left \\langle \\phi (z), \\eta -\n\\frac{1}{n}\\sum_{i=1}^{n-1} {\\phi(\\bar{z}_i)} \\right \\rangle_{\\mathcal{H}}}\\ ,\\ \\forall\\ n\\gt 2\\ .$$\n \nThe *RKHS kernel herding sampling* scheme represents an iterative optimization\nfor drawing examples $\\{\\bar{z}_i\\}$ by minimizing the objective function\n\n$$\\{\\bar{z}_i\\}=\\min_{\\{\\bar{z}_i\\}}{\\left |\\eta - \\frac{1}{n}\\sum_{i=1}^n {\\phi(\\bar{z}_i)}\\right |}_{\\mathcal{H}}\\ .$$\n\nIn this optimization, the target vector $\\eta$ may or may not be in the\nembedded kime-phase distribution, which makes the RKHS herding applicable to \nany vector elements in the RKHS $\\mathcal{H}$. \n\nFor instance, letting $\\eta = \\hat{\\mu}_{T+1}$ allows us to draw random\nsamples $\\bar{S}_{T+1}=\\{\\bar{z}_1, \\bar{z}_2, \\cdots, \\bar{z}_{n_{T+1}} \\}$\nas proxy samples from the true phase distribution\n${S}_{T+1}=\\{{z}_1, {z}_2, \\cdots, {z}_{n_{T+1}} \\}$, which is unknown.\n\nIn practice, computing $\\bar{S}_{T+1}$ requires explicit protocols for solving\nmultiple inverse (preimage) problems, which may be difficult in many situations.\n*RKHS kernel herding sampling* represents an *approximate projection*\n$\\mathcal{H}\\longrightarrow \\mathcal{H}$ since $\\forall\\ \\eta\\in \\mathcal{H}$\n$\\exists\\ p_{\\eta}\\in \\mathcal{P}$, a preimage in the phase family of symmetric phase\ndistributions, associated with the sample $\\{\\bar{z}_1, \\bar{z}_2, \\cdots, \\bar{z}_n, \\}$. \n\nThe following [algorithm for extrapolating the kime-phase distribution dynamics was proposed by Lampert in 2014](https://doi.org/10.1109/CVPR.2015.7298696). \n\n**Algorithm for extrapolating the kime-phase distribution**\n\n - *Inputs*: \n   - *Kernel* function $\\kappa:\\mathcal{Z}\\times\\mathcal{Z}\\to \\mathbb{C}$\n   - *observed datasets* $S_t=\\left \\{z_1^{(t)}, z_2^{(t)}, , \\cdots, z_{n_t}^{(t)}, \\right \\}$, where at each fixed time point $t$, we have repeated measurements $\\{z_i^{(t)}\\}_i$.\n   - Regularization parameter $\\lambda\\geq 0$.\n - *Compute*:\n   - the matrix $K_{(T-1)\\times(T-1)}= \\left ( K_{st}\\right )$, where $K_{st}=\\frac{1}{n_s\\ n_t}\\sum_{i=1}^{n_s} \\sum_{j=1}^{n_t} {\\kappa\\left (z_i^{(s)}, z_j^{(t)}\\right )}$.\n   - the vector $\\kappa_{(T-1)\\times 1}=(\\kappa_t)$, where $\\kappa_t= \\frac{1}{n_s\\ n_T}\\sum_{i=1}^{n_s} \\sum_{j=1}^{n_T} {\\kappa\\left (z_i^{(s)}, z_j^{(T)}\\right )}$.\n   - the effects $\\beta^*=(\\beta^*_1, \\beta^*_2, \\cdots, \\beta^*_{T-1})=(K+\\lambda I)^{-1} \\kappa_{(T-1)\\times 1}\\in\\mathbb{C}^{T-1}$.\n - *Outputs*:\n   - Forward-time random draw data sampled from the RKHS kime-phase distribution model: $\\hat{S}_{T+1}=\\cup_{t=1}^{T} \\left \\{ \\frac{\\beta_t}{n_t}\\cdot z_1^{(t)},\\ \\frac{\\beta_t}{n_t}\\cdot z_2^{(t)},\\cdots, \\frac{\\beta_t}{n_t}\\cdot z_{n_t}^{(t)}  \\right \\}$, where $\\beta_t=\\beta_{t-1}^*$.\n - *RKHS kernel herding*:\n   - *Input*: Set desired output size $m$ and iterate by induction.\n   - *Initialize*: $\\bar{z}_1=\\arg\\max_{z\\in\\mathcal{Z}} {\\sum_{t=2}^T {\\frac{\\beta_t}{n_t}\\sum_{i=1}^{n_t} {\\kappa(z, z_i^{(t)})}}}$.\n   - *Loop*: $\\forall\\ n=2, 3,\\cdots, m$ **do**: \n     - $\\bar{z}_n =\\arg\\max_{z\\in\\mathcal{Z}} \\left [\\sum_{t=2}^T {\\frac{\\beta_t}{n_t}\\sum_{i=1}^{n_t} {\\kappa(z, z_i^{(t)})}} - \\frac{1}{n}\\sum_{i=1}^{n-1} {\\kappa(z,\\bar{z}_i)} \\right ]$.\n     - **end for** loop\n   - *Output*: synthetically generated prospective simulated dataset $\\hat{S}_{T+1}=\\{\\bar{z}_1,\\bar{z}_2, \\cdots, \\bar{z}_m \\}$.\n\n**Task**: Pilot test this with real fMRI data...",
      "word_count": 4656
    },
    {
      "title": "Case Study: Predicting a specialized Fokker Planck equation",
      "content": "We characterize the failure modes for the kernel embedding algorithm in terms of predicting the toy Fokker Planck equation. In the linear potential scenario $U(x)=cx$, the 1D Fokker planck equation is the Smoluchowski equation\n\\begin{equation}\n    \\partial_t P(x,t\\mid x_0,t_0) = \\partial_x D (\\partial_x+\\beta c) P(x,t\\mid x_0,t_0)\n\\end{equation}\nWith initial impulse condition $P(x,t_0\\mid x_0,t_0)=\\delta(x-x_0)$, where $D$ is the diffusion constant, $\\beta=\\frac{1}{k_BT}$, $k_B$ being the Boltzman constant. The solution is\n\\begin{equation}\n    P(x,t\\mid x_0,t_0)=\\frac{1}{\\sqrt{4\\pi D(t-t_0)}}\\exp(-\\frac{(x-x_0+D\\beta c(t-t_0))^2}{4D(t-t_0)})\n\\end{equation}\n\n\\noindent which after assuming $D=1$, $\\beta c=1$, and setting $x_0,t_0=0$ and the equation simplifies to\n\\begin{equation}\n    p_t\\sim \\mathcal{N}(\\mu = -t,\\sigma = \\sqrt{2t})\n\\end{equation}\n\n## Visualizing the time varying probability evolution\n\nAssume we are given access to the temporal samples at $S_{0.2}\\sim p_{0.2},S_{0.4}\\sim p_{0.4},...,S_{1.2}\\sim p_{1.2}$. Can we predict forward $p_{1.4}$ or generate samples effectively?\nA visualization of these distributions is the following:\n\n\n\n\n** Initialize the Data **\n\n\n\n## Lampert's method on predicting future behavior of a time-varying probability distribution\n\nThe following implements the [algorithm for extrapolating the kime-phase distribution dynamics was proposed by Lampert in 2014](https://doi.org/10.1109/CVPR.2015.7298696). \n\n**Remark:**\nSince all the samples in the same batch $S_t$ are weighted equally. Thus, their sample mean would likely stay the same. Namely, $\\bar{S}_{1.2}=-1.2,\\bar{S}_{0.6}=-0.6$ then it is impossible for the $\\hat{S}_{T+1}$ to reach a minimal first moment -1.4, as the maximal achievable smallest first moment from this sample is $-1.2$. In other words, since the weight-averaging of all prior temporal sample means, each greater than 1.2, one can't extrapolate out side the sample mean range $[\\min\\bar{S_t},\\max\\bar{S_t}]=[-1.2,0]$. Also, $\\beta_t<0$ correspond to unphysical sampling. In these cases, herding needs to be applied.\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Package the experiments for different size n\n\n## Classical Approach: Order statistics regression\n\nThe following heuristic transforms the probability transform into pointswise time series prediction. The obvious drawback is that \\textbf{order statistics does not generalize straightforwardly in high dimension}.\n\n*  We sample $n$ data points each from $p_{0},p_{0.2},p_{0.4},\\cdots,p_{1.2}$ obtaining $S_0,\\cdots,S_{1.2}.$\n\n*  For each group, obtain the order statistics by sorting the $n$ data points.\n\n*  Fit $n$ arima models to predict the next time step for each order statistic. For example, pick the smallest value from $S_{0},...,S_{1.2}$ and perform time series prediction (for more efficient compute, we can alternatively perform arima on certain percentiles, e.g, min, max, median,...).\n\n*  Aggregate the generated sample data.\n\n\n\nThis only works for low dimensional examples -  this methodology however does not generalize straightforwardly to higher dimension.\n\n### Classical Alternative: Dynamic Time warping\nA highly related problem in ***speech recognition***, which seeks to optimize\n\\begin{equation}\n    \\min_{\\psi}\\{\\int_{0}^1\\|x(t)-y(\\psi(t))\\|dt: \\psi:[0,1]\\to[0,1], increasing\\}\n\\end{equation}\nThis would flexibly model this scenario as the time varying process is just a reparametrization that can be implemented by $\\psi$. This strike analogy to quantile representation for random variables.\n\n\nAs a crude approximation we will use the $\\hat{\\psi}_{1;1.2}$ to approximate $p_{1.4}$. The optimzation problem is now\n\\begin{equation}\n    \\min_{Q_{p_{1.4}}}\\|Q_{p_{1.4}}-(Q_{p_{1.2}}(\\hat{\\psi}_{1;1.2}))\\|\n\\end{equation}\nwhere $Q_{p_{1.4}}$ is the quantile function. The protocol is to estimate the order matching $\\hat{\\psi}_{1;1.2}$ (alignment6 variable) and use the sampled datapoints at the last timestep ($S_{1.2}$) to forward match the samples, which achieves a desirable mean that is approximately -1.4 (See DTW below).\n\n\n\n\n\n\n## Fourier random feature - classical cosines\nWe visualize the Fourier random features method for encoding the probability distribution via finite samples and then aim to find a strategy to capture the dynamics of $\\kappa_t$\n\n\n\nThe oscillations at the end are due to the fact that the end points are unstable since $x,y\\sim Unif(-5,5)$ and the minimum is sampled from $x=-5,y=5$ which are two extreme values.\n\n\n\n\n\nThis numerical strategy makes sense as \n$$\n    \\mathcal{F}(e^{-ax^2})(k) = \\sqrt{\\frac{\\pi}{a}}e^{-\\pi^2k^2/a}\n$$\nSince our diffusion process has a gradually increasing a, it becomes more evident in Figure \\ref{fig:compare_gaus} that the maximum value ($\\frac{\\pi}{a}$) decreases. However, when we try to do the decoding, the distribution is almost symmetric. \n\nThis approach, like the lampert's approach may generalize to higher dimension flexibly.\nHowever, the main problem in the cosines is that the \"translation information\"(phase factor) is completely lost in the purely real treatment. Namely,\n\n   $$ \\mathcal{F}{g(t-a)} = e^{-iw a}G(w)$$\n\nwhere the $e^{-iwa}$ is essential in recovering the translational distributional information. The full prediction result is much better if we incorporate the full complex domain information\n\n### A Proposed approach: Implement every thing in the complex fourier space and then convert back\n\n\n**Input**: Batch inputs: $S_0,S_1,\\cdots,S_{T}$ each sampled iid from time-varying probability distribution $p_T$\n\n\n**Output**: Batch Output $S_{T+1}$ and $p_{T+1}$\n\n***(A) Bundling the observations to fixed Fourier transformed points***\n\n*Sample the corresponding coupled frequency domain $\\omega\\in\\mathbb{R}^d$, $\\pmb{\\omega}=(\\omega_1,\\omega_2,..,\\omega_N)$\n\n*Compute the empirical average for Fourier Kernel $\\hat{k}_t(\\omega_j)=\\frac{1}{|S_t|}\\sum_{x\\in S_t}e^{i x^T \\omega_j}, 0\\leq t\\leq T, 1\\leq j\\leq N$ ($\\kappa_t(\\omega_j)=\\mathbb{E}_{x\\sim p_t}[e^{i x^T\\omega_j}]$)\n\n***(B) Forward prediction of the bundled frequency points***\n\n* For each $\\omega_j$\n\n* Forward predict $\\hat{\\kappa_{t+1}}(\\omega_j)$ (using proper arima model, separate imaginary/real prediction)\n\n***(C) Inverse transform***\n\n* Approximate $p_{T+1}(x)=\\frac{1}{(2\\pi)^d}\\int_{\\mathbb{R}^d}\\hat{\\kappa}_{t+1}(\\omega_j)e^{-i \\omega_j^T x}d\\omega_j$\n\n* In 1 dimension, this can be effectively computed\n\n$$\\hat{p}_{T+1}(x)=\\frac{1}{C}\\sum_{j=2}^{N}\\hat{\\kappa}_{t+1}(\\omega_{(j)})e^{-i\\omega_{(j)}^T x}\\Big(\\omega_{(j)}-\\omega_{(j+1)}\\Big)$$\n\nwhere $w_{(j)}$ is the sorted sequence for $\\pmb{\\omega}$, where $C$ is some normalization factor.\n\n***(D) Post process smoothing***\n\n\n\n\n\n\n\n\nThe system in general is still a bit off in mean as the auto-arima prediction with hyperparameter search is a bit unstable. \n\n***Note:*** All methodologies are trained on n=100 batch sample from each distribution.",
      "word_count": 849
    },
    {
      "title": "Appendix",
      "content": "... *Improve this simulation example* ...\n\n##  RKHS for noisy trigonometric signal\n\nWe consider a sample of size n = 50, ($y_1, y_2, y_3, ..., y_{50}$), from the model \n$y_i = \\cos(\\pi x_i) + \\varepsilon_i$ where $\\varepsilon \\sim N\\left (\\mu=0, \\sigma^2=(3/2)^2\\right )$. Let's simulate $x$ and $y$, and then run a direct search for the *GCV* optimal smoothing parameter $\\lambda$ using the *GCV* metric.\n\nStart with a function fitting cubic smoothing splines using the *RKHS*.\n\n\n## RKHS Appendix - Matern kernels\n\n**(Multivariate Kernels) The RKHS for Matern kernels are the Sobolev spaces.The Mat&#233;rn kernels are the Green's function for the $d$-dimensional modified Helmholtz operator**\n\n$$(-\\nabla^2+ \\epsilon^{2} I)^m $$\n \nThese facts can be derived from several crucial facts. First of all, the inner product for Sobolev space is (See [P176](https://num.math.uni-goettingen.de/schaback/teaching/AV_2.pdf))\n \n$$\\langle f,g\\rangle_{W_2^{m}(\\mathbb{R}^d)}=\\int_{\\mathbb{R}^d}(1+\\|\\omega\\|^2)^m \\hat{f}(w) \\overline{\\hat{g}(w)}dw \\ .$$\nAfter inserting into a translation-invariant kernel ansatz and using the reproducing kernel property $f(x)=\\langle f,K(x-\\cdot)\\rangle_{W_2^m(\\mathbb{R}^d)}$. This leads to the Fourier translation property $\\hat{K}(x-\\cdot)(w)=e^{-ix^Tw}\\hat{K}(w)$.\n \nAlso we note that the inverse Fourier transform for $(\\epsilon^2+\\|w\\|^2)^{-m}$\n\n$$K(x-y) = (2\\pi)^{-d/2}\\int_{\\mathbb{R}^d}(\\epsilon^2+\\|w\\|_2^2)^{-m}e^{i(x-y)^Tw}dw= \\frac{2^{1-m}}{\\Gamma(m)}\\Big(\\frac{\\|x-y\\|}{\\epsilon}\\Big)^{m-\\frac{d}{2}}K_{\\frac{d}{2}-m}(\\epsilon\\|x-y\\|)$$\nis the Matern kernel, see the [explicit calculation here, P76 of Scattered approximation by Wendland](https://www-cambridge-org.proxy.lib.umich.edu/core/books/scattered-data-approximation/980EEC9DBC4CAA711D089187818135E3). A rough sketch of the proof involves\nwriting out the gamma function definition and then applying a scaling and insert the $1/(1+|x|^2)^{m}$ form and then plug into the fourier transform. Finally applying three variable transformations to arrive at the Modified Bessel function of second kind integral representation.\n\nFinally, the operator and the inverse Fourier transform $(\\epsilon^2+\\|w\\|^2)^{-m}$ can be established from integration by parts:\n\\begin{equation}\n-\\nabla^2\\to -i^2\\|w\\|^2 = \\|w\\|^2, (-\\nabla^2+\\epsilon^2 I)^m\\to (\\epsilon^2+\\|w\\|^2)^m\n\\end{equation}\n \n**Matern kernels shape**\n \n\n\n\n\n## RKHS Appendix - Kernelized ridge regression{#kernelridge}\n\nWe use a similar setup that $X=[x_1,x_2,...,x_n]^T\\in\\mathbb{R}^{n\\times d}$. \nIn the kernelized ridge regression case, we establish the differences and synergies between feature modeling and the RHKS modeling. Namely, there are two possible starting points \n\n\\begin{equation}\n\\begin{split}\n\\text{Linear Vector Modeling: }y_i &= \\mathbf{w}^T\\phi(x_i)=\\sum_{j=1}^D w_j \\phi(x_i)_j\\\\\n\\text{Function Modeling (From representer theorem): }f(\\cdot) & = \\sum_{j=1}^n \\tilde{w_j} \\phi_{x_j}(\\cdot)= \\sum_{j=1}^n \\tilde{w_j} \\kappa(x_j, \\cdot)\n\\end{split}\n\\end{equation}\n\nThe first modeling maps data points to feature vectors $x_i\\in\\mathbb{R}^d$ and $\\phi(x_i)\\in\\mathbb{R}^D$ - finite dimensional, while the latter modeling maps data points to feature functions $\\phi_{x_i}(\\cdot)$ - infinite dimensional. Note that in the first modeling, the $D$ is specified by the nonlinear transformation $\\phi$ and can be arbitrary (does not have to be $D=d$, where $x_i\\in \\mathbb{R}^d$). The vector modeling optimization problem is \n\n\\begin{equation}\n\\min_{\\mathbf{w}\\in\\mathbb{R}^D}\\|y-\\Phi^T \\mathbf{w}\\|_2^2+\\lambda\\|\\mathbf{w}\\|_2^2, \\Phi_{ij}=\\phi(x_i)_j,\\Phi\\in \\mathbb{R}^{D\\times n}\n(\\#eq:opt1)\n\\end{equation}\n\n$$\\mathbf{w} = (\\lambda I_D+\\Phi\\Phi^T)^{-1}\\Phi y$$\nIf we set up the problem using reproducing kernel property where $K_{ij}=\\kappa(x_i,x_j)$ and penalize on the raw $\\mathbf{\\tilde{w}}$, the solution will be different\n\n\\begin{align}\n    \\min_{\\mathbf{\\tilde{w}}\\in \\mathbb{R}^n}\\|y-K\n    \\mathbf{\\tilde{w}}\\|_2^2 + \\lambda\\|\\mathbf{\\tilde{w}}\\|_2^2,     \\mathbf{\\tilde{w}} = (KK+\\lambda I_n)^{-1}Ky\n\\end{align}\n\nHowever, the correspondence between the two approaches can be realized by instead penalizing the norm of the weights for the \"features\"\n\n\\begin{align}\n    \\min_{\\mathbf{\\tilde{w}}\\in \\mathbb{R}^n}\\|y-K\n    \\mathbf{\\tilde{w}}\\|_2^2 + \\lambda\\|\\Phi\\mathbf{\\tilde{w}}\\|_2^2\n    (\\#eq:opt2)\n\\end{align}\n\nHence the two optimization problem \\@ref(eq:opt1)  and \\@ref(eq:opt2)  are equivalent upon the fixed linear transformation $\\Phi\\mathbf{\\tilde{w}}=\\mathbf{w}$. \n\n***Lemma:*** In L2 ridge regression it is equivalent to express the fitted parameter as \n\\begin{equation}\n\\mathbf{w} = (\\lambda I_d+\\Phi\\Phi^T)^{-1}\\Phi y = \\Phi(\\Phi^T\\Phi+\\lambda I_n)^{-1}y\n(\\#eq:ridge-reg)\n\\end{equation}\n\n***Proof:*** A neat trick that could perform the matrix inverse for \\@ref(eq:opt1) in a smaller space is $(P^{-1}+B^TR^{-1}B)^{-1}B^TR^{-1}=PB^T(BPB^T+R)^{-1}$ for non-square matrix $B$ ([See  Max Welling's notes here](https://web2.qatar.cmu.edu/~gdicaro/10315-Fall19/additional/welling-notes-on-kernel-ridge.pdf).). Namely, if $B\\in \\mathbb{R}^{n\\times d}$, then the left hand side inverse performs in $d\\times d$ while the right hand side performs inverse in $n \\times n$. That is, the right hand side manipulation is more preferrable if $n < d$ and vice versa.  Let $\\Phi=B^T\\in \\mathbb{R}^{d\\times n}, P^{-1}=\\lambda I_d, R^{-1}= I_n$, we establish the equivalence that\n\\begin{equation}\n\\mathbf{w} = (\\lambda I_d+\\Phi\\Phi^T)^{-1}\\Phi y = \\Phi(\\Phi^T\\Phi+\\lambda I_n)^{-1}y\n(\\#eq:ridge-reg)\n\\end{equation}\n\n***Note:*** From this derivation, we see that the exact calculation for $\\mathbf{w}$ is intractable. Thus, it is hard to precisely interpret the value since $\\Phi$ is not tractable. However, since $\\Phi\\tilde{w}=w.$ The observation-wise weights are calculable $\\mathbf{\\tilde{w}} = (\\Phi^T\\Phi+\\lambda I_n)^{-1}y=(K+\\lambda I_n)^{-1}y$.\n\nHere, we also used $d$ to denote the number of features and $n$ denoting the number of samples. The $\\mathbf{w}$ encodes the weights of the data-cases, at test time, for an unknown data $x_{test}$, it will be interpolated using Equation \\@ref(eq:ridge-reg) \n\\begin{equation}\n\\hat{y}(x_{test}) = f(x_{test}) =  \\mathbf{w^T} \\Phi(x_{test}) = y^T(K+\\lambda I_n)^{-1}\\kappa(x_{test}) \n\\end{equation}\nwhere $K(x_i,x_j)=\\Phi(x_i)^T\\Phi(x_j)$ and $\\kappa(x_i,x_{test})=\\Phi(x_i)^T\\Phi(x_{test})$, and $\\kappa(x_{test})=[\\kappa(x_1,x_{test}),....,\\kappa(x_n,x_{test})]^T$\n\nRecall that the **hat matrix** implements a mapping between the raw observed $\\mathbf{y}$ to the fitted $\\hat{\\mathbf{y}}$, i.e, $\\hat{\\mathbf{y}}= Hy$. In this derivation, the hat matrix is \n\\begin{equation}\n\\hat{y} = \\Phi^T \\mathbf{w} = \\underbrace{K(K+\\lambda I_n)^{-1}}_{H}\\mathbf{y}\n\\end{equation}\nThe approximation for Generalized cross validation is then\n\\begin{equation}\nGCV = \\frac{n*RSS}{(n-tr(H))^2}\n\\end{equation}\n\n## RKHS Appendix - Kernelized LASSO Shrinkage{#kernellasso}\n\nThe canonical set up for LASSO is\n\n\\begin{equation}\n    \\min_{\\beta\\in \\mathbb{R}^d}\\|y-X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1\n\\end{equation}\n\nUnlike ridge regression, it is especially important to make an important distinction of either enforcing the [shrinkage on the instances or on the features](https://arxiv.org/abs/1202.0515). Here $X=[x_1,x_2,...,x_n]^T\\in\\mathbb{R}^{n\\times d}$ corresponding to $n$ samples, and each $x_i\\in \\mathbb{R}^d$ has $d$ dimensional features.\nThere are two ways of applying nonlinear feature mapping leading to different interpretations of modeling and shrinkage.\n\n### Applying nonlinear mapping on the instances (observations)\n\nApplying a nonlinear feature map on the instances give $\\Phi^T=[\\varphi(x_i),\\varphi(x_2),...,\\varphi(x_n)]^T\\in\\mathbb{R}^{n\\times d'}$. The kernel maps\n\n\\begin{equation}\n    \\kappa(x_i,x_j)=\\varphi(x_i)^T\\varphi(x_j)\n\\end{equation}\n\nWe can explicitly expand the RKHS property\n\n\\begin{equation}\n    f(\\cdot) = \\sum_{i=1}^n \\alpha_i \\kappa(\\cdot, x_i)=\\sum_{i=1}^n\\alpha_i \\varphi_{x_i}(\\cdot)\n\\end{equation}\n\nThe linear superposition correspond to the ***summation weighting of the $n$ observations***. The linear equation can be set up by interpolating the $x_j$'s. \n\\begin{equation}\n    y_j = f(x_j) = \\sum_{i=1}^n \\alpha_i \\kappa(x_i,x_j) \\forall j=1,2,3,...,n\n\\end{equation}\n\nCrucially, the LASSO problem is then\n\\begin{equation}\n    \\min_{\\alpha\\in \\mathbb{R}^n}\\|y-A\\alpha\\|_2^2 + \\lambda\\|\\alpha\\|_1\n    (\\#eq:opt-lasso)\n\\end{equation}\nwhere $A_{ij} = \\kappa(x_i,x_j)$, $A=\\Phi^T\\Phi\\in \\mathbb{R}^{n\\times n}$. The matrix form for $A$ is often by plugging in the parametric form for the kernel $\\kappa$. ***The optimization problem enforces the shrinkage on the observation instances***.\n\n$\\blacktriangleright$ For example, for the linear kernel $\\kappa(x_i,x_j)=x_i\\cdot x_j$. The problem \\@ref(eq:opt-lasso) would translate into\n\n\\begin{align}\n    \\min_{\\beta\\in \\mathbb{R}^n}\\|y-XX^T\n    \\alpha\\|_2^2 + \\lambda\\|\\alpha\\|_1    \n\\end{align}\n\nThe solution is then \n\n\\begin{equation}\n    \\hat{\\alpha_j}^{(LASSO)} = S_{\\lambda}(\\hat{\\alpha_j}^{(OLS)})=\\hat{\\alpha_j}^{(OLS)}\\max(0,1-\\frac{\\lambda}{|\\hat{\\alpha_j}^{(OLS)}|}),\n\\end{equation}\n\n\\begin{equation}\n    \\hat{\\alpha}^{OLS}= (XX^TXX^T)^{-1}XX^Ty\n\\end{equation}\n\nThe **more general nonlinear version is**\n\n\\begin{equation}\n    \\hat{\\alpha}^{OLS}= (\\Phi^T\\Phi\\Phi^T\\Phi)^{-1}\\Phi^T\\Phi y=(KK)^{-1}Ky=K^{-1}y\n\\end{equation}\n\nHowever, as we have seen in ridge regression if we adjust the optimization problem \\@ref(eq:opt-lasso) into (which is similar to \\@ref(eq:opt2)),  but enforces sparsity in the transformed feature space as $\\Phi\\alpha\\in \\mathbb{R}^{d'}$\n\n$$\\min_{\\alpha\\in \\mathbb{R}^n}\\|y-\\Phi^T\\Phi\\alpha\\|_2^2 + \\lambda\\|\\Phi\\alpha\\|_1 .$$\n\nThis becomes \n\n$$\\min_{\\alpha'\\in \\mathbb{R}^{d'}}\\|y-\\Phi^T\\alpha' \\|_2^2 + \\lambda\\|\\alpha'\\|_1 .$$\nHere, $\\hat{\\alpha'}^{OLS}=(\\Phi\\Phi^T)^{-1}\\Phi y$, however since $\\Phi$ is not known and we do not have a trick as in ridge regression to do the evaluation, this optimization is hard to realize just by knowing $K$.\n\n### Applying the nonlinear mapping on the features\nApplying a nonlinear feature mapping on the features $X=[x^1,x^2,...,x^d]\\in\\mathbb{R}^{n\\times d}\\to [\\phi(x^1),...,\\phi(x^d)]\\in\\mathbb{R}^{n'\\times d}$. Crucially, the $y$ outcome also needs to be transformed $\\phi(y)\\in\\mathbb{R}^{n'}$. \nWe can explicitly expand the RKHS property\n\n\\begin{equation}\n    f(\\cdot) = \\sum_{i=1}^d \\beta_i \\kappa(\\cdot, x^i)=\\sum_{i=1}^d\\beta_i \\phi_{x_i}(\\cdot)\n\\end{equation}\n\nThus, the linear system is\n\n\\begin{equation}\n    \\phi_{y}(x^j)=\\kappa(x^j,y)=\\sum_{i=1}^d\\beta_i \\phi_{x^i}(x^j)=\\sum_{i=1}^d\\beta_i\\kappa(x^i,x^j)\n\\end{equation}\n\nThe LASSO problem is \n\n\\begin{equation}\n    \\min_{\\beta\\in \\mathbb{R}^d}\\|\\phi(y)-C\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1\n\\end{equation}\n\nwhere $\\phi(y)=[\\kappa(x^1,y),...,\\kappa(x^d,y)]^T\\in \\mathbb{R}^d$ and $C_{ij}=\\kappa(x^i,x^j), C\\in\\mathbb{R}^{d\\times d}$.\n\n$\\blacktriangleright$ For example, when the linear kernel condition is $\\kappa(x^i,x^j)=x^i\\cdot x^j$. Then, $\\phi(y)=X^Ty$, $C=X^TX$. The solution is then\n\n\\begin{equation}\n    \\hat{\\beta_j}^{(LASSO)} = S_{\\lambda}(\\hat{\\beta_j}^{(OLS)})=\\hat{\\beta_j}^{(OLS)}\\max(0,1-\\frac{\\lambda}{|\\hat{\\beta_j}^{(OLS)}|}),\n\\end{equation}\n\n\\begin{equation}\n    \\hat{\\beta}^{OLS}=(C^TC)^{-1}C^T\\phi(y)= (X^TXX^TX)^{-1}X^TXX^Ty\n\\end{equation}\n\nDepending on the optimization objective, there may be a $N$ couples to the $\\lambda$ factor when one minimizes for the $\\|y-X\\beta\\|_2^2$ or $\\frac{1}{N}\\|y-X\\beta\\|_2^2$ \nIn the general nonlinear case,\n\n\\begin{equation}\n    \\hat{\\beta}^{OLS}=C^{-1}C_y, where C_y = [\\kappa(x^1,y),\\kappa(x^2,y),...,\\kappa(x^d,y)]^T=\\phi(y)\n\\end{equation}\n\n***Remark***:\nIn both cases, it seems unlikely that the OLS estimate will be the same as the raw OLS in the glmnet ($\\alpha=1, \\hat{\\beta}=(X^TX)^{-1}X^Ty$), and if we still stick with the second feature shrinkage, we will get similar shrinkage performance.\n\nAlso note that the observation shrinkage approach seems to lack feature attribution given the fitted $\\alpha_i$. Whatever the outcome corresponds exactly at test time, the model is fitting a linear combination of the previous $n$ observations.\n\nIn the feature shrinkage approach, there seems to be good feature attribution $\\beta_i$ weighting the importance of each feature, however at test time. It is hard to invert the $\\phi(y)$ to get the outcome variable $y$.\n\n**Computing the GCV**: Since the LASSO estimate is non-linear and non-differentiable. The Hat matrix is not directly computable, according to [Tibshirani](https://www.jstor.org/stable/2346178), this non-differentiable $\\sum_{i=1}^n |\\beta_i|$ may be approximated by $\\sum_{i=1}^n \\frac{\\beta_i^2}{|\\beta_i|}$ and hence the hat matrix for the GCV computation may be approximated by similar to ridge regression\n\n$$\\hat{H}=X(X^TX+\\lambda W^{-})^{-1}X^T$$\n\nwhere $W=diag(\\hat{\\beta_1}^{(LASSO)},...,\\hat{\\beta_n}^{(LASSO)})$ and $W^-$ is the generalized inverse of $W$.\n\n**Note:** The hat matrix is a linear transformation thus in the feature shrinkage approach we define the hat matrix in the transformed space\n$$\\hat{\\phi(y)}=\\hat{H}\\phi(y)=K(KK+\\lambda W^{-})^{-1}K\\phi(y)$$",
      "word_count": 1406
    },
    {
      "title": "References",
      "content": "* [Michael Jordan's Properties of Kernels](https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf).\n* [Review of Kernel Mean Embedding of Distributions](https://arxiv.org/pdf/1605.09522.pdf).\n* [Michael Clark's RKHS Code](https://github.com/m-clark/models-by-example/blob/main/rkhs.Rmd).",
      "word_count": 19
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    toc_depth: '3'\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Kernels",
      "code": "library(GPBayes)\nbesselk_nu <- function(nu) {\n  return(function(x) {\n    return(GPBayes::BesselK(nu, x))\n  })\n}\nmaternkernel <- function(sigma,nu,rho,x){\n  scaled_x = sqrt(2*nu)*x/rho\n  return (sapply(scaled_x,besselk_nu(nu))*scaled_x**nu*sigma**2*2**(1-nu)/gamma(nu))\n}\n\nlibrary(plotly)\np <- plot_ly(type=\"scatter\", mode=\"lines\")\nx_values = seq(from = -5, to = 5, by = 0.01)\np <- p %>%add_trace(x=~x_values, y=~maternkernel(1,1/8,1,abs(x_values)), \n                    name = ~paste(\"nu=\", 1/4), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,1/2,1,abs(x_values)), \n                     name = ~paste(\"nu=\", 1/2), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,1,1,abs(x_values)), \n                     name = ~paste(\"nu=\", 1), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,5,1,abs(x_values)),\n                     name = ~paste(\"nu=\", 5), mode=\"lines\")\np <- p %>%add_trace(x = ~x_values, y = ~maternkernel(1,50,1,abs(x_values)), \n                    name = ~paste(\"nu=\", 50), mode=\"lines\")\np = layout(p, title = \"Matern kernels for different parameter values\",\n          xaxis = list(title=\"X values\"), yaxis = list(title=\"Y values\"))\np",
      "line_count": 27
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(tidyverse)\n# devtools::install_github(\"kupietz/kableExtra\")\n# See: https://stackoverflow.com/questions/76118194/error-when-loading-kableextra-in-markdown-file-after-updating-to-r-4-3-0\nlibrary(kableExtra)\n\nALS.train <- read.csv(\"https://umich.instructure.com/files/1789624/download?download_frd=1\")\n# summary(ALS.train); colnames(ALS.train)\n\ny = ALS.train$ALSFRS_slope\nX = as.matrix(ALS.train[ , 2:6])\nX = apply(X, 2, scales::rescale, to = c(0, 1))",
      "line_count": 11
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "inverse <- function(X, eps = 1e-9) {\n  eig.X = eigen(X, symmetric = TRUE)\n  P = eig.X[[2]] \n  lambda = eig.X[[1]]\n  # to avoid singularities, identify the indices of all eigenvalues > epsilon\n  ind = lambda > eps\n  \n  lambda[ind]  = 1/lambda[ind] \n  lambda[!ind] = 0\n  \n  P %*% diag(lambda) %*% t(P)\n}",
      "line_count": 12
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "rk <- function(s, t) {\n  init_len = length(s)\n  rk = 0\n  for (i in 1:init_len) { rk = s[i]*t[i] + rk }\n  return(rk)\n} ",
      "line_count": 6
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "gram <- function(X, rkfunc = rk) { # compute the `crossprod` using the specified RK\n  apply(X, 1, function(Row) \n    apply(X, 1, function(tRow) rkfunc(Row, tRow)) # specifies the Reproducing Kernel\n  )  \n}",
      "line_count": 5
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "ridge <- function(X, y, lambda,kernel=rk) {\n  K = gram(X,kernel)                            #assumed X to be n*d where observations stacked on the rows, GramMatrix (nxn) XTX (already phi^Tphi)\n  rows = dim(X)[1]\n  n = length(y)                                   \n  #Q = cbind(1, GramMatrix) no need for intercept as the kernel already captures this\n  #S = rbind(0, cbind(0, GramMatrix))\n  \n  M = K + lambda*diag(dim(K)[1])                     # (singularity protection)K+lambda I)^{-1}\n  M_inv = inverse(M)                              # invert M\n  \n  omega_tilde = M_inv %*% y\n  f_hat = K %*% omega_tilde \n  \n  A = K %*% M_inv  # Hat matrix maps response to fitted values\n  tr_A = sum(diag(A))                             # trace of hat matrix\n  \n  rss  = crossprod(y - f_hat)                     # residual sum of squares\n  gcv  = n*rss / (n - tr_A)^2                     # compute GCV score\n  \n  return(list(f_hat = f_hat, omega_tilde = omega_tilde, gcv = gcv, rss=rss))\n}",
      "line_count": 21
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "rk_Laplace <- function(s, t, sigma=1) {\n  if (sigma <= 0) sigma=1  # avoid singularities\n  init_len = length(s)\n  rk_Lap = 0\n  for (i in 1:init_len) { rk_Lap = (s[i] - t[i])^2 + rk_Lap }\n  rk_Lap = exp(-(sqrt(rk_Lap))/sigma)\n  return(rk_Lap)\n} ",
      "line_count": 8
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "lambda = 10^seq(-6, 2, by = 0.5)\nlibrary(furrr)\n\n# For classical sequential parameter search use: \n# gcv_search = map(lambda, function(lam) ridge(X, y, lam))\n\n#  For multicore execution use:\nplan(multisession,  workers = 8)\ngcv_search <- future_map(lambda, function(lam) ridge(X, y, lam,rk_Laplace))\n\nV = map_dbl(gcv_search, function(x) x$gcv)\n\nridge_coefs = map_df(gcv_search, function(x) {\n    data.frame(value = x$omega_tilde, coef =as.character(1:2223))\n  }, \n  .id = 'iter') %>% \n  mutate(lambda = lambda[as.integer(iter)])",
      "line_count": 17
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(patchwork)\nlibrary(plotly)\n\n# gcv_plot = qplot(lambda, V, geom='line', main='GCV score (log scale)', ylab='GCV') +\n#   scale_x_log10()\n\nplot_ly(x=lambda, y=V, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(Ridge) GCV vs. Log10(Lambda)\", \n         xaxis = list(title=\"log10(Lambda)\", type = \"log\"))\n\n# beta_plot = ridge_coefs %>% \n#   ggplot(aes(x = lambda, y = value, color = coef)) +\n#   geom_line() +\n#   scale_x_log10() +\n#   scico::scale_color_scico_d(end = 0.8) +\n#   labs(title = 'Model Effects (Betas) Across Lambda')\n# ggplotly(gcv_plot + beta_plot)\n\nplot_ly(data=ridge_coefs[as.numeric(ridge_coefs$coef)<20,], x=~lambda, y=~value, \n        type=\"scatter\", mode=\"lines\", color=~coef, name=~coef) %>%\n  layout(title=\"(Ridge Estimates) Effects (Beta's) vs. log10(Lambda)\", \n         xaxis = list(title=\"\", type = \"log\"), # title=\"log10(Lambda)\", \n         legend = list(title=\"Predicting Covariates\", orientation = 'h'))",
      "line_count": 23
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# Matern kernel function\nmaternkernel <- function(sigma,nu,rho,x){\n  scaled_x = sqrt(2*nu)*x/rho\n  return (sapply(scaled_x,besselk_nu(nu))*scaled_x**nu*sigma**2*2**(1-nu)/gamma(nu))\n}\nrk_Matern <- function(s, t, nu=0.5) {\n  # if (sigma <= 0) sigma=1  # avoid singularities\n  init_len = length(s)\n  rk_dist = 0\n  for (i in 1:init_len) { rk_dist = (s[i] - t[i])^2 + rk_dist }\n  rk_Mat = maternkernel(1,nu,1, rk_dist+1e-6)\n  return(rk_Mat)\n} \nmaternkernel_nu_mod <- function(nu) {\n  return(function(s, t) {\n    # if (sigma <= 0) sigma=1  # avoid singularities\n    init_len = length(s)\n    rk_dist = 0\n    for (i in 1:init_len) { rk_dist = (s[i] - t[i])^2 + rk_dist }\n    rk_Mat = maternkernel(1,nu,1, sqrt(rk_dist)+1e-6)\n    return(rk_Mat)\n  } )\n}",
      "line_count": 23
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "rk_Matern <- function(s, t, nu=0.5) {\n  # if (sigma <= 0) sigma=1  # avoid singularities\n  init_len = length(s)\n  rk_dist = 0\n  for (i in 1:init_len) { rk_dist = (s[i] - t[i])^2 + rk_dist }\n  rk_Mat = maternkernel(1,nu,1, sqrt(rk_dist)+1e-6)\n  return(rk_Mat)\n} ",
      "line_count": 8
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "poly_d_mod <- function(d) {\n  return(function(s, t) {\n    init_len = length(s)\n    rk = 1\n    for (i in 1:init_len) { rk = s[i]*t[i] + rk }\n    return(rk**d)\n  } )\n}",
      "line_count": 8
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "#Hyperparameter set 1 - Linear Kernel\nlibrary(foreach)\nlibrary(doParallel)\nregisterDoParallel(8)\n# fitted_ridge1<- ridge(X,y,lambda=10)\n# fitted_ridge2<- ridge(X,y,lambda=1)\n# fitted_ridge3<- ridge(X,y,lambda=0.1)\n# fitted_ridge4<- ridge(X,y,lambda=0.01)\n\nlambda <- c(0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 10)\nfitted_ridge <- vector()\nfitted_ridge <- foreach (param = 1:length(lambda), .combine = 'c') %dopar% {\n  ridge(X,y,lambda=lambda[param])\n}\nstopImplicitCluster()",
      "line_count": 15
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "df <- as.data.frame(fitted_ridge)\n\ndf_tbl <- rbind(c(df$gcv[1], df$gcv.1[1], df$gcv.2[1], df$gcv.3[1],\n                  df$gcv.4[1], df$gcv.5[1], df$gcv.6[1], df$gcv.7[1]),\n                c(df$rss[1], df$rss.1[1], df$rss.2[1], df$rss.3[1]))\ncolnames(df_tbl) <- c(paste0(\"lambda=\", lambda[1]), paste0(\"lambda=\", lambda[2]), \n                      paste0(\"lambda=\", lambda[3]), paste0(\"lambda=\", lambda[4]),\n                      paste0(\"lambda=\", lambda[5]), paste0(\"lambda=\", lambda[6]), \n                      paste0(\"lambda=\", lambda[7]), paste0(\"lambda=\", lambda[8]))\nrownames(df_tbl) <- c(\"GCV\", \"RSS\")\n\nlibrary(DT)\ndatatable(df_tbl, caption=\"Linear Ridge Kernel (parameter estimates)\")\n\n# fitted_ridge1$rss\n# fitted_ridge2$rss\n# fitted_ridge3$rss\n# fitted_ridge4$rss",
      "line_count": 18
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# Hyperparameter set 2 - Matern kernel\n# matern1 <- ridge(X,y,lambda=0.1,kernel=rk_Matern)\n# matern2 <- ridge(X,y,lambda=0.1,kernel=maternkernel_nu_mod(1))\n# matern3 <- ridge(X,y,lambda=0.1,kernel=maternkernel_nu_mod(0.01))\n# matern4 <- ridge(X,y,lambda=0.1,kernel=maternkernel_nu_mod(1.5))\n# matern5 <- ridge(X,y,lambda=0.1,kernel=maternkernel_nu_mod(2.5))\n# matern6 <- ridge(X,y,lambda=0.1,kernel=maternkernel_nu_mod(10))\n# matern7 <- ridge(X,y,lambda=0.1,kernel=maternkernel_nu_mod(0.5))\n\nregisterDoParallel(8)\nnu <- c(0.01, 0.1, 0.5, 1, 1.5, 2, 2.5, 10)\nfitted_ridgeMatern <- vector()\nfitted_ridgeMatern <- \n  foreach (param = 1:(length(nu)+1), .combine = 'c') %dopar% {\n    besselk_nu <- function(nu) { \n      return(function(x) { return(GPBayes::BesselK(nu, x)) }) }\n    if (param==1) ridge(X, y, lambda=0.1, kernel=rk_Matern)\n    else ridge(X, y, lambda=0.1, kernel=maternkernel_nu_mod(nu[param-1]))\n  }\nstopImplicitCluster()\n\ndf <- as.data.frame(fitted_ridgeMatern)\n\ndf_tbl <- rbind(c(df$gcv[1], df$gcv.1[1], df$gcv.2[1], df$gcv.3[1], df$gcv.4[1],\n                  df$gcv.5[1], df$gcv.6[1], df$gcv.7[1], df$gcv.8[1], df$gcv.9[1]),\n                c(df$rss[1], df$rss.1[1], df$rss.2[1], df$rss.3[1], df$rss.4[1],\n                  df$rss.5[1], df$rss.6[1], df$rss.7[1], df$rss.8[1], df$rss.9[1]))\ncolnames(df_tbl) <- c(paste0(\"nu=\", 0.5), paste0(\"nu=\", nu[1]), paste0(\"nu=\", nu[2]), \n                      paste0(\"nu=\", nu[3]), paste0(\"nu=\", nu[4]),\n                      paste0(\"nu=\", nu[5]), paste0(\"nu=\", nu[6]),\n                      paste0(\"nu=\", nu[7]), paste0(\"nu=\", nu[8]))\nrownames(df_tbl) <- c(\"GCV\", \"RSS\")\n\nlibrary(DT)\ndatatable(df_tbl, caption=\"Matern Ridge Kernel (parameter estimates)\")",
      "line_count": 35
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# matern1$rss\n# matern2$rss\n# matern3$rss\n# matern4$rss\n# matern5$rss\n# matern6$rss\n# matern7$rss",
      "line_count": 7
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "#Hyperparameter set 3 - Polynomial Kernel kernel\n# poly1 <- ridge(X,y,lambda=0.1,kernel=poly_d_mod(3/2))\n# poly2 <- ridge(X,y,lambda=0.1,kernel=poly_d_mod(1))\n# poly3 <- ridge(X,y,lambda=0.1,kernel=poly_d_mod(0.9))\n# poly4 <- ridge(X,y,lambda=0.1,kernel=poly_d_mod(0.7))\n# poly5 <- ridge(X,y,lambda=0.1,kernel=poly_d_mod(0.5))\n# poly6 <- ridge(X,y,lambda=0.1,kernel=poly_d_mod(0.3))\n\nregisterDoParallel(7)\npoly_d <- c(0.3, 0.5, 0.7, 0.9, 1.5, 2.5, 10)\nfitted_ridgePoly <- vector()\nfitted_ridgePoly <- \n  foreach (param = 1:length(poly_d), .combine = 'c') %dopar% {\n    ridge(X, y, lambda=0.1, kernel=poly_d_mod(poly_d[param]))\n  }\nstopImplicitCluster()\n\ndf <- as.data.frame(fitted_ridgePoly)\n\ndf_tbl <- rbind(c(df$gcv[1], df$gcv.1[1], df$gcv.2[1], df$gcv.3[1], df$gcv.4[1],\n                  df$gcv.5[1], df$gcv.6[1]),\n                c(df$rss[1], df$rss.1[1], df$rss.2[1], df$rss.3[1], df$rss.4[1],\n                  df$rss.5[1], df$rss.6[1]))\ncolnames(df_tbl) <- c(paste0(\"d=\", poly_d[1]), paste0(\"d=\", poly_d[2]), \n                      paste0(\"d=\", poly_d[3]), paste0(\"d=\", poly_d[4]),\n                      paste0(\"d=\", poly_d[5]), paste0(\"d=\", poly_d[6]),\n                      paste0(\"d=\", poly_d[7]))\nrownames(df_tbl) <- c(\"GCV\", \"RSS\")\n\nlibrary(DT)\ndatatable(df_tbl, caption=\"Polynomial Ridge Kernel (parameter estimates)\")",
      "line_count": 31
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# poly1$rss\n# poly2$rss\n# poly3$rss\n# poly4$rss\n# poly5$rss\n# poly6$rss",
      "line_count": 6
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "plan(multisession,  workers = 8)\nX_augmented <- cbind(1, X) # augmented design matrix\ngcv_search <- future_map(lambda, function(lam) ridge(X_augmented, y, lam))\n\nV = map_dbl(gcv_search, function(x) x$gcv)\n\nridge_coefs = map_df(gcv_search, function(x) {\n    data.frame(value = x$omega_tilde, coef =as.character(1:2223))\n  }, \n  .id = 'iter') %>% mutate(lambda = lambda[as.integer(iter)])\n\nfit_ridge <- ridge(X_augmented, y, lambda[which.min(V)])  # fit optimal model\n\nomega_tilde <- fit_ridge$omega_tilde\nbeta_hat    <- crossprod(omega_tilde, X)       # slope and noise term coefficients",
      "line_count": 15
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(glmnet)\n# c(beta_0, beta_hat)\nfit_glmnet = glmnet(X, y, alpha = 0, lambda=lambda, standardize = FALSE)",
      "line_count": 3
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "kable_df = function(..., digits =3, full_width = FALSE) \n  kable(..., digits=digits) %>% kable_styling(full_width = full_width)\n\nrbind(RKHS_Ridge  = c(beta_hat),   # -1 to skip over the intercept estimation\n  GLMNet_Ridge = (coef(fit_glmnet)[, which.max(fit_glmnet$dev.ratio)])[-1]) %>% \n  kable_df()",
      "line_count": 6
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(MASS)\n# observation shrinkage\nlasso_observation <- function(X, y, lambda=0.1, kernel=rk) { \n  K = gram(X,kernel)                            \n  # assumed X to be n*d where observations stacked on the rows, \n  # GramMatrix (nxn) XTX (already phi^Tphi)\n  \n  n = length(y)                                   \n  #Q = cbind(1, GramMatrix) no need for intercept as the kernel already captures this\n  #S = rbind(0, cbind(0, GramMatrix))\n  alpha_hat = inverse(K) %*% y\n\n  for (j in 1:n) { # The explicit formula for LASSO loss\n    alpha_hat[j] =  alpha_hat[j]*max(0,1-length( alpha_hat)*lambda/abs( alpha_hat[j]))\n  }\n  \n  M = K%*%K + lambda*ginv(diag(as.list(abs(alpha_hat))))  # X^TX+lambda W-\n  \n  M_inv = inverse(M)                              # invert M\n\n  f_hat = K %*% alpha_hat \n  \n  A = K %*% M_inv %*% K  # Hat matrix X(X^XX+lambda W-)^_1 X^T\n  tr_A = sum(diag(A))                             # trace of hat matrix\n  \n  rss  = crossprod(y - f_hat)                     # residual sum of squares\n  gcv  = n*rss / (n - tr_A)^2                     # compute GCV score\n  \n  return(list(f_hat = f_hat, alpha_hat = alpha_hat, gcv = gcv, rss=rss))\n}",
      "line_count": 30
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# when lambda is 0\nfitted_lambda0 <- lasso_observation(X, y, 0,rk_Laplace)\nfitted_lambda0$gcv\nfitted_lambda0$rss",
      "line_count": 4
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "lambda = 10^seq(-9, 2, by = 0.5)\nlibrary(furrr)\n\n# For classical sequential parameter search use: \n# gcv_search = map(lambda, function(lam) ridge(X, y, lam))\n\n#  For multicore execution use:\nplan(multisession,  workers = 12)\ngcv_search <- future_map(lambda, function(lam) lasso_observation(X, y, lam,rk_Laplace))\n\nV_lasso = map_dbl(gcv_search, function(x) x$gcv)\n\nlasso_coefs = map_df(gcv_search, function(x) {\n    data.frame(value = x$alpha_hat, coef =as.character(1:2223))\n  }, \n  .id = 'iter') %>% mutate(lambda = lambda[as.integer(iter)])",
      "line_count": 16
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(patchwork)\nlibrary(plotly)\n\nRSS_lasso = map_dbl(gcv_search, function(x) x$rss)\nplot_ly(x=lambda, y=RSS_lasso, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(LASSO) RSS (Observation) vs. Log10(Lambda)\", \n         xaxis = list(title=\"log10(Lambda)\", type = \"log\"))\n# gcv_plot = qplot(lambda, V, geom='line', main='GCV score (log scale)', ylab='GCV') +\n#   scale_x_log10()\n\nplot_ly(x=lambda, y=V_lasso, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(LASSO) GCV (Observation) vs. Log10(Lambda)\", \n         xaxis = list(title=\"log10(Lambda)\", type = \"log\"))\n\n# beta_plot = ridge_coefs %>% \n#   ggplot(aes(x = lambda, y = value, color = coef)) +\n#   geom_line() +\n#   scale_x_log10() +\n#   scico::scale_color_scico_d(end = 0.8) +\n#   labs(title = 'Model Effects (Betas) Across Lambda')\n# ggplotly(gcv_plot + beta_plot)\n\nplot_ly(data=lasso_coefs[as.numeric(lasso_coefs$coef)<20,], x=~lambda, y=~value, \n        type=\"scatter\", mode=\"lines\", color=~coef, name=~coef) %>%\n  layout(title=\"LASSO Effect Estimates (Beta's) vs. log10(Lambda)\", \n         xaxis = list(title=\"\", type = \"log\"), # title=\"log10(Lambda)\", \n         legend = list(title=\"Predicting Covariates\", orientation = 'h'))",
      "line_count": 27
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "index <- 1\nmeta_matrix <- matrix(0,nrow=2223,ncol=length(lambda)) # 2223 is the number of rows\nfor (i in lambda){\n  meta_matrix[,index] <- (lasso_coefs[lasso_coefs$lambda==i,]$value ==0)\n    index <- index + 1\n}\nlibrary(DT)\n\n# image(t(meta_matrix),xlab=\"Lambda values\", yaxt=\"n\",xaxt=\"n\")\n# axis(3, at=seq(0,1, length=length(lambda)), labels=as.character(signif(lambda,2)))\nplot_ly(z=t(meta_matrix), type=\"heatmap\") %>%\n  layout(title=\"Lambda values (x-axis) vs. LASSO Coefficient Estimates (y-axis)\",\n         xaxis = list(tickvals = seq(0, dim(meta_matrix)[1], length=length(lambda)),\n                      ticktext = c(as.character(signif(lambda,2)))))",
      "line_count": 14
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "kernelized_y <- function(X,y, rkfunc = rk) { # compute the `crossprod` using the specified RK\n  apply(X, 1, function(Row) \n     rkfunc(Row, y) # specifies the Reproducing Kernel\n  )  \n}",
      "line_count": 5
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(MASS)\n# observation shrinkage\nlasso_feature <- function(X, y, lambda=0.1, kernel=rk) { \nK = gram(t(X),kernel)                            #assumed X to be n*d where observations stacked on the rows, GramMatrix (nxn) XTX (already phi^Tphi)\n  \n  n = ncol(X)                                   \n  #Q = cbind(1, GramMatrix) no need for intercept as the kernel already captures this\n  #S = rbind(0, cbind(0, GramMatrix))\n  kernel_yval <-kernelized_y(t(X),y,kernel)\n  beta_hat = inverse(K) %*% kernel_yval\n \n  for (j in 1:n) {    # The explicit formula for LASSO loss\n     beta_hat[j] = beta_hat[j]*max(0,1-length(beta_hat )*lambda/abs(beta_hat[j]))\n  }\n  \n  M = K%*%K + lambda*ginv(diag(as.list(abs(beta_hat))))   # X^TX+lambda W-\n  \n  M_inv = inverse(M)                              # invert M\n  \n  f_hat = K %*% beta_hat \n  \n  A = K %*% M_inv %*% K  # Hat matrix X(X^XX+lambda W-)^_1 X^T\n  tr_A = sum(diag(A))                             # trace of hat matrix\n  \n  rss  = crossprod(kernel_yval - f_hat)                     # residual sum of squares\n  gcv  = n*rss / (n - tr_A)^2                     # compute GCV score\n  \n  return(list(f_hat = f_hat, beta_hat = beta_hat, gcv = gcv, rss=rss))\n}",
      "line_count": 29
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# when lambda is 0\nfitted_lambda0 <- lasso_feature(X, y, 0)\nfitted_lambda0$gcv\nfitted_lambda0$rss",
      "line_count": 4
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "lambda = 10^seq(-9, 2, by = 0.5)\nlibrary(furrr)\n\n# For classical sequential parameter search use: \n# gcv_search = map(lambda, function(lam) ridge(X, y, lam))\n\n#  For multicore execution use:\nplan(multisession,  workers = 12)\ngcv_search <- future_map(lambda, function(lam) lasso_feature(X_augmented, y, lam))\n\nV_lasso = map_dbl(gcv_search, function(x) x$gcv)\n\nlasso_coefs = map_df(gcv_search, function(x) {\n    data.frame(value = x$beta_hat, coef =as.character(1:6))\n  }, \n  .id = 'iter') %>% mutate(lambda = lambda[as.integer(iter)])",
      "line_count": 16
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(patchwork)\nlibrary(plotly)\n\nRSS_lasso = map_dbl(gcv_search, function(x) x$rss)\nplot_ly(x=lambda, y=RSS_lasso, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(LASSO) RSS (Feature) vs. Log10(Lambda)\", \n         xaxis = list(title=\"log10(Lambda)\", type = \"log\"))\n# gcv_plot = qplot(lambda, V, geom='line', main='GCV score (log scale)', ylab='GCV') +\n#   scale_x_log10()\n\nplot_ly(x=lambda, y=V_lasso, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(LASSO) GCV (Feature) vs. Log10(Lambda)\", \n         xaxis = list(title=\"log10(Lambda)\", type = \"log\"))\n\n# beta_plot = ridge_coefs %>% \n#   ggplot(aes(x = lambda, y = value, color = coef)) +\n#   geom_line() +\n#   scale_x_log10() +\n#   scico::scale_color_scico_d(end = 0.8) +\n#   labs(title = 'Model Effects (Betas) Across Lambda')\n# ggplotly(gcv_plot + beta_plot)\n\nplot_ly(data=lasso_coefs, x=~lambda, y=~value, \n        type=\"scatter\", mode=\"lines\", color=~coef, name=~coef) %>%\n  layout(title=\"(LASSO Estimates) Effects (Beta's) vs. log10(Lambda)\", \n         xaxis = list(title=\"\", type = \"log\"), # title=\"log10(Lambda)\", \n         legend = list(title=\"Predicting Covariates\", orientation = 'h'))",
      "line_count": 27
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "index <- 1\nmeta_matrix <- matrix(0,nrow=6,ncol=length(lambda)) # 2223 is the number of columns\nfor (i in lambda){\n  meta_matrix[,index] <- (lasso_coefs[lasso_coefs$lambda==i,]$value ==0)\n    index <- index + 1\n}\nlibrary(DT)\n\n# image(t(meta_matrix),xlab=\"Lambda values\", yaxt=\"n\",xaxt=\"n\")\n# axis(3, at=seq(0,1, length=length(lambda)), labels=as.character(signif(lambda,2)))\nplot_ly(z=t(meta_matrix), type=\"heatmap\") %>%\n  layout(title=\"Lambda values (x-axis) vs. LASSO Coefficient Estimates (y-axis)\",\n         xaxis = list(tickvals = seq(0, dim(meta_matrix)[1], length=length(lambda)),\n                      ticktext = c(as.character(signif(lambda,2)))))",
      "line_count": 14
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "fit_lasso = lasso_feature(X_augmented, y, lambda[which.min(V_lasso)])  # fit optimal model\nbeta_hat = fit_lasso$beta_hat",
      "line_count": 2
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(glmnet)\n# c(beta_0, beta_hat)\nfit_glmnet = glmnet(X, y, alpha = 1, lambda=lambda, standardize = FALSE)",
      "line_count": 3
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "kable_df = function(..., digits =3, full_width = FALSE) \n  kable(..., digits=digits) %>% kable_styling(full_width = full_width)\n\nrbind(RKHS_LASSO  = c(beta_hat),\n  GLMNet_LASSO = coef(fit_glmnet)[, which.max(fit_glmnet$dev.ratio)]) %>% \n  kable_df()",
      "line_count": 6
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(kernlab)\nlibrary(mlbench)\n\n# data(spirals)\n# str(spirals) # num [1:300, 1:2]\n\nspirals <- mlbench.spirals(300, cycles=2, sd=0.05)\n# plot(p) # ; str(p)\n\nx = as.matrix(spirals$x[ , 1])\nx = scales::rescale(x, to = c(0, 1)) # rescale predictor to [0,1]\ny = spirals$x[ , 2]\nlabels <- spirals$classes",
      "line_count": 13
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# rk_spline <- function(s, t) {\n#   return(0.5 * min(s, t)^2 * max(s, t) - (1/6) * min(s, t)^3)\n# }\n\nrk_spline <- function(s, t) {\n  init_len = length(s)\n  rk = 1\n  for (i in 1:init_len) { \n    minv <- pmin(s[i],t[i])\n    rk = (1 + s[i]*t[i]*(1 + minv) - (s[i] + t[i])*(minv^2)/2 + (minv^3)/3) * rk \n  }\n  return(rk)\n}",
      "line_count": 13
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# May need to use BCE loss function (instead of RSS)\n# binary_cross_entropy(pred, data.valid,loss.unit=c(\"individuals\",\"L2 units\"),                                 y, L2.unit)\n# https://rdrr.io/cran/autoMrP/src/R/utils.R\n\n###########################################################################\n# binary cross-entropy ----------------------------------------------------\n###########################################################################\nbinary_cross_entropy <- function(dataProb, trueLabels, positiveClassLabel=\"1\",\n                                 w_positive=0.5, w_negative=0.5) {\n    # positiveClassLabel is the positive class\n    # Extract true labels and predicted probabilities\n    truth <- as.numeric(trueLabels == positiveClassLabel) \n    prob <- dataProb # Probabilities for positive class\n\n    # Calculate the BCE balanced log loss\n    BCE <- -mean(\n      (truth * log(prob) * w_positive) + ((1 - truth) * log(1 - prob) * w_negative)\n    )\n    return(BCE)\n}\n\n# Normalize all probability estimates to [0-e, 1-e]\nprobNormalize <- function(p) {\n  epsil <- 1e-4  # avoid singularities\n  m <- min(p)\n  M <- max(p)\n  return ( (p-m+epsil)/(M-m+2*epsil) )\n}\n\n# # ################# BCE test\n# # # smoothing_spline(x, y, lam)\n#   X = x\n#   y = as.numeric(labels)\n# \n#   GramMatrix = gram(X, rkfunc = rk_spline) # GramMatrix matrix (nxn)\n# \n#   n = length(y)\n#   J = cbind(1, X)           # matrix with a basis for the null space of the penalty\n#   Q = cbind(J, GramMatrix)       # design matrix\n#   m = ncol(J)               # dimension of the null space of the penalty\n# \n#   S = matrix(0, n + m, n + m)                        # initialize S\n#   S[(m + 1):(n + m), (m + 1):(n + m)] = GramMatrix        # non-zero part of S\n# \n#   M = crossprod(Q) + lambda[1]*S\n#   M_inv = inverse(M)                                 # inverse of M\n# \n#   gamma_hat = crossprod(M_inv, crossprod(Q, y))\n#   f_hat = Q %*% gamma_hat\n# \n#   A    = Q %*% M_inv %*% t(Q)\n#   tr_A = sum(diag(A))                                # trace of hat matrix\n# \n#   # rss = crossprod(y - f_hat)                       # BCE loss\n#   # pred <- as.factor(ifelse (f_hat > 1.5, \"1\", \"2\"))\n#   pred <- probNormalize(f_hat - 1)\n#   BCE <- binary_cross_entropy(dataProb=pred[, 1], trueLabels=as.character(labels),\n#                               positiveClassLabel=\"1\")\n\n#########################################\n#   Numerical output (regression spline)\n#########################################\nsmoothing_spline <- function(X, y, lambda) {\n  GramMatrix = gram(X, rkfunc = rk_spline) # GramMatrix matrix (nxn)\n  \n  n = length(y)\n  J = cbind(1, X)           # matrix with a basis for the null space of the penalty\n  Q = cbind(J, GramMatrix)       # design matrix\n  m = ncol(J)               # dimension of the null space of the penalty\n  \n  S = matrix(0, n + m, n + m)                        # initialize S\n  S[(m + 1):(n + m), (m + 1):(n + m)] = GramMatrix        # non-zero part of S\n  \n  M = crossprod(Q) + lambda*S\n  M_inv = inverse(M)                                 # inverse of M\n  \n  gamma_hat = crossprod(M_inv, crossprod(Q, y))\n  f_hat = Q %*% gamma_hat\n  \n  A    = Q %*% M_inv %*% t(Q)\n  tr_A = sum(diag(A))                                # trace of hat matrix\n  \n  rss = crossprod(y - f_hat)                         # residual sum of squares (RSS)\n  gcv = n * rss/(n - tr_A)^2                         # compute GCV score\n  \n  return(list(f_hat = f_hat, gamma_hat = gamma_hat, gcv = gcv))\n}\n\n#########################################\n#   Categorical output (binary cross-entropy spline)\n#########################################\nsmoothing_BCE_spline <- function(X, y, lambda) {\n  GramMatrix = gram(X, rkfunc = rk_spline) # GramMatrix matrix (nxn)\n  \n  n = length(y)\n  J = cbind(1, X)           # matrix with a basis for the null space of the penalty\n  Q = cbind(J, GramMatrix)       # design matrix\n  m = ncol(J)               # dimension of the null space of the penalty\n  \n  S = matrix(0, n + m, n + m)                        # initialize S\n  S[(m + 1):(n + m), (m + 1):(n + m)] = GramMatrix        # non-zero part of S\n  \n  M = crossprod(Q) + lambda*S\n  M_inv = inverse(M)                                 # inverse of M\n  \n  gamma_hat = crossprod(M_inv, crossprod(Q, y))\n  f_hat = Q %*% gamma_hat\n  \n  A    = Q %*% M_inv %*% t(Q)\n  tr_A = sum(diag(A))                                # trace of hat matrix\n  \n  # rss = crossprod(y - f_hat)                       # BCE loss\n  # pred <- as.factor(ifelse (f_hat > 1.5, \"1\", \"2\"))\n  # convert fitted values to probabilities in [0, 1] for BCE loss estimation\n  pred <- probNormalize(f_hat - 1) # 1 <= f_hat <=2\n  BCE <- binary_cross_entropy(dataProb=pred[, 1], trueLabels=as.character(labels),\n                              positiveClassLabel=\"1\")\n  \n  return(list(f_hat = f_hat, gamma_hat = gamma_hat, BCE = BCE))\n}",
      "line_count": 120
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "lambda = 10^seq(-6, 0, by = 0.5)\n\n# Slow # gcv_search = map(lambda, function(lam) smoothing_spline(x, y, lam))\n\n#  For quicker multi core execution\nplan(multisession,  workers = 8)\ngcv_search = map(lambda, function(lam) smoothing_spline(x, y, lam))\n\nV = map_dbl(gcv_search, function(x) x$gcv)\n\n## BCE Metric\nplan(multisession,  workers = 8)\nBCE_search = map(lambda, function(lam) smoothing_BCE_spline(x, y, lam))\nV_BCE = map_dbl(BCE_search, function(x) x$BCE)",
      "line_count": 14
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "# gcv_plot = qplot(lambda, V, geom = 'line', main = 'GCV score', ylab = 'GCV') +\n#   scale_x_log10()\n# ggplotly(gcv_plot)\n\nplot_ly(x=lambda, y=V, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(Spirals Simulation) GCV vs. Log10(Lambda)\", xaxis = list(type = \"log\"))\n\n#### BCE\nplot_ly(x=lambda, y=V_BCE, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"(Spirals Simulation) BCE vs. Log10(Lambda)\", xaxis = list(type = \"log\"))",
      "line_count": 10
    },
    {
      "section": "Examples of Regression Modeling using Reproducing Kernel Hilbert Spaces",
      "code": "library(mgcv)\n# https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/gam.models.html\n\nfit_rk  = smoothing_spline(x, y, lambda[which.min(V)])   # fit optimal model\nfit_rk_bin  = smoothing_BCE_spline(x, y, lambda[which.min(V_BCE)])   # fit optimal model\ngamSpline = gam(y ~ s(x,bs=\"cr\",k=100)) # s(x) = smooth function of x\nprint(paste0(\"y ~ s(x) GAM Spline model Log-Likelihood = \", round(logLik(gamSpline), 4)))\n\ngamBin <- gam(labels ~ s(x,y), family = 'binomial')\nprint(paste0(\"labels ~ s(x,y) GAM Binary model Log-Likelihood = \", round(logLik(gamBin), 4)))\n\ndf <- as.data.frame(cbind(X=spirals$x[ , 1], Y=spirals$x[ , 2],\n                          class=as.factor(spirals$classes)))\n\n# fit_plot = df %>%\n#   mutate(fit_rk  = fit_rk$f_hat[, 1],\n#          fit_gam = fitted(fit_gam)) %>%\n#   arrange(X) %>%\n#   pivot_longer(-c(X, Y), names_to = 'fit', values_to = 'value')  %>%\n#   ggplot(aes(X, Y)) +\n#   geom_point(color = '#FF55001A') +\n#   geom_line(aes(y = value, color = fit)) +\n#   scico::scale_color_scico_d(palette = 'hawaii', begin = .2, end = .8)\n# ggplotly(gcv_plot + fit_plot)\n\n# identify the 2 true spirals\n# df_reformat$class <- rep(spirals$classes, 2)\n# table(fit_rk_bin$f_hat[, 1] > 1.499)\n# FALSE  TRUE \n#   152   148  \n  \ndf_reformat <- df %>% \n  mutate(RKHS_mod=fit_rk$f_hat[, 1], \n         RKHS_Spline_Binary=2*as.numeric(fit_rk_bin$f_hat[, 1] > 1.499)-1,\n         GAM_Spline=fitted(gamSpline), GAM_Binary=3*fitted(gamBin)-1.5) %>%\n  arrange(X) %>%\n  pivot_longer(-c(X, Y, class), names_to = 'fit', values_to = 'value')  \n\ndf_reformat %>%\n    plot_ly(x=~X, y=~Y, type=\"scatter\", mode=\"markers\", color=~class,\n            # symbol=~class, symbols=as.character(seq_along(unique(df_reformat$class))),\n            name=~paste0(\"True-Class=\", class)) %>% hide_colorbar() %>%\n    add_trace(x=~X, y=~value, color=~fit, colors = c(\"red\", \"blue\", \"green\", \"purple\"),\n              symbol=~class, symbols=as.character(seq_along(unique(df_reformat$class))),\n              name=~paste0(\" \", fit), mode=\"markers\",  \n              hovertemplate = paste('(Model)')) %>%\n    # colors = RColorBrewer::brewer.pal(3, \"Set2\")[1:2]) %>%\n    layout(title=\"(Binary & Regressive) GAM vs. RKHS Models of Spirals Data\\n \n           Color=Model Fit, Shape=True-Class\", \n           xaxis = list(title=\"X\", showline=TRUE), yaxis = list(title=\"Y\"),\n           legend = list(title=\"Objects\", orientation = 'h'), hovermode  = 'x')",
      "line_count": 51
    },
    {
      "section": "Predicting the Future Behavior of Time-Varying Probability Distributions",
      "code": "library(animation)\nlibrary(circular)\nlibrary(plotly)\nepsilon <- 0.1\nsampleSize <- 1000   # total number of phases to sample for 3 different processes (x, y, z)\nsizePerTime <- 100   # number of phases to use for each fixed time (must divide sampleSize)\ncircleUniformPhi <- seq(from=-pi, to=pi, length.out=sizePerTime)\n\noopt = ani.options(interval = 0.2)\nset.seed(1234)\n# sample the the kime-phases for all 3 different processes and the r time points\nx <- rvonmises(n=sampleSize, mu=circular(pi/5), kappa=3)\ny <- rvonmises(n=sampleSize, mu=circular(-pi/3), kappa=5)\nz <- rvonmises(n=sampleSize, mu=circular(0), kappa=10)\nr <- seq(from=1, to=sampleSize/sizePerTime, length.out=10)\n\n# Define a function that renormalizes the kime-phase to [-pi, pi)\npheRenormalize <- function (x) {\n  out <- ifelse(as.numeric(x) <= pi, as.numeric(x)+pi, as.numeric(x)-pi)\n  return (out)\n}\n\n# transform Von Mises samples from [0, 2*pi) to [-pi, pi)\nx <- pheRenormalize(x)\ny <- pheRenormalize(y)\nz <- pheRenormalize(z)\n\n# vectorize the samples\nvectorX = as.vector(x)\nvectorY = as.vector(y)\nvectorZ = as.vector(z)\n# Starting phases, set the first phase index=1\nplotX = c(vectorX[1])\nplotY = c(vectorY[1])\nplotZ = c(vectorZ[1])\n\n# pl_list <- list()\npl_scene <- plot_ly(type='scatter3d', mode=\"markers\")\nplotX <- list() \nplotY <- list() \nplotZ <- list() \n\nplotX_df <- list()   # need separate data frames to store all time foliations\nplotY_df <- list()\nplotZ_df <- list()\n\nfor (t in 1:length(r)) {  # loop over time\n  # loop over kime-phases\n  plotX[[t]] <- as.numeric(x[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  plotY[[t]] <- as.numeric(y[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  plotZ[[t]] <- as.numeric(z[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  \n  tempX = circular(unlist(plotX[[t]]))\n  tempY = circular(unlist(plotY[[t]]))\n  tempZ = circular(unlist(plotZ[[t]]))\n  \n  resx <- density(tempX, bw=25, xaxt='n', yaxt='n')\n  resy <- density(tempY, bw=25, xaxt='n', yaxt='n')\n  resz <- density(tempZ, bw=25, xaxt='n', yaxt='n')\n\n  unifPhi_df <- as.data.frame(cbind(t=t, circleUniformPhi=circleUniformPhi))\n  plotX_df[[t]] <- as.data.frame(cbind(t=t, plotX=unlist(plotX[[t]])))\n  plotY_df[[t]] <- as.data.frame(cbind(t=t, plotY=unlist(plotY[[t]])))\n  plotZ_df[[t]] <- as.data.frame(cbind(t=t, plotZ=unlist(plotZ[[t]])))\n  \n  pl_scene <- pl_scene %>% add_trace(data=unifPhi_df, showlegend=FALSE,\n                      x = ~((t-epsilon)*cos(circleUniformPhi)), \n                      y = ~((t-epsilon)*sin(circleUniformPhi)), z=0,\n                      name=paste0(\"Time=\",t), line=list(color='gray'),\n                      mode = 'lines', opacity=0.3) %>%\n    add_markers(data=plotX_df[[t]], x=~(t*cos(plotX)), y=~(t*sin(plotX)), z=0,\n                      type='scatter3d', name=paste0(\"X: t=\",t), \n                      marker=list(color='green'), showlegend=FALSE,\n                      mode = 'markers', opacity=0.3) %>%\n    add_markers(data=plotY_df[[t]], x=~((t+epsilon)*cos(plotY)),\n                    y=~((t+epsilon)*sin(plotY)), z=0-epsilon, showlegend=FALSE,\n                    type='scatter3d', name=paste0(\"Y: t=\",t), \n                    marker=list(color='blue'),\n                    mode = 'markers', opacity=0.3) %>%\n    add_markers(data=plotZ_df[[t]], x=~((t+2*epsilon)*cos(plotZ)),\n                y=~((t+2*epsilon)*sin(plotZ)), z=0+epsilon, showlegend=FALSE,\n                type='scatter3d', name=paste0(\"Z: t=\",t), \n                marker=list(color='red'),\n                mode = 'markers', opacity=0.3)\n} \n\nmeans_df <- as.data.frame(cbind(t = c(1:length(r)),\n                                plotX_means=unlist(lapply(plotX, mean)),\n                                plotY_means=unlist(lapply(plotY, mean)),\n                                plotZ_means=unlist(lapply(plotZ, mean))))\npl_scene <- pl_scene %>% \n  # add averaged (denoised) phase trajectories\n  add_trace(data=means_df, x=~(t*cos(plotX_means)), \n        y=~(t*sin(plotX_means)), z=0,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs X\", \n        line=list(color='green', width=15), opacity=0.8) %>%\n  add_trace(data=means_df, x=~(t*cos(plotY_means)), \n        y=~(t*sin(plotY_means)), z=0-epsilon,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs X\", \n        line=list(color='blue', width=15), opacity=0.8) %>%\n  add_trace(data=means_df, x=~(t*cos(plotZ_means)), \n        y=~(t*sin(plotZ_means)), z=0+epsilon,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs X\", \n        line=list(color='red', width=15), opacity=0.8) %>%\n  add_trace(x=0, y=0, z=c(-2,2), name=\"Space\", showlegend=FALSE,\n             line=list(color='gray', width=15), opacity=0.8) %>%\n  layout(title=\"Pseudo Spacekime (1D Space, 2D Kime) Kime-Phase Sampling and Foliation\",\n          scene = list(xaxis=list(title=\"Kappa1\"), yaxis=list(title=\"Kappa2\"),\n                        zaxis=list(title=\"Space\"))) %>% hide_colorbar()\npl_scene",
      "line_count": 110
    },
    {
      "section": "Predicting the Future Behavior of Time-Varying Probability Distributions",
      "code": "library(plotly)\n# 3D Wiener Process\nN=500  # Number of random walk steps\n# Define the X, Y, and Z, coordinate displacements independently\n# xdis = rnorm(N, 0 , 1)\n# ydis = rnorm(N, 0 , 2)\n# zdis = rnorm(N, 0 , 3)\n# xdis = cumsum(xdis)\n# ydis = cumsum(ydis)\n# zdis = cumsum(zdis)\n\n# To use simulated 3D MVN Distribution\ndis <- mixtools::rmvnorm(N, mu=c(0,0,0), \n                         sigma=matrix(c(1,0,0, 0,2,0, 0,0,3), ncol=3))\n\n# aggregate the displacements to get the actual 3D Cartesian Coordinates\nxdis = cumsum(dis[,1])\nydis = cumsum(dis[,2])\nzdis = cumsum(dis[,3])\n\n# add Poisson noise\nat = rpois(N, 0.1)\n\nfor(i in c(1:N)) {\n  if(at[i] != 0) {\n    xdis[i] = xdis[i]*at[i]\n    ydis[i] = ydis[i]*at[i]\n    zdis[i] = ydis[i]*at[i]\n  }\n}\n# plot(xdis, ydis, type=\"l\", \n#   main =\"Brownian Motion in Two Dimension with Poisson Arrival Process\", \n#   xlab=\"x displacement\", ylab = \"y displacement\")\n\nplot_ly(x=xdis, y=ydis, z=zdis, type=\"scatter3d\", mode=\"markers+lines\", \n        text=~c(1:N), hoverinfo='text', \n        marker=list(color='gray'), showlegend=F) %>%\n  # Emphasize the starting and ending points\n  add_markers(x=xdis[1], y=ydis[1], z=zdis[1], marker=list(size=20,color=\"green\"),\n              text=paste0(\"Starting Node 1\")) %>%\n  add_markers(x=xdis[N], y=ydis[N], z=zdis[N],\n              marker=list(size=20,color=\"red\"), text=paste0(\"Ending Node \", N)) %>%\n  layout(title=\"3D Brownian Motion Simulation (MVN & Poisson)\",\n         scene=list(xaxis=list(title=\"X displacement\"), \n                    yaxis=list(title=\"Y displacement\"),\n                    zaxis=list(title=\"Z displacement\")))\n\n# 1D Wiener Process\n# dis = rnorm(N, 0, 1);\n# at = rpois(N,1)\n# for(i in 1:N) {\n#   if(at[i] != 0){\n#     dis[i]= dis[i]*at[i]\n#   } \n# }\n# dis = cumsum(dis)\n# plot(dis, type= \"l\",\n#   main= \"Brownian Motion in One Dimension with Poisson Arrival Process\", \n#   xlab=\"time\", ylab=\"displacement\")\n\n# ub = 20; lb = -20\n# xdis = rnorm(N, 0 ,1)\n# xdis1 = rep(1,N)\n# xdis1[1] = xdis[1]\n# for(i in c(1:(N-1))){   \n#   if(xdis1[i] + xdis[i+1] > ub) { xdis1[i+1] <- ub }    \n#   else if(xdis1[i] + xdis[i+1] < lb) { xdis[i+1] = lb }   \n#   else { xdis1[i+1] = xdis1[i] + xdis[i+1] } \n# } \n# \n# plot(xdis1, type=\"l\",main=\"Brownian Motion with bound in 1-dim\",      xlab=\"displacement\",ylab=\"time\")\n\n# Compute the row Euclidean differences\ndf <- data.frame(cbind(xdis, ydis, zdis))\nrowEuclidDistance <- dist(df)\nplot_ly(z=as.matrix(rowEuclidDistance), type=\"heatmap\") %>%\n  layout(title=\"Heatmap of Euclidean Distances between Consecutive Steps of Wiener Process\")\nplot_ly(x=as.vector(as.matrix(rowEuclidDistance)), type = \"histogram\") %>%\n  layout(title=\"Histogram of Euclidean Distances between Consecutive Steps of the Wiener Process\")",
      "line_count": 79
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "diffusion_sol <- function(x,t,x_init,t_init,D=1) {\n  return(1/sqrt(4*pi*D*(t-t_init))*exp(-(x-x_init+D*(t-t_init))^2/(4*D*(t-t_init))))\n}",
      "line_count": 3
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "library(plotly)\n# legend is false\nfig <- plotly::subplot(\n  plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),0.01,0,0), type = 'scatter', mode = 'lines'),\n  plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),0.8,0,0), type = 'scatter', mode = 'lines'),\n    plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),0.2,0,0), type = 'scatter', mode = 'lines'),\n  plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1,0,0), type = 'scatter', mode = 'lines'),\n    plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),0.4,0,0), type = 'scatter', mode = 'lines'),\n  plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1.2,0,0), type = 'scatter', mode = 'lines'),\n    plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),0.6,0,0), type = 'scatter', mode = 'lines'),\n  plot_ly(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1.4,0,0), type = 'scatter', mode = 'lines'),\n  nrows = 4, margin = 0.05\n)\n# move legends to annotations\nannotations = list( \n  list( \n    x = 0.2,  \n    y = 1.0,  \n    text = TeX(\"\\\\textbf{t=0.01}, N(-0.01,\\\\sqrt{0.02})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",\n    font = list(size = 12),\n    yanchor = \"bottom\",  \n    showarrow = FALSE \n  ),  \n  list( \n    x = 0.2,  \n    y = 0.7,  \n    text = TeX(\"\\\\textbf{t=0.2}, N(-0.2,\\\\sqrt{0.4})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",\n    font = list(size = 15),\n    yanchor = \"bottom\",  \n    showarrow = FALSE \n  ),  \n  list( \n    x = 0.2,  \n    y = 0.45,  \n    text = TeX(\"\\\\textbf{t=0.4}, N(-0.2,\\\\sqrt{0.8})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",  \n    yanchor = \"bottom\", \n    font = list(size = 15),\n    showarrow = FALSE \n  ),\n  list( \n    x = 0.2,  \n    y = 0.2,  \n    text = TeX(\"\\\\textbf{t=0.6}, N(-0.6,\\\\sqrt{1.2})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",  \n    yanchor = \"bottom\", \n    font = list(size = 15),\n    showarrow = FALSE \n  ),  \n  list( \n    x = 0.8,  \n    y = 1,  \n    text = TeX(\"\\\\textbf{t=0.8}, N(-0.8,\\\\sqrt{1.6})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    font = list(size = 12),\n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE \n  ),  \n  list( \n    x = 0.8,  \n    y = 0.7,  \n    text = \"t=1\",  \n    xref = \"paper\",  \n    yref = \"paper\",\n    font = list(size = 15),\n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE \n  ),\n  list( \n    x = 0.8,  \n    y = 0.45,  \n    text = TeX(\"\\\\textbf{t=1.2}, N(-1.2,\\\\sqrt{2.4})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",\n    font = list(size = 15),\n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE \n  ),\n  list( \n    x = 0.8,  \n    y = 0.2,  \n    text = TeX(\"\\\\textbf{t=1.4}, N(-1.4,\\\\sqrt{2.8})\"),  \n    xref = \"paper\",  \n    yref = \"paper\",\n    font = list(size = 15),\n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE \n  ))\nfig <- fig %>% layout(title = \"Fokker Planck solution p(x,t)\",\n                      annotations = annotations,showlegend=FALSE) %>% config(mathjax = 'cdn')\n\nfig\n",
      "line_count": 108
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "n <- 100\nX0 <- rnorm(n,-0.01,sqrt(2*0.01))\nX1 <- rnorm(n,-0.2,sqrt(2*0.2))\nX2 <- rnorm(n,-0.4,sqrt(2*0.4))\nX3 <- rnorm(n,-0.6,sqrt(2*0.6))\nX4 <- rnorm(n,-0.8,sqrt(2*0.8))\nX5 <- rnorm(n,-1,sqrt(2*1))\nX6 <- rnorm(n,-1.2,sqrt(2*1.2))",
      "line_count": 8
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "#Kernels\nlibrary(GPBayes)\n# Matern kernel function\nbesselk_nu <- function(nu){\n  return(function(x){\n    return (besselK(x,nu,expon.scaled=TRUE))\n  })\n}\nmaternkernel <- function(nu,x,sigma=1,rho=1){\n  scaled_x = sqrt(2*nu)*x/rho\n  return (sapply(scaled_x,besselk_nu(nu))*scaled_x**nu*sigma**2*2**(1-nu)/gamma(nu))\n}\nlinear_exp<- function (s,t){\n  return (exp(-abs(s-t)))\n}\nquad_exp <- function (s,t){\n  return (exp(-(s-t)^2))\n}\npoly_exp <- function (s,t){\n  return (exp(-s*t))\n}\nmatern <- function(nu){\n  return(function(s,t){\n    return (maternkernel(nu,s-t))\n  })\n}\n",
      "line_count": 27
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "# Implement the estimation for kappa kernel sum\nkappa <- function(s,t,f=linear_exp){\n  ns <- length(s)\n  nt <- length(t)\n  tot <- 0\n  for (i in 1:ns){\n    for (j in 1:nt){\n      tot <- tot+ f(s[i],t[j])\n    }\n  }\n  return (tot/(ns*nt))\n}",
      "line_count": 12
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "K_st <- function(X, func = linear_exp){\n  ncol = dim(X)[2]\n  K_matrix <- matrix(0, ncol, ncol)\n  for (i in 1:ncol){\n    for (j in 1:ncol){\n      K_matrix[i,j] <- kappa(X[,i],X[,j], func)\n    }\n  }\n  return(K_matrix)\n}",
      "line_count": 10
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "K_t <- function(X,Y, func = linear_exp){\n  ncol = dim(X)[2]\n  K_matrix <- matrix(0, ncol, 1)\n  for (i in 1:ncol){\n      K_matrix[i,1] <- kappa(X[,i],Y,func)\n  }\n  return(K_matrix)\n}",
      "line_count": 8
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "sum_kernels <- function(val,beta, func=quad_exp){\n  XX <- list(X0,X1,X2,X3,X4,X5,X6)\n  tot <- 0\n  for (i in 1:7){\n    tot <- tot+ beta[i]*sum(func(val,XX[[i]]))\n  }\n  return(tot)\n}",
      "line_count": 8
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "lampert <- function(n,lambda, func = quad_exp){\n# Kst <- gram(t(cbind(X0,X1,X2,X3,X4,X5,X6)))\nKst <- K_st(cbind(X0,X1,X2,X3,X4,X5),func)\nkt <- K_t(cbind(X0,X1,X2,X3,X4,X5),X6,func)\nbeta <- solve(Kst+lambda*diag(6))%*%kt\nbeta <- c(beta,beta[6])\nprint(beta)\n# sample 1 to 6 according to the weights in beta\n# sample <- sample(1:7,1000,prob=beta,replace=TRUE)\n\n# clist <- cbind(X0,X1,X2,X3,X4,X5,X6)\n# # drew according to X0,X1,X2,X3,..,X6 according to sample\n# sampled <- c()\n# for (i in 1:1000){\n#   bucket <- clist[,sample[i]]\n#   # sample 1,2,.., n uniformly\n#   s <- length(bucket)\n#   # sample 1,2,3,4.., s uniformly\n#   sampled <- c(sampled,bucket[sample(1:s,1)])\n# }\n# Herding\ntar <- seq(-10,10,0.01)\nlargest_x <- -10\nlargest_y <- -100\nfor (val in tar){\n  curr_y <- sum_kernels(val,beta, func)\n  if (curr_y > largest_y){\n    largest_y <- curr_y\n    largest_x <- val\n  }\n}\nlargest_xs <- c(largest_x)\n# append the largest largest_x in largest_xs\nfor (i in 1:(n-1)){\n  largest_x <- -10\n  largest_y <- -100\n  for (val in tar){\n  curr_y <- sum_kernels(val,beta,func)\n  n_av <- length(largest_xs)\n  curr_y <- 1/100*curr_y - sum(func(val,largest_xs))/(n_av+1)\n  if (curr_y > largest_y){\n    largest_y <- curr_y\n    largest_x <- val\n  }\n  }\n  largest_xs <- c(largest_xs,largest_x)\n}\nreturn (largest_xs)\n}",
      "line_count": 49
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "x <- lampert(n,1/n,quad_exp)\nfit <- density(x)\nplot_ly(x = x) %>% \n  # probability density\n  add_histogram(histnorm =\"probability\" ,name=\"Empirical sampled histogram\") %>% \n  add_lines(x = fit$x, y = fit$y, fill = \"tozeroy\", yaxis = \"y2\", name = \"Fitted density\") %>% \n  add_lines(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1.4,0,0), type = 'scatter', mode = 'lines', name = 't=1.4') %>%\n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))",
      "line_count": 8
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "order_stats_reg <- function(n){\n#sort the data X0 from small to large\nX0d <- sort(X0)\nX1d <- sort(X1)\nX2d <- sort(X2)\nX3d <- sort(X3)\nX4d <- sort(X4)\nX5d <- sort(X5)\nX6d <- sort(X6)\nX7 <- rep(0,n)\nfor (i in 1:n){\n  # fit a arima model given X0(i),X1(i),X2(i),X3(i),X4(i),X5(i),X6(i) fit to X7(i)\n  # predict X7(i) given X0(i),X1(i),X2(i),X3(i),X4(i),X5(i),X6(i)\n  # sample X7(i) according to the predicted distribution\n  arma_model <- arima(c(X0d[i],X1d[i],X2d[i],X3d[i],X4d[i],X5d[i],X6d[i]),order=c(0,1,1))\n  X7[i] <- predict(arma_model,n.ahead=1)$pred[1]\n}\nreturn (X7)\n}",
      "line_count": 19
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "x <- order_stats_reg(n)\nfit <- density(x)\nplot_ly(x = x) %>% \n  # probability density\n  add_histogram(histnorm =\"probability\" ,name=\"Empirical sampled histogram\") %>% \n  add_lines(x = fit$x, y = fit$y, fill = \"tozeroy\", yaxis = \"y2\", name = \"Fitted density\") %>% \n  add_lines(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1.4,0,0), type = 'scatter', mode = 'lines', name = 't=1.4') %>%\n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))",
      "line_count": 8
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "library(dtw);\n# X0 <- rnorm(100,-0.01,1)*sqrt(2*0.01)\n# Y0 <- rep(0,10)\n# X1 <- rnorm(100,-0.2,1)*sqrt(2*0.2)\n# X2 <- rnorm(100,-0.4,1)*sqrt(2*0.4)\n# X3 <- rnorm(100,-0.6,1)*sqrt(2*0.6)\n# X4 <- rnorm(100,-0.8,1)*sqrt(2*0.8)\n# X5 <- rnorm(100,-1,1)*sqrt(2*1)\n# X6 <- rnorm(100,-1.2,1)*sqrt(2*1.2)\nalignment<-dtw(sort(X0),sort(X1),keep=TRUE);\n## Display the warping curve, i.e. the alignment curve\n# jpeg(file=\"three-way.jpg\")\nalignment2<-dtw(sort(X1),sort(X2),keep=TRUE);\nalignment3<-dtw(sort(X2),sort(X3),keep=TRUE);\nalignment4<-dtw(sort(X3),sort(X4),keep=TRUE);\nalignment5<-dtw(sort(X4),sort(X5),keep=TRUE);\nalignment6<-dtw(sort(X5),sort(X6),keep=TRUE);\nplot(alignment,type=\"threeway\")\n\n\n#plot(alignment2)\n# superimpose plot(alignment) and plot(alignment2)\n# jpeg(file=\"comparisons.jpg\")\nplot(alignment,label=\"S0 and S0.2\",style=\"l\")\nlines(alignment2$index1,alignment2$index2,col=\"red\",label=\"S0.2 and S0.4\")\nlines(alignment3$index1,alignment3$index2,col=\"blue\",label=\"S0.4 and S0.6\")\nlines(alignment4$index1,alignment4$index2,col=\"green\",label=\"S0.6 and S0.8\")\nlines(alignment5$index1,alignment5$index2,col=\"yellow\",label=\"S0.8 and S1\")\nlines(alignment6$index1,alignment6$index2,col=\"purple\",label=\"S1 and S1.2\")\nlegend(\"bottomright\",col=c(\"black\",\"red\",\"blue\",\"green\",\"yellow\",\"purple\"),lty=1,legend=c(\"S0 and S0.2\",\"S0.2 and S0.4\",\"S0.4 and S0.6\",\"S0.6 and S0.8\",\"S0.8 and S1\",\"S1 and S1.2\"))",
      "line_count": 30
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "# Use the alignment score (estimated psi) to predict the new quantile function\nsorted_X6 <- sort(X6)\nlist_out <- c()\nfor(i in 1:length(alignment6$index1)){\n  list_out <- c(list_out,sorted_X6[alignment6$index1[i]])\n\n}\nhist(list_out)",
      "line_count": 8
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "fit <- density(list_out)\nplot_ly(x = list_out) %>% \n  # probability density\n  add_histogram(histnorm =\"probability\" ,name=\"Empirical sampled histogram\") %>% \n  add_lines(x = fit$x, y = fit$y, fill = \"tozeroy\", yaxis = \"y2\", name = \"Fitted density\") %>% \n  add_lines(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1.4,0,0), type = 'scatter', mode = 'lines', name = 't=1.4') %>%\n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))",
      "line_count": 7
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "d <- n\n# number of points in frequency\nD <- 100\n# sample for x and y\nx <- matrix(runif(D,-5,5))\ny <- matrix(runif(D,-5,5))\nbias_x0 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x0 <- sqrt(2)*cos(X0%*%t(x)+bias_x0)\nz_y0 <- sqrt(2)*cos(X0%*%t(y)+bias_x0)\nbias_x1 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x1 <- sqrt(2)*cos(X1%*%t(x)+bias_x1)\nz_y1 <- sqrt(2)*cos(X1%*%t(y)+bias_x1)\nbias_x2 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x2 <- sqrt(2)*cos(X2%*%t(x)+bias_x2)\nz_y2 <- sqrt(2)*cos(X2%*%t(y)+bias_x2)\nbias_x3 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x3 <- sqrt(2)*cos(X3%*%t(x)+bias_x3)\nz_y3 <- sqrt(2)*cos(X3%*%t(y)+bias_x3)\nbias_x4 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x4 <- sqrt(2)*cos(X4%*%t(x)+bias_x4)\nz_y4 <- sqrt(2)*cos(X4%*%t(y)+bias_x4)\nbias_x5 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x5 <- sqrt(2)*cos(X5%*%t(x)+bias_x5)\nz_y5 <- sqrt(2)*cos(X5%*%t(y)+bias_x5)\nbias_x6 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x6 <- sqrt(2)*cos(X6%*%t(x)+bias_x6)\nz_y6 <- sqrt(2)*cos(X6%*%t(y)+bias_x6)\n\nk_x0 <- z_x0*z_y0\n# apply mean rowwise\nk_x0 <- apply(k_x0,2,mean)\nk_x1 <- z_x1*z_y1\nk_x1 <- apply(k_x1,2,mean)\nk_x2 <- z_x2*z_y2\nk_x2 <- apply(k_x2,2,mean)\nk_x3 <- z_x3*z_y3\nk_x3 <- apply(k_x3,2,mean)\nk_x4 <- z_x4*z_y4\nk_x4 <- apply(k_x4,2,mean)\nk_x5 <- z_x5*z_y5\nk_x5 <- apply(k_x5,2,mean)\nk_x6 <- z_x6*z_y6\nk_x6 <- apply(k_x6,2,mean)\nk_x7 <- rep(0,D)\n\nt <- y[,1]-x[,1]\n#reorder k_x0 according to sorting order in t\nk_x0 <- k_x0[order(t)]\n\n# plot t versus k_x0,k_x1,k_x2,k_x3,k_x4,k_x5,k_x6 in plotly\nplot_ly(x = sort(t), y = k_x0[order(t)], type = 'scatter', mode = 'lines', name = 'k_x0') %>%\n  add_trace(y = k_x1[order(t)], type = 'scatter', mode = 'lines', name = 'k_x1') %>%\n  add_trace(y = k_x2[order(t)], type = 'scatter', mode = 'lines', name = 'k_x2') %>%\n  add_trace(y = k_x3[order(t)], type = 'scatter', mode = 'lines', name = 'k_x3') %>%\n  add_trace(y = k_x4[order(t)], type = 'scatter', mode = 'lines', name = 'k_x4') %>%\n  add_trace(y = k_x5[order(t)], type = 'scatter', mode = 'lines', name = 'k_x5') %>%\n  add_trace(y = k_x6[order(t)], type = 'scatter', mode = 'lines', name = 'k_x6') %>%\n  # add_trace(y = t, type = 'scatter', mode = 'markers', name = 't') %>%\n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))\n\n# fit a arima model\n# for (i in 1:D){\n#   # fit a arima model given X0(i),X1(i),X2(i),X3(i),X4(i),X5(i),X6(i) fit to X7(i)\n#   arma_model <- arima(c(k_x0[i],k_x1[i],k_x2[i],k_x3[i],k_x4[i],k_x5[i],k_x6[i]),order=c(0,1,1))\n#   k_x7[i] <- predict(arma_model,n.ahead=1)$pred[1]\n# }\n# return (list(k_x7, y-x))\n# }",
      "line_count": 68
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "# use a smoothing spline to smooth K_x1\nspar_coe <- 0.9\nlowpass.spline1 <- smooth.spline(sort(t),k_x1[order(t)],spar=spar_coe)\nlowpass.spline2 <- smooth.spline(sort(t),k_x2[order(t)],spar=spar_coe)\nlowpass.spline3 <- smooth.spline(sort(t),k_x3[order(t)],spar=spar_coe)\nlowpass.spline4 <- smooth.spline(sort(t),k_x4[order(t)],spar=spar_coe)\nlowpass.spline5 <- smooth.spline(sort(t),k_x5[order(t)],spar=spar_coe)\nlowpass.spline6 <- smooth.spline(sort(t),k_x6[order(t)],spar=spar_coe)\n# # plot t versus lowpass.spline1, ..., lowpass.spline6 in plotly\nplot_ly(x = sort(t), y = lowpass.spline1$y, type = 'scatter', mode = 'lines', name = 'k_x1') %>%\n  add_trace(x = sort(t),y = lowpass.spline2$y, type = 'scatter', mode = 'lines', name = 'k_x2') %>%\n  add_trace(x = sort(t),y = lowpass.spline3$y, type = 'scatter', mode = 'lines', name = 'k_x3') %>%\n  add_trace(x = sort(t),y = lowpass.spline4$y, type = 'scatter', mode = 'lines', name = 'k_x4') %>%\n  add_trace(x = sort(t),y = lowpass.spline5$y, type = 'scatter', mode = 'lines', name = 'k_x5') %>%\n  add_trace(x = sort(t),y = lowpass.spline6$y, type = 'scatter', mode = 'lines', name = 'k_x6') %>%\n  layout(yaxis = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))\n  # add_trace(y = t, type = 'scatter', mode = 'markers', name = 't') %>%\n  ",
      "line_count": 18
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "t_h <- sort(t)\nkx66 <- lowpass.spline6$y\nkx6_alt <- k_x6[order(t)]\nempirical_x7 <- function(t){\n  t_h_diff <- diff(t_h)\nt_h_diff <- c(t_h_diff,median(t_h_diff))\n\n  return (abs(sum(exp(1i*t_h*t)*kx6_alt*t_h_diff)))\n}\n",
      "line_count": 10
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "# first order difference of t_h\nempiri_out <- c()\nfor (i in seq(-10,10,0.05)){\n  empiri_out <- c(empiri_out,empirical_x7(i))\n}\ntot_den <- sum(empiri_out)*0.05\nempiri_out <- empiri_out/tot_den",
      "line_count": 7
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "lowpass.spline1 <- smooth.spline(seq(-10,10,0.05),empiri_out,spar=1)\nplot_ly(x = x) %>% \n  add_lines(x = seq(-10,10,0.05), y = empiri_out, fill = \"tozeroy\", yaxis = \"y2\", name = \"Fitted density\") %>% \n  add_lines(x = seq(-10,10,0.05), y = lowpass.spline1$y, fill = \"tozeroy\", yaxis = \"y2\", name = \"Smoothed Fitted density\") %>% \n  add_lines(x = seq(-10,10,0.1), y = diffusion_sol(seq(-10,10,0.1),1.4,0,0), yaxis = \"y2\", type = 'scatter', mode = 'lines', name = 't=1.4') %>%\n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))",
      "line_count": 6
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "library(forecast)\nd <- n\nD <- 100\n# sample x and constrain to low freuency modes\nx <- matrix(runif(D,-2,2))\n# y <- matrix(runif(D,-5,5))\nbias_x0 <- t(matrix(rep(runif(D,0,2*pi),n),nrow=D,ncol=n))\nz_x0 <- exp(1i*X0%*%t(x))\nz_y1 <- exp(1i*X1%*%t(x))\nz_y2 <- exp(1i*X2%*%t(x))\nz_y3 <- exp(1i*X3%*%t(x))\nz_y4 <- exp(1i*X4%*%t(x))\nz_y5 <- exp(1i*X5%*%t(x))\nz_y6 <- exp(1i*X6%*%t(x))\nmean0 <- apply(z_x0,2,mean)\nmean1 <- apply(z_y1,2,mean)\nmean2 <- apply(z_y2,2,mean)\nmean3 <- apply(z_y3,2,mean)\nmean4 <- apply(z_y4,2,mean)\nmean5 <- apply(z_y5,2,mean)\nmean6 <- apply(z_y6,2,mean)\n# perform arima regression on Re(mean0), Re(mean1), Re(mean2), Re(mean3), Re(mean4), Re(mean5), Re(mean6)\noutput_val <- c()\nfor (j in 1:D){\n  # fit a arima model given X0(i),X1(i),X2(i),X3(i),X4(i),X5(i),X6(i) fit to X7(i)\n  arima_model <- auto.arima(c(Re(mean0[j]),Re(mean1[j]),Re(mean2[j]),Re(mean3[j]),Re(mean4[j]),Re(mean5[j]),Re(mean6[j])), ic = \"aic\")\n  #arma_model <- arima(c(Re(mean0[i]),mean1[i],mean2[i],mean3[i],mean4[i],mean5[i],mean6[i]),order=c(0,1,1))\n  \n  real_val<-forecast( arima_model)$mean[1]\n  # fit the imaginary part\n  arima_model <- auto.arima(c(Im(mean0[j]),Im(mean1[j]),Im(mean2[j]),Im(mean3[j]),Im(mean4[j]),Im(mean5[j]),Im(mean6[j])), ic = \"aic\")\n  imag_val<- forecast( arima_model)$mean[1]\n  output_val <- c(output_val,real_val+1i*imag_val)\n  \n}\n",
      "line_count": 36
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "t_h <- sort(x[,1])\nempirical_kime_x7 <- function(t){\n  t_h_diff <- diff(t_h)\nt_h_diff <- c(t_h_diff,median(t_h_diff))\n  return (abs(sum(exp(-1i*t_h*t)*output_val[order(x[,1])]*t_h_diff)))\n}\n",
      "line_count": 7
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "# first order difference of t_h\nempiri_out <- c()\nfor (i in seq(-10,5,0.05)){\n  empiri_out <- c(empiri_out,empirical_kime_x7(i))\n}\ntot_den <- sum(empiri_out)*0.05\nempiri_out <- empiri_out/tot_den",
      "line_count": 7
    },
    {
      "section": "Case Study: Predicting a specialized Fokker Planck equation",
      "code": "lowpass.spline1 <- smooth.spline(seq(-10,5,0.05),empiri_out,spar=0.8)\nplot_ly(x = x) %>% \n  add_lines(x = seq(-10,5,0.05), y = empiri_out, fill = \"tozeroy\", yaxis = \"y2\", name = \"Fitted density\") %>% \n  add_lines(x = seq(-10,5,0.05), y = lowpass.spline1$y, fill = \"tozeroy\", yaxis = \"y2\", name = \"Smoothed Fitted density\") %>% \n  add_lines(x = seq(-10,5,0.1), y = diffusion_sol(seq(-10,5,0.1),1.4,0,0), yaxis = \"y2\", type = 'scatter', mode = 'lines', name = 't=1.4') %>%\n  layout(yaxis2 = list(overlaying = \"y\", side = \"right\",name=\"Ground truth density\"))",
      "line_count": 6
    },
    {
      "section": "Appendix",
      "code": "###### Data ######## \nset.seed(3) \nn <- 100\nx <- matrix(runif(n, min = -2, max = 2), ncol = 1)\nx.star <- matrix(sort(x), ncol = 1)             # sorted x, used by plot\ny <- cos(pi * x.star) + rnorm(n, sd = 3/2)\n\n#### Reproducing Kernel for <f,g>=int_0^1 { fâ€™â€™(x) gâ€™â€™(x) dx } #####\nrk.1 <- function(s, t) {\n  return(0.5 * min(s, t) ^ 2) * (max(s, t) + (1 / 5) * (min(s, t)) ^ 3)\n}\n\nget.GramMatrix.1 <- function(X) {\n  n <- dim(X)[1]\n  GramMatrix <- matrix(0, n, n) #initializes GramMatrix array\n  #i=index for rows\n  #j=index for columns\n  GramMatrix <- as.matrix(GramMatrix) # GramMatrix matrix\n  for (i in 1:n) {\n    for (j in 1:n) {\n      GramMatrix[i, j] <- rk.1(X[i, ], X[j, ])\n    }\n  }\n  return(GramMatrix)\n}\n\nsmoothing.spline <- function(X, y, lambda) {\n  GramMatrix <- get.GramMatrix.1(X)     # Gram matrix (nxn)\n  n <- dim(X)[1]              # n=length of y\n  J <- matrix(1, n, 1)        # vector of ones dim\n  T <- cbind(J, X)            # matrix with a basis for the null space of the penalty\n  Q <- cbind(T, GramMatrix)        # design matrix\n  m <- dim(T)[2]              # dimension of the null space of the penalty\n  S <- matrix(0, n+m, n+m)    #initialize S\n  S[(m + 1):(n + m), (m + 1):(n + m)] <- GramMatrix # non-zero part of S\n  M <- (t(Q) %*% Q + lambda * S)\n  M.inv <- inverse(M)          # inverse of M\n  gamma.hat <- crossprod(M.inv, crossprod(Q, y))\n  f.hat <- Q %*% gamma.hat\n  A <- Q %*% M.inv %*% t(Q)\n  tr.A <- sum(diag(A))        # trace of hat matrix\n  rss <- t(y - f.hat) %*% (y - f.hat) # residual sum of squares\n  gcv <- n * rss / (n - tr.A) ^ 2 # obtain GCV score\n  return(list(f.hat = f.hat,  gamma.hat = gamma.hat, gcv = gcv))\n}\n\n### Now we have to find an optimal lambda using GCV...\n### Plot of GCV\nlambda <- 1e-8 \nn <- 100\nV <- rep(0, n) \nfor (i in 1:n) {\n  V[i] <- smoothing.spline(x.star, y, lambda)$gcv # obtain GCV score\n  lambda <- lambda * 1.5 # increase lambda\n} \n\nplot_ly(x=1:n, y=V, type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"GCV vs. Log10(Lambda)\", xaxis = list(type = \"log\"))\n\ni <- (1:n)[V == min(V)] # extract index of min(V)\nfit_rk <- smoothing.spline(x.star, y, 1.5 ^ (i - 1) * 1e-8) \n\n# Graph (Trigonometric Signal)\nplot_ly(x=x.star, y=fit_rk$f.hat[,1], type=\"scatter\", mode=\"lines\",\n            name=\"Ridge RKHS Model\") %>% \n    add_trace(x=x.star, y=y[,1], name=\"Data\", mode=\"markers\", \n              hovertemplate = paste('(%{x},%{y})')) %>%\n    add_trace(x=x.star, y=cos(pi * x.star)[,1], name=\"Exact Source\") %>%\n    layout(title=\"Exact Source, Noisy data, and RKHS Model\", \n           xaxis = list(title=\"X\", showline=TRUE), yaxis = list(title=\"Y\"),\n           legend = list(title=\"Objects\", orientation = 'h'), hovermode  = 'x')",
      "line_count": 71
    },
    {
      "section": "Appendix",
      "code": "library(GPBayes)\nbesselk_nu <- function(nu) {\n  return(function(x) {\n    return(GPBayes::BesselK(nu, x))\n  })\n}\nmaternkernel <- function(sigma,nu,rho,x){\n  scaled_x = sqrt(2*nu)*x/rho\n  return (sapply(scaled_x,besselk_nu(nu))*scaled_x**nu*sigma**2*2**(1-nu)/gamma(nu))\n}",
      "line_count": 10
    },
    {
      "section": "Appendix",
      "code": "library(plotly)\np = plot_ly()\nx_values = seq(from = -5, to = 5, by = 0.01)\n p <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,1/2,1,abs(x_values)), name = ~paste(\"Plot for nu =\", 1/2), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,1,1,abs(x_values)), name = ~paste(\"Plot for nu =\", 1), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,3/2,1,abs(x_values)), name = ~paste(\"Plot for nu =\", 3/2), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,2,1,abs(x_values)), name = ~paste(\"Plot for nu =\", 2), mode=\"lines\")\np <- p %>%add_trace( x = ~x_values, y = ~maternkernel(1,10,1,abs(x_values)), name = ~paste(\"Plot for nu =\", 10), mode=\"lines\")\n\np = layout(p, title = \"Matern kernels with varying nu\",\n          xaxis = list(title = \"X values\"), yaxis = list(title = \"Y values\"))\np",
      "line_count": 12
    }
  ]
}