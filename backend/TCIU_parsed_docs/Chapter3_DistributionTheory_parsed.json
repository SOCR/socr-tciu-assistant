{
  "metadata": {
    "created_at": "2025-05-15T17:01:01.133819",
    "total_sections": 6,
    "total_code_chunks": 5,
    "total_tables": 1,
    "r_libraries": [
      "animation",
      "circular",
      "plotly",
      "reshape2",
      "webshot"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"Spacekime Analytics -- Distribution Theory\"\nsubtitle: \"Mathematical Formalism of Distribution Random Sampling\"\nauthor: \"SOCR Team (Yueyang Shen, Jun Chen, Ivo Dinov)\"\ndate: \"`r format(Sys.time(),'%m/%d/%Y')`\"\noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: TCIU_header.html\n    toc: yes\n    number_sections: yes\n    toc_depth: 3\n    toc_float:\n      collapsed: no\n      smooth_scroll: yes\n    code_folding: hide\n  word_document:\n    toc: yes\n\nIn this [TCIU section](https://http://tciu.predictive.space/), we discuss the *duality* of *functions* and *functionals*, i.e., *generalized functions* or *distributions.* Specifically, we will define *distributions* as *functionals* that are function-like objects that **act** as linear operators on smooth functions. *Distributions* are necessary as *ordinary functions* are insufficient to describe many\nphysical phenomena and don't always permit non-smooth solutions of some differential equation models.",
      "word_count": 111
    },
    {
      "title": "Overview",
      "content": "*Generalized functions* (distributions) play an important role in solving differential equations, harmonic analysis representation of functions, or signals, as superpositions of base functions or basic waves, and for defining complex operations on difficult functions, e.g., defining *functional derivatives* of functions that are not differentiable. *Distributions* are related to, but more general than, the more commonly known *probability density functions* (PDFs), and their counterparts *cumulative distribution functions* (CDFs).\n\nWe will consider the *Dirac delta* generalized function, $\\delta(x)$, also known as the point mass function. \nIt is not well-defined as a function as it's infinite at the origin and zero everywhere else, whereas its integral $\\int_{\\mathbb{R}}{\\delta(x)\\ dx}=1$. The Dirac delta is **not** a real function, but a generalized function, or functional acting as a linear operator on a special class of functions called *test functions.* In a measure-theoretic sense, functions can take on infinite values at single singularity points (of trivial Lebesgue or Borel measure) without impacting the integral of the function. The *Dirac delta* generalized function can not be defined as a function that is infinite at $0$ and integrates to $1$ since this leads to contradictory interpretations of (1) linear transformations of $\\delta(\\cdot)$, e.g., $5\\times \\delta=5\\times 0=0$, (2) its (function) derivatives, and (3) its Fourier transform. We will see the defining *Dirac delta* as a\ngeneralized function (distribution), permits consistent interpretations of its linear superpositions, \nits generalized functional derivatives (distribution derivatives), and its Fourier transform.\n\nThe power of generalized functions come from the fact that distributions as functionals enable integration, differentiation, and Fourier transformations of a larger class of functions and avoid \nconflicts like $5\\delta=\\delta$. More specifically, generalized functions are *not functions of real numbers* in the conventional sense, rather they are *linear functionals* that transform other functions (test functions) to the real numbers. The functions acted upon by the distributions are called \"*test functions*\". Hence, \ndistributions $f$ act on test functions $\\phi$ as linear functions by mapping the test functions to the \nreal numbers:\n\n$$\\underbrace{f}_{distribution}: \\underbrace{\\phi}_{test\\ function} \\longrightarrow \n\\underbrace{\\int_{\\mathbb{R}} {f(x)\\ \\phi (x)\\ dx}}_{\\in\\ \\mathbb{R}}\\ .$$\n\nSo the distribution $f$ is loosely speaking a linear function, but more accurately, a *linear functional*, a member of the *dual space* of the test functions\n\n$$\\underbrace{f}_{distribution}: \\underbrace{\\alpha\\phi+ \\beta\\psi}_{superposition\\ of\\\\ test\\ functions} \\longrightarrow \n\\underbrace{\\alpha \\int_{\\mathbb{R}} {f(x)\\ \\phi (x)\\ dx}+\n\\beta \\int_{\\mathbb{R}} {f(x)\\ \\psi (x)\\ dx}}_{\\in\\ \\mathbb{R}}\\ ,\n\\ \\forall\\ \\alpha,\\beta\\in \\mathbb{R}\\ .$$\n\nWe will illustrate that the delta distribution $\\delta$ is concentrated at zero and \nit operates on a test function $\\phi$ by returning evaluating the test function at the origin \n\n$$\\delta: \\phi \\longrightarrow \n\\int_{\\mathbb{R}} {\\delta(x)\\ \\phi (x)\\ dx}=\\phi(0)\\ ,$$\n\nsince $\\int_{\\mathbb{R}}{\\delta(x)\\ dx}=1$ and \n$\\delta(x)\\equiv 0,\\ \\forall\\ x\\in \\mathbb{R}\\setminus\\{0\\}$.\n\nIn general, generalized functions can be represented as limits of traditional functions.\n\n*Generalized differentiation* (*distributional derivatives*) are defined to\nagree with derivatives of conventional functions. Suppose $f(x)$ is a \ndifferentiable univariate function and $\\phi(x)\\in \\mathcal{C}_c^{\\infty}(\\mathbb{R})$ is an infinitely differentiable test function that is zero outside of some finite interval. Using\nclassical *integration by parts*\n\n$$\\underbrace{f'}_{distributional\\\\ derivative\\ is\\\\ a\\ distribution} : \\phi \\longrightarrow \n\\int_{-\\infty}^\\infty f'(x)\\, \\phi(x) \\, dx = \\\\\n\\underbrace{f(x)\\, \\phi(x) \\bigg|_{-\\infty}^{\\infty}}_{0,\\ \\phi\\in \\mathcal{C}_c^{\\infty}(\\mathbb{R})}\n-\\int_{-\\infty}^\\infty f(x)\\, \\phi'(x) \\ dx= -\\int_{-\\infty}^\\infty f(x)\\, \\phi'(x) \\ dx\\ .$$\n\nEven when the generalized function $f(x)$ is not differentiable in a classical sense,\nthe *distributional derivative* definition above is well-defined in terms of the \nderivative of the test function $\\phi$, which the distribution $f$ acts on.\n\nTechnically, the functional $f$ not as a function of real numbers, but as a \ndistribution that operates on test functions producing real numbers. In\nother words, $f$ is a linear functional on the space of tests functions by\nmapping $\\phi$ to $\\int {f(x) \\phi(x) dx}\\in \\mathbb{R}$. \n\nSimilarly, higher order distributional derivative of integrable generalized functions\n$f$, $f^{(n)}$ are transformed to the reciprocal higher order derivatives of the infinitely\ndifferentiable test functions $\\phi$, i.e., $\\phi^{(n)}$\n\n$$f^{(n)}:  \\phi \\longrightarrow -\\int_{-\\infty}^\\infty f(x) \\, \\phi^{(n)}(x)\\, dx\\ .$$\n\nEven if $f$ is not smooth or differentiable, it suffices to be integrable to define its \n*distributional (generalized* or *weak) derivatives* of any order.\n\nFor instance, the delta functional is not even a function, however, its\ndistributional derivative is well-defined.\n\nThis property provides a strong motivation for introducing generalized functions,\nas they are differentiable and expand the space of potential functions\nrepresenting solutions to difficult differential equations, which may not permit\nclassical function solutions. Distributions also may enhance the search for\nclassical equation solutions, just like linting dimensionality in many data science \nproblems provides meaningful ways to solve difficult optimization problems \nand fit AI models, see \n[TCIU Chapter 3 Appendix on RKHS](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_ReproducingKernelHilbertSpaces.html). Proving the existence of a generalized function equation solution often\nprovides clues to deriving a classical function solution from the distribution solution.\n\nLet's consider a heuristic example of generalized solutions yielding classical solutions. \nConsider an *integral programming problem* of minimization problem for an objective (cost) function\nwith integer arguments,\nsee [DSPA Chapter 13 (Optimization)](https://socr.umich.edu/DSPA2/DSPA2_notes/13_FunctionOptimization.html). \nThe more general optimization of the same objective function over the reals\nmay be easier to find first using classical gradient descent minimization. \nHaving the real-minimum, we can check the nearby integers to \nidentify the desired integer solutions of the original integral programming problem.\nGeneralized distributional derivatives can be used analogously to approximate\nthe derivatives of difficult (non-smooth) functions. \nLooking for solutions over a larger state space is a common mathematical trick\nto estimate approximate solutions over the original (constrained) state space.\n\nGeneralized functions are related to *probability distributions*. *Probability density functions*\nare non-negative real-valued functions, $f:\\mathbb{R}\\to\\mathbb{R}^+$\nthat integrate to unity, $\\int f(x)dx=1$. \nAll observed phenomena and *events* (sets of observable outcomes) can be \nrepresented by some (marginal, joint, or conditional) probability distributions \nby integrating the probability density function over the event/set. \n\nThe probability density also facilitates the calculation of the expected value\nof a test function by integrating the function against the probability density. \nSo, distributions are generalized functions that can be integrated or \ndifferentiated similarly to integrating or differentiating test functions acted upon\nthe distributions.\n\nOf course, the values of probability density functions are just real numbers, not probabilities, \nbut the *densities are integrants that yield the desired probabilities.* \nSimilarly, by themselves, generalized functions are not functions but linear\noperators that act on special kinds of (test) functions to yield real numbers.\n\nThe class of test functions operated on by distributions can vary. The most \nuseful class of test functions is the space of infinitely differentiable \nfunctions with compact support, $\\mathcal{C}_c^{\\infty}(\\mathbb{R})$. \nIn Fourier analysis, the set of *Schwartz functions* \n\n$$\\mathcal{S}(\\mathbb{R^n})=\\{f\\in C^{\\infty}(\\mathbb{R^n}):\n||f||_{\\alpha,\\beta}<\\infty \\,, \\forall \\alpha, \\beta\\} \\tag{1}$$\n\nrepresent another useful class of natural test functions consisting of \nall *infinitely differentiable functions of rapid decay*, i.e., functions $\\phi$\nsuch that $x^n \\phi(x)\\underset{x\\to\\pm\\infty}{\\longrightarrow} 0,\\ \\forall\\ n\\in\\mathbb{N}$. \nThis rapid decay conditions guarantees that the Fourier transforms of\nSchwartz functions are also Schwartz functions. \n\nNote that compactly supported (time-limited) functions are not well-suited for\nFourier analysis since their compact support guarantees that the support of their \nFourier transforms will be infinite! This is due to the Heisenberg uncertainty \nprinciple indicating that the more concentrated a function is in spacetime, \nthe less concentrated the support of its Fourier transform is in the (frequency domain)\nFourier space. No function can be both time-limited *and* band-limited. \n\nTo draw direct connection to spacekime analytics, and specifically kime-representations\nof repeated measurement longitudinal data, we will review the more fundamental \nproperties of generalized functions.",
      "word_count": 1211
    },
    {
      "title": "The Dirac Delta Distribution",
      "content": "The Dirac delta distribution $\\delta(x)$ represents the most basic *generalized function* (or *distribution*). It can be defined as a limit of *test functions*\n$$\\delta_n(x) = \\frac{1}{\\sqrt{\\pi\\frac{1}{n^2}}} e^{-\\frac{x^2}{(1/n)^2}} \\underset{n\\to\\infty}{\\longrightarrow} \\delta(x)\\ .$$\n\nClearly, these test functions are classical Gaussian distributions with progressively decaying variance $\\sigma_n^2=\\frac{1}{n^2}$.\n\n\nUsing the Taylor series expansion of $f$, the convolution of $\\delta_n(x)$ with any bounded and continuous function, $\\forall\\ f(x)\\in C^{\\infty}(\\Omega), \\ \\Omega\\subseteq \\mathbb{R}^d$,\n\n$$\\underbrace{(f*\\delta)}_{convolution}(x_o)\n\\overset{n\\to\\infty}{\\longleftarrow} \n(f*\\delta_n)(x_o)=\\int_{-\\infty}^{\\infty} {f(x)\\delta_n(x-x_o)\\ dx}\\ \n\\overset{n\\to\\infty}{\\approx} f(x_o)\\underbrace{\\int_{-\\infty}^{\\infty} {\\delta_n(x-x_o)\\ dx}}_\n{1}=f(x_o)\\ .$$\n\nHence we can define the generalized function (distribution) $\\delta$ as\na linear functional over $C^{\\infty}(\\Omega)$, i.e., $\\delta$ is the *dual* of all bounded and continuous functions, $\\delta:C^{\\infty}(\\Omega)\\longrightarrow C^{\\infty}(\\Omega)$, acting by\n\n$$\\delta(f)(x_o)\\equiv  (f*\\delta)(x_o)\\equiv f(x_o),\\ \\forall f\\in C^{\\infty}(\\Omega),\\ \\forall x_o\\in\\Omega \\ .$$\n\nThe generalized Dirac delta function *picks out* the value of the *function* ($f$) it maps at *the specified *point* in the domain $x_o$.\n\nA *linear functional* $\\mathcal{L}$ is a *linear mapping* from a vector\nspace to a field, e.g., $\\mathcal{L}:V\\longrightarrow\\mathbb{C}$ where\n$$\\mathcal{L}(\\alpha_1 v_1 + \\alpha_2 v_2)=\n\\alpha_1 \\mathcal{L}(v_1) + \\alpha_2 \\mathcal{L}(v_2),\\ \\forall v_1,v_2\\in V,\\ \\forall \\alpha_1,\\alpha_2\\in\\mathbb{C}.$$\n\nRecall that the differential operator is a linear functional from the vectors space of differentiable functions to the reals (or complex plane),\n$\\forall d\\in C^1(\\mathbb{R}),\\ d(x)\\longrightarrow d'(0)\\in\\mathbb{R}$.\n\nSimilarly, the Dirac delta linear functional \n$\\delta:C^{\\infty}(\\Omega)\\longrightarrow \\mathbb{C}$ is a linear functional acting by $f(x) \\longrightarrow (f*\\delta)(x_o)=f(x_o)$. \nMore generally, all bounded and integrable functions $p(x)$ are also linear functions, $p:C^{\\infty}(\\Omega)\\longrightarrow \\mathbb{C}$ \nacting by mapping \n$C^{\\infty}(\\Omega)\\ni f(x) \\longrightarrow \\int_{-\\infty}^{\\infty} {f(x)p(x)\\ dx}\\in\\mathbb{R}$. \nNote that most of the time the *reals* represent the *base field*, $\\mathbb{R}\\subseteq \\mathbb{C}$.\n\nWe should recognize the **duality** of $p(x)$ as being a *function* in \n$C^{\\infty}(\\Omega)$ and a *functional* transforming functions to scalars in the base field.\n\nLet's define the set of *test functions* as the set of infinitely differentiable functions with bounded support, $\\mathcal{D}=C_0^{\\infty}(\\Omega)$. Then, a *distribution* $d$ is a \ncontinuous linear functional $d(\\phi): \\mathcal{D}\\longrightarrow \\mathbb{R}$, such that\n\n$$d(\\alpha_1 \\phi_1 + \\alpha_2 \\phi_2)=\n\\alpha_1 d(\\phi_1) + \\alpha_2 d(\\phi_2),\\ \\forall \\phi_1,\\phi_2\\in \\mathcal{D},\\ \\forall \\alpha_1,\\alpha_2\\in\\mathbb{C}\\ .$$\n\n*Relations between distribution and functions*:\n\n - All continuous *functions* $d(x)$ are also *distributions* acting by $\\mathcal{D}\\ni \\phi(x) \\overset{g}{\\longrightarrow} \\int_{-\\infty}^{\\infty} {d(x)\\phi(x)\\ dx}\\in\\mathbb{R}$.\n - All *distributions* can be *approximated by functions*, $\\exists \\{d_n(x)\\}_n\\in \\mathcal{D}$ such that $\\forall \\phi\\in\\mathcal{D}$, $d(\\phi) = \\underset{n\\to\\infty}{\\lim} {\\int_{\\mathbb{R}}d_n(x)\\phi(x)\\ dx}$.\n\n## First-principles examples of distributions\n\n * Linear mixtures of delta functionals are distributions. For instance, $d(x)=-2\\delta(x+3) + \\delta(x)$, $\\forall \\phi\\in\\mathcal{D}$, $d(\\phi)=\\int_{-\\infty}^{\\infty} {d(x)\\phi(x)\\ dx}=-2\\phi(-3)+\\phi(0)\\in\\mathbb{R}$.\n \n * *Heaviside* (step function) $H(x)=\\begin{cases} 0 & x\\leq 0  \\\\ 1 & x\\gt 0 \\end{cases}$ is a generalized function because $\\forall \\phi\\in\\mathcal{D}$, \n\n$$d(\\phi)\\equiv H(\\phi)\\equiv \\int_{-\\infty}^{\\infty} {H(x)\\times(-x)\\phi'(x)\\ dx}\\\\ = \\int_{0}^{\\infty} {(-x)\\phi'(x)\\ dx}\n\\underset{by\\ parts}{=} \\underbrace{-x\\phi(x)\\Biggr|_{0}^{\\infty}}_{0} -\n(-1)\\int_{0}^{\\infty} {\\phi(x)\\ dx}\\\\ \\equiv \\int_{-\\infty}^{\\infty} {H(x)\\phi(x)\\ dx}\\ .$$\n\n## Distributional derivatives\n\n[For mode details see the TCIU section on Radon-Nikodym derivatives](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_Radon_NikodymDerivatives.html). \n\nLet's demonstrate a couple of examples of differentiating non-differentiable functions interpreted as distributions. The *distributional derivative* of $d(\\phi)$ is defined to be the (derivative) *distribution* $d'(\\phi)\\equiv-d(\\phi')$.\nAssume there is a smooth function $d(x)$ such that $d(\\phi)=\\underset{\\mathbb{R}}{\\int}{d(x)\\phi(x)\\ dx}$. Then, integration by parts yields\n\n$$d'(\\phi)\\underset{def}{=} \\underset{\\mathbb{R}}{\\int}{d'(x)\\phi(x)\\ dx}\n\\underset{d'(\\phi)\\equiv-d(\\phi')}{=} -\\underset{\\mathbb{R}}{\\int}{d(x)\\phi'(x)\\ dx}=\n-d(\\phi'),\\ \\forall \\phi\\in\\mathcal{D}\\ .$$\n\n - Distributional derivative of the Heavyside (step) function, $H(x)$: $H'(\\phi)\\underset{def}{=}  -\\underset{\\mathbb{R}}{\\int}{H(x)\\phi'(x)\\ dx}= \\int_{0}^{\\infty}{\\phi'(x)\\ dx}\\\\ =\\phi(0)=\\int_{-\\infty}^{\\infty}{\\delta(x-0)\\phi(x)\\ dx}=\\int_{-\\infty}^{\\infty}{H'(x)\\phi(x)\\ dx},\\ \\forall \\phi\\in\\mathcal{D}\\ .$ Therefore, the distributional derivative of the Heavyside function is the Dirac generalized function $H'(x)=\\delta(x)$.\n\n - Distributional derivative of the Dirac delta functional, $\\delta'(\\phi)=-\\delta(\\phi')=-\\phi'(0)$.\n\n### Higher-order distributional derivatives\n\nThe higher-order distributional derivatives are defined inductively using repeated integration by parts.\n\n$$d^{(n)}(\\phi)\\underset{def}{=} \\underset{\\mathbb{R}}{\\int}{d^{(n)}(x)\\phi(x)\\ dx}\n= (-1)^n d(\\phi^{(n)}),\\ \\forall \\phi\\in\\mathcal{D}\\ .$$\n\n*Example 1*: Second distributional derivative of the Heaviside functional:\n\n$$H''(\\phi) = \\left (H'(\\phi)\\right )' = \\left (\\delta(\\phi)\\right )'=\n\\delta'(\\phi)=-\\phi'(0),\\ \\forall \\phi\\in\\mathcal{D}\\ .$$\n\n*Example 2*: Next, let's look at the second-order derivative of $f(x) = |x|$, which is clearly not differentiable as a function at the origin. Observe that $\\forall \\phi\\in\\mathcal{D}$, splitting the domain into two regions and integrating by parts gives\n\n$$f''(\\phi) = (-1)^2 f(\\phi'')= \n\\int_{\\mathbb{R}}{f(x)\\phi''(x)\\ dx}=\n\\int_{-\\infty}^0{(-x)\\phi''(x)\\ dx}+\\int_0^{\\infty}{x\\phi''(x)\\ dx}=\\\\\n\\underbrace{-x\\phi'(x)\\Biggr|_{-\\infty}^0}_{0} + \n\\underbrace{x\\phi'(x)\\Biggr|_{0}^{\\infty}}_{0}+\n\\underbrace{\\phi(x)\\Biggr|_{-\\infty}^0}_{\\phi(0)} - \n\\underbrace{\\phi(x)\\Biggr|_{0}^{\\infty}}_{\\phi(0)}=2\\phi(0)\\ .$$\n\nTherefore, the second order distributional derivative $|x|''=2\\delta(x)$.",
      "word_count": 650
    },
    {
      "title": "Formal Definitions of Distributions",
      "content": "Often in practice it is more meaningful to discuss *physical quantities* in terms of *generalized functions* using \\(\\int f(x)\\varphi(x)dx\\) rather than using specific functional values, \\(f(x)\\). \n\nThe space of *test functions* is denoted by \\(D(\\Omega)\\ni \\{\\psi\\}\\) and represents the space of smooth functions with compact support on a domain \\(\\Omega\\). A *test function* $\\psi: \\Omega \\to \\mathbb{R}$ has following properties:\n\n - It vanishes outside a bounded subset of \\(\\Omega\\) that stays away from the boundary of \\(\\Omega\\), $\\partial \\Omega$.\n - It has continuous derivatives of all orders.\n\nThen, the definition of a *generalized function*, i.e., a *distribution*, is based on the space of test functions. There are three equivalent definitions of distributions.\n\n## Distributions as continuous linear functionals\n\nThe *class of distributions* is all continuous linear functionals on \\(\\Omega\\) mapping from the space of test functions \\(D(\\Omega)\\) to real or complex numbers.\n\n*Notes*:\n\n1. The mapping \\(T: D(\\Omega)\\to \\mathbb{C}\\) should be *linear.*\n$$T_f(a\\varphi_1 + b\\varphi_2 )\\equiv \\langle f, a\\varphi_1 + b\\varphi_2 \\rangle = a\\langle f, \\varphi_1\\rangle + b\\langle f,\\varphi_2\\rangle\\ .$$\nfor any test functions \\(\\{\\varphi_1, \\varphi_2\\} \\in D(\\Omega)\\), the space of test functions.\n\n2. The mapping $T$, note that we can suppress the continuous function $f$ in the subindex $T_f$, should also be *continuous*:\n\nFor any test function \\(\\varphi \\in D(\\Omega)\\) and a corresponding sequence of test functions \\(\\{ \\varphi_{n} \\} \\in D(\\Omega)\\) with \\(\\varphi_n\\stackrel{D}\\longrightarrow\\varphi\\), \\(T(\\varphi_n)\\longrightarrow T(\\varphi)\\) is always true.\n\n3. All continuous functions $f$ are distributions, but not vice-versa. \n\nDenote \\(T_f = \\langle f, \\varphi \\rangle\\), which is a linear and continuous mapping from \\(D\\) to real or complex numbers. Thus, each continuous function \\(f\\) represents a distribution. Whereas the Dirac $\\delta$ distribution is not a continuous function.\n\n## Distributions as a limit of ordinary functions\n\nSince we could approximate an arbitrary distribution using a sequence of test functions, there is a new way to define the distribution. \n\nConsider a series of test functions \\({f_n}\\) approximating a function \\(f\\). Define \\(T_n\\) to be the distribution series, where \\(T_n (\\varphi)= \\langle\\varphi_n,\\varphi\\rangle\\). Then \\(T_f = \\langle f, \\varphi\\rangle = \\lim_{n \\to \\infty}\\langle \\varphi_n,\\varphi \\rangle\\) is a new distribution. \n\nSimilarly, we can also say that if \\({f_n}\\) is a sequence of distributions for which the limit as \\(n\\to\\infty\\) of \\(\\langle f_n, \\varphi \\rangle\\) exists for all test functions \\(\\varphi\\), then \\(\\langle f, \\varphi\\rangle = \\lim_{n \\to \\infty} \\langle f_n, \\varphi \\rangle\\) defines a distribution as limiting (fixed-point), \\(f_n \\longrightarrow f\\).\n\nThis is supported by the theorem that for any distribution \\(f\\), there always exists a test function sequence \\(\\{\\varphi_n\\}\\) such that \\(\\{\\varphi_n \\}\\longrightarrow f\\). \n\n*Notations:* There might exist an analogous relationship between the convergence of test functions to a distribution and the convergence of test samples to a population. We need to go further. \n\n## Convolution quotients \n\n(This section still needs to be refined!)\n\nDefinition of the convolution quotient: \\[(f * d)(x) = \\int f(x-y)d(y)dy\\]\n\nHere we first introduce a concept of the Schwartz class S, which refers to the set of functions of which the all the partial derivatives are *rapidly decreasing*. There are two ways to define the *rapidly decreasing* functions:\n\n(1) There exists constants $M_N$, such that $$|f(x)| \\leq M_N|x|^{-N}  as\\space x \\longrightarrow \\infty $$ for $N = 1, 2, 3...$.\n(2) For any polynomial $p(x)$, $p(x)f(x)$ always goes to zero when $x \\longrightarrow \\infty$. \n\n$D(\\mathbb{R}^n)$ is the subset of $S(\\mathbb{R}^n)$, and $e^{-|x|^2}$ is an example that belongs to $S(\\mathbb{R}^n)$ but doesn't belong to $D(\\mathbb{R}^n)$ since it does not have bounded support. \n\n(3) The concept of *tempered distributions* refers to the linear functionals defined on $S$.\n\nHere we can define an arbitrary tempered distribution $g(x)$:\n$$g(x) = \\psi(x) * f(x) = \\int \\psi(x-y)f(y)dy = \\langle f, \\tau_{-x}\\tilde{\\psi} \\rangle ,$$\nwhere $\\tau_{-x}\\tilde{\\psi} = \\psi(x-y)$, $f,\\psi \\in S$.\n\nThis defines the tempered distributions by convolution quotient $\\psi(x) * f(x)$. \n\n*Example:* $S$ is the subset of the set of tempered distributions $S'$.\nTake $f = \\delta$ and an arbitrary $\\psi \\in S$, and get\n\n$$\\psi = \\langle \\delta, \\tau_{-x}\\tilde{\\psi} \\rangle =  \\psi * \\delta(x) \\in S'\\ .$$\nConsider two test functions \\(\\phi\\) and \\(\\psi\\), where  \\(\\phi\\)  is a convolution kernel, and  \\(\\psi\\) is a convolution operator. Here we define a new distribution f:\n\n$$\\langle f , \\phi * \\psi \\rangle = \\lim_{n \\to \\infty} \\langle f_n, \\phi * \\psi \\rangle .$$\n\n*Notation*:\n\nThe above three definitions of *distributions*, as generalized functions, are equivalent. \n\nFor an arbitrary function \\(f\\) in (2), since the continuous function series \\(\\varphi_n\\) uniformly converges to \\(f\\), \\(f\\) is also a continuous function, satisfying the first definition of distribution. And according to the theorem, for any distribution defined by the first definition, we could always find the test function sequence \\(\\{\\varphi_n\\}\\) such that \\(\\{\\varphi_n\\}\\) uniformly converges to \\(f\\) and \\(f\\) is a distribution defined by the second definition.\n\nThe third definition only defines tempered distributions, but it agrees with the first definition, which means that for any tempered distribution defined by the convolution quotient, it can also be written as the form in the first definition. Take an arbitrary $g(x) = \\psi(x) * f(x)$ for example\n\n$$\n\\langle g, \\varphi \\rangle = \\langle \\psi * f, \\varphi \\rangle = \\langle f, \\tilde{\\psi} * \\varphi \\rangle \\in \\mathbb{R} \\space or\\space  \\mathbb{C},\n$$\n\n\nwhere $f\\in S'$, $\\tilde{\\psi} * {\\phi} \\in S$. This means that $g$ represents the linear functionals mapping from the space of Swarz class to real or complex numbers. \n\n*Question*: In the third definition, we considered a larger set of input functions, i.e. $S$ instead of the set of test functions $D$. Can we say these three definitions are equivalent?",
      "word_count": 921
    },
    {
      "title": "Distribution Properties",
      "content": "## Synergies between distributions and real numbers\n\nThe *relationship between distributions and functions* is analogous to the *relation between real numbers and rational numbers*.\n\nRecall the Euler *natural number* approximation\n\n$$\\lim_{n \\to \\infty}\\left (1 + \\frac{1}{n}\\right )^n = e,$$\nby a sequence of rational numbers \\(\\left (1 + \\frac{1}{n}\\right)^n\\) approximating the real (transcendental) Euler number \\(e\\approx 2.718282\\cdots\\).\n\nSimilarly, infinite sequences of functions converge to distributions in the same sense. Take the Dirac delta function $\\delta$ as an example. \n\nConsider a sequence of test functions \\({f_k(x)}\\) that satisfies the following three conditions:\n\n(1) \\(f_k(x) = 0\\) unless \\(|k| \\leq \\frac{1}{k}\\). \n(2) \\(\\int_{-\\frac{1}{k}}^{\\frac{1}{k}}f_k(x)dx = 1\\).\n(3) \\(f_k\\) is smooth.\n\nHere we get \\(\\langle f_k, \\varphi \\rangle\\) converges to \\(\\langle \\delta ,\\varphi \\rangle\\) so \n\\[\\lim_{n \\to \\infty}f_k = \\delta\\ .\\]\n\nSummarizing, both distributions and real numbers are extensions of (locally integrable) functions and rational numbers, respectively. That's why the distributions are also called *generalized* functions. \n\nFor any locally integrable function \\(f\\), we associate a distribution \\(f\\), given by \\(T_f = \\langle f,\\varphi \\rangle\\). For other distributions that are not locally integrable functions can be represented as the limit of the locally integrable function sequence, like \\(\\delta\\).\n\n## Fourier transforms and Convolutions of tempered distributions \n\nRecall the Schwartz class $S$ and *tempered distributions* we mentioned before. \n\nSince we need to consider Fourier transforms of the distributions and convolutions of distributions, we just consider tempered distributions here. \n\n(1) \\(\\int \\hat{f}(x)\\varphi(x)dx = \\int f(x)\\hat{\\varphi}(x)dx \\), where \\( f \\in S^{'}$, $\\varphi \\in S\\), and \\(\\hat{f(\\xi)} = \\int f(x)e^{ix \\xi}dx\\). \\(\\hat{f}\\) is the distribution Fourier transform of f.\n\n(2) \\(\\langle \\psi * f, \\varphi \\rangle = \\langle f, \\tilde{\\psi} * \\varphi \\rangle\\), where \\( f \\in S^{'}\\), \\(\\varphi,\\psi \\in S\\), and \\(\\tilde{\\psi}(x) = \\psi(-x) \\). \n\n\n## The Structure of Distributions\n\nGenerally speaking, if $T$ is a distribution, $T$ can be written as a infinite sum of derivatives of functions:\n\n$$T = \\sum_{n=1}^{\\infty}f_n^{(n)}(x)\\ .$$\nFor example, \n$$T = \\sum_{n=1}^{\\infty}\\delta^{(n)}(x-n) $$\nis a distribution which can be written as an infinite sum of the derivatives related to the delta function. \n\nParticularly, if $T$ is a distribution with compact support, or a tempered distribution, \n$T$ can be written as a finite sum of derivatives of functions.\n\n$$T = \\sum_{n=1}^{N}f_n^{(n)}(x) . $$\n\n\nThis leads to the Structure Theorem for \\(D'\\) and \\(S'\\)：\n\n*Note*: Here, \\(f_n,\\ \\forall n= 1,2,3,\\cdots \\) can be different functions.\n\n*Structure Theorem for D':* If $T$ is a distribution on $R^n$, then there exists continuous functions \\(f_{\\alpha}\\) such that\n\n$$T = \\sum_{\\alpha} \\left (\\frac{\\partial}{\\partial x}\\right )^{\\alpha}f_{\\alpha},$$\nwhere for every bounded open set \\(\\Omega\\), all but a finite number of distributions \\((\\frac{\\partial}{\\partial x})^\\alpha f_\\alpha\\) vanishes identically on \\(\\Omega\\).\n\nHere the right hand side is an infinite sum:\n\n$$\\left(\\frac{\\partial}{\\partial x}\\right )^{\\alpha} = \\left (\\frac{\\partial}{\\partial x_1}\\right )^{\\alpha_1}\\left (\\frac{\\partial}{\\partial x_2}\\right)^{\\alpha_2}\\cdots \\left(\\frac{\\partial}{\\partial x_n}\\right)^{\\alpha_n},$$\nand \n$$|\\alpha| = \\alpha_1+\\alpha_2 + \\cdots +\\alpha_n $$\n\n*Structure Theorem for $S'$:* If $T$ is a tempered distribution on \\(R^n\\), then there exists a finite number of continuous functions \\(f_{\\alpha}\\) satisfying \n$$|f_{\\alpha}(x)| \\leq c(1+|x|)^n\\ ,$$\n\n\nsuch that \n$$T = \\sum\\left (\\frac{\\partial}{\\partial x}\\right )^{\\alpha}f_{\\alpha},$$\n\nwhere the right hand side is a finite sum. The Structure Theorem allows us further define the *order* of distributions. \n\n*Note*: \n\n(1) This representation is not unique, and there may exist multiple representations of one distribution \\(T\\).\n\n(2) Using the structure theorem, here we may introduce the concept - *Order of Distribution* - The highest order derivative in \\(T =  \\sum(\\frac{\\partial}{\\partial x})^{\\alpha}f_{\\alpha} \\). \n \n## Positiveness of Distributions\n\n*Positiveness:* If for any non-negative test functions \\(\\varphi\\)(i.e. \\(\\varphi \\geq 0\\),\n\n$$\nT = \\langle f, \\varphi \\rangle = \\int f(x)\\varphi(x)dx \\geq 0\n$$\n\nis always true, then \\(T\\) is a *positive* distribution. Obviously, \\(T\\) is a positive distribution if and only if \n\n$$\\int f^{-}(x)dx = 0 $$\n\nwhere \\(f^{-}(x) = \\max(-f(x), 0)\\).\n\n*Example*: \\(\\delta\\) function is a positive function. For any  \\(\\varphi \\geq 0\\), there is \n\n$$\\langle \\delta, \\varphi \\rangle = \\varphi(0) \\geq 0\\ .$$\n\n*Note*:\n(1) The definition of positive distributions comes from positive measure. There is one-to-one correspondence between positive distribution \\(T\\) and positive measure \\(\\mu\\), such that \n\n$$\\langle T, \\varphi \\rangle = \\int \\varphi d\\mu\\ .$$\n\n*Proof: * Using [Riesz Representation Theorem](https://en.wikipedia.org/wiki/Riesz_representation_theorem):\n\nLet \\(X\\) be a locally compact Hausdroff space and \\(\\psi\\) is a positive linear functional on \\(C_c(X)\\) - the space of compactly supported complex-valued continuous functions. Then there exists a Borel  \\(\\sigma-\\) algebra \\(\\Sigma\\) on \\(X\\) and a unique positive Borel measure \\(\\mu\\) on \\(X\\) such that \n\n$$\\psi(f) = \\int_{X}f(x)d\\mu(x), f \\in C_c(X).$$\nBy using Riesz Representation Theorem, we draw a connection between linear functionals and measures. The theorem tells us that any positive linear functional has a unique associated positive measure. \n\n## The Continuity of Distribution\n\n*Continuity at \\( \\varphi \\)*: for every \\(\\epsilon > 0\\), and \\(R\\) sufficiently large, there exists \\(\\delta(R), m(R)\\), such that \n\n$$|\\langle T, \\varphi \\rangle - \\langle T, \\varphi_0 \\rangle| \\leq \\epsilon,$$\nwhere \n\n$$||(\\frac{\\partial}{\\partial x})^\\alpha\\varphi - (\\frac{\\partial}{\\partial x})^\\alpha\\varphi_0||_{\\infty}  = sup_{x} |(\\frac{\\partial}{\\partial x})^\\alpha\\varphi - (\\frac{\\partial}{\\partial x})^\\alpha\\varphi_0 | \\leq \\delta $$\n\nfor every \\(\\alpha \\leq m\\) and \\(supp(\\varphi_1), supp(\\varphi_2) \\subset \\{|x| \\leq R\\}\\).\n\n*Continuity of $T$: * For every test function \\(\\varphi\\), distribution \\(T\\) is continuous at \\(\\varphi\\). This also gives us a corollary that for continuous distribution \\(T\\), when \\(\\varphi_j \\longrightarrow \\varphi\\), \\(\\langle T, \\varphi_j \\rangle \\longrightarrow \\langle T, \\varphi \\rangle\\). \n\n## Probability Distribution and Distribution in the sense of Generalized Functions\n\nBoth probability distributions and distributions(generalized functions) try to solve the uneven distribution of physical quantities. The Riesz Representation Theorem establishes a link between *linear functionals* and *measures.* We try to find the associations between these two terms. \n\nRecall the definition of the *Probability Measure.* Considering the general notation of measure, the probability measure \\(\\mu\\) has additional requirements:\n\n(1) \\(0 \\leq \\mu\\ \\leq 1\\), and \\(\\mu\\) returns zero for the empty set and one for the entire space.\n\n(2) \\(\\mu\\) has the countable additivity property. If \\(A_j, j = 1,...,+\\infty\\) are countable disjoints sets, there is\n\n$$\\mu(\\cup_{j=1}^{\\infty}E_i) = \\sum_{i\\in N}\\mu(E_i) \\ .$$\n\nConsider the integral with respect to the probability measure:\n\n$$\\int fd\\mu = \\int f(x)dF(x),$$\nwhere \\(F(x)\\) is the c.d.f. of the corresponding probability measure. This expression can be regarded as an \"expectation\" of the function with respect to the probability measure. For each point \\(x\\) in the entire space, the integral assigns a weight(i.e.the probability) to each function f(x). \n\nRecall what we discussed before in *Positiveness of Distributions*, there is a one-to-one correspondence between positive distributions \\(T\\) and positive measures \\(\\mu\\). Each probability measure \\(P\\) on \\(\\Omega\\) is a kind of positive measure with an additional requirement \\(P(\\Omega) = 1\\). Thus, there is a one-to-one correspondence between a subset of positive distributions and probability measures. \n\nFor any probability measure \\(\\mu\\), it can be regarded as a distribution and there exists a positive distribution \\(T\\) acting on \\(D\\), such that\n\n$$T_\\mu = \\langle \\mu, \\varphi \\rangle = \\int\\varphi d\\mu.$$\n\nThe probability measure \\(\\mu\\) assigns the whole sample space to be one. This is equivalent with \\(\\langle \\mu, 1\\rangle = 1\\) in the distribution sense.\n\n*Notation: * Actually, The constant function \\(\\varphi = 1\\) isn't an element in the space of test functions. More rigorously, this statement means that \n\n$$\\lim_{k \\longrightarrow \\infty} \\langle \\mu, \\varphi_k \\rangle = 1,$$\n\nwhere \\(\\{\\varphi_k\\}\\) is an arbitrary test function series that approximates the constant function \\(\\varphi = 1\\).\n\nThe probability measure \\(\\mu\\) also assigns the whole sample space to be zero. Obviously, this is equivalent with \\(\\langle \\mu, 0 \\rangle = 0\\) in the distribution sense.\n\nWe already know that each probability measure \\(\\mu\\) is associated with a positive distribution. And \\(\\mu\\) also requires that \\(\\langle \\mu, 1 \\rangle = 1\\) and \\(\\langle \\mu, 0 \\rangle = 0\\). The latter one is always true for any distribution. Intuitively, we may draw a connection between positive distributions with \\(\\langle \\mu, 1 \\rangle = 1\\) and probability measures:\n\nThere is a one-to-one correspondence between positive distribution with \\(\\langle \\mu, 1 \\rangle = 1\\) and probability distribution.\n\nThis is a true statement supported by the true converse statement - For any positive distribution with \\(\\langle \\mu, 1 \\rangle = 1\\), there is a corresponding probability measure \\(\\mu\\). \n\n*Example:*  Consider the *Heaviside function*,\n\n$$H(x) = \\left\\{\n\\begin{aligned}\n1 & , & x\\geq 0 \\\\\n0 & , & x < 0\n\\end{aligned}\n\\right.\\ ,$$\n\nwhich is the cumulative distribution function of a constant random variable which is almost surely on 0. The corresponding probability measure here is the delta measure, where \n\n$$\\delta(A) = \\left\\{\n\\begin{aligned}\n0 & , & 0 \\notin A \\\\\n1 & , & 0 \\in A\n\\end{aligned}\n\\right.\\ .$$\n\nThe corresponding distribution here is the generalized function \\(\\delta\\).\n\n$$T(\\varphi) = \\int\\delta(x)\\varphi(x)dx = \\int\\varphi d\\delta =\n\\int\\varphi(x)dH(x) = \\varphi(0).$$",
      "word_count": 1436
    },
    {
      "title": "Random drawing (sampling) from distributions",
      "content": "## George Marsaglia's polar sampling method\n\n[George Marsaglia's polar sampling method](https://doi.org/10.1137/10060) is a \n*pseudo-random number sampling* algorithm for generating a pair of independent \nstandard normal random variable. Mind the strong relation to Cartesian representation of kime $\\kappa=(\\kappa_1, \\kappa_2)\\in\\mathbb{C}$! The difference is that whereas\nthe kime Cartesian coordinates could follow any bivariate distribution,\nMarsaglia polar sampling generates random kime coordinates \n$\\kappa_1, \\kappa_2\\sim N(0,1)$.\n\nThe Marsaglia polar random draw selects points in the  unit square \n$I\\times I = \\{(\\kappa_1, \\kappa_2)\\in \\mathbb{C} | 0\\leq \\kappa_1, \\kappa_2\\leq 1\\}$\nsubject to the constraint until $0 < z=\\kappa_1^2 + \\kappa_2^2 < 1$.\n\nThe corresponding random $z\\in\\mathbb{C}$, i.e., corresponding pair of random \nnormal variables is $\\left (\\kappa_1\\sqrt{\\frac{-2\\ln(z)}{z}}, \\ \\kappa_2\\sqrt{\\frac{-2\\ln(z)}{z}}\\right )$, \n$\\forall\\ z\\in\\mathbb{C}$, s.t., $0 < z=\\kappa_1^2 + \\kappa_2^2 < 1$.\n\nNote that the components $\\frac{\\kappa_i}{\\sqrt{z}}$ represent the *cosine* and the\n*sine* of the kime-phase angle the angle between the vector \n$\\vec{\\kappa}=(\\kappa_1, \\kappa_2)$ and the $x$-axis. This is because\n$\\kappa_i\\sqrt{\\frac{-2\\ln(z)}{z}}\\equiv \\frac{\\kappa_i}{\\sqrt{z}}\\sqrt{-2\\ln(z)}\\ , \\ i\\in\\{1,2\\}$.\n\n**Lemma**. Suppose $u\\sim Unif(-1,1)$, then the point \n$\\kappa=\\left (\\underbrace{\\cos(\\pi u)}_{\\kappa_1}, \\underbrace{\\sin(\\pi u)}_{\\kappa_2}\\right )\\in \\mathbb{C}$ is uniformly distributed on the unit circle\n$\\{(\\kappa_1, \\kappa_2)\\in \\mathbb{C} | \\kappa_1^2 + \\kappa_2^2=1\\}$.\n\nMultiplying the point $\\kappa\\in\\mathbb{C}$ by an independent random variable\n(kime-magnitude, or time) $\\rho\\gt 0$ whose distribution is\n$P(\\rho<a)=\\int_0^a {r\\ e^{-r^2/2}\\ dr}$ yields a point\n\n$$\\rho\\kappa\\equiv \\kappa'=\\left (\\underbrace{\\rho\\cos(\\pi u)}_{\\kappa'_1},\\ \n\\underbrace{\\rho\\sin(\\pi u)}_{\\kappa'_2}\\right )\\in \\mathbb{C},$$\n\nwhose Cartesian coordinates are jointly bivariate normal, and their\nmarginals are independent standard normal random variables, i.e., \n$\\kappa'_1,\\kappa'_2\\sim N(0,1)$ and \n\n$$(\\kappa'_1,\\kappa'_2)\\sim N\\left (\\mu=\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \n\\Sigma=\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right )\\ .$$\n\n*Proof*. The rationale for this is based in the simple trick of deriving \nthe bivariate normal distribution from the square of independent Normal marginals.\n\nLet $I=\\int_{-\\infty}^\\infty e^{-\\frac{\\kappa_1^2}{2}}\\ d\\kappa_1$ and compute\nthe square \n$$\\underbrace{I^2}_{squared\\\\ distribution}=\\left (\\int_{-\\infty}^\\infty e^{-\\frac{\\kappa_1^2}{2}}\\ d\\kappa_1\\right )\n\\times \\left (\\int_{-\\infty}^\\infty e^{-\\frac{\\kappa_1^2}{2}}\\ d\\kappa_1\\right )\\\\\n=\\left (\\int_{-\\infty}^\\infty e^{-\\frac{\\kappa_1^2}{2}}\\ d\\kappa_1\\right )\n\\times \\left (\\int_{-\\infty}^\\infty e^{-\\frac{\\kappa_2^2}{2}}\\ d\\kappa_2\\right )\\\\\n=\\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty e^{-\\frac{\\kappa_1^2+\\kappa_2^2}{2}}\\ \nd\\kappa_1d\\kappa_2 \\\\ \\underbrace{=}_{Polar\\\\ coord}\n\\int_{-\\pi}^\\pi\\int_{0}^\\infty \\rho\\ e^{-\\frac{\\rho^2}{2}}\\ \nd\\rho d\\theta = 2\\pi \\int_{0}^\\infty \\rho\\ e^{-\\frac{\\rho^2}{2}}\\ d\\rho\\ .$$\n\nObserve that the polar coordinate transformation yields a constant \nuniform *kime-phase distribution* $\\theta\\sim Unif(-\\pi,\\pi)$, whereas\nthe density of the *radial distance* $\\rho^2$ is \n$\\rho\\ e^{-\\frac{\\rho^2}{2}}\\sim \\chi^2_{df=2}$. \n\nTherefore, the bivariate Cartesian coordinates \n$$\\kappa'_1=\\rho\\cos(\\pi u)\\ {\\text{and }} \\kappa'_2=\\rho\\sin(\\pi u)\\sim N(0,1)\\ .$$\n**Task** Expand this approach of producing a pair of *independent standard normal variables* \nby radial projection of uniformly random points from the unit circle \nat a distance representing the square root of a $\\chi^2_2$ variable to non-uniform\nsymmetric, circular distributions, such as the (truncated) Laplace kime-phase distribution.\n\n*Note*: This is related to the [Box–Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)\n\n$$\\kappa_1=\\sqrt{\\underbrace{-2\\ln(\\overbrace{u_1}^{\\sim Unif(0,1)})}_{\\sim \\chi^2_1}}\n\\cos\\left (\\pi \\underbrace{u_2}_{\\sim Unif(-1,1)}\\right ),\\\\\n\\kappa_2=\\sqrt{\\underbrace{-2\\ln(\\overbrace{u_1}^{\\sim Unif(0,1)})}_{\\sim \\chi^2_1}}\n\\sin\\left (\\pi \\underbrace{u_2}_{\\sim Unif(-1,1)}\\right )\\ .$$\n\nUsing the Euler's formula $e^{i \\kappa} = \\cos(\\kappa) + i \\sin(\\kappa)$,\nlet $\\rho = \\sqrt{-2 \\ln(u_1)}$ and $\\kappa = \\pi u_2$. Then\n\n$$re^{i \\kappa} = \\sqrt{-2 \\ln(u_1)} e^{i \\pi u_2} =\n\\sqrt{-2 \\ln(u_1)}\\left (\\cos(\\pi u_2) + i \\sin(\\pi u_2)\\right )\\ .$$\n\nA random bivariate point $(\\kappa_1,\\kappa_2)$ inside the unit disk is projected\nonto the unit circle by setting $z=\\kappa_1^2+\\kappa_2^2$</math> and \ngenerating the polar random variable\n$\\left( \\frac{\\kappa_1}{\\sqrt{z}}, \\frac{\\kappa_2}{\\sqrt{z}} \\right)$ \nwithout computing the *cosine* and *sine* functional values. \nThe random point on the circumference is then radially projected along\nthe random kime-magnitude (time) displacement by $\\sqrt{-2\\ln(z)}$, where\n$z\\sim Unif(0,1)$ is independent of the random point on the circumference.\n\nIn this example, we generate $1,000$ kime samples using the Box-Muller transform.\n\n\nWe can also use Box-Miller to generate the SVG plots.\n\n\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number <img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script\n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"</script>\n\t\t<script type=\"text/javascript\"_uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 660
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    toc_depth: '3'\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "knitr::opts_chunk$set(echo = TRUE)",
      "line_count": 1
    },
    {
      "section": "The Dirac Delta Distribution",
      "code": "library(plotly)\nx <- seq(from=-4, to=4, length.out=2000)\nindexN <- seq(1:10)\ny <- list()\n\np <- plot_ly(type = \"scatter\", mode=\"lines\")\nfor (i in indexN) {\n  y[[i]] <- dnorm(x, mean=0, sd=1/indexN[i])\n  p <- p %>% add_trace(x=x, y=y[[i]], \n                       hovertemplate = paste0(\"Distribution=N(mu=0,sd=1/\", indexN[i], \")\\n\", \"(%{x}, %{y})\"),\n                       name=paste0(\"N(mu=0,sd=1/\", indexN[i], \")\"))\n}\np <- p %>% \n  layout(title=\"Gaussian Test Functions Approximating Dirac Delta\",\n     xaxis=list(title=\"Critical Values\"), yaxis=list(title=\"Densities\"))\np",
      "line_count": 16
    },
    {
      "section": "Random drawing (sampling) from distributions",
      "code": "set.seed(2)\nu1 = runif(1000, min = 0, max = 1)\nu2 = runif(1000, min = 0, max = 1)\ns = -log(u1)\nr = sqrt(2*s)\ntheta = 2*pi*u2\ntheta2 = theta * 180/pi\nk1 = sqrt(2*s) * cos(theta)\nk2 = sqrt(2*s) * sin(theta)\n\n\nlibrary(plotly)\ndata1 = data.frame(r = r, theta = theta2)\nscatter_polar = plot_ly(data1, r = ~r, theta = ~theta, type = 'scatterpolar', \n                        mode = 'markers',name = \"Polar Scatter Plot\", color=\"green\")\n\n\ndata2 = data.frame(kappa_1 = k1, kappa_2 = k2)\nscatter_plot = plot_ly(data2, x = ~kappa_1, y = ~kappa_2, type = 'scatter', \n                       mode='markers', name=\"Cartesian Scatter Plot\", color=\"green\")\nlayout_polar <- list(polar = list(radialaxis = \"Kime Magnitude\",\n                                  angularaxis = \"Kime Phase\"), showlegend=TRUE)\nscatter_polar <- scatter_polar %>% layout(layout_polar)\n\nlayout_plot <- list(xaxis = \"Kappa 1\", yaxis = \"Kappa 2\", showlegend=TRUE)\n\nscatter_plot <- scatter_plot %>% layout(layout_plot)\n\nscatter_polar\nscatter_plot",
      "line_count": 30
    },
    {
      "section": "Random drawing (sampling) from distributions",
      "code": "html_file_path_polar = \"/Users/user/Desktop/SOCR/interactive_polar_plot.html\"\nhtml_file_path_scatter = \"/Users/user/Desktop/SOCR/interactive_scatter_plot.html\"\n\nhtmlwidgets::saveWidget(scatter_polar, file = html_file_path_polar)\nhtmlwidgets::saveWidget(scatter_plot, file = html_file_path_scatter)\n\nlibrary(webshot)\n\n# Set file paths\nsvg_file_path_polar <- \"/Users/user/Desktop/SOCR/interactive_polar_plot.svg\"\nsvg_file_path_scatter <- \"/Users/user/Desktop/SOCR/interactive_scatter_plot.svg\"\n\n# Use webshot to convert HTML to SVG - fail\n#webshot(html_file_path_polar, file = svg_file_path_polar, vwidth = 1200, vheight = 800)\n#webshot(html_file_path_scatter, file = svg_file_path_scatter, vwidth = 1200, vheight = 800)",
      "line_count": 15
    },
    {
      "section": "Random drawing (sampling) from distributions",
      "code": "library(animation)\nlibrary(circular)\nlibrary(plotly)\nepsilon <- 0.1\nsampleSize <- 1000   # total number of phases to sample for 3 different processes (x, y, z)\nsizePerTime <- 100   # number of phases to use for each fixed time (must divide sampleSize)\ncircleUniformPhi <- seq(from=-pi, to=pi, length.out=sizePerTime)\ntotalTimes <- as.integer(sampleSize/sizePerTime)\n\nu <- seq(from=0, to=1, length.out=sizePerTime)  # Uniform[0,1]\nv <- seq(from=0, to=1, length.out=sampleSize %/% sizePerTime)  # Uniform[0,1]\nkappa1 <- cos(pi * u) # Cartesian kime coordinates kappa 1 (x-axis)\nkappa2 <- sin(pi * u) # kappa 2 (y-axis)\n# kappaGrid <- matrix(expand.grid(u, v))\nkappaGrid <- expand.grid(u, v)\n\noopt = ani.options(interval = 0.2)\nset.seed(1234)\n# sample the the kime-phases for all 3 different processes (A,B,C) and the r time points\nA <- rvonmises(n=sampleSize, mu=circular(pi/5), kappa=3)\nB <- rvonmises(n=sampleSize, mu=circular(-pi/3), kappa=5)\nC <- rvonmises(n=sampleSize, mu=circular(0), kappa=10)\nr <- seq(from=1, to=sampleSize/sizePerTime, length.out=10)\n\n# Define a function that renormalizes the kime-phase to [-pi, pi)\npheRenormalize <- function (x) {\n  out <- ifelse(as.numeric(x) <= pi, as.numeric(x)+pi, as.numeric(x)-pi)\n  return (out)\n}\n\n# transform Von Mises samples from [0, 2*pi) to [-pi, pi)\nA <- pheRenormalize(A)\nB <- pheRenormalize(B)\nC <- pheRenormalize(C)\n\n# vectorize the samples\nvectorA = as.vector(A)\nvectorB = as.vector(B)\nvectorC = as.vector(C)\n# Starting phases, set the first phase index=1\nplotA = c(vectorA[1])\nplotB = c(vectorB[1])\nplotC = c(vectorC[1])\n\n# pl_list <- list()\npl_scene <- plot_ly(type='scatter3d', mode=\"markers\")\nplotA <- list() \nplotB <- list() \nplotC <- list() \n\nplotA_df <- list()   # need separate data frames to store all time foliations\nplotB_df <- list()\nplotC_df <- list()\n\nfor (t in 1:length(r)) {  # loop over time\n  # loop over kime-phases\n  plotA[[t]] <- as.numeric(A[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  plotB[[t]] <- as.numeric(B[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  plotC[[t]] <- as.numeric(C[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  \n  tempA = circular(unlist(plotA[[t]]))\n  tempB = circular(unlist(plotB[[t]]))\n  tempC = circular(unlist(plotC[[t]]))\n  \n  resA <- density(tempA, bw=25, xaxt='n', yaxt='n')\n  resB <- density(tempB, bw=25, xaxt='n', yaxt='n')\n  resC <- density(tempC, bw=25, xaxt='n', yaxt='n')\n\n  unifPhi_df <- as.data.frame(cbind(t=t, circleUniformPhi=circleUniformPhi))\n  plotA_df[[t]] <- as.data.frame(cbind(t=t, plotA=unlist(plotA[[t]])))\n  plotB_df[[t]] <- as.data.frame(cbind(t=t, plotB=unlist(plotB[[t]])))\n  plotC_df[[t]] <- as.data.frame(cbind(t=t, plotC=unlist(plotC[[t]])))\n  \n  pl_scene <- pl_scene %>% add_trace(data=unifPhi_df, showlegend=FALSE,\n                      x = ~((t-epsilon)*cos(circleUniformPhi)), \n                      y = ~((t-epsilon)*sin(circleUniformPhi)), z=0,\n                      name=paste0(\"Time=\",t), line=list(color='gray'),\n                      mode = 'lines', opacity=0.3) %>%\n    add_markers(data=plotA_df[[t]], x=~(t*cos(plotA)), y=~(t*sin(plotA)), z=0,\n                      type='scatter3d', name=paste0(\"A: t=\",t),\n                      marker=list(color='green'), showlegend=FALSE,\n                      mode = 'markers', opacity=0.3) %>%\n    add_markers(data=plotB_df[[t]], x=~((t+epsilon)*cos(plotB)),\n                    y=~((t+epsilon)*sin(plotB)), z=0-epsilon, showlegend=FALSE,\n                    type='scatter3d', name=paste0(\"B: t=\",t), \n                    marker=list(color='blue'),\n                    mode = 'markers', opacity=0.3) %>%\n    add_markers(data=plotC_df[[t]], x=~((t+2*epsilon)*cos(plotC)),\n                y=~((t+2*epsilon)*sin(plotC)), z=0+epsilon, showlegend=FALSE,\n                type='scatter3d', name=paste0(\"C: t=\",t), \n                marker=list(color='red'),\n                mode = 'markers', opacity=0.3)\n} \n# pl_scene\n\nmeans_df <- as.data.frame(cbind(t = c(1:length(r)),\n                                plotA_means=unlist(lapply(plotA, mean)),\n                                plotB_means=unlist(lapply(plotB, mean)),\n                                plotC_means=unlist(lapply(plotC, mean))))\npl_scene <- pl_scene %>% \n  # add averaged (denoised) phase trajectories\n  add_trace(data=means_df, x=~(t*cos(plotA_means)),\n        y=~(t*sin(plotA_means)), z=0,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs A\",\n        line=list(color='green', width=15), opacity=0.8) %>%\n  add_trace(data=means_df, x=~(t*cos(plotB_means)), \n        y=~(t*sin(plotB_means)), z=0-epsilon,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs B\", \n        line=list(color='blue', width=15), opacity=0.8) %>%\n  add_trace(data=means_df, x=~(t*cos(plotC_means)), \n        y=~(t*sin(plotC_means)), z=0+epsilon,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs C\", \n        line=list(color='red', width=15), opacity=0.8) %>%\n  add_trace(x=0, y=0, z=c(-2,2), name=\"Space\", showlegend=FALSE,\n             line=list(color='gray', width=15), opacity=0.8) %>%\n  # Add Uniform Grid [-1,1] * [0,1]\n  add_trace(x=kappaGrid[[1]], y=kappaGrid[[2]], z=0, name=\"Uniform Grid\", showlegend=FALSE,\n             marker=list(color='orange', width=15), opacity=0.8) %>%\n  layout(title=\"Pseudo Spacekime (1D Space, 2D Kime) Kime-Phase Sampling and Foliation\",\n          scene = list(xaxis=list(title=\"Kappa1\"), yaxis=list(title=\"Kappa2\"),\n                        zaxis=list(title=\"Space\"))) %>% hide_colorbar()\npl_scene\n\n#################################################################\n# Organize data - dfXYZP - dataframe, \n# (X,Y,Z) Cartesian Coordinates, t=time, P=Phi, (A,B,C) Processes\n#################################################################\ndf <- array(0, c(length(r), sizePerTime, 6))\ndfXYZP <- data.frame()\n\nfor (t in 1:length(r)) {   # loop over time\n  for (phi in 1:sizePerTime)  { # loop over phase index\n    for (coord in 1:6) {     # loop over X,Y,Z coordinates, t, phi, Phi_Value\n      if (coord==1)      df[t, phi, coord]  <- A[(t-1)*sizePerTime + phi]  # X process\n      else if (coord==2) df[t, phi, coord]  <- B[(t-1)*sizePerTime + phi]\n      else if (coord==3) df[t, phi, coord]  <- C[(t-1)*sizePerTime + phi]\n      else if (coord==4) df[t, phi, coord]  <- t    # t: Parametric Grid\n      else if (coord==5) df[t, phi, coord]  <- phi  # Phi: Parametric Grid\n      else df[t, phi, coord]  <- circleUniformPhi[phi]  # actual value of the phase Phi\n      \n      ind = (t-1)*sizePerTime + phi  # dfXYZP row index\n      if (coord <= 3) {  # The 3 simulated processes\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 1] <- \n              t*cos(df[t, phi, coord]) ## X    (X-Cartesian Coordinate)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 2] <- \n              t*sin(phi) ## Y    (Y-Cartesian Coordinate)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 3] <- \n              (4-coord)*epsilon ## Z    (Z-Cartesian Coordinate)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 4] <- t ## t  (kime-magnitude)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 5] <- phi ## Phi  (kime-phase)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 6] <- coord  ## Process (color)\n      } else if (coord==4) {   # t indexing\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 1] <- t # phi/sizePerTime # U[0,1]\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 2] <- t # (t-1)/length(r) # U[0,1]\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 3] <- t # z\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 4] <- t # t\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 5] <- t # phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 6] <- coord\n      } else if (coord==4) {   # phi indexing\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 1] <- phi/sizePerTime\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 2] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 3] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 4] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 5] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 6] <- coord\n      } else if (coord==5) {   # phi Value\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 1] <- phi/sizePerTime\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 2] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 3] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 4] <- t\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 5] <- phi \n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 6] <- circleUniformPhi[phi] # phi # ?\n      } else {  # if (coord==6) #  Uniform[0,1] * Uniform[0,1] parametric Grid\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 1] <- u[phi] * (sampleSize %/% sizePerTime)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 2] <- v[t] * (sampleSize %/% sizePerTime)\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 3] <- 0\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 4] <- t\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 5] <- phi\n        dfXYZP[length(r)*sizePerTime*(coord-1) + ind, 6] <- coord\n      }\n    }\n  }\n} \ncolnames(dfXYZP) <- c(\"X\", \"Y\", \"Z\", \"t\", \"Phi\", \"Process\")\npl_scene <- dfXYZP[-c(3001:5000), ] %>%\n  highlight_key(~Phi) %>% plot_ly(type='scatter3d', mode=\"markers+text\") %>%\n  add_markers(x=~X, y=~Y, z=~Z, text=~Process, \n              name=~paste0(\"Process: \", Process), #, \"<br>t: \", t, \"<br>Phi: \", Phi, \"<extra></extra>\"), \n              showlegend=FALSE, color=~Process, textposition=\"top\", opacity=0.3,\n              hoverinfo = ~paste(\"Process: \", Process, \"<br>X: %{x}<br>\",\n                      \"Y: %{y}<br>\", \"Z: %{z}<br>\", \n                      \"t: \", t, \"<br>Phi: \", Phi, \"<extra></extra>\")) %>% #  \"x+y+color\") %>%\n  highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\", dynamic=TRUE) %>%\n  layout(title=\"Marsaglia Polar Random Kime-Phase Sampling \\n Three Different Processes (Colors)\\n Yellow Grid shows the Time * Phase indexing Corresponding to Kime\\n Mouse-over Shows the Corresponding Longitudinal Trajectories for a Fixed Phase Index\",\n          scene = list(xaxis=list(title=\"Kappa1\"), yaxis=list(title=\"Kappa2\"),\n                        zaxis=list(title=\"Process\"))) %>% hide_colorbar()\npl_scene\n\n## 3D array to a dataframe\n# library(reshape2)\n# df1 <- melt(df)  # Var1=time, Var2=Phase, Var3=Process, value=Value\n# colnames(df1)<- c(\"time\", \"phi\", \"Process\", \"value\")\n# df2 <- df1[, -1]  # \"Time * phi\", \"Process\", \"value\"\n# \n# pl_scene <- plot_ly(data=dfXYZP, type='scatter3d', mode=\"markers\")\n# \n# pl_scene <- dfXYZP %>%\n#   highlight_key(~Phi) %>% plot_ly(type='scatter3d', mode=\"markers+text\") %>%\n#   add_markers(x=~X, y=~Y, z=~Z, text=~Process, name=~paste0(\"Process: \", Process), \n#               marker=list(color=~as.factor(Process)), showlegend=FALSE,\n#               textposition = \"top\", hoverinfo = \"x+y\", opacity=0.3) %>%\n#   highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") \n#     # %>%\n#     # add_trace(x=0, y=0,  z=c(-2,2), name=\"Space\", showlegend=FALSE,\n#     #           line=list(color='gray', width=15), opacity=0.8) # Vertical Space\n# \n# pl_scene\n\n# pl_scene <- plot_ly(data = df, type='scatter3d', mode=\"markers\")\n# \n# for (t in 1:length(r)) {   # loop over time  # t=1\n#   pl_scene <- as.data.frame(df[t,,]) %>%\n#     highlight_key(~V1) %>% plot_ly(type='scatter3d', mode=\"markers+text\") %>%\n#     add_markers(x=~t*cos(V2), y=~t*sin(V2), text = ~V2,\n#                   z=0+epsilon/2, type='scatter3d', name=paste0(\"X: t=\",t), # Process 1\n#                   marker=list(color='green'), showlegend=FALSE,\n#                   textposition = \"top\", hoverinfo = \"x+y\", opacity=0.3) %>%\n#     add_markers(x=~(t+epsilon)*cos(V3), text = ~V3,                # Process 2\n#                   y=~(t+epsilon)*sin(V3), z=0-epsilon, showlegend=FALSE,\n#                   name=paste0(\"Y: t=\",t), \n#                   marker=list(color='blue'), mode='markers', opacity=0.3)  %>%\n#     add_markers(x=~(t+2*epsilon)*cos(V4),               # Process 3\n#                   y=~(t+2*epsilon)*sin(V4), z=0+epsilon, showlegend=FALSE,\n#                   type='scatter3d', name=paste0(\"Z: t=\",t), \n#                   marker=list(color='red'), mode='markers', opacity=0.3) %>%\n#     add_markers(x=~(t*epsilon)*V5,   # Process 4 (Grid)\n#                   y=t, z=0, showlegend=FALSE,\n#                   type='scatter3d', name=paste0(\"Grid: t=\",t), \n#                   marker=list(color='lightgray'), mode='markers', opacity=0.3) %>%\n#     highlight(on = \"plotly_hover\", off = \"plotly_doubleclick\") \n#     # %>%\n#     # add_trace(x=0, y=0,  z=c(-2,2), name=\"Space\", showlegend=FALSE,\n#     #           line=list(color='gray', width=15), opacity=0.8) # Vertical Space\n# }\n# \n# pl_scene\n\n# pl_scene <- plot_ly(type='scatter3d', mode=\"markers\")\n# \n# pl_scene <- \n#     pl_scene %>% add_trace(data=unifPhi_df, showlegend=FALSE, # Radial Circles\n#                       x = ~((t-epsilon)*cos(circleUniformPhi)), \n#                       y = ~((t-epsilon)*sin(circleUniformPhi)), z=0,\n#                       name=paste0(\"Time=\",t), line=list(color='gray'),\n#                       mode = 'lines', opacity=0.3) %>%\n#     add_markers(x=(t*cos(df[t, , 1])), y=(t*sin(df[t, , 1])), \n#                 z=0+epsilon/2, type='scatter3d', name=paste0(\"X: t=\",t), # Process 1\n#                 marker=list(color='green'), showlegend=FALSE,\n#                 mode = 'markers', opacity=0.3) %>%\n#     add_markers(x=(t+epsilon)*cos(df[t, , 2]),                 # Process 2\n#                 y=(t+epsilon)*sin(df[t, , 2]), z=0-epsilon, showlegend=FALSE,\n#                 type='scatter3d', name=paste0(\"Y: t=\",t), \n#                     marker=list(color='blue'), mode='markers', opacity=0.3) %>%\n#     add_markers(x=(t+2*epsilon)*cos(df[t, , 3]),               # Process 3\n#                 y=(t+2*epsilon)*sin(df[t, , 3]), z=0+epsilon, showlegend=FALSE,\n#                 type='scatter3d', name=paste0(\"Z: t=\",t), \n#                 marker=list(color='red'), mode='markers', opacity=0.3) %>%\n#     add_markers(x=(t*epsilon)*(df[t, , 5]),   # Process 4 (Grid)\n#                 y=t, z=0, showlegend=FALSE,\n#                 type='scatter3d', name=paste0(\"Grid: t=\",t), \n#                 marker=list(color='lightgray'), mode='markers', opacity=0.3) %>%\n#   add_trace(x=0, y=0, z=c(-2,2), name=\"Space\", showlegend=FALSE,   # Vertical Space\n#              line=list(color='gray', width=15), opacity=0.8) \n# pl_scene\n\npl_scene <- pl_scene %>% # Add Uniform Grid [-1,1] * [0,1]\n  add_trace(x=5*kappaGrid[[1]], y=5*kappaGrid[[2]], z=0, name=\"Uniform Grid\", \n            showlegend=FALSE, marker=list(color='orange',width=15), opacity=0.8)\n  \nfor (t in 1:length(r)) {   # loop over time\n  tempA = circular(unlist(df[t, , 1]))  # X Process\n  tempB = circular(unlist(df[t, , 2]))  # Y\n  tempC = circular(unlist(df[t, , 3]))  # Z\n  unifPhi_df <- as.data.frame(cbind(t=t, circleUniformPhi=circleUniformPhi))\n  \n  pl_scene <- \n    pl_scene %>% add_trace(data=unifPhi_df, showlegend=FALSE, # Radial Circles\n                      x = ~((t-epsilon)*cos(circleUniformPhi)), \n                      y = ~((t-epsilon)*sin(circleUniformPhi)), z=0,\n                      name=paste0(\"Time=\",t), line=list(color='gray'),\n                      mode = 'lines', opacity=0.3) %>%\n    add_markers(x=(t*cos(df[t, , 1])), y=(t*sin(df[t, , 1])), \n                z=0, type='scatter3d', name=paste0(\"X: t=\",t), # Process 1\n                marker=list(color='green'), showlegend=FALSE,\n                mode = 'markers', opacity=0.3) %>%\n    add_markers(x=(t+epsilon)*cos(df[t, , 2]),                 # Process 2\n                y=(t+epsilon)*sin(df[t, , 2]), z=0-epsilon, showlegend=FALSE,\n                type='scatter3d', name=paste0(\"Y: t=\",t), \n                    marker=list(color='blue'), mode='markers', opacity=0.3) %>%\n    add_markers(x=(t+2*epsilon)*cos(df[t, , 3]),               # Process 3\n                y=(t+2*epsilon)*sin(df[t, , 3]), z=0+epsilon, showlegend=FALSE,\n                type='scatter3d', name=paste0(\"Z: t=\",t), \n                marker=list(color='red'), mode='markers', opacity=0.3) %>%\n    add_markers(x=(t*epsilon)*(df[t, , 4]/sizePerTime),   # Process 4 (Grid)\n                y=t, z=0+2*epsilon, showlegend=FALSE,\n                type='scatter3d', name=paste0(\"Grid: t=\",t), \n                marker=list(color='lightgray'), mode='markers', opacity=0.3)\n}",
      "line_count": 308
    }
  ]
}