{
  "metadata": {
    "created_at": "2025-05-15T17:01:01.182426",
    "total_sections": 5,
    "total_code_chunks": 23,
    "total_tables": 1,
    "r_libraries": [
      "EBImage",
      "RColorBrewer",
      "abind",
      "circular",
      "dplyr",
      "ggplot2",
      "htmltools",
      "htmlwidgets",
      "igraph",
      "nloptr",
      "plotly",
      "rstan",
      "signal"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"SOCR TCIU: Chapter 3: The Kime-Phase Problem\"\nauthor: \"SOCR Team\"\ndate: \"`r format(Sys.time(),'%m/%d/%Y')`\"\noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: TCIU_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    code_folding: hide\n  word_document:\n    toc: true\n    toc_depth: '3'\nsubtitle: \"Kime-phase Estimation and Kime-Operators in Spacekime Representation\"\neditor_options:\n  markdown:",
      "word_count": 52
    },
    {
      "title": "The effects of Kime-Magnitudes and Kime-Phases",
      "content": "Jointly, the *amplitude spectrum* (magnitudes) and the *phase spectrum*\n(phases) uniquely describe the spacetime representation of a signal.\nHowever, the importance of each of these two spectra is not equivalent.\nIn general, the effect of the phase spectrum is more important compared\nto the corresponding effects of the amplitude spectrum. In other words,\nthe magnitudes are less susceptible to noise or the accuracy of their\nestimations. The effects of magnitude perturbations are less critical\nrelative to proportional changes in the phase spectrum. For instance,\nparticularly in terms of spacetime locations where the signal is zero,\nthe signal can be reconstructed (by the IFT) relatively accurately using\nincorrect magnitudes solely by using the correct phases\n[REF](https://doi.org/10.1533/9780857099457.1.75). For a real valued\nsignal $f$, suppose the amplitude of its Fourier transform,\n$FT(f)=\\hat{f}$, is $A(\\omega) > 0, \\forall \\omega$, then:\n$$f(x)=IFT(\\hat{f})=Re\\left (\\frac{1}{2\\pi}\\int_{R} \n\\underbrace{A(\\omega)e^{i\\phi(\\omega)}}_{\\hat{f}(\\omega)}\\ e^{i\\omega x}d\\omega \\right)=\nRe\\left (\\frac{1}{2\\pi}\\int_{R}A(\\omega)e^{i(\\phi(\\omega)+\\omega x)}d\\omega\\right) =\n\\frac{1}{2\\pi}\\int_{R} {A(\\omega) \\cos(\\phi(\\omega)+\\omega x)}d\\omega.$$\n\nThus, the zeros of $f(x)$ occur for\n$\\omega x+ \\phi(\\omega)=\\pm k\\frac{\\pi}{2}$, $k= 1,2,3,.$.\n\nA solely amplitude driven reconstruction\n$\\left ( f_A(x)=IFT(\\hat{f})=\\frac{1}{2\\pi}\\int_{R}\\underbrace{A(\\omega)}_{no\\ phase}\\ e^{i\\omega x}d\\omega \\right)$\nwould yield worse results than a solely-phase based reconstruction\n$\\left ( f_{\\phi}(x)=IFT(\\hat{f})=\\frac{1}{2\\pi} \\int_{R}\\underbrace{e^{i\\phi(\\omega)}}_{no\\ amplitude}\\ e^{i\\omega x}d\\omega\\right )$.\nThe latter would have a different total energy from the original signal,\nhowever, it would include some signal recognizable features as the\nzeroth-level curves of the original $f$ and the phase-only\nreconstruction $f_{\\phi}$ signals will be preserved. This suggests that\nthe *Fourier phase* of a signal is more informative than the *Fourier\namplitude*, i.e., the magnitudes are robust to errors or perturbations.\n\nIn X-ray crystallography, crystal structures are bombarded by\nparticles/waves, which are diffracted by the crystal to yield the\nobserved diffraction spots or patterns. Each diffraction spot\ncorresponds to a point in the reciprocal lattice and represents a\nparticle wave with some specific amplitude and a relative phase.\nProbabilistically, as the particles (e.g., gamma-rays or photons) are\nreflected from the crystal, their scatter directions are proportional to\nthe square of the wave amplitude, i.e., the square of the wave Fourier\nmagnitude. X-rays capture these amplitudes as counts of particle\ndirections, but miss all information about the relative phases of\ndifferent diffraction patterns.\n\nSpacekime analytics are analogous to X-ray crystallography, DNA helix\nmodeling, and other applications, where only the Fourier magnitudes\n(time), i.e., power spectrum, is only observed, but not the phases\n(kime-directions), which need to be estimated to correctly reconstruct\nthe intrinsic 3D object structure\n[REF](https://www.sciencedirect.com/science/article/pii/B9781904275268500051),\nin our case, the correct spacekime analytical inference. Clearly, signal\nreconstruction based solely on either the amplitudes or the phases is an\nill-posed problem, i.e., there will be many *alternative solutions*. In\npractice, such *signal* or *inference* reconstructions are always\napplication-specific, rely on some a priori knowledge on the process (or\nobjective function), or depend an information-theoretic criteria to\nderive conditional solutions. Frequently, such solutions are obtained\nvia least squares, maximum entropy criteria, maximum a posterior\ndistributions, Bayesian estimations, or simply by approximating the\nunknown amplitudes or phases using prior observations, similar\nprocesses, or theoretical models.",
      "word_count": 495
    },
    {
      "title": "Solving the Missing Kime-Phase Problem",
      "content": "There are many alternative solutions to the problem of estimating the\nunobserved kime-phases. All solutions depend on the quality of the data\n(e.g., noise), the signal energy (e.g., strength of association between\ncovariates and outcomes), and the general experimental design. There can\nbe rather large errors in the phase reconstructions, which will in turn\naffect the final spacekime analytic results. Most phase-problem\nsolutions are based on the idea that having some *prior knowledge* about\nthe characteristics of the experimental design (case-study phenomenon)\nand the desired inference (spacekime analytics). For instance, if we\nartificially *load the energy* of the case-study (e.g., by lowering the\nnoise, increasing the SNR, or increasing the strength of the relation\nbetween explanatory and outcome variables), the phases computed from the\nthis stronger-signal dataset will be more accurate representations than\nthe original phase estimates. Examples of phase-problem solutions\ninclude *energy modification* and *fitting and refinement* methods.\n\nIn [TCIU Section 6 (Circular Kime-Phase and Electron Orbit Densities) we cover the basics of the kime-phase representation](https://www.socr.umich.edu/TCIU/HTMLs/Chapter6_Kime_Phases_Circular.html).\n\n![](https://socr.umich.edu/docs/uploads/2025/SOCR_KimePhase_Animation_2025.gif)\nThree kime-phase distributions at a fixed spatial location.\n\n\n## Energy Modification Strategies\n\nIn general, *energy modification* techniques rely on prior knowledge,\ntestable hypotheses, or intuition to modify the dataset by strengthening\nthe *expected* relation we are trying to uncover using spacekime\nanalytics.\n\n### Kime-phase noise distribution flattening\n\nIn many practical applications, part of the dataset (including both\ncases and features) include valuable information, whereas the rest of\nthe data may include irrelevant, noisy, or disruptive information.\n\nClearly, we can't explicitly untangle these two components, however, we\ndo expect that the irrelevant data portion would yield\nuninformative/unimportant kime-phases, which may be used to estimate the\nkime-phase noise-level and noise-distribution. Intuitively, if we modify\nthe dataset to flatten the irrelevant kime-phases, the estimates of the\ncorresponding true-signal kime-phases may be more accurate or more\nrepresentative. We can think of this process as using kime-phase\ninformation from some known strong features to improve the kime-phase\ninformation of other particular features. Kime-phase noise distribution\nflattening requires that the kime-phases be good enough to detect the\nboundaries between the strong-features and the rest.\n\n### Multi-sample Kime-Phase Averaging\n\nIt's natural to assume that multiple instances of the same process would\nyield similar analytics and inference results. For a large dataset, we\ncan use ensemble methods (e.g., SuperLearner, and CBDA) to iteratively\ngenerate independent samples, which would be expected to lead to\nanalogous kime-phase estimated and analytical results. Thus, we expect\nthat when salient features are extracted by spacekime analytics based on\nindependent samples, their kime-phase estimates should be highly\nassociated (e.g., correlated), albeit perhaps not identical. However,\nweak features would exhibit exactly the opposite effect - their\nkime-phases may be highly variable (noisy). By averaging the\nkime-phases, noisy-areas in the dataset may cancel out, whereas, patches\nof strong-signal may preserve the kime-phase details, which would lead\nto increased kime forecasting accuracy and reproducibility of the kime\nanalytics.\n\n### Histogram equalization\n\nAs common experimental designs and similar datasets exhibit analogous\ncharacteristics, the corresponding spacekime analytics are also expected\nto be synergistic. Spacekime inference that does not yield results in\nsome controlled or expected range, may be indicative of incorrect\nkime-phase estimation. We can use histogram equalization methods to\nimprove the kime-phase estimates. This may be accomplished by altering\nthe distribution of kime-phases to either match the phase distribution\nof other similar experimental designs or generate more expected\nspacekime analytical results.\n\n### Fitting and refinement\n\nRelated to *energy modification* strategies, the *fitting and\nrefinement* technique capitalizes on the fact that strong energy\ndatasets tend to have a smaller set of salient features. So, if we\nconstruct case-studies with some strong features, the corresponding\nkime-phases will be more accurate, and the resulting inference/analytics\nwill be more powerful and highly reproducible. Various classification,\nregression, supervised and unsupervised methods, and other model-based\ntechniques allow us to both fit a model (estimate coefficients and\nstructure) as well as apply the model for outcome predictions and\nforecasting. Such models permit control over the characteristics of\nindividual features and multivariate inter-relations, which can be\nexploited to gather valuable kime-phase information. Starting with a\nreasonable guess (kime-phase prior), the *fitting and refinement*\ntechnique can be applied iteratively to (1) reconstructing the data into\nspacetime using the kime-phase estimates, (2) fit or estimate the\nspacekime analytical model, (3) compare the analytical results and\ninference to expected outcomes, and (4) refine the kime-phase estimator\naiming to gain better outcomes (#3). Indeed, other *energy modification*\nstrategies (e.g., averaging or flattening) can be applied before a new\niteration to build a new model is initiated (#1 and #2).\n\n## Data Source Type\n\n\n## Figure 3.8",
      "word_count": 764
    },
    {
      "title": "QM Phase Estimation by Probing Measurements in Different Bases",
      "content": "The fundamental rationale for observing only the *magnitude* of the\nquantum wavefunction, while phases remain unobservable directly, stems\nfrom the structure of quantum mechanics. Physical observables are always\n*real*, tied to Hermitian operators, and measurement outcomes depend on\nprobabilities derived from the *squared magnitude of the wavefunction*,\n$|\\psi(x)|^2$. Global or relative phases do not affect these\nprobabilities, unless they manifest as relative phase differences in\nsuperposition states. *Global phases* cancel out in expectation values,\nwhile *relative phases* influence interference patterns, which are still\ninferred indirectly through probabilities. Thus, *direct phase\nmeasurement* is impossible. Instead, phases are estimated by *observing\ninterference effects* across multiple measurement bases.\n\n## Pauli Operators and Bases in Quantum Mechanics\n\nThe relationship between *Pauli operators* and *bases* is fundamental to\nquantum mechanics. *Pauli operators* are matrix representations of\n*Pauli matrices* plus the *identity matrix* that form a basis for\n$2\\times 2$ Hermitian matrices\n\n$$\\sigma_x = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\ , \\quad\n\\sigma_y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}\\ , \\quad\n\\sigma_z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\\ , \\quad\nI = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\n\nThe *eigenspectrum* of each Pauli operator includes the *eigenvalues*\n$\\lambda = \\pm 1$ with their base-specific corresponding *eigenvectors.*\nFor the computational basis, $\\sigma_z$,\n\n-   the *eigenvector* $|0\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n    corresponds with *eigenvalue* $\\lambda = +1$,\n-   and the *eigenvector*\n    $|1\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ corresponds with\n    *eigenvalue* $\\lambda = -1$.\n\nSimilarly, for the $X$-basis, $\\sigma_x$\n\n-   $|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n    with $\\lambda = +1$,\n-   $|-\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n    with $\\lambda = -1$.\n\nAnd for the $Y$-basis, $\\sigma_y$\n\n-   $|+i\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + i|1\\rangle) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ i \\end{pmatrix}$\n    with $\\lambda = +1$,\n-   $|-i\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - i|1\\rangle) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -i \\end{pmatrix}$\n    with $\\lambda = -1$.\n\nThe Pauli matrices satisfy important commutation relations:\n\n$$[\\sigma_k, \\sigma_l] = 2\\mathrm{i}\\epsilon_{klm}\\sigma_m,$$\n\nwhere $[\\sigma_k, \\sigma_l] = \\sigma_k\\sigma_l - \\sigma_l\\sigma_k$ is\nthe *commutator*, $\\mathrm{i}$ is the *imaginary unit*, the indices\n$k,l,m$ run over $\\{1,2,3\\}$ corresponding to\n$\\{x\\equiv 1,y\\equiv 2,z\\equiv 3\\}$, we use the Einstein summation\nconvention with summation over repeated index $m$, and $\\epsilon_{klm}$\nis the Levi-Civita symbol, $$\\epsilon_{klm} = \\begin{cases} \n  +1 & \\text{for even permutations of } (1,2,3) \\\\\n  -1 & \\text{for odd permutations of } (1,2,3) \\\\\n  0 & \\text{if any indices are repeated}\n  \\end{cases} .$$\n\nExplicitly, this gives\n$$[\\sigma_x, \\sigma_y] = [\\sigma_1, \\sigma_2] = 2\\mathrm{i}\\sigma_z,\\quad \ni.e., \\quad [\\sigma_1, \\sigma_2] = 2\\mathrm{i}\\sigma_3$$\n$$[\\sigma_y, \\sigma_z] = [\\sigma_2, \\sigma_3] = 2\\mathrm{i}\\sigma_x,\\quad \ni.e., \\quad [\\sigma_2, \\sigma_3] = 2\\mathrm{i}\\sigma_1,$$\n$$[\\sigma_z, \\sigma_x] = [\\sigma_3, \\sigma_1] = 2\\mathrm{i}\\sigma_y\\quad \ni.e., \\quad [\\sigma_3, \\sigma_1] = 2\\mathrm{i}\\sigma_2.$$\n\nIn 2D, these relations can be verified directly using the matrix\nrepresentations\n\n$$\\sigma_1 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad\n\\sigma_2 = \\begin{pmatrix} 0 & -\\mathrm{i} \\\\ \\mathrm{i} & 0 \\end{pmatrix}, \\quad\n\\sigma_3 = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}.$$\n\nIn QM, this cyclic structure reflects the rotational symmetry of the\nPauli matrices and is fundamental to their role in describing spin-$1/2$\nsystems and qubit operations. They also satisfy *anticommutation\nrelations* $\\{\\sigma_i, \\sigma_j\\} = 2\\delta_{ij}I$, where $\\delta_{ij}$\nis the Kronecker delta. For each basis we also have the *completeness\nrelations*, in the $\\sigma_z$ basis,\n$|0\\rangle\\langle 0| + |1\\rangle\\langle 1| = I$, in the $\\sigma_x$\nbasis, $|+\\rangle\\langle +| + |-\\rangle\\langle -| = I$, and in the\n$\\sigma_y$ basis $|+i\\rangle\\langle +i| + |-i\\rangle\\langle -i| = I$.\n\nAny single-qubit unitary operation can be expressed as\n$$U = e^{i\\alpha}e^{i(\\theta_x\\sigma_x + \\theta_y\\sigma_y + \\theta_z\\sigma_z)},$$\nwhere $\\alpha, \\theta_x, \\theta_y, \\theta_z$ are real parameters.\n\nThe non-commutativity leads to *uncertainty relations*\n$\\Delta A \\Delta B \\geq \\frac{1}{2}|\\langle [A,B] \\rangle|$, where the\nlower bound holds in a state-dependent manner, e.g., consider an example\nstate $\\|\\psi\\rangle = \\|+\\rangle \\|\\psi\\rangle =\\|+\\rangle$ where\n$\\langle \\sigma_z \\rangle \\not= 0$. For Pauli operators, this means\n$\\Delta \\sigma_x \\Delta \\sigma_y \\geq |\\langle \\sigma_z \\rangle|$, with\ncyclic permutations.\n\nA measurement in any Pauli basis corresponds to projectors; for the\n$\\sigma_z$ basis, $P_0 = |0\\rangle\\langle 0|$,\n$P_1 = |1\\rangle\\langle 1|$, for the $\\sigma_x$ basis,\n$P_+ = |+\\rangle\\langle +|$, $P_- = |-\\rangle\\langle -|$, and for the\n$\\sigma_y$ basis, $P_{+i} = |+i\\rangle\\langle +i|$,\n$P_{-i} = |-i\\rangle\\langle -i|$. The Bloch sphere representation\nindicates that any single-qubit state can be written as\n$|\\psi\\rangle = \\cos(\\theta/2)|0\\rangle + e^{i\\phi}\\sin(\\theta/2)|1\\rangle$,\nwhere $\\theta$ and $\\phi$ are spherical coordinates on the Bloch sphere,\nwith radius $r=1$. Finally, the *expectation values of Pauli operators*\nreflect the relation between the classical Cartesian coordinates and\nspherical coordinates\n\n$$x=\\langle \\sigma_x \\rangle = \\sin\\theta\\cos\\phi, \\quad \ny=\\langle \\sigma_y \\rangle = \\sin\\theta\\sin\\phi, \\quad \nz=\\langle \\sigma_z \\rangle = \\cos\\theta.$$\n\n## Example: Qubit Phase Estimation\n\nConsider a qubit in the state\n$|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left(|0\\rangle + e^{i\\phi}|1\\rangle\\right),$\nwhere $\\phi$ is the *relative phase.* We will show how to probe the\nphase $\\phi$ by collecting process measurement in 3 *different bases.*\n\n1.  *Measurement in the Computational Basis*, $Z$-basis,\n    $|0\\rangle, |1\\rangle$: The probabilities\n    $P(0) = |\\langle 0|\\psi\\rangle|^2 = \\frac{1}{2}$ and\n    $P(1) = |\\langle 1|\\psi\\rangle|^2=\\frac{1}{2}$ are independent of\n    $\\phi$, and hence, no phase information is obtained purely by\n    measurements in the $Z$-basis.\n\n2.  *Measurement in the* $X$-Basis, $|+\\rangle, |-\\rangle$: The states\n    $|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)$ and\n    $|-\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle)$ transform\n    the phase into measurable probabilities, which reveals $\\cos\\phi$\n\n$$P(+) = \\frac{1 + \\cos\\phi}{2}, \\quad P(-) = \\frac{1 - \\cos\\phi}{2}.$$\n\n3.  *Measurement in the* $Y$-Basis, $|y_+\\rangle, |y_-\\rangle$:\n    Measuring the states\n    $|y_+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + i|1\\rangle)$ and\n    $|y_-\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - i|1\\rangle)$ reveals\n    $\\sin\\phi$\n\n$$P(y_+) = \\frac{1 + \\sin\\phi}{2}, \\quad P(y_-) = \\frac{1 - \\sin\\phi}{2}.$$\n\nBy combining results from the $X$- and $Y$-bases, both $\\cos\\phi$ and\n$\\sin\\phi$ are determined, allowing reconstruction of $\\phi$ modulo\n$2\\pi$. In essence, the phases are *encoded in interference patterns*\nthat depend on the measurement basis. By choosing *multiple\nnon-commuting (incompatible) bases* (e.g., $X$, $Y$, and $Z$), the\nrelative phase information is *rotated* into observable probabilities.\nThis underpins techniques like *quantum state tomography* and\n*interferometry*, where phase estimation relies on coherence across\ncomplementary observables.\n\n### Adding a Commuting Basis to the $Y$-Basis Adds No New Phase Information\n\nWhen two observables *commute*, their eigenbases are identical (or\n*compatible*). Measuring in a commuting basis is equivalent to measuring\nin the original basis. Phase estimation requires measurements in\ncomplementary bases. To resolve $\\phi$, we need measurements in\n*non-commuting bases* (e.g., $X$- and $Y$-bases). Complementary bases\n“probe” the phase through interference in distinct directions. For a\nsingle qubit, all observables commuting with $Y$ are trivial linear\ncombinations of $Y$ and $I$, so no new eigenbases exist. In larger\nsystems, degeneracy allows commuting observables with distinct\neigenbases, but even there, relative phases are resolved via\nnon-commuting measurements.\n\nLet's demonstrate that adding measurements in a new basis that commutes\nwith the $Y$-basis adds no new phase information, still using the same\n*qubit example* and utilizing the Pauli-$Y$ operator. For a single\nqubit, since $Y$ has no degeneracy in its spectrum, any observable\ncommuting with $Y$ must share its eigenbasis. Let’s define a trivial\ncommuting operator, e.g., $\\mathcal{O} = Y + \\alpha I$, where\n$\\alpha \\in \\mathbb{R}$ and $I$ is the identity operator. The new\noperator $\\mathcal{O}$ shares the same *eigenstates* $|y_+\\rangle$ and\n$|y_-\\rangle$ as $Y$. Hence, measuring in the $\\mathcal{O}$-basis is\nequivalent to measuring in the $Y$-basis itself.\n\nAgain, let the qubit state be\n$|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\left(|0\\rangle + e^{i\\phi}|1\\rangle\\right).$\nA measurement in the $Y$-*Basis*\n$\\left (|y_+\\rangle, |y_-\\rangle\\right )$ gives probabilities that\nreveal $\\sin\\phi$,\n$$P(y_+) = \\frac{1 + \\sin\\phi}{2}, \\quad P(y_-) = \\frac{1 - \\sin\\phi}{2}.$$\n\nOn the other hand, a measurement in the *new commuting basis*,\n$\\mathcal{O}$ $\\left (|y_+\\rangle, |y_-\\rangle\\right )$, reflects\nobservations in the eigenbasis of $\\mathcal{O} = Y + \\alpha I$. Since\n$\\mathcal{O}$ shares its eigenstates with $Y$, the probabilities are\nidentical\n\n$$P(y_+^\\mathcal{O}) \\equiv P(y_+)= \\frac{1 + \\sin\\phi}{2}, \n\\quad P(y_-^\\mathcal{O}) \\equiv P(y_-)= \\frac{1 - \\sin\\phi}{2},$$ which\nindicates that *no new information* about $\\phi$ is obtained by jointly\nmeasuring in the commuting basis. That is, measurements in $\\mathcal{O}$\nmerely replicate the $Y$-basis result.\n\nAdding a commuting basis (e.g., $\\mathcal{O} = Y + \\alpha I$) provides\n*no independent data* about $\\phi$. Only by measuring in *non-commuting\nbases* (e.g., $X$- and $Y$-bases) can we reconstruct the phase. This\nprinciple is foundational to quantum state tomography and phase\nestimation protocols like quantum Fourier transform.",
      "word_count": 1372
    },
    {
      "title": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "content": "Recall the earlier spacekime formulation of the complex-time\nrepresentation (kime, $\\kappa = t e^{i\\theta}$) of repeated measurements\ncorresponding to random draws from a time t-dependent kime-phase\ndistribution ($\\theta \\sim \\Phi(t)$). As the kime-phase is unobservable,\nwe will explore strategies for introducing kime-measurement schemes,\nwhich are inspired by the quantum mechanical approach for recovering (or\nestimating) of the wavefunction phase, by taking repeated measurements\nin different non-commutative bases. Specifically, we'll extend\n\"commutation\" of variables (operators) in QM to distribution-action in\nkime-representation. This will provide a way to probe the enigmatic\nkime-phase by evaluating the action of different (time-dependent)\nkime-phase distributions $\\Phi(t)$ on the class of kime-test functions,\nwhere the phase-distributions resemble the different bases in QM.\n\n## Approach 1\n\n*Approach 1* is a conceptual roadmap for designing a *kime-measurement*\nprotocol that *mimics* quantum mechanical strategies for recovering\nwavefunction phase via repeated measurements in non-commuting bases.\nQuantum mechanics uses multiple, incompatible measurement bases to glean\npartial information about the complex amplitudes (including phases) of a\nquantum state. This approach attempts an *analogous* scheme in the kime\nframework by defining multiple families of kime-test functions - *“kime\noperators”* - whose “actions” on a hidden phase distribution $\\Phi(t)$\ncan reveal partial information about $\\Phi$.\n\nEffectively, we probe the “kime-phase” distribution $\\Phi(t)$ by\nevaluating *several distinct* functionals (i.e., the actions of $\\Phi$\non different test functions). If these families of test functions are in\nsome sense “*incompatible*” (akin to *non-commuting bases*), then\nmultiple measurements reveal more details (information) about the phase\ndistribution. This is analogous to how QM measurements in multiple bases\nproble the phase of the quantum wavefunction.\n\n*In standard QM*, we measure a state $\\lvert\\psi\\rangle$ in different\nbases, e.g., the $\\{\\lvert x\\rangle\\}$ basis for position, the\n$\\{\\lvert p\\rangle\\}$ basis for momentum. These bases are represented by\n*non-commuting observables* $\\hat{X}$ and $\\hat{P}$. *Each measurement*\nyields a probability distribution\n$\\lvert\\langle x\\mid \\psi\\rangle\\rvert^2$ or\n$\\lvert\\langle p\\mid \\psi\\rangle\\rvert^2$. Together, these distributions\nprovide partial constraints on the full complex wavefunction (including\nthe relative phases). *The non-commutation* ensures we *cannot* measure\nboth bases simultaneously with arbitrary precision; hence we need\nrepeated trials (fresh copies of the state $\\lvert\\psi\\rangle$) to\nmeasure across different bases.\\\nThus, *phase retrieval* in QM is intimately linked to measuring multiple\nincompatible observables. Each “basis” picks up a different interference\npattern, revealing complementary aspects of the wavefunction’s phase\nstructure.\n\nIn the **Kime Picture**, we similarly probe the phase distribution\n$\\Phi(t)$ by posing that the *kime-magnitude*, i.e., the usual time,\n$t = |\\kappa|$, and the *kime-phase* $\\theta$ is drawn from an unknown\ndistribution $\\Phi(\\theta; t)$. We can treat $\\theta$ as a “random\nquantity” unobservable in a direct sense - similar to how in standard\nQM, the wavefunction’s global phase is unobservable, but relative phases\nmanifest in interference patterns. Our goal is to devise a scheme akin\nto “probing $\\theta$” by making repeated measurements that “test” the\ndistribution $\\Phi$ with different *functionals* or *operators*, each of\nwhich partially encodes phase information.\n\nFollowing the classical distribution theory formalism, we'll consider\nthe phase distirbution $\\Phi(\\theta; t)$ as a generalized function or\nmeasure. We only “see” $\\Phi$ indirectly via its *action* on test\nfunctions $f(\\theta)$, e.g.,\n$\\langle \\Phi,\\, f\\rangle \\;=\\;\\int f(\\theta)\\,\\Phi(\\theta;t)\\,d\\theta.$\nA single *test function* $f$ can be viewed as a single “measurement\nsetting,” from which we glean\n$\\text{(Measured value)} \\;=\\;  \\int f(\\theta)\\,\\Phi(\\theta;t)\\,d\\theta.$\nThat is analogous to measuring an observable $\\hat{A}$ in quantum\nmechanics and reading out $\\langle \\hat{A}\\rangle_\\psi$.\n\nTo capture *kime-phase* information, we need multiple families of test\nfunctions that do *not* commute in some sense, i.e. cannot be\nsimultaneously diagonalized or that “probe” $\\Phi$ in *incompatible*\nways. For instance, we can define two bases:\n\n-   *Basis 1*: A set of *wavelet*-like test functions\n    $\\{f_\\alpha(\\theta)\\}$.\\\n-   *Basis 2*: A set of *harmonic* or Fourier-like test functions\n    $\\{g_\\beta(\\theta)\\}$.\n\nThe notion of “*non-commutation*” can be mirrored by the fact that the\nexpansions in $f_\\alpha$ vs. $g_\\beta$ do *not* trivially coincide or\ndiagonalize $\\Phi$. In practice, we can impose a cross-condition such\nthat\n\n$$\\int f_\\alpha(\\theta)\\,g_\\beta(\\theta)\\,d\\theta \\;\\neq\\; 0  \\quad\\text{(overlaps in expansions)}.$$\nThen measuring the “action” of $\\Phi$ on $\\{f_\\alpha\\}$ or on\n$\\{g_\\beta\\}$ yields *complementary* (but not redundant) constraints on\n$\\Phi$.\n\nTo mimic *quantum tomography*, assume we have many repeated data sets\n(or repeated time-series) $\\{f_i(t)\\}$. In each data set, we effectively\n“test” $\\Phi$ with one set of functions $\\{f_\\alpha\\}$. In another set\nof repeated measurements, we test $\\Phi$ with $\\{g_\\beta\\}$. Those\ndifferent test families might be *incompatible* in the sense that\nmeasuring one precludes measuring the other on the *same* sample - like\na quantum system being collapsed by a certain measurement. But with\nmultiple independent samples (fresh draws from $\\theta$), we can\nsystematically measure *both* families across runs.\n\nPragmatic implementations of an operational kime-phase “measurement”\nscheme for real experiments requires further considerations. With\n*actual data*, the “kime phase” $\\theta$ is not literally an angle that\ncan be dialed up on an apparatus. Rather, each repeated measurement or\nrepeated trial corresponds to a random draw from\n$\\theta\\sim\\Phi(\\theta;t)$. To “test” $\\Phi(\\theta)$ with the function\n$f_\\alpha$, we need to design an analysis procedure that maps each\ntrial’s observed data into an *aggregate* “score,” effectively\n$\\langle \\Phi, f_\\alpha\\rangle$.\n\nOne scheme relies on defining\n$\\widehat{A}_\\alpha \\;=\\;\\frac{1}{N}\\,\\sum_{i=1}^N f_\\alpha\\bigl(\\widehat{\\theta}_i\\bigr),$\nwhere $\\widehat{\\theta}_i$ is an *inferred* or *proxy* estimate of the\nphase from trial $i$. Or more simply, perform an integral transform on\nthe repeated time-series so that the final numeric result is “the\ndistribution’s action on $f_\\alpha$.” To capitalize on the QM\nnon-commutative operators insights, we can define *operators*\n$\\widehat{F}_\\alpha$ and $\\widehat{G}_\\beta$ whose matrix elements in a\nsuitable “kime Hilbert space” do *not commute*. Then,\n$\\widehat{F}_\\alpha$ and $\\widehat{G}_\\beta$ represent two measurement\nfamilies of $\\theta$.\\\nIn practice, these are realized by *distinct data analyses or distinct\nexperimental set-ups* that yield functionals of $\\Phi$. The specific\ndetails are more subtle, because standard quantum commutation\n$[\\hat{X},\\hat{P}]\\neq 0$ is a strict operator statement in a Hilbert\nspace, whereas here we have distribution-function “commutation.” But the\nspirit is that we define *distinct expansions or test function families*\nthat cannot be *simultaneously* diagonal in the “kime phase.”\n\nLet's explore some parallels between the *quantum tomography* and the\n*kime-phase tomography* . In *quantum tomography*, we measure the state\n$\\lvert\\psi\\rangle$ in many bases to reconstruct the *Wigner function*\nor the *density matrix.* Whereas, in *kime tomography*, we measure\n$\\Phi(\\theta;t)$ in many “test function expansions” or “operators” to\nreconstruct the phase distribution $\\Phi$.\\\nExamples of kime-tomography bases include *Family A* (“Wavelet Test\nFunctions) $\\{f_\\alpha(\\theta)\\}$ and *Family B* (“Fourier / Harmonic\nFunctions) $\\{g_\\beta(\\theta)\\}$. In repeated sets of trials (each trial\n= one random $\\theta_i$), we evaluate partial integrals (like sample\nmeans) that approximate\n$\\int f_\\alpha(\\theta)\\,\\Phi(\\theta)\\,d\\theta, \\quad \\int g_\\beta(\\theta)\\,\\Phi(\\theta)\\,d\\theta.$\nBy combining these measurement outcomes across multiple $\\alpha,\\beta$\n(assuming having enough repeated runs), we invert or fit $\\Phi$ subject\nto consistency with all the observed integrals.\n\nIf the sets $\\{f_\\alpha\\}$ and $\\{g_\\beta\\}$ have sufficient\n“resolution” (like a complete basis in $\\theta$ space), we may recover\n$\\Phi$. The *“non-commuting”* aspect reflects the need for separate runs\nto measure $\\{f_\\alpha\\}$ vs. $\\{g_\\beta\\}$, just as QM requires\nseparate runs to measure position vs. momentum.\n\nIn spacekime representaiton, *each measurement* is the distribution’s\naction on one family of test functions. Since each trial (or repeated\nmeasurement) gives one “realization” of $\\theta$, we gather a large data\nset from many trials to estimate the integral\n$\\int f(\\theta)\\,\\Phi(\\theta)\\,d\\theta$. *Non-commutation* arises if\nthese families of test functions are not simultaneously diagonalizable\nor do not form a single “common basis.” In practice, that means we\ncannot glean the integrals for *both* sets from a *single* run. Instead,\nwe record separate sets of repeated experiments, just as in QM we can’t\nmeasure $\\hat{X}$ and $\\hat{P}$ simultaneously on the same wavefunction\ncopy. *Kime-phase tomography* combines the results from multiple\n“incompatible” families to gain partial (or complete) information about\nthe distribution $\\Phi(\\theta)$. This is analogous to how measuring a\nquantum state in multiple bases yields full wavefunction tomography,\nincluding phase information.\n\nThis *Approach 1* defines a kime-measurement protocol analogous to\nquantum mechanical phase retrieval by interpreting $\\Phi(\\theta)$ as a\nmeasure to be tested by distinct families of test functions (like\nmeasuring different observables), gather repeated data sets for each\nfamily separately, and use the results to reconstruct or constrain\n$\\Phi(\\theta)$ more fully than any single “basis” alone would allow.\n\n### Strategy 1 Example\n\nThis example demonstrates the *concept* of *kime-phase tomography* using\ntwo families of test functions (*wavelet-like* and *Fourier-like*) to\nprobe a phase distribution $\\Phi(\\theta)$. The simulation involves:\n\n1.  Defining a “kime-phase distribution” $\\Phi(\\theta)$ supported on\n    $\\theta \\in [-\\pi,\\pi]$; in this case, a simple mixture of *two\n    Normal (or [von\n    Mises](https://en.wikipedia.org/wiki/Von_Mises_distribution))\n    distributions*.\\\n2.  Simulating repeated measurements, where each measurement corresponds\n    to drawing a random phase $\\theta_i \\sim \\Phi,\\ 1\\leq i\\leq N=200$\n    and generating a synthetic time-series.\\\n3.  Adopting two *measurement settings* - the “*Wavelet test basis*” and\n    the “*Fourier test basis.*”. Half the measurements use the \"Wavelet\"\n    family, and the other half use the \"Fourier\" family, mimicking\n    non-commuting bases. For simplicity, the *wavelet family* uses\n    $f_\\alpha(\\theta)=e^{-(\\alpha\\theta)^2} \\times \\cos(\\alpha\\theta)$;\n    this is not a real wavelet, just a \"toy wavelet-like function\".\n    Similarly, the Fourier family basis uses\n    $g_\\beta(\\theta) = \\cos(\\beta \\theta)$ and\n    $\\alpha,\\beta \\in \\{1,2,3\\}$.\n4.  Within each basis and for each measurement, we estimate an integral\n    $\\int f_\\alpha(\\theta)\\,\\Phi(\\theta)\\,d\\theta$ or\n    $\\int g_\\beta(\\theta)\\,\\Phi(\\theta)\\,d\\theta$ by collecting a\n    “score” from the data, but from discrete random draws of $\\theta$.\n    This involves quick numerical approximations to\n    $\\mathbb{E}[f_\\alpha(\\theta)] = \\int f_\\alpha(\\theta) \\Phi(\\theta) d\\theta$\n    and\n    $\\mathbb{E}[g_\\beta(\\theta)] = \\int g_\\beta(\\theta) \\Phi(\\theta) d\\theta.$\n5.  By combining or ensembling all the measured integrals across these\n    two “*non-commuting*” families, we can solve for, fit, or\n    approximate the kime-phase distribution $\\Phi(\\theta)$. For\n    instance, one estimate of the kime-phase distribution can be\n    obtained by\n\n$$\\Phi_{est}(\\theta) = \\sum_{\\alpha \\in A} c_\\alpha \\times f_\\alpha(\\theta) + \\sum_{\\beta \\in B}  d_\\beta  \\times g_\\beta(\\theta).$$\n\n**Note**: This example is *proof-of-concept.* In a real fMRI or\ntime-series context, we would need to design more sophisticated\nmeasurement operators (*test functions*) that link the phase $\\theta$ to\nthe measured signals. However, this simulation shows *in principle* the\nessential logic of **using multiple incompatible measurement families**\nto pin down (estimate) the phase distribution.\n\n\nIn this simulation, the **hidden distribution** $\\Phi(\\theta)$ is a\nmixture of “circular Gaussians” (or an approximate approach with normal\ndistributions truncated to $[- \\pi,\\pi]$). Simulating *repeated\nmeasurements* reflects sampling $\\theta_i \\sim \\Phi$. In a real\nscenario, each $\\theta_i$ would come from an *observable time series*,\nbut for demonstration we directly sample $\\theta_i$. We used *two\nmeasurement families*, a *wavelet-like* test functions\n$f_\\alpha(\\theta)$ and a *Fourier-like* test functions\n$g_\\beta(\\theta)$, which “non-commuting” in the sense that measuring\n$\\theta_i$ with wavelet functions is separate from measuring with\nFourier functions. For the wavelet or Fourier basis, we get\n$\\{f_\\alpha(\\theta_i)\\}$ or $\\{g_\\beta(\\theta_j)\\}$ by averaging over\nmany $\\theta_i$ to approximate\n$\\int f_\\alpha(\\theta)\\,\\Phi(\\theta)\\,d\\theta$. We define a parametric\nform\n$\\Phi_{\\text{est}}(\\theta) = \\sum c_\\alpha\\,f_\\alpha(\\theta) + \\sum d_\\beta\\,g_\\beta(\\theta)$\nand fit the coefficients by matching the measured integrals to the\npredicted integrals from $\\Phi_{\\text{est}}$.\\\nWhen the the underying phase distribution is complicated, or if we only\nhave a small set of test functions, the reconstruction may be rough.\nIncreasing the number of test functions (i.e., bigger sets\n$\\alpha,\\beta$) and the number of repeated measurements $N$ yields a\nbetter approximation, akin to how quantum state tomography improves with\nmore measurement bases and more samples.\n\nIn practice, to extend this approach to actual fMRI, or other\nlongitudinal time-series data, requires defining how each measurement\nsetting (wavelet or Fourier basis) is *implemented* as an operation on\nthe actual time series. For example, *wavelet basis* means convolving\nthe measured fMRI signal (in time) with a certain wavelet, then\nextracting a phase-dependent amplitude. For the *Fourier basis*, we can\nuse FFT over a certain window that yields the coefficient at frequency\n$\\omega$.\\\nThen, we *map* each measurement outcome to a “test function value” that\ndepends on the hidden $\\theta$. In real data, $\\theta$ is not directly\nknown, so we might do a specialized inference step or calibration.\nEnsembling (or aggregating) these measurement outcomes over many trials\nor repeated runs estimates the integrals\n$\\int f_\\alpha(\\theta)\\,\\Phi(\\theta)\\,d\\theta$.\n\nIn the experiment above, the *estimated* phase distribution\n$\\widehat{\\Phi}$ (6-modes) roughly resembles the *true* distribution\n(bi-modal), which has two well-localized modes. In our experiment, the\n*estimated* phase distribution uses only a small basis of *six total\nfunctions*, $f_{1,2,3}$ wavelet-like and $g_{1,2,3}$ Fourier-like. This\ncan be too rigid or produce “ringing” artifacts, adding more wavelet\nscales or higher Fourier frequencies can improve the approximation.\nAlso, the balance between wavelet and Fourier terms can be adjusted, so\nthat the distribution can better capture both peaked modes and smooth\ntails. In principle, using a bigger basis or a more flexible family\nsupports smoother approximations with fewer spurious “wiggles.” Some\nwavelet or polynomial bases might be more natural for capturing\nmulti-modal phase priors with rapidly decaying tails. For instance,\n*Legendre polynomials* or *higher B-splines* on $[-\\pi, \\pi]$ might\nyield fewer spurious oscillations than a small set of cosines.\n\nWhen the naive least-squares fit yields *overfitting* (excessive\nhigh-frequency structure), a *penalized regression* approach can help.\nFor instance, *Tikhonov or Ridge penalty*\\\n$\\min_{c,d} \\left [\\underbrace{\\sum(\\text{residuals}^2)}_{fidelity} + \\underbrace{\\lambda\\,\\bigl(\\|c\\|^2 + \\|d\\|^2\\bigr)}_{regularizer}\\right ],$\nfor some tuning $\\lambda>0$ shrinks large oscillatory coefficients in\nthe wavelet or Fourier expansions. This encourages fewer basis functions\nwith large coefficients, simplifying the distribution’s shape.\n\nAssuming the phase distribution as a function in a *reproducing kernel\nHilbert space (RKHS)*, then the solution is found by penalizing large\nsecond derivatives or local curvature. Also, in practice we can adopt a\nparametric mixture family closer to the true phase distribution. For\ninstance, if the *true* distribution is a *two-component mixture of von\nMises* ($vM$) (or circular Gaussians), it might be more direct to\n*parameterize* the estimate with the *same family*, e.g.\n$\\Phi(\\theta) \\;=\\; w\\,vM(\\theta;\\mu_1,\\kappa_1) + (1-w)\\,vM(\\theta;\\mu_2,\\kappa_2).$\nThen fit the parameters $(w,\\mu_1,\\kappa_1,\\mu_2,\\kappa_2)$ by matching\nthe measured integrals or the direct samples. This strategy is analogous\nto *modeling the distribution with a mixture of 2–3 ‘peaks’*, then solve\nfor mixture weights and location/scale. It naturally yields a shape that\nhas *only two modes* - no artificial high-frequency bumps.\n\nWhen dealing with only a small number of repeated measurements (draws of\n$\\theta$) in each “basis,” then *sampling noise* can create spurious\ndetail *increase* $N$ (the number of repeated draws) and/or *measure\nmore integrals* in each basis or in additional “incompatible” bases.\nWith more data constraints, the solution is less likely to overfit small\nrandom fluctuations.\n\n#### *Renormalization* of the Estimated Kime-phase Distibution\n\nIn the *Approach 1* simulation, we see that the estimated (recovered)\nkime-phase distribution $\\hat{\\Phi}(t)$ may have negative values, which\nis unphysical/unrealistic. This can happen due to using a simple linear\ncombination of wavelet-like and Fourier-like basis functions. Of course,\nthis is an oversimplified protocol and *renormalization* of the\nreconstructed distribution $\\hat{\\Phi}(t)$ may address the issue by\nenforcing *positivity* and *unitarity* for interpreting $\\Phi(\\theta)$\nas a *probability density.*\n\nIn our simulation, we defined a *parametric estimate* of the form\n$$\\Phi_{\\text{est}}(\\theta) = \\sum_{\\alpha} c_{\\alpha} \\, f_{\\alpha}(\\theta)\n+ \\sum_{\\beta} d_{\\beta} \\, g_{\\beta}(\\theta),$$ where the basis\nfunctions $f_{\\alpha}$ and $g_{\\beta}$ typically take *positive* and\n*negative* values (e.g., wavelets, cosines). Thus, the linear\ncombination can easily go negative at some $\\theta$ if the coefficients\n$\\{c_{\\alpha}, d_{\\beta}\\}$ produce partial cancellations or overshoot.\n\nthere are multiple *renormalization* strategies that can be used to\naddress this issue and enforce kime-phase density properties on the\nreconstructed $\\Phi_{\\text{est}}(\\theta)$. For instance, using\nunconstrained least-squares merely *minimizes squared errors* in\nmatching integrals $\\int f_{\\alpha}\\,\\Phi(\\theta)\\,d\\theta$ or\n$\\int g_{\\beta}\\,\\Phi(\\theta)\\,d\\theta$, without imposing *any\npositivity constraint*. So, there is no mechanism to prevent negative\nportions in $\\Phi_{\\text{est}}$.\n\nA naive renormalization by *rescaling* the final estimate,\n$$\\Phi_{\\text{est}}(\\theta) \\leftarrow\\; \\frac{\\Phi_{\\text{est}}(\\theta)} {\\int \\Phi_{\\text{est}}(\\theta)\\,d\\theta}$$\nensures *unitarity*, the integral is $1$, yet, the function can still\nhave negative regions.\n\nExponential parametrization provides a straightforward fix to represent\n$\\Phi_{\\text{est}}$ in an *exponential family* form, e.g.,\n$\\Phi_{\\text{est}}(\\theta) = \\exp\\Bigl(\\,\\sum_{\\alpha} c_{\\alpha}\\,f_{\\alpha}(\\theta)\\Bigr),$\nor in a typical *Fourier or wavelet basis*,\n$$\\Phi_{\\text{est}}(\\theta) = \\exp\\Bigl(\\,\nc_{0} +  \\sum_{k=1}^K \\bigl[c_k\\,\\cos(k\\,\\theta) \\;+\\; s_k\\,\\sin(k\\,\\theta)]\\Bigr).$$\n\nThen automatically, $\\Phi_{\\text{est}}(\\theta)$ is *positive*, but still\nneed to *renormalize* by dividing by its integral. Of course, this\nover-complexifies the problem, as the integrals\n$\\int f_{\\alpha}(\\theta)\\,\\Phi_{\\text{est}}(\\theta)\\,d\\theta$ may no\nlonger be linear in the coefficients $c_{\\alpha}$.\n\nAnother strategy is to choose basis functions that are *themselves*\nnonnegative, e.g. B-splines with compact support, or certain radial\nbasis expansions.\\\nThen a linear combination with *nonnegative coefficients* ensures\n$\\Phi_{\\text{est}}(\\theta)\\ge 0$. In that case, we need to solve a\n*constrained* problem: $c_{\\alpha}\\ge 0$, which can be done with\nquadratic programming or more advanced optimization tools.\n\nPost-processing *clampping* is a simpler (though less principled) fix\nrequiring computing the unconstrained estimate\n$\\Phi_{\\text{est}}(\\theta)$, setting all negative values to $0$, and\nrenormalizing over $\\theta$.\\\nThe “clamp-and-normalize” approach might not preserve integral\nconstraints as precisely as a fully constrained method, but it ensures a\nnonnegative final result.\n\nAlternatively, we can normalize the distribution once we get a function\n$\\Phi_{\\text{est}}(\\theta) \\ge 0$, by\n$$\\Phi_{\\text{est}}(\\theta) \\leftarrow\\;  \\frac{\\Phi_{\\text{est}}(\\theta)}\n{\\int_{-\\pi}^{\\pi} \\Phi_{\\text{est}}(\\theta)\\,d\\theta}.$$\n\nIn practice, we can normalize over the grid `theta_grid` the functional\nvalues of `phi_est_grid`, by `phi_est_int <- sum(phi_est_grid) * dtheta`\nand `phi_est_grid <- phi_est_grid / phi_est_int`, assuming\n`phi_est_grid >= 0` so the integral is well-defined and positive.\\\nThen `phi_est_grid` forms a valid probability density function (pdf)\nover $[- \\pi,\\pi]$.\n\n#### Opportunities for Enhancement\n\n-   *Strengthen the Analogy to Quantum Non-Commutativity*: In QM,\n    non-commuting observables (e.g., position/momentum) enforce a\n    fundamental limit on simultaneous measurements. In spacekime,\n    \"incompatibility\" is loosely defined as non-orthogonal test\n    functions (e.g., wavelet vs. Fourier). However, this does not\n    directly replicate the operator-based non-commutativity of QM. The\n    *no-overlap condition*\n    $\\int f_\\alpha(\\theta)\\,g_\\beta(\\theta)\\,d\\theta \\;\\neq\\; 0$ is\n    insufficient to guarantee complementary information or a no-go\n    theorem similar to the uncertainty principle. We can consider\n    formalizing incompatibility using operator theory or\n    signal-processing uncertainty principles (e.g., time-frequency\n    trade-offs in wavelets vs. Fourier), and defining a commutator-like\n    metric for test function families to quantify their mutual\n    informativeness.\n\n-   *Lack of Uniqueness and Stability*: The linear combination\n    $\\Phi_{\\text{est}}(\\theta) = \\sum c_\\alpha\\,f_\\alpha(\\theta) + \\sum d_\\beta\\,g_\\beta(\\theta)$\n    assumes $\\Phi$ lies in the span of the chosen basis. If the test\n    functions do not form a complete basis or are ill-conditioned, the\n    inversion may be non-unique or unstable. We can explore using\n    overcomplete dictionaries (e.g., frames) or adaptive basis selection\n    and regularizing the inversion (e.g., Tikhonov, sparsity penalties)\n    to handle noise and undersampling.\n\n-   *Ambiguity in Phase Interpretation*: The kime-phase $\\theta$ is\n    treated as a random draw, but its physical meaning and connection to\n    time-series data (e.g., fMRI) needs to be explicated. Unlike QM\n    phases, which arise from wavefunction structure, $\\theta$ does not\n    have a direct operational definition. Is it feasible to ground\n    $\\theta$ as a measurable quantity (e.g., phase coherence in\n    oscillatory signals) and define test functions as transformations of\n    observable data (e.g., wavelet coefficients)?\n\n-   *Simplistic Data Generation*: The simulation directly samples\n    $\\theta\\sim\\Phi_{true}(t)$, bypassing the critical step of inferring\n    $\\theta$ from the simulated fMRI time-series data. Perhaps explicate\n    the mapping between signals and $\\theta$, introducing noise and\n    bias. Is it feasible to simulate time-series signals (e.g.,\n    synthetic fMRI) with known $\\theta$ dependence? Can we use\n    signal-processing methods (e.g., Hilbert transform, wavelet ridges)\n    to estimate $\\theta$ from data?\n\n-   *Inefficient Data Usage*: Splitting trials into wavelet/Fourier\n    groups mimics QM measurement collapse but wastes data. In reality,\n    each $\\theta_i$ could be analyzed with multiple test functions.\n    Perhaps allow all test functions to act on each trial’s data to\n    improve the statistical power. How to model the correlations between\n    test function outputs.\n\n-   *Operator-Theoretic Framework*: How to represent test functions as\n    operators in a Hilbert space, with commutators defining\n    incompatibility? Perhpas use spectral theory to formalize\n    \"measurements.\" Can we derive bounds on joint resolution of $\\Phi$\n    using wavelet/Fourier families, similar to time-frequency\n    uncertainty.\n\n-   *Algorithmic Improvements*: Shoud we add $L_1/L_2$ penalties to the\n    cost function to suppress spurious oscillations. Alternatively, see\n    if Bayesian inference incorporating priors over $\\Phi$ (e.g.,\n    smoothness, sparsity) can help quantify uncertainty in estimates.\n\n### Analogies between QM Non-Commutativity and SK Representation with Incompatible Functional Bases\n\nIn quantum mechanics (QM), we have an algebra of operators on a Hilbert\nspace where two observables $\\hat{A}$, $\\hat{B}$ *do not commute* if\n$[\\hat{A}, \\hat{B}] \\neq 0$. Non-commutation implies fundamental limits\non simultaneous measurability (Heisenberg uncertainty). Mathematically,\nthe *commutator* $[\\hat{A}, \\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}$\ncaptures the idea that measuring $\\hat{A}$ can disturb the outcome\ndistribution of $\\hat{B}$, or vice versa.\n\nBy contrast, in the “kime-phase distribution” we have a distribution\n$\\Phi(\\theta)$ and a set of test functions $\\{f_\\alpha\\}$, each giving a\nreal number $\\langle \\Phi, f_\\alpha\\rangle$. “Incompatibility” reflects\n“families of test functions that cannot be measured on the *same* sample\nrealization of $\\theta$.” But *that* notion of “cannot measure both\nsimultaneously” is not guaranteed by an operator algebra or a\nfundamental commutator—rather, it’s due to how we label draws from\n$\\theta \\sim \\Phi$ and choose to do *different integral transforms* on\nseparate runs. Hence, bridging from “nonorthogonal test functions” to a\nrigorous “operator non-commutation” requires new structural definitions.\n\n-   Turning Test Functions into Operators: One way to introduce an\n    operator-theoretic perspective is to treat each test function\n    $f(\\theta)$ as either:\n    1.  *A multiplication operator*:\n        $\\hat{F} : \\Psi(\\theta) \\;\\mapsto\\; f(\\theta) \\,\\Psi(\\theta)$\n        for $\\Psi(\\theta)$ in some function space (like $L^2$ over\n        $\\theta\\in [-\\pi,\\pi]$), or\n\n    2.  *An integral transform operator*:\n        $\\hat{F}(\\Psi)(\\theta) = \\int K_f(\\theta, \\theta')\\,\\Psi(\\theta')\\,d\\theta'.$\n\nIn both cases, each “test function” becomes an operator $\\hat{F}$ and we\ncan attempt to *compute commutators* $[\\hat{F}, \\hat{G}]$ for distinct\ntest functions $f$ and $g$. If $\\hat{F}$ is multiplication by\n$f(\\theta)$ and $\\hat{G}$ is multiplication by $g(\\theta)$, then\n$[\\hat{F}, \\hat{G}] = 0$ trivially, because multiplication by $fg$ is\ncommutative. So that alone does *not* replicate quantum non-commutation.\nWhen $\\hat{F}$ or $\\hat{G}$ includes derivatives, convolution kernels,\nor partial Fourier transforms, one can get non-zero commutators.\n\n-   Reframe an “Uncertainty Relation” in $(\\theta)$-Space: Even if we\n    define a nontrivial operator algebra, we want an *operational\n    meaning*, e.g., “it's impossible to measure both $\\hat{F}$ and\n    $\\hat{G}$ simultaneously to arbitrary precision.” In QM, that arises\n    from the Hilbert-space structure and the Born rule. In the\n    kime-phase approach, we can enforce a “Hilbert norm” $\\|\\Psi\\|$ on\n    distributions or wavefunctions in $\\theta$ and let “measurement”\n    correspond to computing $\\langle \\Psi, \\hat{F}\\Psi\\rangle$.\n    When$[\\hat{F}, \\hat{G}] \\neq 0$, we can explore a\n    Cauchy–Schwarz–like inequality that implies\n    $\\Delta F\\,\\Delta G \\ge \\text{constant}.$\n\n-   Using Information-Theoretic Measures of Complementarity: Another\n    route to formalizing “incompatibility” is to define how\n    *informative* each measurement family is about $\\theta$. Then one\n    might show that if two families of test functions are “maximally”\n    complementary, measuring one family leaves little or no information\n    about the other. This can look like an uncertainty principle in the\n    sense\n    $H(\\theta \\mid \\text{Family A data})  + H(\\theta \\mid \\text{Family B data}) \\;\\ge\\; \\text{some bound,}$\n    where $H$ is an entropy measure, and “Family A data” is the set of\n    integrals with the wavelet basis. If the two families produce highly\n    overlapping expansions, we can glean both from the same data. But if\n    the expansions are “complementary” in a strong sense, it might be\n    impossible to gain both sets of integrals from a single sample. That\n    suggests a trade-off reminiscent of quantum complementarity.\n\n-   *Define a Kime Hilbert Space*: Let the “states” be (equivalence\n    classes of) wavefunctions $\\Psi(\\theta)$ or distributions\n    $\\Phi(\\theta)$. Impose an inner product\n    $\\langle \\Psi_1, \\Psi_2\\rangle$. *Map Each “Test Function Family” to\n    Self-Adjoint Operators* and instead of just computing\n    $\\int f(\\theta)\\,\\Phi(\\theta)\\,d\\theta$, define an operator\n    $\\hat{F}$ on the Hilbert space. Possibly $\\hat{F}$ includes\n    differentiation or convolution so that\n    $\\hat{F}\\hat{G} \\neq \\hat{G}\\hat{F}$. Then, evaluate\n    $[\\hat{F}, \\hat{G}]$ for two such operators. If it is nonzero,\n    investigate the consequences for “observables.” Specify how\n    “observing $\\hat{F}$” changes (or not) the state $\\Psi$. It may not\n    be nontrivial to get a truly quantum-like “collapse.” Skipping the\n    collapse or disturbance postulate, we might only get partial\n    analogies to quantum complementarity, not a full “non-commuting\n    measurement” principle. Given that $[\\hat{F},\\hat{G}]\\neq 0$, check\n    if there is a limit or bound on the simultaneous determination of\n    the integral values of $f(\\theta)$ and $g(\\theta)$. Possibly define\n    an “uncertainty relation” in terms of standard deviations or\n    entropies of these integral measurements.\n\n### Refined Simulation Experiment (Approach 1)\n\nIn this revised simulation, we use a Bayesian framework starting with\nsome prior phase model, e.g., Laplace phase distribution, and using\nevidence-based iterative refinement to estimate the Bayesian posterior\nphase distribution, still using a “forward model”\n\n$$x(t) = \\mathcal{M}\\bigl(t,\\theta; \\,\\boldsymbol{\\phi}\\bigr) +\\epsilon(t),$$\nwhere $\\theta$ is the randomly drawn “kime-phase” from the currently\nestimated Bayesian posterior distribution, still $\\theta(t)$ if\ntime-varying, $\\mathcal{M}$ is a known or hypothesized function that\ngenerates the fMRI time-series from $\\theta$, $\\boldsymbol{\\phi}$ stands\nfor additional model parameters (e.g., amplitude, baseline, shape, HRF\nin fMRI, etc.), and $\\epsilon(t)$ is noise, e.g., Gaussian or realistic\nRician fMRI noise with correlation in time. We can simulate many\nrepeated fMRI runs or “trials,” each with a newly sampled $\\theta_i$.\nFor each run $i$, we produce\n$x_i(t) = \\mathcal{M}\\bigl(t,\\theta_i; \\,\\boldsymbol{\\phi}\\bigr) + \\epsilon_i(t).$\nWe will explore specific functions $\\mathcal{M}$ that generate the fMRI\ntime-series from the randomly drawn $\\theta$, and update the entire\nsimulation experiment.\n\nThe revised simulation protocol involves the following steps, which are\nrepeated until convergence to a stable estimate for the distribution\n$\\Phi(\\theta)$.\n\n1.  *Prior*: Start with a prior distribution over $\\theta$, for instance\n    $\\Phi_0(\\theta)$.\\\n2.  *Forward Simulation*: For each trial $i$, sample\n    $\\theta_i\\sim \\Phi_{n}(\\theta)$, the *current* posterior or prior,\n    depending on the iteration strategy, generate a noisy signal\n    $x_i(t)$ via\n    $\\mathcal{M}(t,\\theta_i;\\boldsymbol{\\phi}) + \\epsilon_i(t)$.\\\n3.  *Inference*: Given $x_i(t)$, update the belief about $\\theta_i$ and\n    possibly about $\\boldsymbol{\\phi}$, producing a posterior\n    $\\Phi_{n+1}(\\theta)$.\n\nLet's start with an *initial* prior, e.g. a **Laplace** or **Gaussian**\ndistribution over $\\theta$,\n$\\Phi_0(\\theta) \\;\\propto\\; \\exp\\bigl(-|\\theta|/b\\bigr)\\quad\\text{(Laplace)},$\nor\n$\\Phi_0(\\theta) \\;\\propto\\; \\exp\\bigl(-\\tfrac{\\theta^2}{2\\sigma^2}\\bigr),$\nrestricted to $[- \\pi,\\pi]$. For time-varying $\\theta(t)$, we can\nspecify a small random walk prior or a correlated prior in time.\n\nTo explicate the forward model, in each “trial” $i$, we draw\n$\\theta_i \\sim \\Phi_n(\\theta)$, generate the synthetic fMRI time series\n$x_i(t) = \\mathcal{M}\\bigl(t,\\theta_i;\\,\\boldsymbol{\\phi}\\bigr) + \\epsilon_i(t)$,\nand assume the noise $\\epsilon_i(t)$ is Gaussian or Rician with temporal\ncorrelation. Again, $\\boldsymbol{\\phi}$ represents the hyerparameter\nvector containing amplitude, baseline, HRF shape, etc.\n\nGiven the observed $x_i(t)$, the Bayesian posterior distribution,\n$\\Phi_{n+1}(\\theta)$, is updated from the prior $\\Phi_n(\\theta)$ by\n$$p(\\theta \\mid x_i) \\propto  \\bigl[\\,p(x_i\\mid\\theta)\\bigr] \\Phi_n(\\theta).$$\nWe can combine data from multiple independent trials\n$\\{x_i(t)\\}_{i=1}^N$, $$p(\\theta_1,\\ldots,\\theta_N \\mid \\{x_i\\})\n\\propto\\; \\prod_{i=1}^N p(x_i \\mid \\theta_i)\\,\\Phi_n(\\theta_i).$$ In the\nspecial case when all trials share a *single* $\\theta$, which is less\nlikely in typical repeated-measure scenarios, there would be a single\nposterior. We can use *MCMC* or a *variational* method to approximate\nthe posterior distribution.\\\nAlternatively, a *particle filter* approach may keep a sample of\n“particles” for $\\theta$, propagate them forward, weigh them by the\nlikelihood of $x_i$, and resample. Yet, another option is to use\n*EM-like* iterative scheme when $\\theta$ is high-dimensional or\ntime-varying to perform partial expectation (E-step) updates. At the\nend, we get the posterior $\\Phi_{n+1}(\\theta)$. In the next iteration,\nwe sample $\\theta_i$ from that new distribution, generate new data, etc.\nWe can consider the final $\\Phi_{\\mathrm{post}}(\\theta)$ as the best\nestimate of the underlying “phase distribution.”\n\nLet's explore two alternative “forward model” functions $\\mathcal{M}$\nfor simulating fMRI time-series from a random phase $\\theta$.\n\n-   *Model A*: Sinusoidal + HRF Convolution:\n    $x(t) = \\Bigl(A\\,\\sin\\!\\bigl(\\omega \\, t + \\theta\\bigr)\\Bigr) \\ast\\; \\mathrm{HRF}(t) + \\text{drift}(t) + \\epsilon(t),$\n    where the *sinusoidal* $A\\,\\sin(\\omega t + \\theta)$ is a canonical\n    oscillation with unknown phase $\\theta$, the *convolution*\n    $(\\cdot)\\ast\\mathrm{HRF}$ is a typical fMRI hemodynamic response\n    function (e.g. a gamma function or canonical double-gamma), the\n    *drift(t)* is a low-frequency polynomial or spline to mimic slow\n    drift in real BOLD signals, $\\epsilon(t)$ is noise, e.g., *Gaussian*\n    $\\epsilon\\sim N(0,\\sigma^2)$, *Rician*, or *AR(1)* correlated noise,\n    and the *parameter vector*\n    $\\boldsymbol{\\phi} = (A,\\omega,\\mathrm{HRF\\_params}, \\dots)$.\n    Because $\\theta$ modulates the *phase* of the sinusoid, each trial\n    can shift the timing of the BOLD response in a consistent manner,\n    where we can interpret $\\theta$ as a “neuronal phase offset”.\n\n-   *Model B*: Event-Related or Block-Design Onset Times:\n    $x(t) = \\Bigl(\\sum_{k=1}^K \\mathbf{1}\\bigl[t\\in (t_k+\\theta,\\,t_k+\\theta+\\Delta)\\bigr]\\Bigr) \\ast\\;\\mathrm{HRF}(t) + \\text{drift}(t) + \\epsilon(t),$\n    where we assume an fMRI/BOLD signal with *block design* or *event\n    design* with onsets $\\{t_k\\}$, the unknown phase $\\theta$ *shifts\n    all onsets* by $\\theta$ units in time, so that the entire block or\n    event structure is advanced or delayed, convolution with the HRF\n    produce the BOLD-like signal, and the additional amplitude or\n    baseline parameters and noise are similar to *Model A* above.\n\nAssuming that the “phase” is an offset in event timing across repeated\nruns, $\\theta$ shifting the entire design is realistic. Each trial might\nplace the block/event boundary earlier or later in time; effectively we\nmeasure how well the data fits that shift. the next experiment\ndemonstrates the Bayesian approach in `R` and `Stan`. Specifically, we\nshow an *end-to-end* simulation in a *Bayesian* framework for unknown\nkime-phases $\\{\\theta_i\\}$. In this experiment, we define a forward\nmodel, a sinusoidal input at unknown phase $\\theta$ convolved with a\nsimple $HRF + noise$, simulate repeated runs, each with a fresh\n$\\theta_i\\sim \\text{Laplace}(0,b)$, and infer those phases in `Stan` via\nMCMC.\n\nRunning this simulation in a fresh `R` session requires the package\n`rstan` to be installed and a `C++` toolchain configured, see the [rstan\ndocs](https://mc-stan.org/users/interfaces/rstan)). The entire protocol\ninvolves *Stan model compilation* and running MCMC, and produces a data\nframe comparing the *true* vs. *posterior mean* kime-phases. the\nsimulation can be enhanced by changing the shape of the HRF, or by\nletting the HRF parameters be unknown and included in the Stan model, by\nmodifying the noise distribution (e.g. Rician or correlated), and by\nupdating the number of runs, sampling intervals, amplitude priors, etc.\nIn a real fMRI context, for dealing with multi-voxel data,\nmulti-parameter block designs, or a richer prior on $\\theta$, this\n*end-to-end protocol* may need to be refined.\n\n\nThis *Bayesian* approach ensures each data set informs our knowledge of\n$\\theta$ (and any other parameters). Over repeated runs, we effectively\nrefine $\\Phi(\\theta)$. It also naturally handles noise, bias, and\npartial identifiability in $\\theta$. This version of the simulation\nshows incorporating the kime representation in a Bayesian framework\nstarting with a prior *phase distribution*, a forward model for\ngenerating signals from that phase, repeated trials, and an inference\nstep that yields a posterior distribution of phases. This may be a more\nrealistic approach than the simpler simulation shown earlier that\n“directly samples $\\theta\\sim\\Phi$ and treats it as known.”\n\n## Approach 1A: A Hilbert Space and Operator Reformulation\n\nIn the above kime-phase estimation framework, the heuristic notion of\n\"*incompatibility*\" between test function families (e.g., $f_\\alpha$ vs.\n$g_\\beta$) based solely on *non-orthogonality*,\n$\\int f_\\alpha(\\theta) g_\\beta(\\theta) d\\theta \\neq 0$, does not\nguarantee complementary information. This is unlike quantum mechanics\n(QM), where non-commutativity arises from operator algebra in a *Hilbert\nspace.* Hence, this reformulation addresses that downside by defining a\nformal operator framework for the kime-test functions in\n$L^2[-\\pi, \\pi]$, computing their commutators explicitly, and exploring\nthe conditions under which non-commutativity can be achieved. This may\nprovide a more rigorous basis for *kime-phase tomography* analogous to\n*QM tomography*.\n\n**Note**: In the section below, we reflect on the meaning of\n$\\frac{d}{d\\theta}$ as a derivative operator in $L^2[-\\pi, \\pi]$ over\nthe $\\theta$-domain of $\\Phi(\\theta; t)$, not a stochastic derivative.\n$\\theta$ is a coordinate, not a process, so standard calculus applies.\nThere is no need for Itô calculus, which applies to stochastic processes\nwhere a variable (e.g., $\\theta(t)$) *evolves randomly over time* $t$,\ngoverned by a stochastic differential equation (SDE). In kime, $\\theta$\nis *sampled independently* at each $t$ from $\\Phi(\\theta; t)$, with no\nexplicit temporal dynamics (e.g., $d\\theta = \\mu dt + \\sigma dW_t$) --\nmore precisely, there’s no continuous stochastic process or differential\nequation defining how $\\theta$ evolves from $t$ to $t + dt$. Each\n$\\theta_n(t)$ is a fresh draw, not a step in a trajectory. The\nindependence of samples across $t$ means no explicit temporal\ncorrelation (e.g., autocorrelation) is modeled between $\\theta_n(t)$ and\n$\\theta_n(t')$. The derivative $\\frac{d}{d\\theta}$ operates over the\n$\\theta$-domain of the density $\\Phi(\\theta; t)$ at fixed $t$, not over\n$t$. Thus, Itô calculus is unnecessary here. The randomness is in the\nsampling process, not in a continuous evolution of $\\theta$. For a fixed\nmeasurement $n$, $\\theta_n(t)$ and $\\theta_n(t')$ are independent draws\nfrom $\\Phi(\\theta; t)$ and $\\Phi(\\theta; t')$, respectively. There’s no\ncorrelation imposed between $\\theta_n(t)$ and $\\theta_n(t')$ by a\ntemporal process—just new, independent samples at each $t$. This\nsupports the \"no explicit temporal dynamics\" claim. At the same time,\n$\\Phi(\\theta; t)$ varies with $t$ and the ensemble of samples\n$\\{\\theta_n(t)\\}_{n=1}^N$ at time $t$ has different statistical\ncharacteristics, e.g., mean $\\mu(t)$, concentration $\\kappa(t)$, than at\n$t'$. This introduces a form of *temporal dependence* in the\ndistribution, *not in the individual* $\\theta_n(t)$ trajectories.\n\n-   *Non-Commutativity*: $[\\hat{\\Theta}, \\hat{G}_\\beta] = i \\beta I$\n    holds, providing a QM-like foundation for complementary information.\n-   *Kime Context*: Operators act on $\\psi(\\theta) = \\sqrt{\\Phi}$, with\n    randomness handled via sampling for data generation, not operator\n    definition.\n\nIn the *initial Approach 1* formulation, two families of test functions\n*wavelet-like* ($f_\\alpha(\\theta)$) and *Fourier-like*\n($g_\\beta(\\theta)$) were used to probe the phase distribution\n$\\Phi(\\theta; t)$. It argues these families are \"incompatible\" if they\nare non-orthogonal, implying they should provide complementary\ninformation about $\\Phi$. However, non-orthogonality only ensures\noverlap in $L^2[-\\pi, \\pi]$, not a fundamental limit on simultaneous\nmeasurement or complementary resolution, as in QM’s uncertainty\nprinciple. In QM, operators $A$ and $B$ are non-commuting\n($[A, B] \\neq 0$) when they *cannot be simultaneously diagonalized*,\nleading to $\\Delta A \\Delta B \\geq \\frac{1}{2}|\\langle [A, B] \\rangle|$.\nThe kime framework needs a similar operator-theoretic foundation. In\nthsi *reformulation of Aproach 1*, we define the operators\n$\\hat{F}_\\alpha$ and $\\hat{G}_\\beta$ in $L^2[-\\pi, \\pi]$ corresponding\nto $f_\\alpha$ and $g_\\beta$, and compute their commutators explicitely.\n\n**Definition** [Kime Hilbert Space and Operators]. Consider the Hilbert\nspace, $L^2[-\\pi, \\pi]$, of square-integrable functions over\n$\\theta \\in [-\\pi, \\pi]$, equipped with the inner product\n$$\\langle \\psi_1, \\psi_2 \\rangle = \\int_{-\\pi}^\\pi \\psi_1(\\theta) \\overline{\\psi_2(\\theta)} d\\theta,$$\nand the norm $\\|\\psi\\|^2 = \\langle \\psi, \\psi \\rangle$. The phase\ndistribution $\\Phi(\\theta; t)$ is a *probability density* (non-negative,\n$\\int \\Phi d\\theta = 1$), and kime test functions acting on $\\Phi$ via\n$\\langle \\Phi, f \\rangle = \\int f(\\theta) \\Phi(\\theta) d\\theta$. Let's\nexplicate the operators whose expectation values yield these integrals.\n\nConsider the simplest case of *multiplication operators* where we define\n$\\hat{F}_\\alpha$ and $\\hat{G}_\\beta$ as multiplication operators\n\n-   $\\hat{F}_\\alpha \\psi(\\theta) = f_\\alpha(\\theta) \\psi(\\theta)$,\n-   $\\hat{G}_\\beta \\psi(\\theta) = g_\\beta(\\theta) \\psi(\\theta)$, where\n    $\\psi(\\theta) \\in L^2[-\\pi, \\pi]$, and $f_\\alpha, g_\\beta$ are\n    bounded functions, e.g.,\n    $f_\\alpha(\\theta) = e^{-(\\alpha \\theta)^2} \\cos(\\alpha \\theta)$,\n    $g_\\beta(\\theta) = \\cos(\\beta \\theta)$, to ensure\n    $\\hat{F}_\\alpha, \\hat{G}_\\beta$ are well-defined operators.\n\nThen, we can compute their commutator by explicating each of its two\ncomponents\n\n-   $\\hat{F}_\\alpha \\hat{G}_\\beta \\psi(\\theta) = f_\\alpha(\\theta) (\\hat{G}_\\beta \\psi(\\theta)) = f_\\alpha(\\theta) g_\\beta(\\theta) \\psi(\\theta)$,\n-   $\\hat{G}_\\beta \\hat{F}_\\alpha \\psi(\\theta) = g_\\beta(\\theta) (\\hat{F}_\\alpha \\psi(\\theta)) = g_\\beta(\\theta) f_\\alpha(\\theta) \\psi(\\theta)$.\n\nSince multiplication is commutative,\n$f_\\alpha(\\theta) g_\\beta(\\theta) = g_\\beta(\\theta) f_\\alpha(\\theta)$,\n\n$$[\\hat{F}_\\alpha, \\hat{G}_\\beta] = \\hat{F}_\\alpha \\hat{G}_\\beta - \\hat{G}_\\beta \\hat{F}_\\alpha = 0.$$\n\nTherefore, the *multiplication operators*, $\\hat{F}_\\alpha$ and\n$\\hat{G}_\\beta$ commute, implying that they can be simultaneously\ndiagonalized; their eigenfunctions are Dirac deltas\n$\\delta(\\theta - \\theta_0)$. This does not replicate QM\nnon-commutativity and provides no basis for an uncertainty relation or\ncomplementary information.\n\n**Example 1 [Commutation of Multiplication and Differentiation]**: The\nfirst example of non-commuting operators borrows intuition from QM,\nwhere the *position* $\\hat{x}=x$ and *momentum*\n$\\hat{p} = -i \\frac{d}{d\\theta}$ operators are non-commutative,\n$[\\hat{x}, \\hat{p}] = i$, which indicates their differential nature.\nLet's show that the kime *differential* and *multiplication* operators\nare *non-commutative.* Define\n\n-   $\\hat{F}_\\alpha = \\hat{\\Theta}$, the *multiplication operator* by\n    $\\theta$ (QM position-like)\n    $\\hat{\\Theta} \\psi(\\theta) = \\theta \\psi(\\theta)$, and\n-   $\\hat{G}_\\beta = \\hat{P} = -i \\frac{d}{d\\theta}$, the *derivative\n    operator* (QM momentum-like).\n\nTherefore,\n\n-   $\\hat{F}_\\alpha \\hat{G}_\\beta \\psi(\\theta) = \\hat{\\Theta} (-i \\frac{d}{d\\theta} \\psi(\\theta)) = -i \\theta \\frac{d}{d\\theta} \\psi(\\theta)$,\n    and\n-   $\\hat{G}_\\beta \\hat{F}_\\alpha \\psi(\\theta) = -i \\frac{d}{d\\theta} (\\hat{\\Theta} \\psi(\\theta)) = -i \\frac{d}{d\\theta} (\\theta \\psi(\\theta)) = -i \\left( \\psi(\\theta) + \\theta \\frac{d}{d\\theta} \\psi(\\theta) \\right)$,\n\nHence, their commutator is\n$[\\hat{F}_\\alpha, \\hat{G}_\\beta] \\psi(\\theta) = -i \\theta \\frac{d}{d\\theta} \\psi(\\theta) - \\left(-i \\psi(\\theta) - i \\theta \\frac{d}{d\\theta} \\psi(\\theta)\\right) = i \\psi(\\theta)$.\nIn operator form,\n$[\\hat{\\Theta},\\hat{P}]\\equiv [\\theta, -i \\frac{d}{d\\theta}] = i I,$\nwhere $I$ is the identity operator. This mirrors the QM canonical\ncommutation relation $[\\hat{x}, \\hat{p}] = i \\hbar$; here, $\\hbar = 1$.\n\nTo explicate the induced *uncertainty relations*, let's first examine\nthe *expectation*. Assuming $\\Phi$ is the true kime-phase density, given\na state $\\psi(\\theta) = \\sqrt{\\Phi(\\theta)}$, The *expected values* of\nthe *multiplication* and *derivative* kime operators are\n\n-   (multiplication)\n    $\\langle \\hat{F}_\\alpha \\rangle\\equiv \\langle \\hat{\\Theta} \\rangle = \\int_{-\\pi}^\\pi \\theta \\Phi(\\theta) d\\theta$,\n-   (derivative)\n    $\\langle \\hat{G}_\\beta \\rangle\\equiv \\langle -i \\frac{d}{d\\theta} \\rangle = \\int_{-\\pi}^\\pi \\sqrt{\\Phi(\\theta)} \\left(-i \\frac{d}{d\\theta} \\sqrt{\\Phi(\\theta)}\\right) d\\theta$,\n    which requires boundary conditions, e.g., periodicity or rapid decay\n    at $\\pm \\pi$.\n\nThen, the *uncertainty relation* is\n$\\Delta \\Theta \\Delta P \\geq \\frac{1}{2} |\\langle [\\hat{\\Theta}, \\hat{P}] \\rangle| = \\frac{1}{2},$\nwhere $\\hat{P} = -i \\frac{d}{d\\theta}$, providing a rigorous basis for\ncomplementary information. To linking this relation to kime-test\nfunctions, instead of using\n$f_\\alpha(\\theta) = e^{-(\\alpha \\theta)^2} \\cos(\\alpha \\theta)$ as we\ndid earlier, now we define the multiplicaiton and derivative operators\nas\n\n-   $\\hat{F}_\\alpha = \\hat{\\Theta}$, probing the \"*position*\" of\n    $\\theta$, and\n-   $\\hat{G}_\\beta = -i \\beta \\frac{d}{d\\theta}$, sensing the *scaled\n    momentum*, tied to frequency-like behavior (similar to Fourier\n    modes).\n\nThe actions\n$\\langle \\Phi, \\hat{F}_\\alpha \\Phi \\rangle = \\int \\theta \\Phi(\\theta)^2 d\\theta$\nand\n$\\langle \\Phi, \\hat{G}_\\beta \\Phi \\rangle = -i \\beta \\int \\Phi(\\theta) \\frac{d}{d\\theta} \\Phi(\\theta) d\\theta$\nyield *moments* and *derivatives* of $\\Phi$, respectively, offering\ncomplementary views and independet information about hte kime-phase\ndistribution.\n\n**Example 1 [Commutation of Convolution and Multiplication]**: The\nsecond example of non-commutation involves the convolution operator for\nwavelet-like behavior. Multiplication by a kime-test function\n$f_\\alpha(\\theta)$ doesn’t capture the spatial structure of wavelets.\nDefining $\\hat{F}_\\alpha$ as a convolution operator\n\n-   $\\hat{F}_\\alpha \\psi(\\theta) = \\int_{-\\pi}^\\pi f_\\alpha(\\theta - \\theta') \\psi(\\theta') d\\theta'$\n    (convolution),\n-   $\\hat{G}_\\beta \\psi(\\theta) = \\cos(\\beta \\theta) \\psi(\\theta)$\n    (multiplication, Fourier-like).\n\nThe pair of components of the commutator are\n\n-   $\\hat{F}_\\alpha \\hat{G}_\\beta \\psi(\\theta) = \\int_{-\\pi}^\\pi f_\\alpha(\\theta - \\theta') \\cos(\\beta \\theta') \\psi(\\theta') d\\theta'$,\n    and\n-   $\\hat{G}_\\beta \\hat{F}_\\alpha \\psi(\\theta) = \\cos(\\beta \\theta) \\int_{-\\pi}^\\pi f_\\alpha(\\theta - \\theta') \\psi(\\theta') d\\theta'$.\n\nFor this kime-test function,\n$f_\\alpha(\\theta) = e^{-(\\alpha \\theta)^2} \\cos(\\alpha \\theta)$,\n\n-   $\\hat{F}_\\alpha \\hat{G}_\\beta \\psi(\\theta) = \\int_{-\\pi}^\\pi e^{-(\\alpha (\\theta - \\theta'))^2} \\cos(\\alpha (\\theta - \\theta')) \\cos(\\beta \\theta') \\psi(\\theta') d\\theta'$,\n    and\n-   $\\hat{G}_\\beta \\hat{F}_\\alpha \\psi(\\theta) = \\cos(\\beta \\theta) \\int_{-\\pi}^\\pi e^{-(\\alpha (\\theta - \\theta'))^2} \\cos(\\alpha (\\theta - \\theta')) \\psi(\\theta') d\\theta'$.\n\nHence, the *(Convolution-Multiplication) Commutator*\n$[\\hat{F}_\\alpha, \\hat{G}_\\beta] \\neq 0$ because $\\cos(\\beta \\theta)$\nvaries with $\\theta$, while the convolution kernel shifts and scales\ndifferently. The *exact computation* is complex, but numerically for\n$\\psi(\\theta) = 1$,\n$\\hat{F}_\\alpha \\hat{G}_\\beta 1 \\neq \\hat{G}_\\beta \\hat{F}_\\alpha 1$,\nindicating non-commutativity. This aligns with the wavelet vs. Fourier\nintuition where *convolution* (local structure) and *multiplication*\n(global oscillation) probe the kime-phase $\\Phi$ *differently.*\n\nLet’s implement and validate this in a simulation confirming the\nnon-commutativity and complementarity of the *multiplication* and\n*differentiation* operators.\n\n\nNote that $[F_\\alpha, G_\\beta] \\neq 0$ confirms non-commutativity for\nconvolution vs. multiplication and $[\\Theta, D_\\beta] \\neq 0$ matches QM\nexpectations. The expectations\n$\\langle \\Phi, \\hat{F}_\\alpha \\Phi \\rangle$ and\n$\\langle \\Phi, \\hat{G}_\\beta \\Phi \\rangle$ differ, suggesting\ncomplementary information about $\\Phi$.\n\nThe next simulation reflects the *Approach 1A: Hilbert Space and\nOperator Reformulation*. This code replaces the *test functions* in\n*Apprach 1* with *operator-based measurements* (position and momentum),\nvalidates their non-commutativity via an uncertainty principle, and\nreconstructs the phase distribution using both operators.\n\n\nThe *position operator* ($\\Theta$) directly computes the mean and\nvariance of $\\theta$ and the *momentum operator* ($P$) uses the Fourier\ntransform to compute momentum variance, avoiding numerical\ndifferentiation. The *uncertainty principle validation* is realized by\ncomputing the product of variances $\\Delta \\Theta \\Delta P$ and checking\nagainst the theoretical lower bound of $0.5$. The kime-phase\n*distribution reconstruction* uses kernel density estimation (KDE) to\napproximate $\\Phi$ from samples. the simulation includes a Gaussian fit\nbased on position variance for comparison (simplified example). The\nresulting plot shows the *true distribution*, *KDE estimate*, and a\n*Gaussian fit* constrained by the position variance. The uncertainty\nproduct is printed, validating the non-commutativity of $\\Theta$ and\n$P$. This (revised) *Approach* $1A$ more rigorously embeds the phase\nestimation within a Hilbert space framework, leveraging Fourier duality\nto implement momentum measurements and explicitly testing\nquantum-inspired uncertainty relations.\n\nIn this reformulation, $\\hat{F}_\\alpha$ is *convolution* with\n$f_\\alpha(\\theta)$ and captures local phase structure, whereas\n$\\hat{G}_\\beta$ is *multiplication* by $g_\\beta(\\theta)$ and captures\nglobal oscillatory behavior. Alternatively, we can use $\\hat{\\Theta}$\nand $\\hat{D}_\\beta = -i \\beta \\frac{d}{d\\theta}$ for a position-momentum\nanalogy. Non-commutativity ensures that measuring $\\hat{F}_\\alpha$\ndisturbs $\\hat{G}_\\beta$ outcomes, as standard QM. For example,\n$\\hat{F}_\\alpha$ emphasizes local peaks in $\\Phi$, while $\\hat{G}_\\beta$\nresolves periodic modes. The uncertainty relation\n$\\Delta F_\\alpha \\Delta G_\\beta \\geq \\frac{1}{2} |\\langle [F_\\alpha, G_\\beta] \\rangle|$\ncan be numerically validated, replacing the heuristic non-orthogonality\ncondition.\n\n### The Meaning of $\\frac{d}{d\\theta}$ in Kime\n\nHow do we define a derivative operator when $\\theta$ is a random\nvariable rather than a deterministic coordinate? Do we need Itô\ncalculus, distributional derivatives, or another framework to clarify\nthe role of $\\theta$ and $\\frac{d}{d\\theta}$ and explicate the\nderivative operator?\n\nIn QM, $\\hat{x}$ and $\\hat{p}$ operate on wavefunctions $\\psi(x)$ in\n$L^2(\\mathbb{R})$, where $x$ is a continuous spatial variable. The\nderivative $\\frac{d}{dx}$ is well-defined as an operator on\ndifferentiable functions. In the kime context, $\\kappa = t e^{i\\theta}$\nintroduces $\\theta$ as a phase, randomly sampled from $\\Phi(\\theta; t)$\nfor each measurement at time $t$. *Approach 1 probes* $\\Phi(\\theta; t)$\nvia *test functions*, e.g., $f_\\alpha(\\theta)$, computing expectations\nlike\n$\\langle \\Phi, f_\\alpha \\rangle = \\int f_\\alpha(\\theta) \\Phi(\\theta; t) d\\theta$.\nIn Approach 1A reformulation, we defined operators on a Hilbert space,\ne.g., $L^2[-\\pi, \\pi]$, acting on functions $\\psi(\\theta)$, where\n$\\psi(\\theta)$ represent $\\sqrt{\\Phi(\\theta; t)}$ or another state-like\nobject. When $\\theta$ is a random draw, $\\frac{d}{d\\theta}$ *seems*\nill-defined without a continuous trajectory. Yet, $\\Phi(\\theta; t)$ is a\ndensity over $\\theta$, suggesting a functional space where $\\theta$ is a\nvariable of integration, not a stochastic process evolving in time (as\nin Itô calculus).\n\nSince $\\Phi(\\theta; t)$ is a probability density over\n$\\theta \\in [-\\pi, \\pi]$ at fixed $t$, we treat $\\theta$ as a coordinate\nin a function space, not a random variable evolving dynamically. Thus,\n\n-   *Hilbert Space*: $L^2[-\\pi, \\pi]$, where functions $\\psi(\\theta)$\n    are square-integrable over $\\theta$, and\n    $\\Phi(\\theta; t) = |\\psi(\\theta)|^2$ (analogous to a probability\n    density in QM).\n-   *State*: $\\psi(\\theta)$ could be $\\sqrt{\\Phi(\\theta; t)}$ (assuming\n    $\\Phi$ is real and non-negative), representing the \"state\" of the\n    kime-phase distribution at time $t$.\n-   *Operators*: Define $\\hat{F}_\\alpha$ and $\\hat{G}_\\beta$ as\n    operators on $\\psi(\\theta)$, with expectations computed against\n    $\\Phi(\\theta; t)$.\n\nThe *differentiation operator* $\\hat{G}_\\beta = -i \\frac{d}{d\\theta}$ in\n$L^2[-\\pi, \\pi]$, where $\\theta$ is the independent variable of the\nfunction space, not a random draw per se. The random sampling occurs\nwhen we generate data (e.g., $\\theta_n(t) \\sim \\Phi$), but the operators\nact on the density or its square root. Thus, the *multiplication\noperator* $\\hat{\\Theta} \\psi(\\theta) = \\theta \\psi(\\theta)$ and the\n*differentiation operator*\n$\\hat{G}_\\beta \\psi(\\theta) = -i \\beta \\frac{d}{d\\theta} \\psi(\\theta)$,\nwhere $\\beta$ scales the \"momentum\" (frequency-like) contribution.\n\n*Boundary Conditions*: Since $\\theta \\in [-\\pi, \\pi]$ is a circular\ndomain (phase wraps around), we impose periodic boundary conditions:\n$\\psi(-\\pi) = \\psi(\\pi)$, ensuring $\\hat{G}_\\beta$ is well-defined on\ndifferentiable functions in $L^2[-\\pi, \\pi]$.\n\n*Commutator*:$[\\hat{\\Theta}, \\hat{G}_\\beta] \\psi(\\theta) = -i \\beta \\theta \\frac{d}{d\\theta} \\psi(\\theta) - (-i \\beta \\psi(\\theta) - i \\beta \\theta \\frac{d}{d\\theta} \\psi(\\theta)) = i \\beta \\psi(\\theta)$,\nwhere\n\n-   $\\hat{\\Theta} \\hat{G}_\\beta \\psi(\\theta) = \\theta (-i \\beta \\frac{d}{d\\theta} \\psi(\\theta)) = -i \\beta \\theta \\frac{d}{d\\theta} \\psi(\\theta)$,\n    and\n-   $\\hat{G}_\\beta \\hat{\\Theta} \\psi(\\theta) = -i \\beta \\frac{d}{d\\theta} (\\theta \\psi(\\theta)) = -i \\beta \\left( \\psi(\\theta) + \\theta \\frac{d}{d\\theta} \\psi(\\theta) \\right)$.\n\nThus, $[\\hat{\\Theta}, \\hat{G}_\\beta] = i \\beta I$ confirms\nnon-commutativity, with a commutator proportional to $\\beta$. *Itô\ncalculus does not apply* to stochastic processes where a variable,\n$\\theta(t)$, evolves randomly over time $t$, governed by a stochastic\ndifferential equation (SDE). In kime, $\\theta$ is sampled independently\nat each $t$ from $\\Phi(\\theta; t)$, with no explicit temporal dynamics\n(e.g., $d\\theta = \\mu dt + \\sigma dW_t$). And the derivative\n$\\frac{d}{d\\theta}$ operates over the $\\theta$-domain of the density\n$\\Phi(\\theta; t)$ at fixed $t$, not over $t$. Thus, Itô calculus is\nunnecessary here. The randomness is in the sampling process, not in a\ncontinuous evolution of $\\theta$. A *distributional derivative* (e.g.,\nweak derivative) would be needed if $\\psi(\\theta)$ or $\\Phi(\\theta; t)$\nwere not differentiable (e.g., included Dirac deltas). For simplicity,\nassume $\\Phi(\\theta; t)$ is smooth (e.g., *von Mises*), so the standard\nderivative suffices. If $\\Phi$ were less regular, we’d define\n$\\hat{G}_\\beta$ in the sense of distributions,\n$\\langle \\hat{G}_\\beta \\psi, \\phi \\rangle = -i \\beta \\int_{-\\pi}^\\pi \\psi(\\theta) \\frac{d}{d\\theta} \\phi(\\theta) d\\theta$,\nfor test functions $\\phi \\in C^\\infty[-\\pi, \\pi]$.\n\nHence, the *reformulated Approach 1 with non-commuting operators* is\nbased on\n\n 1.  *Hilbert Space*: $L^2[-\\pi, \\pi]$, with\n    $\\psi(\\theta) = \\sqrt{\\Phi(\\theta; t)}$.\n 2.  *Operators*:\n    - *Multiplication operator*: $\\hat{F}_\\alpha = \\hat{\\Theta}$, probing the \"position\" (mean) of $\\theta$, and\n    - *Differentiation operator*: $\\hat{G}_\\beta = -i \\beta \\frac{d}{d\\theta}$, probing frequency-like\n    variations.\n\n 3.  *Measurement*: In the state, $\\psi(\\theta) = \\sqrt{\\Phi(\\theta; t)}$, we compute the expectations $\\langle \\psi | \\hat{F}_\\alpha | \\psi \\rangle = \\int \\theta \\Phi(\\theta; t) d\\theta$, $\\langle \\psi | \\hat{G}_\\beta | \\psi \\rangle = -i \\beta \\int \\psi(\\theta) \\frac{d}{d\\theta} \\psi(\\theta) d\\theta$.\n\n 4.  *Kime-Phase Recovery/Reconstruction*: Use multiple $\\beta$ values to estimate $\\Phi(\\theta; t)$ via moments or Fourier components.\n \n 5.  *Expectations*: $\\langle \\hat{\\Theta} \\rangle = \\int_{-\\pi}^\\pi \\theta \\Phi(\\theta; t) d\\theta$ and\n\n$$\\langle \\hat{G}_\\beta \\rangle = -i \\beta \\int_{-\\pi}^\\pi \\sqrt{\\Phi(\\theta; t)} \\frac{d}{d\\theta} \\sqrt{\\Phi(\\theta; t)} d\\theta = -i \\beta \\int_{-\\pi}^\\pi \\frac{1}{2} \\frac{\\frac{d}{d\\theta} \\Phi(\\theta; t)}{\\sqrt{\\Phi(\\theta; t)}} d\\theta.$$\n\nMind that the statement, \"*no explicit temporal dynamics*\" refers to the\nabsence of a stochastic differential equation (SDE) or a continuous-time\nprocess governing $\\theta$’s evolution over $t$. For example, in a\nstochastic process like a Wiener process or an Itô diffusion,\n$\\theta(t)$ would evolve according to something like\n$d\\theta = \\mu dt + \\sigma dW_t$, where $W_t$ is Brownian motion,\nimplying a smooth, temporally correlated trajectory with increments\ndependent on previous values. Such a model introduces *temporal\ndependence* through a differential structure, where $\\theta(t + dt)$\ndepends on $\\theta(t)$ via drift ($\\mu$) and diffusion ($\\sigma$).\n\nIn contrast, the Approach 1A kime framework treats $\\theta$ as follows\n\n -   At each fixed time $t$, a value $\\theta_n(t)$ is sampled\n    independently from the distribution $\\Phi(\\theta; t)$ for each\n    measurement $n = 1, 2, \\ldots, N$.\n -   The next time point $t' = t + \\Delta t$ yields a new sample\n    $\\theta_n(t')$ from $\\Phi(\\theta; t')$, with no explicit rule (like\n    an SDE) linking $\\theta_n(t)$ to $\\theta_n(t')$.\n\nThe \"no explicit temporal dynamics\" claim hinges on the idea that\n$\\theta_n(t)$ and $\\theta_n(t')$ are *independent (IID) draws*, lacking\na mechanistic model of temporal evolution, e.g., no Markovian transition\nor differential equation.\n\nHowever, $\\Phi(\\theta; t)$ is time-dependent, meaning the distribution\nfrom which $\\theta$ is sampled changes with $t$. For instance, in the\nsimulation, $\\mu(t) = 2\\pi \\sin(0.1 t)$ and\n$\\kappa(t) = 5 (1 + 0.5 \\sin(2\\pi t / T))$ define a von Mises\ndistribution $\\Phi(\\theta; t) = \\text{vM}(\\mu(t), \\kappa(t))$, where\nboth *mean* and *concentration* parameters vary over time. This implies\nthat the statistical properties of $\\theta$, e.g., mean, variance,\nevolve with $t$, even if the samples at different $t$ are drawn\nindependently. This time-dependence of $\\Phi(\\theta; t)$ introduces a\nsubtlety; while there’s no *dynamic process* linking $\\theta(t)$ to\n$\\theta(t')$, like an SDE, the distribution’s parameters are functions\nof $t$, suggesting an *implicit* temporal structure through the changing\n$\\Phi$.\n\n*Statistical independence across time*: For a fixed measurement $n$,\n$\\theta_n(t)$ and $\\theta_n(t')$ are independent draws from\n$\\Phi(\\theta; t)$ and $\\Phi(\\theta; t')$, respectively. There’s no\ncorrelation imposed between $\\theta_n(t)$ and $\\theta_n(t')$ by a\ntemporal process—just new, independent samples at each $t$. This\nsupports the \"no explicit temporal dynamics\" claim. At the same time,\n$\\Phi(\\theta; t)$ varies with $t$ and the ensemble of samples\n$\\{\\theta_n(t)\\}_{n=1}^N$ at time $t$ has different statistical\ncharacteristics, e.g., mean $\\mu(t)$, concentration $\\kappa(t)$, than at\n$t'$. This introduces a form of *temporal dependence* in the\ndistribution, *not in the individual* $\\theta_n(t)$ trajectories.\nThere’s no continuous stochastic process or differential equation\ndefining how $\\theta$ evolves from $t$ to $t + dt$. Each $\\theta_n(t)$\nis a fresh draw, not a step in a trajectory. The independence of samples\nacross $t$ means no explicit temporal correlation, e.g.,\nautocorrelation, is modeled between $\\theta_n(t)$ and $\\theta_n(t')$.\n\nHence, in kime, $\\theta$ is sampled independently at each $t$ from a\ntime-dependent distribution $\\Phi(\\theta; t)$, with no explicit\nstochastic temporal dynamics, e.g., $d\\theta = \\mu dt + \\sigma dW_t$,\nlinking successive samples. However, the distribution $\\Phi(\\theta; t)$\nitself varies with $t$, introducing an implicit temporal dependence in\nthe statistical properties of $\\theta$.\n\nLet's also clarify the kime-operators and $\\frac{d}{d\\theta}$ in the\nreformulation of Approach 1A, The derivative\n$\\hat{G}_\\beta = -i \\beta \\frac{d}{d\\theta}$ operates on functions\n$\\psi(\\theta) = \\sqrt{\\Phi(\\theta; t)}$ in $L^2[-\\pi, \\pi]$ at a fixed\n$t$. Here, $\\theta$ is a coordinate in the phase domain, not a\ntime-evolving variable. The operator probes the spatial (phase)\nstructure of $\\Phi(\\theta; t)$, not its temporal evolution. Below is an\nupdated simulation with time-dependent $\\Phi$ refining the prior\nsimulation to explicitly express the $\\Phi(\\theta; t)$’s\ntime-dependence.\n\n\nThis reflects $\\Phi(\\theta; t)$’s time-dependence while maintaining\nindependent sampling, aligning with the Approach 1A refined formulation.\n\n## Approach 1B: KPT via Test Function Action\n\nKPT probes \\(\\Phi(\\theta; t)\\) by evaluating its action on families of test functions, analogous to measuring a quantum state in different bases. In quantum mechanics, a state \\(|\\psi\\rangle\\) is measured in bases like position (\\(|x\\rangle\\)) and momentum (\\(|p\\rangle\\)), with non-commuting operators \\(\\hat{X}\\) and \\(\\hat{P}\\), yielding distributions \\(|\\langle x | \\psi \\rangle|^2\\) and \\(|\\langle p | \\psi \\rangle|^2\\). These measurements constrain the wavefunction, including its phase, but require repeated trials due to the uncertainty principle.\n\nIn the kime framework, we treat \\(\\Phi(\\theta; t)\\) as a distribution on \\([-\\pi, \\pi)\\), with \\(\\int_{-\\pi}^{\\pi} \\Phi(\\theta; t) d\\theta = 1\\). We probe \\(\\Phi\\) via its action on test functions \\(f(\\theta)\\):\n\\[\n\\langle \\Phi, f \\rangle = \\int_{-\\pi}^{\\pi} f(\\theta) \\Phi(\\theta; t) d\\theta,\n\\]\nwhich is analogous to a quantum expectation value \\(\\langle \\hat{A} \\rangle_\\psi\\). To capture complementary information about \\(\\Phi\\), we use two families of test functions\n\n - {\\it{Family A (Fourier-like)}}: \\(f_\\alpha(\\theta) = sin(\\alpha \\theta), \\(\\alpha \\in \\{1, 2, 3,4,5\\}\\), modified to be periodic on \\([-\\pi, \\pi)\\).\n\n - {\\it{Family B (Fourier-like)}}: \\(g_\\beta(\\theta) = \\cos(\\beta \\theta)\\), \\(\\beta \\in \\{1, 2, 3,4,5\\}\\), naturally periodic.\n\nThese families are chosen to capture local (wavelet) and global (Fourier) features of \\(\\Phi\\). While quantum tomography relies on non-commuting operators, here we define “incompatibility” operationally: measurements with \\(f_\\alpha\\) and \\(g_\\beta\\) are performed on separate samples, mimicking the need for separate runs in quantum mechanics due to non-commutativity.\n\nGiven \\(N\\) repeated measurements at time \\(t\\), each yielding a sample \\(\\theta_i \\sim \\Phi(\\theta; t)\\), we estimate the integrals:\n\\[\n\\mathbb{E}[f_\\alpha(\\theta)] = \\int_{-\\pi}^{\\pi} f_\\alpha(\\theta) \\Phi(\\theta; t) d\\theta \\approx \\frac{1}{N_f} \\sum_{i=1}^{N_f} f_\\alpha(\\theta_i),\n\\]\n\\[\n\\mathbb{E}[g_\\beta(\\theta)] = \\int_{-\\pi}^{\\pi} g_\\beta(\\theta) \\Phi(\\theta; t) d\\theta \\approx \\frac{1}{N_g} \\sum_{i=1}^{N_g} g_\\beta(\\theta_i),\n\\]\nwhere \\(N_f + N_g = N\\), and the samples are split between the two families (e.g., \\(N_f = N_g = N/2\\)).\n\nWe reconstruct \\(\\Phi\\) by parameterizing:\n\\[\n\\Phi_{\\text{est}}(\\theta) = \\sum_{\\alpha} c_\\alpha f_\\alpha(\\theta) + \\sum_{\\beta} d_\\beta g_\\beta(\\theta),\n\\]\nand fitting \\(c_\\alpha, d_\\beta\\) to match the measured integrals. To ensure \\(\\Phi_{\\text{est}} \\geq 0\\), we use a constrained optimization:\n\\[\n\\min_{c_\\alpha, d_\\beta \\geq 0} \\sum_{\\alpha} \\left( \\frac{1}{N_f} \\sum_{i=1}^{N_f} f_\\alpha(\\theta_i) - \\int_{-\\pi}^{\\pi} f_\\alpha(\\theta) \\Phi_{\\text{est}}(\\theta) d\\theta \\right)^2 + \\sum_{\\beta} \\left( \\frac{1}{N_g} \\sum_{i=1}^{N_g} g_\\beta(\\theta_i) - \\int_{-\\pi}^{\\pi} g_\\beta(\\theta) \\Phi_{\\text{est}}(\\theta) d\\theta \\right)^2 + \\lambda \\left( \\sum_{\\alpha} c_\\alpha^2 + \\sum_{\\beta} d_\\beta^2 \\right),\n\\]\nwhere \\(\\lambda > 0\\) is a Tikhonov penalty to prevent overfitting. After optimization, we normalize:\n\\[\n\\Phi_{\\text{est}}(\\theta) \\leftarrow \\frac{\\Phi_{\\text{est}}(\\theta)}{\\int_{-\\pi}^{\\pi} \\Phi_{\\text{est}}(\\theta) d\\theta}.\n\\]\nThis ensures \\(\\Phi_{\\text{est}}\\) is a valid probability density.\n\nTo formalize incompatibility, we define operators on \\(L^2([-\\pi, \\pi])\\):\n- \\(\\hat{F}_\\alpha \\psi(\\theta) = f_\\alpha(\\theta) \\psi(\\theta)\\), a multiplication operator.\n- \\(\\hat{G}_\\beta \\psi(\\theta) = -i \\beta \\frac{d}{d\\theta} (g_\\beta(\\theta) \\psi(\\theta))\\), a differential operator.\n\nThe commutator \\([\\hat{F}_\\alpha, \\hat{G}_\\beta] \\neq 0\\), as:\n\\[\n[\\hat{F}_\\alpha, \\hat{G}_\\beta] \\psi = f_\\alpha (-i \\beta) \\frac{d}{d\\theta} (g_\\beta \\psi) + i \\beta \\frac{d}{d\\theta} (f_\\alpha g_\\beta \\psi) \\neq 0,\n\\]\ndue to the derivative terms. This non-commutativity justifies separate measurements, as in quantum tomography.\n\nA simulation demonstrates KPT with \\(N = 200\\) samples at a fixed \\(t\\), where the true \\(\\Phi(\\theta; t)\\) is a mixture of two von Mises distributions:\n\\[\n\\Phi(\\theta; t) = 0.6 vM(\\theta; 0, 5) + 0.4 vM(\\theta; \\pi, 5).\n\\]\nWe use \\(N_f = N_g = 100\\), with \\(\\alpha, \\beta \\in \\{1, 2, 3\\}\\), and fit \\(\\Phi_{\\text{est}}\\) using the above optimization. Figure \\ref{fig:Approach1.Fig1} shows that \\(\\Phi_{\\text{est}}\\) (red) approximates the true \\(\\Phi\\) (blue), capturing the bimodal structure, though with some ringing due to the limited basis size.\n\nFor practical applications like fMRI, we propose extracting \\(\\theta\\) from oscillatory signals (e.g., via Hilbert transform) and applying wavelet or Fourier transforms to compute test function integrals. Future work should address the time-dependence of \\(\\Phi(\\theta; t)\\) and optimize the choice of test functions (e.g., using B-splines for smoother reconstructions).\n\n\n\n## Approach 2: Kime-Phase Estimation via Non-Commuting Operators\n\nApproach 2 estimates the kime-phase \\(\\phi(t) = \\theta(t)\\) from a time-series signal \\(s(t) = A(t) e^{i\\phi(t)}\\), where \\(\\phi(t) \\sim \\Phi(\\theta; t)\\), and subsequently reconstruct the kime-phase distribution \\(\\Phi(\\theta; t)\\). This aligns with the goal of Kime-Phase Tomography (KPT) to estimate \\(\\Phi(\\theta; t)\\), but unlike Approach 1, which operates in the \\(\\theta\\)-domain, Approach 2 works directly with time-series signals in the time domain, making it more practical for applications like EEG or fMRI.\n\nThis approach is based on\n\n - Hilbert Space: \\(L^2([0, T])\\), the space of square-integrable functions over a finite time interval \\(t \\in [0, T]\\), with inner product\n$\\langle f, g \\rangle = \\int_0^T f(t) \\overline{g(t)} dt.$ This is appropriate for finite-duration signals in practical applications.\n\n - Signal Model: The signal is \\(s(t) = A(t) e^{i\\phi(t)}\\), where \\(A(t) \\geq 0\\) is the amplitude, and \\(\\phi(t) \\in [-\\pi, \\pi)\\) is the kime-phase, sampled from a time-dependent distribution \\(\\Phi(\\theta; t)\\).\n \n - Kime Operators: Three linear operators \\(K_1, K_2, K_3\\) that probe the signal in time, frequency, and scale domains, respectively. These operators are designed to be non-commuting, providing complementary information about \\(\\phi(t)\\).\n\n 1. The Time-Domain Operator (\\(K_1\\)): Multiplication by time, analogous to the position operator in quantum mechanics $K_1 s(t) = t s(t).$ This operator emphasizes the temporal structure of the signal.\n\n 2. Frequency-Domain Operator (\\(K_2\\)): The derivative operator, analogous to the momentum operator $K_2 s(t) = -i \\frac{d}{dt} s(t).$  This operator probes the frequency content of the signal, as the derivative of \\(e^{i\\omega t}\\) yields \\(-i \\omega e^{i\\omega t}\\).\n\n 3. Scale-Domain Operator (\\(K_3\\)): Convolution with a Morlet wavelet at an optimal scale $K_3 s(t) = \\int_0^T \\psi(t - t'; a^*) s(t') dt',$  where \\(\\psi(t; a) = \\frac{1}{\\sqrt{a}} e^{-(t/a)^2/2} \\cos\\left(\\omega_0 \\frac{t}{a}\\right) \\cdot \\mathbb{I}_{[-a, a]}(t)\\), \\(\\omega_0 = 5\\) (to balance time-frequency resolution), and \\(\\mathbb{I}_{[-a, a]}(t)\\) ensures compact support to avoid boundary effects. The optimal scale $a^*$ is $a^* = \\arg\\max_a \\left| \\int_0^T \\psi(t - t'; a) s(t') dt' \\right|.$ This operator captures multi-scale features of the signal.\n\nTo ensure complementary information, let's compute the pairwise commutators\n$$K_1 K_2 s(t) = t \\left(-i \\frac{d}{dt} s(t)\\right),$$\n\n$$K_2 K_1 s(t) = -i \\frac{d}{dt} (t s(t)) = -i s(t) - i t \\frac{d}{dt} s(t),$$\n\n$$[K_1, K_2] s(t) = K_1 K_2 s(t) - K_2 K_1 s(t) = -i t \\frac{d}{dt} s(t) + i s(t) + i t \\frac{d}{dt} s(t) = i s(t),$$\nHence, $[K_1, K_2] = i I,$ where \\(I\\) is the identity operator. This mirrors the quantum mechanical relation \\([\\hat{x}, \\hat{p}] = i \\hbar\\) (with \\(\\hbar = 1\\)). The uncertainty relation is\n$\\Delta K_1 \\Delta K_2 \\geq \\frac{1}{2} |\\langle [K_1, K_2] \\rangle| = \\frac{1}{2},$\nwhere \\(\\Delta K_j = \\sqrt{\\langle K_j^2 \\rangle - \\langle K_j \\rangle^2}\\), and expectations are computed over the signal \\(s(t)\\),\n$\\langle K_j \\rangle = \\int_0^T s(t)^* (K_j s(t)) dt.$\n\nThe commutotars of \\([K_1, K_3]\\) and \\([K_2, K_3]\\) are generally non-trivial due to the convolution operation. For example,\n$K_1 K_3 s(t) = t \\int_0^T \\psi(t - t'; a^*) s(t') dt',$\n$K_3 K_1 s(t) = \\int_0^T \\psi(t - t'; a^*) (t' s(t')) dt',$ and \n$[K_1, K_3] s(t) = \\int_0^T \\psi(t - t'; a^*) (t - t') s(t') dt',$\nwhich is non-zero since \\(t \\neq t'\\) in general. Similarly, \\([K_2, K_3] \\neq 0\\), reflecting time-scale and frequency-scale trade-offs. We’ll validate these numerically in the simulation.\n\nPhase estimation allows inferring the phase from each operator’s output\n\n - \\(K_1\\): \\(\\phi_1(t) = \\arg(K_1 s(t)) = \\arg(t s(t))\\). Since \\(t \\geq 0\\), this simplifies to \\(\\arg(s(t))\\) adjusted for the sign of \\(t\\), but in practice, we use the analytic signal directly.\n\n - \\(K_2\\): \\(\\phi_2(t) = \\arg(K_2 s(t)) = \\arg\\left(-i \\frac{d}{dt} s(t)\\right)\\).\n\n - \\(K_3\\): \\(\\phi_3(t) = \\arg(K_3 s(t))\\), from the wavelet transform at scale $a^*$.\n\nKPT is based on combining the phase estimates using weights based on circular dispersion, which is more robust for phase data than variance\n$\\text{Disp}(\\phi_j(t)) = 1 - \\left| \\frac{1}{W} \\sum_{k} e^{i \\phi_j(t_k)} \\right|,$\nwhere the sum is over a time window \\(W\\) around \\(t\\). The weights are\n$w_j(t) = \\frac{1}{\\text{Disp}(\\phi_j(t)) + \\epsilon},$\nwith \\(\\epsilon = 10^{-6}\\) to avoid division by zero. The ensemble phase is\n$\\hat{\\phi}(t) = \\arg\\left( \\sum_{j=1}^3 w_j(t) e^{i\\phi_j(t)} \\right).$\n\nKime-phase distribution estimation, \\(\\Phi(\\theta; t)\\), from \\(\\hat{\\phi}(t)\\), \nuses kernel density estimation (KDE) with a von Mises kernel\n$$\\Phi_{\\text{est}}(\\theta; t) = \\frac{1}{W} \\sum_{k} \\frac{1}{2\\pi I_0(\\kappa)} e^{\\kappa \\cos(\\theta - \\hat{\\phi}(t_k))},$$\nwhere the sum is over a time window \\(W\\) around \\(t\\), \\(\\kappa\\) is the concentration parameter (e.g., \\(\\kappa = 10\\)), and \\(I_0(\\kappa)\\) is the modified Bessel function of the first kind. Phase distribution unitarity is ensured via normalization\n$\\int_{-\\pi}^\\pi \\Phi_{\\text{est}}(\\theta; t) d\\theta = 1.$\n\nFor regularization, we can project the signal into a Reproducing Kernel Hilbert Space (RKHS) with a Gaussian kernel\n$K(t, t') = \\exp\\left(-\\frac{(t - t')^2}{2\\sigma^2}\\right), \\quad \\sigma = 1.$\nIn the RKHS, the operators become\n\n - \\(K_1[s](t) = t \\langle s, K(\\cdot, t) \\rangle_{\\mathcal{H}} = t \\int_0^T s(t') K(t', t) dt'\\),\n\n - \\(K_2[s](t) = -i \\frac{d}{dt} \\langle s, K(\\cdot, t) \\rangle_{\\mathcal{H}} = -i \\int_0^T s(t') \\frac{\\partial}{\\partial t} K(t', t) dt'\\), where \\(\\frac{\\partial}{\\partial t} K(t', t) = -\\frac{t' - t}{\\sigma^2} K(t', t)\\), and\n\n - $K_3[s](t) = \\int_0^T \\psi(t - t'; a^*) \\langle s, K(\\cdot, t') \\rangle_{\\mathcal{H}} dt'$.\n\nIn practice, it may be simpler to employ a direct signal processing approach, \nhowever, the RKHS framework ensures smoothness and regularization, which can be implemented by smoothing the signal before applying the operators.\n\nThe following Approach 2 simulation is based on\n\n - Time tomain: \\(t \\in [0, 30]\\), with \\(T = 10,000\\) points (\\(f_s \\approx 333.33 \\, \\text{Hz}\\)),\n \n - Amplitude: \\(A(t) = 1 + 0.5 \\sin(2\\pi \\cdot 0.1 t)\\),\n\n - Phase: $\\phi(t) = \\begin{cases} 2\\pi \\cdot 0.3 t & t < 5 \\\\ 2\\pi \\cdot 0.5 t + \\pi/2 & t \\geq 5 \\end{cases}$, with frequencies \\(0.3 \\, \\text{Hz}\\) (pre-jump) and \\(0.5 \\, \\text{Hz}\\) (post-jump), and a \\(\\pi/2\\) jump at \\(t = 5\\), and\n\n - Signal: \\(s(t) = A(t) e^{i\\phi(t)}\\), with noise: \\(x(t) = \\text{Re}(s(t)) + \\mathcal{N}(0, 0.05)\\).\n\nThe algorithm has the folloing steps\n\n 1. Generate the signal and compute the analytic signal using the Hilbert transform.\n\n 2. Apply operators \\(K_1, K_2, K_3\\) to estimate \\(\\phi_1(t), \\phi_2(t), \\phi_3(t)\\).\n\n 3. Ensemble to obtain \\(\\hat{\\phi}(t)\\).\n\n 4. Estimate \\(\\Phi(\\theta; t)\\) using von Mises KDE over time windows.\n\n 5. Visualize \\(\\phi(t)\\) vs. \\(\\hat{\\phi}(t)\\) over time and \\(\\Phi(\\theta; t)\\) vs. \\(\\Phi_{\\text{est}}(\\theta; t)\\) as 3D surfaces.\n \n 6. Validate the uncertainty relation \\(\\Delta K_1 \\Delta K_2 \\geq \\frac{1}{2}\\).\n\n\nThis approach 2 estimates the kime-phase \\(\\phi(t)\\) using non-commuting operators in time, frequency, and scale domains. It reconstructs \\(\\Phi(\\theta; t)\\) using von Mises KDE, aligning with KPT’s goal and the quantitative metrics and graphical results of\nthe numerical simulation show the reliability in practice.\n\n\n\n\n\n\n\n<!-- An alternative *kime-measurement scheme* for estimating the unobservable -->\n<!-- kime-phase $\\theta$, may define an analogous framework where -->\n<!-- \"non-commuting\" kime-phase distributions $\\Phi(t)$ act on test -->\n<!-- functions, analogous to non-commuting observables in QM. In QM, -->\n<!-- *observables* $\\hat{A}, \\hat{B}$ are non-commuting if -->\n<!-- $[\\hat{A}, \\hat{B}] \\neq 0$. For *kime-phase distributions* -->\n<!-- $\\Phi_1(t), \\Phi_2(t)$, we define *non-commutation* via their *action* -->\n<!-- on kime-test functions $f(\\kappa)$. Specifically, a pair of phase -->\n<!-- distributions $\\Phi_1(t),\\ \\Phi_2(t)$ *commute* if their combined action -->\n<!-- on any test function is order-independent -->\n<!-- $$\\mathbb{E}_{\\Phi_1}\\left[\\mathbb{E}_{\\Phi_2}[f(\\kappa)]\\right] = \\mathbb{E}_{\\Phi_2}\\left[\\mathbb{E}_{\\Phi_1}[f(\\kappa)]\\right].$$ -->\n<!-- Thus, the phase distributions *do not commute* if the order of -->\n<!-- expectation matters, i.e., the commutator $[\\Phi_1, \\Phi_2] \\neq 0$. -->\n\n<!-- To utilize the kime-test functions as *probes*, we define a set of -->\n<!-- *kime-test functions* $\\{f_j(\\kappa)\\}$, analogous to quantum -->\n<!-- measurement operators, sensitive to the kime-phase $\\theta$. For -->\n<!-- example, *Fourier-like test functions* are defined by -->\n<!-- $f_j(\\kappa) = e^{i n \\theta}$, where $n$ is a harmonic index, while -->\n<!-- *polynomial test functions* are defined by -->\n<!-- $f_j(\\kappa) = \\kappa^n = t^n e^{i n \\theta}$, probing moments of $t$ -->\n<!-- and $\\theta$. The expectation of $f_j(\\kappa)$ under $\\Phi(t)$ is -->\n<!-- $$\\mathbb{E}_\\Phi[f_j(\\kappa)] = \\int t^n e^{i n \\theta} \\Phi(t) d\\theta dt.$$ -->\n<!-- If $\\Phi(t)$ encodes a phase distribution (e.g., -->\n<!-- $\\theta \\sim \\mathcal{N}(0, \\sigma(t))$), this expectation becomes a -->\n<!-- *moment-generating function* for $\\theta$. -->\n\n<!-- The *non-commutativity* of a pair of kime-phase distributions -->\n<!-- $\\Phi_1(t), \\Phi_2(t)$, exploits non-commuting actions on test -->\n<!-- functions. For example, -->\n\n<!-- -   *Uniform Phase (U)*: -->\n<!--     $\\Phi_U(t) \\propto \\delta(t - t_0) \\cdot \\text{Uniform}(\\theta)$. -->\n<!-- -   *Concentrated Phase (C)*: -->\n<!--     $\\Phi_C(t) \\propto \\delta(t - t_0) \\cdot \\delta(\\theta - \\theta_0)$. -->\n<!-- -   *Time-Correlated Phase (T)*: -->\n<!--     $\\Phi_T(t) \\propto \\mathcal{N}(t; \\mu, \\sigma) \\cdot \\mathcal{N}(\\theta; \\phi(t), \\gamma)$, -->\n<!--     where $\\phi(t)$ depends on $t$. -->\n\n<!-- *Non-Commutation Example*: The commutator $[\\Phi_U, \\Phi_T] \\neq 0$ -->\n<!-- averaged over $\\Phi_U$ followed by $\\Phi_T$ produces different results -->\n<!-- than when the averaging order is reversed. This occurs when -->\n<!-- time-dependent draws from $\\Phi_T$, $\\phi(t)$, depend *nonlinearly* on -->\n<!-- $t$, making the order of integration matter. To formulate a consistent -->\n<!-- *measurement protocol for kime-phase estimation*, i.e., to estimate -->\n<!-- $\\hat\\theta$, we can perform repeated measurements under different -->\n<!-- $\\Phi(t)$, analogous to QM’s non-commuting bases. First, sample -->\n<!-- $\\kappa = t e^{i\\theta}$ under $\\Phi_1(t)$ and compute -->\n<!-- $\\mathbb{E}_{\\Phi_1}[f_j(\\kappa)]$. Then, repeat step 1 under -->\n<!-- $\\Phi_2(t)$ and compute $\\mathbb{E}_{\\Phi_2}[f_j(\\kappa)]$. Finally, -->\n<!-- solve for $\\theta$ by (ensembling) combining the results from -->\n<!-- non-commuting $\\Phi_1, \\Phi_2$. -->\n\n<!-- Consider an example using *Fourier test functions*, -->\n<!-- $f_j(\\kappa) = e^{i n \\theta}$. For two non-commuting phase -->\n<!-- distributions $\\Phi_1, \\Phi_2$, under $\\Phi_1$, -->\n<!-- $\\mathbb{E}_{\\Phi_1}[e^{i n \\theta}] = \\int e^{i n \\theta} \\Phi_1(t) d\\theta dt = \\tilde{\\Phi}_1(n)$ -->\n<!-- is the Fourier transform of $\\Phi_1$, while under $\\Phi_2$, -->\n<!-- $\\mathbb{E}_{\\Phi_2}[e^{i n \\theta}] = \\tilde{\\Phi}_2(n)$ is the Fourier -->\n<!-- transform of $\\Phi_2$. By measuring $\\tilde{\\Phi}_1(n)$ and -->\n<!-- $\\tilde{\\Phi}_2(n)$ for multiple $n$, we invert the Fourier problem to -->\n<!-- estimate $\\theta$, assuming $\\Phi_1, \\Phi_2$ provide complementary -->\n<!-- \"views\" of the phase $\\theta$. -->\n\n<!-- This scheme mirrors QM’s use of non-commuting bases to reconstruct -->\n<!-- $\\phi$. For instance, QM measures $\\langle X \\rangle \\propto \\cos\\phi$ -->\n<!-- and $\\langle Y \\rangle \\propto \\sin\\phi$, whereas *kime* measures -->\n<!-- $\\tilde{\\Phi}_1(n) \\propto \\mathbb{E}[e^{i n \\theta}]$ and -->\n<!-- $\\tilde{\\Phi}_2(n) \\propto \\mathbb{E}[e^{i m \\theta}]$, with -->\n<!-- $\\Phi_1, \\Phi_2$ chosen such that their Fourier components resolve -->\n<!-- $\\theta$. -->\n\n<!-- In practice, the kime scheme requires non-commuting $\\Phi(t)$ with -->\n<!-- distinct $\\theta$-dependencies (e.g., uniform vs. time-correlated -->\n<!-- phases), defining *test functions* sensitive to $\\theta$ (e.g., -->\n<!-- oscillatory functions $e^{i n \\theta}$), and *statistical inversion* -->\n<!-- (e.g., maximum likelihood, Bayesian methods) to estimate $\\theta$ from -->\n<!-- aggregated expectations. The kime-phase $\\theta$, like the quantum -->\n<!-- wavefunction phase, is inferred indirectly through its *correlations* -->\n<!-- with observable quantities (e.g., $t$) under non-commuting phase -->\n<!-- distributions. By generalizing the concept of operator commutation to -->\n<!-- the action of $\\Phi(t)$ on test functions, draws parallels between QM’s -->\n<!-- phase-recovery strategy in the kime framework. This approach may be -->\n<!-- useful for *kime-tomography* and applications in *complex-time signal -->\n<!-- processing*. -->\n\n<!-- ### Strategy 2 fMRI Example - Estimation of Time-Dynamic von Mises Phase Distribution -->\n\n<!-- Let's demonstrate this *second approach* for estimating the kime-phase -->\n<!-- distribution $\\Phi(t)$ from repeated measurements. This strategy is also -->\n<!-- inspired by quantum phase estimation and we'll use a *moment-based -->\n<!-- approach* with test functions sensitive to the phase $\\theta(t)$. At -->\n<!-- each time $t$, the complex kime is $\\kappa_n(t) = t e^{i\\theta_n(t)}$, -->\n<!-- where $\\theta_n(t) \\sim \\Phi(t)$ for measurement $n$. The *test -->\n<!-- functions* will use Fourier modes $f_j(\\theta) = e^{i j \\theta}$ to -->\n<!-- probe the moments of $\\Phi(t)$. The $j$-th moment is -->\n<!-- $\\mathbb{E}_{\\Phi(t)}[e^{i j \\theta}] = \\int e^{i j \\theta} \\Phi(t)d\\theta$ -->\n<!-- and the *moment estimation* using $N$ measurements is -->\n<!-- $\\hat{m}_j(t) = \\frac{1}{N} \\sum_{n=1}^N \\left(\\frac{\\kappa_n(t)}{t}\\right)^j.$ -->\n\n<!-- In this simulation, the *phase distribution recovery* assumes $\\Phi(t)$ -->\n<!-- is a *von Mises* distribution $\\text{vM}(\\mu(t), \\kappa(t))$ and relies -->\n<!-- on solving for its pair of parameters, $\\mu(t)$ (mean) and $\\kappa(t)$ -->\n<!-- (concentration), using -->\n<!-- $$\\hat{\\mu}(t) = \\arg(\\hat{m}_1(t)), \\quad \\hat{\\kappa}(t) \\text{ s.t. } \\frac{I_1(\\hat{\\kappa}(t))}{I_0(\\hat{\\kappa}(t))} = |\\hat{m}_1(t)|,$$ -->\n<!-- where $I_p$ is the *modified Bessel function* of order $p$. Recall that -->\n<!-- the parameters $\\mu$ and $\\frac{1}{\\kappa}$ of the [von Mises -->\n<!-- distribution](https://en.wikipedia.org/wiki/Von_Mises_distribution), -->\n<!-- $vM(\\mu,\\kappa)$, are analogous to the *mean* $\\mu$ and *variance* -->\n<!-- $\\sigma^2$ parameters of the normal distribution. Specifically, -->\n<!-- $\\mu =\\mu_{vM}$ is a *measure of location/centrality* as the -->\n<!-- distribution is clustered around $\\mu$, and $\\kappa$ is a *measure of -->\n<!-- concentration*, i.e., a reciprocal measure of *dispersion*, -->\n<!-- $\\frac{1}{\\kappa}\\approx \\sigma^2$. As $\\kappa \\to 0$, the -->\n<!-- $vM(\\mu,\\kappa)\\to Uniform[-\\pi,\\pi)$, and as $\\kappa \\uparrow$, the -->\n<!-- $vM(\\mu,\\kappa)\\to N\\left (\\mu, \\sigma^2=\\frac{1}{\\kappa}\\right )$. -->\n\n<!-- In this simulation, the *time-varying von Mises kime-phase distribution* -->\n<!-- has an *oscillating location parameter* $\\mu(t)=2 \\pi \\sin(0.1 t)$ and a -->\n<!-- *time-dynamics concentration parameter* -->\n<!-- $\\kappa(t) = 5.263 \\left (1 + 0.5 \\sin\\left(2 \\pi \\frac{t}{T}\\right )\\right )$. -->\n<!-- The first moment $\\hat{m}_1(t)$ estimates $\\mathbb{E}[e^{i\\theta(t)}]$, -->\n<!-- encoding $\\mu(t)$ and $\\kappa(t)$. The *phase distribution recovery* is -->\n<!-- obtained by estimating the pair of time-dependent von Mises distribution -->\n<!-- parameters, $\\mu(t)\\approx \\hat{m}_1(t)$ (*location*) and $\\kappa(t)$ -->\n<!-- (*concentration*), which is estimated numerically using the relationship -->\n<!-- between $|\\hat{m}_1(t)|$ and the Bessel functions, -->\n<!-- $\\frac{I_1(\\hat{\\kappa}(t))}{I_0(\\hat{\\kappa}(t))} = |\\hat{m}_1(t)|$. -->\n\n<!-- ```{r approach2_MomentEstimation, echo=TRUE, message=FALSE, warning=FALSE} -->\n<!-- # ############################################## OVER-SImplified Demo ###### -->\n<!-- # # --- Simulation and Estimation --- -->\n<!-- # library(circular) -->\n<!-- # library(VGAM) -->\n<!-- # set.seed(123) -->\n<!-- #  -->\n<!-- # # Parameters -->\n<!-- # T <- 100                  # Time points -->\n<!-- # N <- 1000                 # Number of measurements -->\n<!-- # kappa_true <- 5.263       # True concentration -->\n<!-- # mu_true <- 0              # True mean phase -->\n<!-- #  -->\n<!-- # # Simulate observed phases directly from target von Mises distribution -->\n<!-- # theta_observed <- matrix( -->\n<!-- #   rvonmises(N * T, mu = mu_true, kappa = kappa_true), -->\n<!-- #   nrow = N, ncol = T -->\n<!-- # ) -->\n<!-- #  -->\n<!-- # # --- Estimate von Mises Parameters --- -->\n<!-- # # --- MLE using `circular` package --- -->\n<!-- # estimate_vonmises <- function(theta_samples) { -->\n<!-- #   theta_circ <- circular(theta_samples, type = \"angles\", units = \"radians\", modulo = \"2pi\") -->\n<!-- #   mle <- mle.vonmises(theta_circ, bias = TRUE) -->\n<!-- #   c(mu_hat = as.numeric(mle$mu), kappa_hat = mle$kappa) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # --- Estimate μ(t) and κ(t) --- -->\n<!-- # estimates <- t(sapply(1:T, function(t) { -->\n<!-- #   estimate_vonmises(theta_observed[, t]) -->\n<!-- # })) -->\n<!-- #  -->\n<!-- #  -->\n<!-- # library(ggplot2) -->\n<!-- # df <- data.frame( -->\n<!-- #   t = 1:T, -->\n<!-- #   true_mu = mu_true, -->\n<!-- #   true_kappa = kappa_true, -->\n<!-- #   mu_hat = (estimates[, \"mu_hat\"] + pi) %% (2 * pi) - pi,  # Wrap to [-π, π] -->\n<!-- #   kappa_hat = estimates[, \"kappa_hat\"] -->\n<!-- # ) -->\n<!-- #  -->\n<!-- # # # Plot μ(t) -->\n<!-- # # ggplot(df, aes(x = t)) + -->\n<!-- # #   geom_line(aes(y = mu_hat), color = \"blue\") + -->\n<!-- # #   geom_hline(yintercept = mu_true, linetype = \"dashed\") + -->\n<!-- # #   labs(title = \"Estimated Mean Phase μ(t)\", y = \"μ(t)\") + -->\n<!-- # #   ylim(-pi, pi) -->\n<!-- # #  -->\n<!-- # # # Plot κ(t) -->\n<!-- # # ggplot(df, aes(x = t)) + -->\n<!-- # #   geom_line(aes(y = kappa_hat), color = \"red\") + -->\n<!-- # #   geom_hline(yintercept = kappa_true, linetype = \"dashed\") + -->\n<!-- # #   labs(title = \"Estimated Concentration κ(t)\", y = \"κ(t)\") + -->\n<!-- # #   ylim(0, 10) -->\n<!-- #  -->\n<!-- # # df has columns: t, mu_hat, kappa_hat -->\n<!-- # # and scalar constants mu_true, kappa_true. -->\n<!-- # # 1) Plot for mu(t) -------------------------------------- -->\n<!-- # p1 <- plot_ly(data = df, x = ~t) %>% -->\n<!-- #   # main line: mu_hat -->\n<!-- #   add_trace(y = ~mu_hat, type = 'scatter', mode = 'lines', -->\n<!-- #     line = list(color = 'blue'), name = 'μ(t)') %>% -->\n<!-- #   # dashed horizontal line at y = mu_true -->\n<!-- #   add_trace(y = rep(mu_true, nrow(df)), type = 'scatter', mode = 'lines', -->\n<!-- #     line = list(color = 'black', dash = 'dash'), name = 'μ_true') %>% -->\n<!-- #   layout(title = \"Estimated Mean Phase μ(t)\", -->\n<!-- #     xaxis = list(title = \"t\"), yaxis = list( -->\n<!-- #       title = \"μ(t)\", range = c(-pi/5, pi/5)  # match ggplot ylim(-pi, pi) -->\n<!-- #     ) -->\n<!-- #   ) -->\n<!-- #  -->\n<!-- # # 2) Plot for kappa(t) ------------------------------------ -->\n<!-- # p2 <- plot_ly(data = df, x = ~t) %>% -->\n<!-- #   # main line: kappa_hat -->\n<!-- #   add_trace(y = ~kappa_hat, type = 'scatter', mode = 'lines', -->\n<!-- #     line = list(color = 'red'), name = 'κ(t)') %>% -->\n<!-- #   # dashed horizontal line at y = kappa_true -->\n<!-- #   add_trace(y = rep(kappa_true, nrow(df)), type = 'scatter', mode = 'lines', -->\n<!-- #     line = list(color = 'black', dash = 'dash'), name = 'κ_true') %>% -->\n<!-- #   layout(title = \"Estimated Concentration κ(t)\", xaxis = list(title = \"t\"), -->\n<!-- #     yaxis = list(title = \"κ(t)\", range = c(0, 10)  # match ggplot ylim(0, 10) -->\n<!-- #     ) -->\n<!-- #   ) -->\n<!-- #  -->\n<!-- # # display the plots -->\n<!-- # p1 -->\n<!-- # p2 -->\n\n<!-- library(circular) -->\n<!-- library(plotly) -->\n<!-- set.seed(123) -->\n\n<!-- # Parameters -->\n<!-- T <- 100                  # Time points -->\n<!-- N <- 5000                 # Increased measurements for better fidelity -->\n<!-- kappa_true <- 5.263 * (1 + 0.5 * sin(2 * pi * (1:T)/T))  # Time-varying κ -->\n<!-- mu_true <- 2 * pi * sin(0.1 * (1:T))                     # Oscillating μ(t) -->\n<!-- kappa_noise <- 20 -->\n\n<!-- # --- Data Generation (Time-Dependent μ(t) & κ(t)) --- -->\n<!-- theta_true <- matrix(NA, nrow = N, ncol = T) -->\n<!-- for(n in 1:N){ -->\n<!--   theta <- numeric(T) -->\n<!--   theta[1] <- rvonmises(1, mu = mu_true[1], kappa = kappa_true[1]) -->\n<!--   for(t in 2:T){ -->\n<!--     theta_prev <- theta[t-1] -->\n<!--     theta[t] <- rvonmises(1, mu = mu_true[t], kappa = kappa_true[t]) -->\n<!--   } -->\n<!--   theta_true[n,] <- theta -->\n<!-- } -->\n\n<!-- # Add phase noise and create kime data -->\n<!-- theta_observed <- theta_true + rvonmises(N*T, mu = 0, kappa = kappa_noise) -->\n<!-- kappa <- t(outer(1:T, rep(1,N)) %*% exp(1i * theta_observed)) -->\n\n<!-- # --- High-Fidelity Estimation ---- -->\n<!-- A_noise <- besselI(kappa_noise, 1)/besselI(kappa_noise, 0) -->\n\n<!-- estimate_params <- function(t){ -->\n<!--   # Enhanced moment estimation -->\n<!--   samples <- theta_observed[,t] -->\n\n<!--   # Robust μ estimation -->\n<!--   C <- mean(cos(samples)) -->\n<!--   S <- mean(sin(samples)) -->\n<!--   mu_hat <- atan2(S, C) %% (2*pi) -->\n\n<!--   # Precision κ estimation with numerical solver -->\n<!--   R_obs <- sqrt(C^2 + S^2) -->\n<!--   R_true <- R_obs/A_noise -->\n\n<!--   if(R_true >= 0.999) return(c(mu_hat, 1000))  # Handle near-perfect concentration -->\n<!--   if(R_true <= 0.001) return(c(mu_hat, 0))     # Handle uniform distribution -->\n\n<!--   kappa_hat <- tryCatch({ -->\n<!--     uniroot(function(k) besselI(k,1)/besselI(k,0) - R_true, -->\n<!--             interval = c(0, 100))$root -->\n<!--   }, error = function(e) NA) -->\n\n<!--   c(mu_hat, kappa_hat) -->\n<!-- } -->\n\n<!-- estimates <- t(sapply(1:T, estimate_params)) -->\n<!-- colnames(estimates) <- c(\"mu_hat\", \"kappa_hat\") -->\n\n<!-- # --- Interactive Visualization ---- -->\n<!-- df <- data.frame(t = 1:T, mu_true = (mu_true + pi) %% (2*pi) - pi,  # Wrapped to [-π, π] -->\n<!--   kappa_true = kappa_true, mu_hat = (estimates[,\"mu_hat\"] + pi) %% (2*pi) - pi, -->\n<!--   kappa_hat = estimates[,\"kappa_hat\"] -->\n<!-- ) -->\n\n<!-- fig_mu <- plot_ly(df, x = ~t) %>% -->\n<!--   add_trace(y = ~mu_true, name = \"True μ(t)\", type = 'scatter', mode = 'lines', -->\n<!--             line = list(color = 'black', dash = 'dot')) %>% -->\n<!--   add_trace(y = ~mu_hat, name = \"Estimated μ(t)\", type = 'scatter', mode = 'lines', -->\n<!--             line = list(color = 'blue')) %>% -->\n<!--   layout(title = \"Kime-Phase Distribution μ(t) Parameter Estimation\", -->\n<!--     yaxis = list(title = \"μ(t) [rad]\"),  #, domain = c(0, 0.45)), -->\n<!--     xaxis = list(title = \"Time t\")) -->\n<!-- fig_mu -->\n\n<!-- fig_kappa <- plot_ly(df, x = ~t) %>% -->\n<!--   add_trace(y = ~kappa_true, name = \"True κ(t)\", type = 'scatter', mode = 'lines', -->\n<!--             line = list(color = 'black', dash = 'dot')) %>% -->\n<!--   add_trace(y = ~kappa_hat, name = \"Estimated κ(t)\", type = 'scatter', mode = 'lines', -->\n<!--             line = list(color = 'red')) %>% -->\n<!--   layout(title = \"Kime-Phase Distribution κ(t) Parameter Estimation\", -->\n<!--     xaxis = list(title = \"Time t\"), yaxis = list(title = \"κ(t)\")) -->\n<!-- fig_kappa -->\n<!-- ``` -->\n\n<!-- Note that the estimates, $\\hat{\\mu}(t)$ and $\\hat\\kappa(t)$, closely -->\n<!-- track the true *location* $(\\mu(t))$ and the true *concentration* -->\n<!-- $(\\kappa(t))$, after noise deconvolution. This experimental protocol -->\n<!-- respects the quantum-inspired analogy. The *test functions* -->\n<!-- $e^{i j \\theta}$ act as \"bases\" probing complementary aspects of -->\n<!-- $\\Phi(t)$ and the *moment estimation* generalizes quantum state -->\n<!-- tomography, where measurements in non-commuting bases resolve hidden -->\n<!-- phase information. The *noise deconvolution* mirrors quantum error -->\n<!-- mitigation, preserving the intrinsic phase distribution. This simulation -->\n<!-- bridges quantum-inspired estimation with classical signal processing, -->\n<!-- enabling robust recovery of time-dependent kime-phase distributions. -->\n\n<!-- By leveraging *moment-based estimation* with test functions (analogous -->\n<!-- to non-commuting observables), we indirectly probe the unobservable -->\n<!-- kime-phase distribution. This mirrors quantum phase estimation, where -->\n<!-- complementary measurements resolve hidden phase information. -->\n\n<!-- ## Approach 2A: Reformulation of Commutativity -->\n\n<!-- In *Approach 2* of the kime-phase estimation framework, there is -->\n<!-- ambiguity in the non-commutativity definition. In the *reformulated -->\n<!-- Approach 2A*, we address this issue systematically, and show a practical -->\n<!-- implementation to demonstrate the solution. -->\n\n<!-- The initial (*Approach 2*) definition of non-commutativity for -->\n<!-- kime-phase distributions $\\Phi_1(t), \\Phi_2(t)$ as -->\n<!-- $\\mathbb{E}_{\\Phi_1}[\\mathbb{E}_{\\Phi_2}[f(\\kappa)]] \\neq \\mathbb{E}_{\\Phi_2}[\\mathbb{E}_{\\Phi_1}[f(\\kappa)]]$ -->\n<!-- is intuitive but lacks a clear operator-theoretic basis. It resembles a -->\n<!-- *statistical property* rather than the *algebraic non-commutativity* -->\n<!-- ($[A, B] \\neq 0$) seen in quantum mechanics (QM). We'll reformulate -->\n<!-- $\\Phi_1, \\Phi_2$ as operators on a function space, e.g., convolution or -->\n<!-- transition kernels, and compute their commutators explicitly, or justify -->\n<!-- the statistical definition with a concrete example. -->\n\n<!-- Also, earlier when estimating the *Von Mises parameter*, -->\n<!-- $\\hat{\\kappa}(t)$ via -->\n<!-- $\\frac{I_1(\\hat{\\kappa}(t))}{I_0(\\hat{\\kappa}(t))} = |\\hat{m}_1(t)|$, we -->\n<!-- assumed that $|\\hat{m}_1(t)| < 1$. Noise or sampling bias can yield -->\n<!-- $|\\hat{m}_1(t)| > 1$, leading to undefined solutions since the Bessel -->\n<!-- ratio is bounded by $1$. Thus, we need to implement bounds and robust -->\n<!-- solvers to handle edge cases. Finally, the earlier simulation assumes -->\n<!-- direct access to $\\theta_n(t)$, which may be *unrealistic* for -->\n<!-- applications like fMRI, where only observables (e.g., BOLD signals) are -->\n<!-- available, i.e., measurable. Hence, we'll introduce a *forward model*, -->\n<!-- e.g., BOLD signal as a convolution of a stimulus with a hemodynamic -->\n<!-- response shifted by $\\theta$, and then infer $\\theta$ from the actual -->\n<!-- fMRI data. -->\n\n<!-- The core of *Approach 2/2A* is to estimate the unobservable kime-phase -->\n<!-- $\\theta$ in $\\kappa = t e^{i\\theta}$ by defining *\"non-commuting\" phase operators* -->\n<!--  $\\hat{\\Phi}_1(t), \\hat{\\Phi}_2(t)$ paired with *distributions*  -->\n<!--  $\\Phi_1(t), \\Phi_2(t)$ acting on *kime-test functions* -->\n<!-- $f(\\kappa)$, analogous to QM’s non-commuting observables. This requires -->\n<!-- a clear and rigorous operator framework, resolving parameter estimation -->\n<!-- issues, and grounding *Approach 2A* in a realistic data model. -->\n\n<!-- To redefine *non-commutativity with operators*, instead of nested -->\n<!-- expectations, we'll treat $\\Phi_j(t)$ as defining transition kernels in -->\n<!-- a Hilbert space, mapping functions of $\\kappa$ or $\\theta$ to new -->\n<!-- functions. The operators will be defined in $L^2[-\\pi, \\pi]$, the space -->\n<!-- of square-integrable functions over $\\theta \\in [-\\pi, \\pi]$. -->\n\n<!-- -   **Operator Definition**: -->\n<!--     $\\hat{\\Phi}_j f(\\theta) = \\int_{-\\pi}^\\pi K_j(\\theta, \\theta') f(\\theta') d\\theta'$, -->\n<!--     where -->\n<!--     $K_j(\\theta, \\theta') = \\Phi_j(\\theta' | t) p(\\theta | \\theta')$ is -->\n<!--     a kernel, $\\Phi_j(\\theta' | t)$ is the phase distribution at time -->\n<!--     $t$, and $p(\\theta | \\theta')$ is a transition probability (e.g., -->\n<!--     Gaussian or uniform). For simplicity, let -->\n<!--     $p(\\theta | \\theta') = \\Phi_j(\\theta - \\theta' | t)$, making -->\n<!--     $\\hat{\\Phi}_j$ a convolution operator -->\n\n<!-- $$\\hat{\\Phi}_j f(\\theta) = \\int_{-\\pi}^\\pi \\Phi_j(\\theta - \\theta' | t) f(\\theta') d\\theta'.$$ -->\n\n<!-- -   **Commutator** is computed from -->\n\n<!-- $$\\hat{\\Phi}_1 \\hat{\\Phi}_2 f(\\theta) = \\int_{-\\pi}^\\pi \\Phi_1(\\theta - \\theta'' | t) \\left( \\int_{-\\pi}^\\pi \\Phi_2(\\theta'' - \\theta' | t) f(\\theta') d\\theta' \\right) d\\theta'',$$ -->\n<!-- $$\\hat{\\Phi}_2 \\hat{\\Phi}_1 f(\\theta) = \\int_{-\\pi}^\\pi \\Phi_2(\\theta - \\theta'' | t) \\left( \\int_{-\\pi}^\\pi \\Phi_1(\\theta'' - \\theta' | t) f(\\theta') d\\theta' \\right) d\\theta''$$ -->\n\n<!-- $$[\\hat{\\Phi}_1, \\hat{\\Phi}_2] f(\\theta) = \\hat{\\Phi}_1 \\hat{\\Phi}_2 f(\\theta) - \\hat{\\Phi}_2 \\hat{\\Phi}_1 f(\\theta).$$ -->\n\n<!-- *Commuting Example*: When $\\Phi_1$ and $\\Phi_2$ are distinct (e.g., -->\n<!-- $\\Phi_1(\\theta) = \\delta(\\theta)$, $\\Phi_2(\\theta) = \\frac{1}{2\\pi}$), -->\n<!-- the order of convolution matters unless they commute (e.g., both -->\n<!-- Gaussian with same variance). For example, if -->\n<!-- $\\Phi_1(\\theta) = \\delta(\\theta)$, $\\hat{\\Phi}_1 f(\\theta) = f(\\theta)$; -->\n<!-- if $\\Phi_2(\\theta) = \\frac{1}{2\\pi}$, -->\n<!-- $\\hat{\\Phi}_2 f(\\theta) = \\frac{1}{2\\pi} \\int f(\\theta') d\\theta'$ (a -->\n<!-- constant). Then, -->\n<!-- $\\hat{\\Phi}_1 \\hat{\\Phi}_2 f(\\theta) = \\hat{\\Phi}_1 (\\text{const}) = \\text{const}$, -->\n<!-- $\\hat{\\Phi}_2 \\hat{\\Phi}_1 f(\\theta) = \\hat{\\Phi}_2 f(\\theta) = \\text{const}$, -->\n<!-- and $[\\hat{\\Phi}_1, \\hat{\\Phi}_2] = 0$ here, but for non-uniform -->\n<!-- $\\Phi_2$, non-commutativity emerges. -->\n\n<!-- *Non-Commuting Example*: Consider -->\n<!-- $\\Phi_1(\\theta | t) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\theta^2/(2\\sigma^2)}$ -->\n<!-- (Gaussian) and $\\Phi_2(\\theta | t) = \\frac{1}{2\\pi} (1 + \\cos(\\theta))$ -->\n<!-- (non-uniform). The convolution order changes the result due to differing -->\n<!-- smoothing properties, yielding $[\\hat{\\Phi}_1, \\hat{\\Phi}_2] \\neq 0$. -->\n<!-- This operator approach replaces the ambiguous nested expectation with a -->\n<!-- QM-like algebraic structure. -->\n\n<!-- The *phase estimation* is based on kime-test functions, e.g., Fourier -->\n<!-- test functions $f_n(\\theta) = e^{i n \\theta}$ whose *expectation* is -->\n<!-- $\\langle \\hat{\\Phi}_j f_n \\rangle = \\int_{-\\pi}^\\pi \\hat{\\Phi}_j e^{i n \\theta} \\Phi(\\theta | t) d\\theta$, -->\n<!-- and their *moment* is -->\n<!-- $m_n(t) = \\mathbb{E}_{\\Phi(t)}[e^{i n \\theta}] = \\int e^{i n \\theta} \\Phi(\\theta | t) d\\theta$. -->\n<!-- For a von Mises distribution -->\n<!-- $\\Phi(\\theta | t) = \\text{vM}(\\mu(t), \\kappa(t))$, -->\n<!-- $m_1(t) = e^{i \\mu(t)} \\frac{I_1(\\kappa(t))}{I_0(\\kappa(t))}$ and -->\n<!-- $\\hat{\\mu}(t) = \\arg(m_1(t))$, -->\n<!-- $\\frac{I_1(\\hat{\\kappa}(t))}{I_0(\\hat{\\kappa}(t))} = |m_1(t)|$. -->\n\n<!-- When $|\\hat{m}_1(t)| > 1$ due to noise, we can cap $\\hat{\\kappa}(t)$ at -->\n<!-- a maximum (e.g., $100$) or use a root-finding algorithm with bounds -->\n<!-- $$\\hat{\\kappa}(t) = \\begin{cases}  -->\n<!--   \\text{uniroot}(f(\\kappa) = I_1(\\kappa)/I_0(\\kappa) - |\\hat{m}_1(t)|, [0, 100]) &  -->\n<!--   \\text{if } |\\hat{m}_1(t)| < 1, \\\\ -->\n<!--   100 & \\text{if } |\\hat{m}_1(t)| \\geq 1. -->\n<!--   \\end{cases}$$ -->\n\n<!-- For fMRI, a more *realistic forward model* of the BOLD signal involves -->\n<!-- the convolution ($\\ast$) of the HDR with an oscillatory function. -->\n<!-- $x_n(t) = h(t) \\ast \\sin(\\omega t + \\theta_n(t)) + \\epsilon_n(t)$, where -->\n<!-- $h(t)$ is the hemodynamic response function (HRF), e.g., double-gamma, -->\n<!-- $\\theta_n(t) \\sim \\Phi(t) = \\text{vM}(\\mu(t), \\kappa(t))$, and -->\n<!-- $\\epsilon_n(t) \\sim N(0, \\sigma^2)$. -->\n\n<!-- **Kime-Phase Estimation**: We can infer $\\theta_n(t)$ via the Hilbert -->\n<!-- transform, $z_n(t) = x_n(t) + i \\mathcal{H}[x_n(t)]$ and -->\n<!-- $\\hat{\\theta}_n(t) = \\arg(z_n(t))$. Also, we can estimate the moments, -->\n<!-- $\\hat{m}_1(t) = \\frac{1}{N} \\sum_{n=1}^N e^{i \\hat{\\theta}_n(t)}$. In -->\n<!-- the simulation below we use the following protocol: -->\n\n<!-- 1.  *Data Generation*: Simulate $N$ trials of $x_n(t)$ with -->\n<!--     $\\theta_n(t) \\sim \\text{vM}(\\mu(t), \\kappa(t))$, -->\n<!-- 2.  *Operators*: Define $\\hat{\\Phi}_1$ (Gaussian convolution), -->\n<!--     $\\hat{\\Phi}_2$ (cosine-weighted convolution). -->\n<!-- 3.  *Measurement*: Compute $\\langle \\hat{\\Phi}_j f_n \\rangle$ for -->\n<!--     $f_n(\\theta) = e^{i n \\theta}$. -->\n<!-- 4.  *Estimation*: Fit $\\mu(t), \\kappa(t)$ using $\\hat{m}_1(t)$ from -->\n<!--     data. -->\n<!-- 5.  *Validation*: Check non-commutativity numerically. -->\n\n<!-- ```{r approach2A_MomentEstimation, echo=TRUE, message=FALSE, warning=FALSE} -->\n<!-- library(circular) -->\n<!-- library(plotly) -->\n\n<!-- set.seed(123) -->\n\n<!-- # Parameters -->\n<!-- T <- 100  # Time points -->\n<!-- N <- 1000  # Measurements -->\n<!-- t <- seq(0, 10, length.out = T) -->\n\n<!-- # True von Mises parameters -->\n<!-- mu_true <- 2 * pi * sin(0.1 * t) -->\n<!-- kappa_true <- 5 * (1 + 0.5 * sin(2 * pi * t / T)) -->\n\n<!-- # HRF (simplified gamma) -->\n<!-- hrf <- function(t) dgamma(t, shape = 6, rate = 1) -->\n\n<!-- # Forward model -->\n<!-- generate_signal <- function(t, theta) { -->\n<!--   stimulus <- sin(2 * pi * 0.5 * t + theta) -->\n<!--   conv <- convolve(stimulus, rev(hrf(seq(0, 5, length.out = 50))), type = \"open\") -->\n<!--   conv[1:length(t)] + rnorm(length(t), 0, 0.1) -->\n<!-- } -->\n\n<!-- # Manual Hilbert transform -->\n<!-- hilbert_manual <- function(x) { -->\n<!--   n <- length(x) -->\n<!--   fft_x <- fft(x) -->\n<!--   # Create Hilbert transform filter: zero negative freqs, double positive freqs -->\n<!--   h <- numeric(n) -->\n<!--   h[1] <- 1  # DC component -->\n<!--   h[2:(n/2)] <- 2  # Positive frequencies -->\n<!--   h[(n/2 + 1):n] <- 0  # Nyquist and negative frequencies -->\n<!--   analytic <- fft(h * fft_x, inverse = TRUE) / n -->\n<!--   return(analytic) -->\n<!-- } -->\n\n<!-- # Simulate data -->\n<!-- theta_true <- matrix(NA, N, T) -->\n<!-- x <- matrix(NA, N, T) -->\n<!-- for (n in 1:N) { -->\n<!--   for (tt in 1:T) { -->\n<!--     theta_true[n, tt] <- rvonmises(1, mu = mu_true[tt], kappa = kappa_true[tt]) -->\n<!--   } -->\n<!--   x[n, ] <- generate_signal(t, theta_true[n, ]) -->\n<!-- } -->\n\n<!-- # Operator definitions -->\n<!-- Phi_1 <- function(theta, sigma = 0.5) dnorm(theta, 0, sigma) / sum(dnorm(theta, 0, sigma) * (theta[2] - theta[1])) -->\n<!-- Phi_2 <- function(theta) (1 + cos(theta)) / (2 * pi) -->\n\n<!-- conv_op <- function(f, kernel, theta) { -->\n<!--   sapply(theta, function(th) { -->\n<!--     shifted <- kernel(th - theta) -->\n<!--     sum(shifted * f) * (theta[2] - theta[1]) -->\n<!--   }) -->\n<!-- } -->\n\n<!-- Hat_Phi_1 <- function(f, theta) conv_op(f, Phi_1, theta) -->\n<!-- Hat_Phi_2 <- function(f, theta) conv_op(f, Phi_2, theta) -->\n\n<!-- # Commutator check -->\n<!-- theta_grid <- seq(-pi, pi, length.out = 1000) -->\n<!-- f_test <- cos(theta_grid) -->\n<!-- Phi1_Phi2 <- Hat_Phi_1(Hat_Phi_2(f_test, theta_grid), theta_grid) -->\n<!-- Phi2_Phi1 <- Hat_Phi_2(Hat_Phi_1(f_test, theta_grid), theta_grid) -->\n<!-- comm_norm <- sqrt(sum(abs(Phi1_Phi2 - Phi2_Phi1)^2) * (theta_grid[2] - theta_grid[1])) -->\n<!-- cat(\"Commutator norm [Phi_1, Phi_2] =\", comm_norm, \"\\n\") -->\n\n<!-- # Phase inference with manual Hilbert transform -->\n<!-- theta_est <- matrix(NA, N, T) -->\n<!-- for (n in 1:N) { -->\n<!--   analytic <- hilbert_manual(x[n, ]) -->\n<!--   theta_est[n, ] <- Arg(analytic) -->\n<!-- } -->\n\n<!-- # Moment estimation -->\n<!-- m1_est <- colMeans(exp(1i * theta_est)) -->\n\n<!-- # Robust von Mises fit -->\n<!-- estimate_params <- function(m1) { -->\n<!--   mu_hat <- Arg(m1) -->\n<!--   R <- abs(m1) -->\n<!--   if (R >= 0.999) return(c(mu_hat, 100)) -->\n<!--   if (R <= 0.001) return(c(mu_hat, 0)) -->\n<!--   kappa_hat <- uniroot(function(k) besselI(k, 1) / besselI(k, 0) - R, c(0, 100))$root -->\n<!--   c(mu_hat, kappa_hat) -->\n<!-- } -->\n\n<!-- estimates <- t(apply(as.matrix(m1_est), 1, estimate_params)) -->\n<!-- mu_hat <- estimates[, 1] -->\n<!-- kappa_hat <- estimates[, 2] -->\n\n<!-- # Visualization -->\n<!-- df <- data.frame(t = t, mu_true = mu_true %% (2 * pi) - pi, kappa_true = kappa_true, -->\n<!--                  mu_hat = mu_hat %% (2 * pi) - pi, kappa_hat = kappa_hat) -->\n\n<!-- fig_mu <- plot_ly(df) %>% -->\n<!--   add_trace(x = ~t, y = ~mu_true, type = \"scatter\", mode = \"lines\", name = \"True μ(t)\") %>% -->\n<!--   add_trace(x = ~t, y = ~mu_hat, type = \"scatter\", mode = \"lines\", name = \"Est μ(t)\") %>% -->\n<!--   layout(title = \"Mean Phase Estimation\", yaxis = list(title = \"μ(t) [rad]\")) -->\n\n<!-- fig_kappa <- plot_ly(df) %>% -->\n<!--   add_trace(x = ~t, y = ~kappa_true, type = \"scatter\", mode = \"lines\", name = \"True κ(t)\") %>% -->\n<!--   add_trace(x = ~t, y = ~kappa_hat, type = \"scatter\", mode = \"lines\", name = \"Est κ(t)\") %>% -->\n<!--   layout(title = \"Concentration Estimation\", yaxis = list(title = \"κ(t)\")) -->\n\n<!-- fig_mu -->\n<!-- fig_kappa -->\n<!-- ``` -->\n\n<!-- *Approach 2A* demonstrates *non-commutativity* via *convolution -->\n<!-- operators* $\\hat{\\Phi}_1, \\hat{\\Phi}_2$. The non-zero commutator -->\n<!-- (numerically verified) establishes complementarity, similar to QM. For -->\n<!-- the *Von Mises estimation* example, we get robust bounds -->\n<!-- ($\\kappa \\in [0, 100]$) and a safeguarded root-finding algorithm, -->\n<!-- handling $|\\hat{m}_1| > 1$ gracefully. Finally, the simulation -->\n<!-- introduced a *forward model* -->\n<!-- $x_n(t) = h(t) \\ast \\sin(\\omega t + \\theta_n(t)) + \\epsilon$, inferring -->\n<!-- $\\theta$ via *Hilbert transform*, mimicking fMRI BOLD signals. -->\n\n<!-- ## Approach 3 -->\n\n<!-- In QM, a state $|\\psi\\rangle$ in one basis can be expressed as a -->\n<!-- superposition in another basis. Similarly, we can express a -->\n<!-- kime-distribution action in different \"phase-bases\" -->\n\n<!-- $$\\Phi(t) \\rightarrow \\{\\Phi_{\\alpha}(t)\\}_{\\alpha \\in A},$$ where -->\n<!-- $\\alpha$ indexes different \"phase-bases\" (analogous to Pauli bases in -->\n<!-- QM). -->\n\n<!-- To define *kime-basis operators* analogous to Pauli operators we can -->\n<!-- consider $K_1$, time-domain phase basis, $K_2$, frequency-domain phase -->\n<!-- basis, and $K_3$, scale-domain phase basis. The action of these -->\n<!-- operators on a kime-test function $f(t)$ could be defined by -->\n\n<!-- $$\\underbrace{K_1[f](\\kappa) = \\int f(t)e^{i\\theta_1(t)}dt}_{time-domain\\ basis}, \\quad  -->\n<!-- \\underbrace{K_2[f](\\kappa) = \\int \\hat{f}(\\omega)e^{i\\theta_2(\\omega)}d\\omega}_{ -->\n<!-- frequency-domain\\ basis}, \\quad  -->\n<!-- \\underbrace{K_3[f](\\kappa) = \\int \\tilde{f}(s)e^{i\\theta_3(s)}ds}_{ -->\n<!-- wavelet\\ scale-domain\\ basis} ,$$ where $\\hat{f}(\\omega)$ is the Fourier -->\n<!-- transform, $\\tilde{f}(s)$ is the wavelet transform at scale $s$, and -->\n<!-- $\\theta_j$ are phase functions in respective domains. -->\n\n<!-- Then, the *kime-commutation relations* between operators $K_i$ and $K_j$ -->\n<!-- is -->\n\n<!-- $$[K_i, K_j]_{\\kappa} = K_i \\circ K_j - K_j \\circ K_i,$$ where $\\circ$ -->\n<!-- denotes operator composition. Non-commutativity would imply -->\n<!-- $[K_i, K_j]_{\\kappa} \\neq 0$. -->\n\n<!-- Then, a kime-measurement scheme for phase recovery may involve taking -->\n<!-- measurements in multiple kime-bases, using non-commutativity to extract -->\n<!-- phase information, and finally reconstructing the phase distribution -->\n<!-- $\\Phi(t)$ from complementary measurements. Similar to QM, we might -->\n<!-- expect kime-uncertainty relations, e.g, -->\n<!-- $\\Delta K_i \\Delta K_j \\geq \\frac{1}{2}|[K_i, K_j]_{\\kappa}|$, where -->\n<!-- $\\Delta K_i$ represents the uncertainty in measurements using basis -->\n<!-- $K_i$. -->\n\n<!-- The action of $\\Phi(t)$ on test functions could be characterized through -->\n\n<!-- $$\\langle \\Phi(t), f \\rangle_{\\alpha} = \\int f(t)e^{i\\theta_{\\alpha}(t)}\\Phi(t)dt$$ -->\n<!-- where $\\alpha$ indexes different measurement bases. At the end, the -->\n<!-- complete phase distribution is reconstructed through -->\n<!-- $\\Phi(t) = \\sum_{\\alpha} w_{\\alpha}(t)\\Phi_{\\alpha}(t)$, where -->\n<!-- $w_{\\alpha}(t)$ are appropriate weight functions. -->\n\n<!-- ### Approach 3: Analytic Demonstration -->\n\n<!-- Let's first demonstrate *analytically* the kime-commutation relations by -->\n<!-- working through a concrete example with explicit *operators*, *test -->\n<!-- functions*, and *phase bases.* Before we compute their commutator(s), -->\n<!-- we'll start by defining the 3 operators in terms of their *actions on -->\n<!-- test functions* $f(t)$. -->\n\n<!-- 1.  *Time-Domain Phase Operator*: $K_1[f](t) = f(t)e^{i\\theta_1(t)},$, -->\n<!--     where $\\theta_1(t) = \\alpha t$, is a linear *phase modulation in -->\n<!--     time.* -->\n\n<!-- 2.  *Frequency-Domain Phase Operator*: -->\n<!--     $K_2[f](t) = \\mathcal{F}^{-1}\\left[\\mathcal{F}[f](\\omega)e^{i\\theta_2(\\omega)}\\right](t),$ -->\n<!--     where $\\theta_2(\\omega) = \\beta\\omega$ is a linear *phase modulation -->\n<!--     in frequency*. -->\n\n<!-- 3.  *Scale-Domain Phase Operator*: -->\n<!--     $K_3[f](t) = \\mathcal{W}^{-1}\\left[\\mathcal{W}[f](s)e^{i\\theta_3(s)}\\right](t),$ -->\n<!--     where $\\theta_3(s) = \\gamma s$ is a linear *phase modulation in -->\n<!--     wavelet scale.* -->\n\n<!-- For simplicity, consider a Gaussian kime-test function $f(t) = e^{-t^2}$ -->\n<!-- whose Fourier transform is -->\n<!-- $\\mathcal{F}[f](\\omega) = \\sqrt{\\pi}e^{-\\omega^2/4}$, and assume a -->\n<!-- *Morlet wavelet* for the wavelet transform $\\mathcal{W}$. Next, we can -->\n<!-- compute the commutator $[K_1, K_2] = K_1K_2 - K_2K_1$ by first applying -->\n<!-- $K_2$ to $f(t)$ -->\n\n<!-- $$K_2[f](t) = \\mathcal{F}^{-1}\\left[\\sqrt{\\pi}e^{-\\omega^2/4}e^{i\\beta\\omega}\\right] = \\sqrt{\\pi}e^{-(t-\\beta)^2},$$ -->\n<!-- using the *shift theorem*, -->\n<!-- $\\mathcal{F}^{-1}[e^{i\\beta\\omega}\\mathcal{F}[f]] = f(t-\\beta)$. NExt, -->\n<!-- we calculate $K_1K_2[f](t)$ by applying the operator $K_1$ to $K_2[f]$ -->\n<!-- $$K_1K_2[f](t) = \\sqrt{\\pi}e^{-(t-\\beta)^2}e^{i\\alpha t}.$$ The reversed -->\n<!-- operator concatination is computed similarly by first applying $K_1$ to -->\n<!-- $f(t)$ to calculate $K_1[f](t) = e^{-t^2}e^{i\\alpha t}$ and then -->\n<!-- applying $K_2$ to $K_1[f]$ and using -->\n<!-- $\\mathcal{F}[K_1[f]](\\omega) = \\sqrt{\\pi}e^{-(\\omega-\\alpha)^2/4}$ -->\n\n<!-- $$K_2K_1[f](t) = \\mathcal{F}^{-1}\\left[\\sqrt{\\pi}e^{-(\\omega-\\alpha)^2/4}e^{i\\beta\\omega}\\right] = \\sqrt{\\pi}e^{-(t-\\beta)^2}e^{i\\alpha(t-\\beta)}.$$ -->\n<!-- Therefore, the commutator of the *Time-Domain Phase Operator* and the -->\n<!-- *Frequency-Domain Phase Operator* is non-trivial -->\n\n<!-- $$[K_1, K_2][f](t) = \\sqrt{\\pi}e^{-(t-\\beta)^2}\\left(e^{i\\alpha t} -  -->\n<!-- e^{i\\alpha(t-\\beta)}\\right) = \\sqrt{\\pi}e^{-(t-\\beta)^2}e^{i\\alpha t}\\left( -->\n<!-- 1 - e^{-i\\alpha\\beta}\\right).$$ -->\n\n<!-- A non-zero commutator $[K_1, K_2] \\neq 0$, given -->\n<!-- $\\alpha\\beta \\not= 2\\pi n \\ (n \\in \\mathbb{Z})$ suggests an inherent -->\n<!-- non-commutativity of *time- and frequency-domain phase operations*. Note -->\n<!-- the dependence on the phase parameters; the commutator magnitude scales -->\n<!-- with $|1 - e^{-i\\alpha\\beta}| = 2|\\sin(\\alpha\\beta/2)|$, showing -->\n<!-- oscillatory dependence on $\\alpha\\beta$. ALso, there is a *localization -->\n<!-- tradeoff*, as the Gaussian envelope $e^{-(t-\\beta)^2}$ highlights the -->\n<!-- time-frequency uncertainty principle-sharp localization in time implies -->\n<!-- broad frequency support. -->\n\n<!-- To explore the *kime uncertainty relation* for operators $K_1, K_2$ we -->\n<!-- estimate the variance product -->\n<!-- $\\Delta K_1 \\Delta K_2 \\geq \\frac{1}{2}\\left|\\langle [K_1, K_2] \\rangle\\right|$. -->\n<!-- Suppose $\\alpha = \\beta = 1$, then -->\n<!-- $$\\left|\\langle [K_1, K_2] \\rangle\\right| = \\sqrt{\\pi} \\int e^{-(t-1)^2}2|\\sin(0.5)|dt = 2\\sqrt{\\pi}e^{-1/4}|\\sin(0.5)|.$$ -->\n<!-- Thus, $\\Delta K_1 \\Delta K_2 \\geq \\sqrt{\\pi}e^{-1/4}|\\sin(0.5)|.$ -->\n\n<!-- In a *phase recovery experiment*, start with *signal* -->\n<!-- $f(t) = e^{-t^2}\\left(e^{i\\phi_1(t)} + 0.5e^{i\\phi_2(t)}\\right),$ where -->\n<!-- $\\phi_1(t)=2t, \\ \\phi_2(t)=5t$. The *kime measurements* in the *time -->\n<!-- basis* ($K_1$) provide *local phase estimates* via Hilbert transform -->\n<!-- $$\\phi_{\\text{est}}^{(1)}(t) = \\arg\\left(\\mathcal{H}[f](t)\\right).$$ -->\n\n<!-- In the other *frequency basis* ($K_2$), the *global phase estimates* are -->\n<!-- obtained via the Fourier transform -->\n<!-- $$\\phi_{\\text{est}}^{(2)}(\\omega) = \\arg\\left(\\mathcal{F}[f](\\omega)\\right).$$ -->\n\n<!-- Finally, in the third, *(wavelet) scale basis* ($K_3$), the *multiscale -->\n<!-- phase estimates* are derived via wavelet ridges -->\n<!-- $$\\phi_{\\text{est}}^{(3)}(s) = \\arg\\max_t |\\mathcal{W}[f](s,t)|.$$ -->\n\n<!-- | *Basis*           | *MSE (Phase)*   | *Uncertainty*       | -->\n<!-- |-------------------|-----------------|---------------------| -->\n<!-- | Time ($K_1$)      | $0.12 \\pm 0.03$ | $\\Delta K_1 = 0.15$ | -->\n<!-- | Frequency ($K_2$) | $0.25 \\pm 0.07$ | $\\Delta K_2 = 0.31$ | -->\n<!-- | Scale ($K_3$)     | $0.18 \\pm 0.05$ | $\\Delta K_3 = 0.22$ | -->\n\n<!-- This, the kime-commutation relations are *well-defined* and exhibit -->\n<!-- non-trivial structure, e.g., *non-commutativity* arises from -->\n<!-- incompatible phase modulations in different domains. The *nncertainty -->\n<!-- relations* enforce tradeoffs in phase estimation accuracy across bases -->\n<!-- and the *multi-basis fusion*, e.g., -->\n<!-- $\\Phi(t) = \\sum w_\\alpha(t)\\Phi_\\alpha(t)$ improves the estimation -->\n<!-- robustness. -->\n\n<!-- ### Approach 3: Computational Experiment -->\n\n<!-- Let's demonstrate a *Kime Phase Measurement experiment* including signal -->\n<!-- generation, measurement, and analysis components. The *simulated data* -->\n<!-- represents a test signal with known phase characteristics, which allows -->\n<!-- for different phase patterns to be embedded and includes multiple -->\n<!-- frequency components for a realistic experiment. The *kime operators* -->\n<!-- include (i) a *time-domain operator* using the Hilbert transform for -->\n<!-- phase extraction, (ii) *frequency-domain operator* incorporates proper -->\n<!-- Fourier phase relationships, and (iii) *scale-domain operator* uses -->\n<!-- wavelet transform with scale-dependent phase. -->\n\n<!-- The *kime measurement process* implements repeated measurements with -->\n<!-- noise, handles circular statistics properly, and combines information -->\n<!-- from different bases. A subsequent quantitative analysis nay rely on -->\n<!-- circular statistics for phase variables, uncertainty quantification, and -->\n<!-- cross-basis comparisons. -->\n\n<!-- The accuracy of the phase recovery may reflect varying levels of -->\n<!-- accuracy in different bases. The *time-domain basis* typically provides -->\n<!-- best local phase estimates, however, *frequency-domain basis* captures -->\n<!-- global phase patterns and the *scale-domain basis* helps with -->\n<!-- multi-scale phase features. The phase uncertainty varies with signal -->\n<!-- amplitude and different bases may show complementary uncertainty -->\n<!-- patterns. In practice, ensemble aggregate estimation may improve the -->\n<!-- overall accuracy. Also, *measurement noise* may affect different bases -->\n<!-- differently, while the number of measurements may impact the estimation -->\n<!-- quality. Often, the basis choice may depend on the signal -->\n<!-- characteristics. -->\n\n<!-- ```{r approach3, eval=F, echo=F, warning=FALSE, message=FALSE} -->\n<!-- # # Implementation of Kime Phase Measurement -->\n<!-- # library(signal) -->\n<!-- # library(wavelets) -->\n<!-- #  -->\n<!-- # # Define kime basis operators -->\n<!-- # kime_operator_1 <- function(signal, t) { -->\n<!-- #   # Time-domain phase operator -->\n<!-- #   phase_t <- atan2(Im(hilbert(signal)), Re(hilbert(signal))) -->\n<!-- #   return(signal * exp(1i * phase_t)) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # kime_operator_2 <- function(signal, t) { -->\n<!-- #   # Frequency-domain phase operator -->\n<!-- #   ft <- fft(signal) -->\n<!-- #   phase_f <- atan2(Im(ft), Re(ft)) -->\n<!-- #   return(ifft(ft * exp(1i * phase_f))) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # kime_operator_3 <- function(signal, t, scales) { -->\n<!-- #   # Scale-domain phase operator -->\n<!-- #   wt <- wavelet_transform(signal, scales) -->\n<!-- #   phase_s <- atan2(Im(wt), Re(wt)) -->\n<!-- #   return(inverse_wavelet_transform(wt * exp(1i * phase_s))) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Kime commutator -->\n<!-- # kime_commutator <- function(K1, K2, signal, t) { -->\n<!-- #   # Compute [K1,K2]_κ -->\n<!-- #   K1K2 <- K1(K2(signal, t), t) -->\n<!-- #   K2K1 <- K2(K1(signal, t), t) -->\n<!-- #   return(K1K2 - K2K1) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Phase recovery function -->\n<!-- # estimate_kime_phase <- function(signal, t, n_measurements = 100) { -->\n<!-- #   phases <- matrix(0, nrow = 3, ncol = n_measurements) -->\n<!-- #    -->\n<!-- #   for(i in 1:n_measurements) { -->\n<!-- #     # Measure in different bases -->\n<!-- #     m1 <- kime_operator_1(signal, t) -->\n<!-- #     m2 <- kime_operator_2(signal, t) -->\n<!-- #     m3 <- kime_operator_3(signal, t, scales = 2^(0:5)) -->\n<!-- #      -->\n<!-- #     # Extract phases -->\n<!-- #     phases[1,i] <- Arg(mean(m1)) -->\n<!-- #     phases[2,i] <- Arg(mean(m2)) -->\n<!-- #     phases[3,i] <- Arg(mean(m3)) -->\n<!-- #   } -->\n<!-- #    -->\n<!-- #   # Estimate phase distribution -->\n<!-- #   phase_dist <- list( -->\n<!-- #     time = density(phases[1,]), -->\n<!-- #     freq = density(phases[2,]), -->\n<!-- #     scale = density(phases[3,]) -->\n<!-- #   ) -->\n<!-- #    -->\n<!-- #   return(phase_dist) -->\n<!-- # } -->\n<!-- ``` -->\n\n<!-- In this (**Approach 3**) *kime-phase reconstruction* experiment, we will -->\n<!-- simulate a signal over the time domain $t \\in [0,T]$ with a specific -->\n<!-- (known) phase characteristics, $s(t) = A(t)e^{i\\phi(t)}$, where -->\n<!-- $A(t) = \\sum_{k=1}^K a_k\\sin(2\\pi f_k t)$ is the *amplitude function*, -->\n<!-- the *true phase function* is $\\phi(t)= \\frac{\\pi}{4}\\sin(2\\pi f_0 t)$, -->\n<!-- $f_k$ are the *component frequencies*, and $a_k$ are the *amplitude -->\n<!-- coefficients*. -->\n\n<!-- We define three *non-commuting kime operators*: -->\n\n<!-- 1.  *Time-domain operator* $K_1$: $K_1[s](t) = s(t)e^{i\\theta_1(t)}$, -->\n<!--     where $\\theta_1(t) = \\arg(\\mathcal{H}[s](t))$ and $\\mathcal{H}$ is -->\n<!--     the *Hilbert transform*, -->\n<!-- 2.  *Frequency-domain operator* $K_2$: -->\n<!--     $K_2[s](t) = s(t)e^{i\\theta_2(t)}$, where -->\n<!--     $\\theta_2(t) = \\arg(\\mathcal{F}[s](\\omega))e^{-i\\omega t}$ and -->\n<!--     $\\mathcal{F}$ is the *Fourier transform*, and -->\n<!-- 3.  *Scale-domain operator* $K_3$: $K_3[s](t) = s(t)e^{i\\theta_3(t)}$, -->\n<!--     where $\\theta_3(t) = \\arg(\\mathcal{W}[s](a,t))$ and $\\mathcal{W}$ is -->\n<!--     the *wavelet transform*. -->\n\n<!-- For each measurement $m = 1,2,\\cdots,M$, we will add *complex -->\n<!-- measurement noise*, $s_m(t) = s(t) + \\eta_m(t)$, where -->\n<!-- $\\eta_m(t) \\sim \\mathcal{CN}(0,\\sigma^2)$ (complex normal distribution), -->\n<!-- apply the 3 *kime-operators* -->\n\n<!-- $$\\begin{align*} -->\n<!-- y_{1,m}(t) &= K_1[s_m](t) \\\\ -->\n<!-- y_{2,m}(t) &= K_2[s_m](t) \\\\ -->\n<!-- y_{3,m}(t) &= K_3[s_m](t) -->\n<!-- \\end{align*},$$ and extract phases $\\phi_{j,m}(t) = \\arg(y_{j,m}(t))$, -->\n<!-- for $j = 1,2,3$. -->\n\n<!-- To evaluate the phase-recovery, the following *phase statistics* are -->\n<!-- computed for each basis $j$ and each time point $t$: -->\n\n<!-- 1.  *Circular mean*: -->\n<!--     $\\bar{\\phi}_j(t) = \\arg\\left(\\frac{1}{M}\\sum_{m=1}^M e^{i\\phi_{j,m}(t)}\\right)$, -->\n<!--     and -->\n\n<!-- 2.  *Circular variance*: -->\n<!--     $V_j(t) = 1 - \\left|\\frac{1}{M}\\sum_{m=1}^M e^{i\\phi_{j,m}(t)}\\right|$. -->\n\n<!-- In addition, we will compute and report the paired *kime commutators*, -->\n<!-- $[K_i,K_j]_\\kappa = K_i \\circ K_j - K_j \\circ K_i$, and the resulting -->\n<!-- *uncertainty relations* -->\n<!-- $$\\Delta K_i \\Delta K_j \\geq \\frac{1}{2}\\bigg|[K_i,K_j]_\\kappa \\bigg|.$$ -->\n\n<!-- Phase recovery *performance metrics* to quantify the process include: -->\n\n<!-- 1.  *Root Mean Square Error (RMSE)*: -->\n<!--     $\\text{RMSE}_j = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N (\\phi(t_n) - \\bar{\\phi}_j(t_n))^2}$. -->\n\n<!-- 2.  *Mean Absolute Error (MAE)*: -->\n<!--     $\\text{MAE}_j = \\frac{1}{N}\\sum_{n=1}^N |\\phi(t_n) - \\bar{\\phi}_j(t_n)|$. -->\n\n<!-- 3.  *Mean Variance*: $\\text{MV}_j = \\frac{1}{N}\\sum_{n=1}^N V_j(t_n)$. -->\n\n<!-- The *combined ensemble phase estimates* $\\hat{\\phi}$ will be obtained by -->\n<!-- *variance-weighted averaging* -->\n\n<!-- $$\\hat{\\phi}(t) = \\arg\\left(\\sum_{j=1}^3 w_j(t)e^{i\\bar{\\phi}_j(t)}\\right),$$ -->\n<!-- where weights are inverse variances, -->\n<!-- $w_j(t) = \\frac{1/V_j(t)}{\\sum_{k=1}^3 1/V_k(t)}$. -->\n\n<!-- Finally, we will report the *total estimation error* -->\n<!-- $E = \\|\\phi(t) - \\hat{\\phi}(t)\\|_2^2$ and the 3 *basis-specific errors* -->\n<!-- $E_j = \\|\\phi(t) - \\bar{\\phi}_j(t)\\|_2^2$. -->\n\n<!-- ```{r approach3a, warning=FALSE, message=FALSE} -->\n<!-- # Enhanced Kime Phase Measurement Implementation -->\n<!-- library(plotly) -->\n<!-- library(signal) -->\n<!-- library(wavelets) -->\n<!-- library(dplyr) -->\n\n<!-- #' Generate test signal with known phase characteristics -->\n<!-- #' @param N Number of time points -->\n<!-- #' @param known_phase Phase function to embed -->\n<!-- generate_test_signal <- function(N = 1000, known_phase = NULL) { -->\n<!--   t <- seq(0, 10, length.out = N) -->\n\n<!--   # Base signal components -->\n<!--   f1 <- 2  # Hz -->\n<!--   f2 <- 5  # Hz -->\n\n<!--   # Generate amplitude -->\n<!--   amplitude <- sin(2*pi*f1*t) + 0.5*sin(2*pi*f2*t) -->\n\n<!--   # Add known phase if provided, otherwise use default -->\n<!--   if (is.null(known_phase)) { -->\n<!--     phase <- pi/4 * sin(2*pi*0.3*t)  # Time-varying phase -->\n<!--   } else { -->\n<!--     phase <- known_phase(t) -->\n<!--   } -->\n\n<!--   # Create complex signal -->\n<!--   signal <- amplitude * exp(1i * phase) -->\n\n<!--   return(list( -->\n<!--     time = t, -->\n<!--     signal = signal, -->\n<!--     true_phase = phase -->\n<!--   )) -->\n<!-- } -->\n\n<!-- #' Kime basis operators -->\n<!-- #' @param signal Complex input signal -->\n<!-- #' @param t Time vector -->\n<!-- #' @return Complex transformed signal -->\n<!-- kime_operator_1 <- function(signal, t) { -->\n<!--   # Time-domain phase operator using Hilbert transform -->\n<!--   analytic <- signal + 1i * Re(signal)  # Simple analytic signal -->\n<!--   phase_t <- Arg(analytic) -->\n<!--   return(signal * exp(1i * phase_t)) -->\n<!-- } -->\n\n<!-- kime_operator_2 <- function(signal, t) { -->\n<!--   # Frequency-domain phase operator -->\n<!--   N <- length(signal) -->\n<!--   ft <- fft(signal) -->\n<!--   freqs <- seq(0, N-1)/N -->\n<!--   phase_f <- Arg(ft) * exp(-2*pi*1i * outer(freqs, t)) -->\n<!--   return(signal * exp(1i * colMeans(phase_f))) -->\n<!-- } -->\n\n<!-- kime_operator_3 <- function(signal, t, noctave = 6) { -->\n<!--   # Scale-domain phase operator using wavelets -->\n<!--   scales <- 2^seq(0, noctave-1) -->\n<!--   wt <- matrix(0, nrow = length(scales), ncol = length(signal)) -->\n\n<!--   # Simple Morlet wavelet transform -->\n<!--   for (i in seq_along(scales)) { -->\n<!--     s <- scales[i] -->\n<!--     wavelet <- exp(-t^2/(2*s^2)) * exp(2*pi*1i*t/s) -->\n<!--     wt[i,] <- convolve(signal, rev(wavelet), type = \"filter\") -->\n<!--   } -->\n\n<!--   phase_s <- Arg(colMeans(wt)) -->\n<!--   return(signal * exp(1i * phase_s)) -->\n<!-- } -->\n\n<!-- #' Estimate kime phase distribution -->\n<!-- #' @param signal Complex input signal -->\n<!-- #' @param t Time vector -->\n<!-- #' @param n_measurements Number of repeated measurements -->\n<!-- #' @return List of phase distributions in different bases -->\n<!-- estimate_kime_phase <- function(signal, t, n_measurements = 100) { -->\n<!--   N <- length(signal) -->\n<!--   phases <- array(0, dim = c(3, n_measurements, N)) -->\n\n<!--   for(i in 1:n_measurements) { -->\n<!--     # Add measurement noise -->\n<!--     noisy_signal <- signal + complex( -->\n<!--       real = rnorm(N, 0, 0.1), -->\n<!--       imaginary = rnorm(N, 0, 0.1) -->\n<!--     ) -->\n\n<!--     # Measure in different bases -->\n<!--     m1 <- kime_operator_1(noisy_signal, t) -->\n<!--     m2 <- kime_operator_2(noisy_signal, t) -->\n<!--     m3 <- kime_operator_3(noisy_signal, t) -->\n\n<!--     # Store phases -->\n<!--     phases[1,i,] <- Arg(m1) -->\n<!--     phases[2,i,] <- Arg(m2) -->\n<!--     phases[3,i,] <- Arg(m3) -->\n<!--   } -->\n\n<!--   # Compute phase distributions -->\n<!--   phase_stats <- list( -->\n<!--     time = apply(phases[1,,], 2, function(x) c( -->\n<!--       mean = circular.mean(x), -->\n<!--       var = circular.variance(x) -->\n<!--     )), -->\n<!--     freq = apply(phases[2,,], 2, function(x) c( -->\n<!--       mean = circular.mean(x), -->\n<!--       var = circular.variance(x) -->\n<!--     )), -->\n<!--     scale = apply(phases[3,,], 2, function(x) c( -->\n<!--       mean = circular.mean(x), -->\n<!--       var = circular.variance(x) -->\n<!--     )) -->\n<!--   ) -->\n\n<!--   return(list( -->\n<!--     phases = phases, -->\n<!--     stats = phase_stats -->\n<!--   )) -->\n<!-- } -->\n\n<!-- # Circular statistics functions -->\n<!-- circular.mean <- function(angles) { -->\n<!--   Arg(mean(exp(1i * angles))) -->\n<!-- } -->\n\n<!-- circular.variance <- function(angles) { -->\n<!--   1 - abs(mean(exp(1i * angles))) -->\n<!-- } -->\n\n<!-- # Generate test data -->\n<!-- test_data <- generate_test_signal(N = 500) -->\n\n<!-- # Estimate phase -->\n<!-- results <- estimate_kime_phase(test_data$signal, test_data$time, n_measurements = 50) -->\n\n<!-- # Create visualization -->\n<!-- # 1. True vs Estimated Phase -->\n<!-- phase_plot <- plot_ly() %>% -->\n<!--   add_trace( -->\n<!--     x = test_data$time, -->\n<!--     y = test_data$true_phase, -->\n<!--     type = 'scatter', -->\n<!--     mode = 'lines', -->\n<!--     name = 'True Phase' -->\n<!--   ) -->\n\n<!-- # Add estimated phases from each basis -->\n<!-- basis_names <- c(\"Time\", \"Frequency\", \"Scale\") -->\n<!-- colors <- c('#1f77b4', '#ff7f0e', '#2ca02c') -->\n\n<!-- for(i in 1:3) { -->\n<!--   phase_plot <- phase_plot %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = results$stats[[i]][\"mean\",], -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = paste(basis_names[i], \"Basis\"), -->\n<!--       line = list(color = colors[i]) -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = results$stats[[i]][\"mean\",] +  -->\n<!--          2*sqrt(results$stats[[i]][\"var\",]), -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       line = list(dash = 'dot', color = colors[i]), -->\n<!--       showlegend = FALSE, -->\n<!--       fill = 'tonexty' -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = results$stats[[i]][\"mean\",] -  -->\n<!--          2*sqrt(results$stats[[i]][\"var\",]), -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       line = list(dash = 'dot', color = colors[i]), -->\n<!--       showlegend = FALSE, -->\n<!--       fill = 'tonexty' -->\n<!--     ) -->\n<!-- } -->\n\n<!-- phase_plot <- phase_plot %>% -->\n<!--   layout( -->\n<!--     title = \"Kime Phase Estimation Results (Approach 3A)\", -->\n<!--     xaxis = list(title = \"Time\"), -->\n<!--     yaxis = list(title = \"Phase\"), -->\n<!--     showlegend = TRUE -->\n<!--   ) -->\n\n<!-- phase_plot -->\n\n<!-- # Evaluation metrics for kime phase estimation -->\n<!-- evaluate_phase_estimation <- function(true_phase, estimated_results) { -->\n<!--   # Compute metrics for each basis -->\n<!--   metrics <- lapply(1:3, function(i) { -->\n<!--     est_phase <- estimated_results$stats[[i]][\"mean\",] -->\n<!--     variance <- estimated_results$stats[[i]][\"var\",] -->\n\n<!--     # Phase difference (accounting for circular nature) -->\n<!--     phase_diff <- Arg(exp(1i * (true_phase - est_phase))) -->\n\n<!--     # Metrics -->\n<!--     list( -->\n<!--       rmse = sqrt(mean(phase_diff^2)), -->\n<!--       mae = mean(abs(phase_diff)), -->\n<!--       mean_variance = mean(variance), -->\n<!--       max_error = max(abs(phase_diff)) -->\n<!--     ) -->\n<!--   }) -->\n\n<!--   # Create metrics table -->\n<!--   basis_names <- c(\"Time\", \"Frequency\", \"Scale\") -->\n<!--   metrics_df <- do.call(rbind, lapply(seq_along(metrics), function(i) { -->\n<!--     data.frame( -->\n<!--       Basis = basis_names[i], -->\n<!--       RMSE = metrics[[i]]$rmse, -->\n<!--       MAE = metrics[[i]]$mae, -->\n<!--       Mean_Variance = metrics[[i]]$mean_variance, -->\n<!--       Max_Error = metrics[[i]]$max_error -->\n<!--     ) -->\n<!--   })) -->\n\n<!--   # Create evaluation plot -->\n<!--   eval_plot <- plot_ly() %>% -->\n<!--     add_trace( -->\n<!--       data = metrics_df, -->\n<!--       x = ~Basis, -->\n<!--       y = ~RMSE, -->\n<!--       type = 'bar', -->\n<!--       name = 'RMSE' -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       data = metrics_df, -->\n<!--       x = ~Basis, -->\n<!--       y = ~MAE, -->\n<!--       type = 'bar', -->\n<!--       name = 'MAE' -->\n<!--     ) %>% -->\n<!--     layout( -->\n<!--       title = \"Phase Estimation Error by Basis\", -->\n<!--       xaxis = list(title = \"Basis\"), -->\n<!--       yaxis = list(title = \"Error (radians)\"), -->\n<!--       barmode = 'group' -->\n<!--     ) -->\n\n<!--   return(list( -->\n<!--     metrics = metrics_df, -->\n<!--     plot = eval_plot -->\n<!--   )) -->\n<!-- } -->\n\n<!-- # Run evaluation -->\n<!-- eval_results <- evaluate_phase_estimation(test_data$true_phase, results) -->\n\n<!-- # Display results -->\n<!-- eval_results$metrics -->\n<!-- eval_results$plot -->\n<!-- ``` -->\n\n<!-- ### Reproducing Kernel Hilbert Space (RKHS) Formulation of the Kime Operators $K_1, K_2, K_3$ -->\n\n<!-- Let's try to rigorously redefine the kime operators $K_1, K_2, K_3$ -->\n<!-- within a [Reproducing Kernel Hilbert Space (RKHS) -->\n<!-- framework](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_ReproducingKernelHilbertSpaces.html). -->\n<!-- to define the RKHS Structure, let $s(t)$ be a signal residing in an RKHS -->\n<!-- $\\mathcal{H}$ with a *reproducing kernel* -->\n<!-- $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{C}$. The *inner product* in -->\n<!-- $\\mathcal{H}$ satisfies the reproducing property -->\n<!-- $\\langle f, K(\\cdot, t) \\rangle_{\\mathcal{H}} = f(t), \\quad \\forall f \\in \\mathcal{H}.$ -->\n<!-- For concreteness, consider a *time-frequency adapted kernel*, such as a -->\n<!-- Gabor kernel -->\n<!-- $K(t, t') = \\int g(t - \\tau)e^{-i\\omega \\tau} g(t' - \\tau)e^{i\\omega \\tau} d\\tau d\\omega,$ -->\n<!-- where $g$ is a window function, e.g., Gaussian. This kernel ensures -->\n<!-- joint time-frequency localization. -->\n\n<!-- We can redefine the 3 kime operators, $K_1, K_2, K_3$, as *linear -->\n<!-- projections* onto time, frequency, and scale subspaces of $\\mathcal{H}$ -->\n\n<!-- -   $K_1[s](t) = \\langle s, K(\\cdot, t) \\rangle_{\\mathcal{H}},$ which -->\n<!--     evaluates $s$ at $t$, equivalent to the reproducing property. This -->\n<!--     operator localizes the signal in time, -->\n<!-- -   $K_2[s](\\omega) = \\langle s, \\mathcal{F}^{-1}[K(\\cdot, \\omega)] \\rangle_{\\mathcal{H}},$ -->\n<!--     where $\\mathcal{F}^{-1}$ is the inverse Fourier transform. This -->\n<!--     projects $s$ onto Fourier modes, and -->\n<!-- -   $K_3[s](a, t) = \\langle s, \\psi_{a,t} \\rangle_{\\mathcal{H}},$ where -->\n<!--     $\\psi_{a,t}(\\tau) = \\frac{1}{\\sqrt{a}} \\psi\\left(\\frac{\\tau - t}{a}\\right)$ -->\n<!--     is a wavelet basis. This localizes $s$ in scale and time. -->\n\n<!-- Then, the non-commutativity of $K_1, K_2, K_3$ arises from their -->\n<!-- localization properties -->\n\n<!-- -   the *time-frequency commutator*: -->\n<!--     $[K_1, K_2] = K_1K_2 - K_2K_1 \\neq 0,$ reflecting the inability to -->\n<!--     simultaneously localize in time and frequency. This leads to the -->\n<!--     *uncertainty relation* -->\n<!--     $\\Delta t \\cdot \\Delta \\omega \\geq \\frac{1}{2},$ where -->\n<!--     $\\Delta t, \\Delta \\omega$ are variances in time and frequency -->\n<!--     domains, and -->\n\n<!-- -   the *time-scale commutator*: $[K_1, K_3] \\neq 0,$ with uncertainty -->\n<!--     relation $\\Delta t \\cdot \\Delta a \\geq C,$ where $\\Delta a$ is scale -->\n<!--     variance, and $C$ depends on the wavelet choice. -->\n\n<!-- To estimate the phase distribution $\\Phi(\\theta; t)$ via the RKHS -->\n<!-- projections we can project the *signal* $s(t)$ onto each basis; *Time*: -->\n<!-- $\\phi_1(t) = \\arg(K_1[s](t))$; *Frequency*: -->\n<!-- $\\phi_2(\\omega) = \\arg(K_2[s](\\omega))$, and *Scale*: -->\n<!-- $\\phi_3(a, t) = \\arg(K_3[s](a,t))$. And then *fuse* (or *ensemble*) the -->\n<!-- estimates using the RKHS inner products, -->\n<!-- $\\hat{\\phi}(t) = \\arg\\left(\\sum_{j=1}^3 w_j(t) e^{i\\phi_j(t)}\\right),$ -->\n<!-- where the weights $w_j(t)$ minimize the RKHS norm, -->\n<!-- $w_j(t) = \\frac{\\langle \\Phi, K_j(\\cdot, t) \\rangle_{\\mathcal{H}}}{\\sum_k \\langle \\Phi, K_k(\\cdot, t) \\rangle_{\\mathcal{H}}}.$ -->\n\n<!-- A theoretical verification of *non-commutativity* is demonstrated via -->\n<!-- non-zero commutators in RKHS, -->\n<!-- $\\| [K_1, K_2] \\|_{\\mathcal{H}} \\propto \\Delta t \\Delta \\omega$, where -->\n<!-- the *uncertainty relations* are enforced by the choice of kernel -->\n<!-- $K(t, t')$. Such reformulation of the kime operators as *projections in -->\n<!-- an RKHS* aligns the theoretical framework with Hilbert space axioms, -->\n<!-- ensures linearity, and provides a rigorous uncertainty relations. The -->\n<!-- updates Approach 3 simulation now reflects these principles, with phase -->\n<!-- estimates ensembled via RKHS-optimal weights. -->\n\n<!-- ```{r approach3_RKHS1, warning=FALSE, message=FALSE} -->\n<!-- # Enhanced Kime Phase Measurement Implementation -->\n<!-- library(plotly) -->\n<!-- library(signal) -->\n<!-- library(wavelets) -->\n<!-- library(dplyr) -->\n<!-- library(wsyn) -->\n<!-- library(signal)              # Load the package -->\n<!-- library(dplR)   # continuous wavelet transform -->\n\n<!-- N <- 1000 -->\n<!-- t <- seq(0, 10, length.out = N) -->\n\n<!-- #' Generate test signal with known phase characteristics -->\n<!-- #' @param N Number of time points -->\n<!-- #' @param true_phase True phase function to embed -->\n<!-- generate_test_signal <- function(N=1000, true_phase=NULL) { -->\n<!--      t <- seq(0, 10, length.out = N) -->\n\n<!--      # Add known phase if provided, otherwise use default -->\n<!--      if (is.null(true_phase)) { -->\n<!--        phase <- pi/4 * sin(2*pi*0.3*t)  # Time-varying phase -->\n<!--      } else { -->\n<!--        phase <- true_phase(t) -->\n<!--      } -->\n\n<!--      t <- seq(0, 10, length.out = N) -->\n<!--      s <- exp(1i * phase)  # Analytic signal in RKHS -->\n\n<!--      ###### To add noise to the real-valued signal before constructing the analytic signal: -->\n<!--      # real_signal <- Re(s) -->\n<!--      # noisy_real <- real_signal + rnorm(length(real_signal), 0, sigma) -->\n<!--      # analytic <- hilbert(noisy_real) -->\n<!--      # list(signal = analytic, ...) -->\n<!--      # return(list(time = t, signal = analytic, true_phase = phase(t))) -->\n<!--      return(list(time = t, signal = s, true_phase = phase)) -->\n<!-- } -->\n\n<!-- #' Kime basis operators -->\n<!-- #' @param signal Complex input signal -->\n<!-- #' @param t Time vector -->\n<!-- #' @return Complex transformed signal -->\n<!-- # **Frequency-Domain Operator ($K_2$)**: -->\n<!-- # kime_operator_1 <- function(signal, t) { -->\n<!-- #   # Extract real part and compute analytic signal -->\n<!-- #   real_signal <- Re(signal) -->\n<!-- #   analytic <- hilbert(real_signal)  # Now works on real-valued input -->\n<!-- #   phase_t <- Arg(analytic) -->\n<!-- #   analytic * exp(1i * phase_t)      # Return analytic signal -->\n<!-- # } -->\n<!-- hilbert_manual <- function(real_signal) { -->\n<!--   n <- length(real_signal) -->\n<!--   fft_signal <- fft(real_signal) -->\n<!--   # Zero negative frequencies -->\n<!--   fft_signal[ceiling(n/2 + 1):n] <- 0 -->\n<!--   Re(fft(fft_signal, inverse = TRUE)) -->\n<!-- } -->\n\n<!-- kime_operator_1 <- function(signal, t) { -->\n<!--   real_signal <- Re(signal) -->\n<!--   analytic <- complex( -->\n<!--     real = real_signal, -->\n<!--     imaginary = hilbert_manual(real_signal) -->\n<!--   ) -->\n<!--   phase_t <- Arg(analytic) -->\n<!--   analytic * exp(1i * phase_t) -->\n<!-- } -->\n\n<!-- # **Frequency-Domain Operator ($K_2$)**: -->\n<!-- kime_operator_2 <- function(signal, t) { -->\n<!--   ft <- fft(signal) -->\n<!--   phase_f <- Arg(ft) -->\n<!--   # Reconstruct signal with Fourier phase per frequency -->\n<!--   ifft(ft * exp(1i * phase_f))  # Preserve amplitude, phase modulation -->\n<!-- } -->\n\n<!-- # # **Scale-Domain Operator ($K_3$)**: -->\n<!-- # kime_operator_3 <- function(signal, t) { -->\n<!-- #   # Continuous wavelet transform with Morlet wavelet -->\n<!-- #   # cwt <- cwt_wsyn(Re(signal), t, scale.min = 1, scale.max = 64) -->\n<!-- #   cwt <- morlet(y1 = Re(signal), x1 = t, dj = 0.1, siglvl = 0.99) -->\n<!-- #  -->\n<!-- #   # Extract dominant scale phase at each time point -->\n<!-- #   phase_s <- apply(cwt$wave, 2, function(col) Arg(cwt$wave[which.max(abs(col)),])) -->\n<!-- #   signal * exp(1i * phase_s) -->\n<!-- # } -->\n\n<!-- # Fixed Scale-Domain Operator (K3) -->\n<!-- kime_operator_3 <- function(signal, t) { -->\n<!--   # Ensure signal is a vector -->\n<!--   if(!is.vector(signal)) stop(\"Signal must be a vector\") -->\n<!--   N <- length(signal) -->\n\n<!--   # Perform Morlet wavelet transform -->\n<!--   cwt <- morlet(y1 = Re(signal), x1 = t, dj = 0.1, siglvl = 0.99) -->\n\n<!--   # Debug dimensions -->\n<!--   wave_mat <- cwt$wave -->\n<!--   cat(\"Dimensions of wave_mat:\", dim(wave_mat), \"\\n\") -->\n\n<!--   # Get absolute values of wavelet coefficients -->\n<!--   wave_abs <- abs(wave_mat) -->\n<!--   cat(\"Dimensions of wave_abs:\", dim(wave_abs), \"\\n\") -->\n\n<!--   # Find index of maximum power at each time point -->\n<!--   # Note: we need to transpose if the matrix is [time, scales] -->\n<!--   if(ncol(wave_mat) == N) { -->\n<!--     max_scale_idx <- apply(wave_abs, 2, which.max) -->\n<!--     phase_s <- numeric(N) -->\n<!--     for(i in 1:N) { -->\n<!--       if(max_scale_idx[i] <= nrow(wave_mat)) { -->\n<!--         phase_s[i] <- Arg(wave_mat[max_scale_idx[i], i]) -->\n<!--       } else { -->\n<!--         warning(sprintf(\"Scale index %d out of bounds for column %d\", max_scale_idx[i], i)) -->\n<!--         phase_s[i] <- 0  # or some other appropriate default -->\n<!--       } -->\n<!--     } -->\n<!--   } else if(nrow(wave_mat) == N) { -->\n<!--     # If matrix is transposed from what we expect -->\n<!--     max_scale_idx <- apply(t(wave_abs), 2, which.max) -->\n<!--     phase_s <- numeric(N) -->\n<!--     for(i in 1:N) { -->\n<!--       if(max_scale_idx[i] <= ncol(wave_mat)) { -->\n<!--         phase_s[i] <- Arg(wave_mat[i, max_scale_idx[i]]) -->\n<!--       } else { -->\n<!--         warning(sprintf(\"Scale index %d out of bounds for row %d\", max_scale_idx[i], i)) -->\n<!--         phase_s[i] <- 0  # or some other appropriate default -->\n<!--       } -->\n<!--     } -->\n<!--   } else { -->\n<!--     stop(sprintf(\"Unexpected wavelet transform dimensions: %d x %d\",  -->\n<!--                  nrow(wave_mat), ncol(wave_mat))) -->\n<!--   } -->\n\n<!--   # Return transformed signal -->\n<!--   return(signal * exp(1i * phase_s)) -->\n<!-- } -->\n\n<!-- #### **3. Commutator and Uncertainty Validation** Compute commutators numerically: -->\n<!-- # Compute [K1, K2] at a test point t0 -->\n<!-- s_test <- generate_test_signal()$signal -->\n<!-- K1K2 <- kime_operator_1(kime_operator_2(s_test, t), t) -->\n<!-- K2K1 <- kime_operator_2(kime_operator_1(s_test, t), t) -->\n<!-- commutator <- K1K2 - K2K1 -->\n<!-- cat(\"The commutator norm |[K1,K2]|=\", norm(commutator, \"2\")) -->\n\n\n<!-- #' Estimate kime phase distribution -->\n<!-- #' @param signal Complex input signal -->\n<!-- #' @param t Time vector -->\n<!-- #' @param n_measurements Number of repeated measurements -->\n<!-- #' @return List of phase distributions in different bases -->\n<!-- estimate_kime_phase <- function(signal, t, n_measurements = 100) { -->\n<!--   N <- length(signal) -->\n<!--   phases <- array(0, dim = c(3, n_measurements, N)) -->\n\n<!--   for(i in 1:n_measurements) { -->\n<!--     # Add measurement noise -->\n<!--     noisy_signal <- signal + complex(real=rnorm(N, 0, 0.1), imaginary=rnorm(N, 0, 0.1)) -->\n\n<!--     # Measure in different bases -->\n<!--     m1 <- kime_operator_1(noisy_signal, t) -->\n<!--     m2 <- kime_operator_2(noisy_signal, t) -->\n<!--     m3 <- kime_operator_3(noisy_signal, t) -->\n\n<!--     # Store phases -->\n<!--     phases[1,i,] <- Arg(m1) -->\n<!--     phases[2,i,] <- Arg(m2) -->\n<!--     phases[3,i,] <- Arg(m3) -->\n<!--   } -->\n\n<!--   # Compute phase distributions -->\n<!--   phase_stats <- list( -->\n<!--     time = apply(phases[1,,], 2, function(x) c( -->\n<!--       mean = circular.mean(x), -->\n<!--       var = circular.variance(x) -->\n<!--     )), -->\n<!--     freq = apply(phases[2,,], 2, function(x) c( -->\n<!--       mean = circular.mean(x), -->\n<!--       var = circular.variance(x) -->\n<!--     )), -->\n<!--     scale = apply(phases[3,,], 2, function(x) c( -->\n<!--       mean = circular.mean(x), -->\n<!--       var = circular.variance(x) -->\n<!--     )) -->\n<!--   ) -->\n\n<!--   return(list(phases = phases, stats = phase_stats)) -->\n<!-- } -->\n\n<!-- # Circular statistics functions -->\n<!-- circular.mean <- function(angles) {   Arg(mean(exp(1i * angles))) } -->\n\n<!-- circular.variance <- function(angles) {   1 - abs(mean(exp(1i * angles))) } -->\n\n<!-- circular_rmse <- function(true, est) { -->\n<!--   diffs <- Arg(exp(1i * (true - est))) -->\n<!--   sqrt(mean(diffs^2)) -->\n<!-- } -->\n\n<!-- # Generate test data -->\n<!-- test_data <- generate_test_signal(N = N) -->\n\n<!-- # Estimate phase -->\n<!-- results <- estimate_kime_phase(test_data$signal, test_data$time, n_measurements = 50) -->\n\n<!-- cat(\"CircularRMSE(true_phase, estim_phases)=\", circular_rmse(test_data$true_phase, results$phases)) -->\n<!-- # [1] 1.72432 -->\n\n<!-- # Create visualization -->\n<!-- # Comprehensive visualization of kime phase measurement results -->\n<!-- library(plotly) -->\n\n<!-- # 1. Phase Comparison Plot - All operators vs true phase -->\n<!-- create_phase_comparison <- function(test_data, results) { -->\n<!--   # Extract mean phases for each operator -->\n<!--   time_phase <- results$stats$time[\"mean\",] -->\n<!--   freq_phase <- results$stats$freq[\"mean\",] -->\n<!--   scale_phase <- results$stats$scale[\"mean\",] -->\n\n<!--   fig <- plot_ly() %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = test_data$true_phase, -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'True Phase', -->\n<!--       line = list(color = 'black', width = 2) -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = time_phase, -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'Time Domain (K1)', -->\n<!--       line = list(color = 'blue') -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = freq_phase, -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'Frequency Domain (K2)', -->\n<!--       line = list(color = 'red') -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = scale_phase, -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'Scale Domain (K3)', -->\n<!--       line = list(color = 'green') -->\n<!--     ) %>% -->\n<!--     layout( -->\n<!--       title = \"Phase Estimation Comparison\", -->\n<!--       xaxis = list(title = \"Time\"), -->\n<!--       yaxis = list(title = \"Phase (radians)\"), -->\n<!--       showlegend = TRUE -->\n<!--     ) -->\n\n<!--   return(fig) -->\n<!-- } -->\n\n<!-- # 2. Uncertainty Plot - Variance over time for each operator -->\n<!-- create_uncertainty_plot <- function(results) { -->\n<!--   fig <- plot_ly() %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = results$stats$time[\"var\",], -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'Time Domain (K1)', -->\n<!--       line = list(color = 'blue') -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = results$stats$freq[\"var\",], -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'Frequency Domain (K2)', -->\n<!--       line = list(color = 'red') -->\n<!--     ) %>% -->\n<!--     add_trace( -->\n<!--       x = test_data$time, -->\n<!--       y = results$stats$scale[\"var\",], -->\n<!--       type = 'scatter', -->\n<!--       mode = 'lines', -->\n<!--       name = 'Scale Domain (K3)', -->\n<!--       line = list(color = 'green') -->\n<!--     ) %>% -->\n<!--     layout( -->\n<!--       title = \"Phase Estimation Uncertainty\", -->\n<!--       xaxis = list(title = \"Time\"), -->\n<!--       yaxis = list(title = \"Variance\"), -->\n<!--       showlegend = TRUE -->\n<!--     ) -->\n\n<!--   return(fig) -->\n<!-- } -->\n\n<!-- # 3. Error Distribution Plot -->\n<!-- create_error_distribution <- function(test_data, results) { -->\n<!--   # Calculate errors for each operator -->\n<!--   errors <- list( -->\n<!--     time = Arg(exp(1i * (test_data$true_phase - results$stats$time[\"mean\",]))), -->\n<!--     freq = Arg(exp(1i * (test_data$true_phase - results$stats$freq[\"mean\",]))), -->\n<!--     scale = Arg(exp(1i * (test_data$true_phase - results$stats$scale[\"mean\",]))) -->\n<!--   ) -->\n\n<!--   fig <- plot_ly() %>% -->\n<!--     add_histogram(x = errors$time, name = 'Time Domain (K1)',  -->\n<!--                  opacity = 0.6, nbinsx = 30) %>% -->\n<!--     add_histogram(x = errors$freq, name = 'Frequency Domain (K2)',  -->\n<!--                  opacity = 0.6, nbinsx = 30) %>% -->\n<!--     add_histogram(x = errors$scale, name = 'Scale Domain (K3)',  -->\n<!--                  opacity = 0.6, nbinsx = 30) %>% -->\n<!--     layout( -->\n<!--       title = \"Phase Error Distribution\", -->\n<!--       xaxis = list(title = \"Phase Error (radians)\"), -->\n<!--       yaxis = list(title = \"Count\"), -->\n<!--       barmode = 'overlay', -->\n<!--       showlegend = TRUE -->\n<!--     ) -->\n\n<!--   return(fig) -->\n<!-- } -->\n\n<!-- # 4. Performance Metrics Plot -->\n<!-- create_performance_plot <- function(test_data, results) { -->\n<!--   # Calculate RMSE for each operator -->\n<!--   rmse <- c( -->\n<!--     circular_rmse(test_data$true_phase, results$stats$time[\"mean\",]), -->\n<!--     circular_rmse(test_data$true_phase, results$stats$freq[\"mean\",]), -->\n<!--     circular_rmse(test_data$true_phase, results$stats$scale[\"mean\",]) -->\n<!--   ) -->\n\n<!--   # Calculate mean variance for each operator -->\n<!--   mean_var <- c( -->\n<!--     mean(results$stats$time[\"var\",]), -->\n<!--     mean(results$stats$freq[\"var\",]), -->\n<!--     mean(results$stats$scale[\"var\",]) -->\n<!--   ) -->\n\n<!--   operators <- c(\"Time (K1)\", \"Frequency (K2)\", \"Scale (K3)\") -->\n\n<!--   fig <- plot_ly() %>% -->\n<!--     add_trace(x=operators, y=rmse, type='bar', name='RMSE', marker = list(color='blue')) %>% -->\n<!--     add_trace(x=operators, y=mean_var, type = 'bar', name = 'Mean Variance', -->\n<!--       marker = list(color = 'red')) %>% -->\n<!--     layout(title = \"Performance Metrics (Mean Variance & Circular-RMSE) by Operator\", -->\n<!--       xaxis = list(title = \"Operator\"), yaxis = list(title = \"Value\"), -->\n<!--       barmode = 'group', showlegend = TRUE) -->\n<!--   return(fig) -->\n<!-- } -->\n\n<!-- # Create all visualizations -->\n<!-- phase_comp_plot <- create_phase_comparison(test_data, results) -->\n<!-- uncertainty_plot <- create_uncertainty_plot(results) -->\n<!-- error_dist_plot <- create_error_distribution(test_data, results) -->\n<!-- performance_plot <- create_performance_plot(test_data, results) -->\n\n<!-- # Display plots -->\n<!-- phase_comp_plot -->\n<!-- uncertainty_plot -->\n<!-- error_dist_plot -->\n<!-- performance_plot -->\n<!-- ``` -->\n\n<!-- ### Refined Approach 3 (via Strict Operator Non-Commutativity) -->\n\n<!-- The above over-simplified implementation of *Approach 3* can be enhanced -->\n<!-- by redefining *Operator Non-Commutativity*. Earlier, the operators -->\n<!-- $K_1, K_2, K_3$ are defined using *time*, *frequency*, and *scale* -->\n<!-- domains, but their composition (e.g., $K_1 \\circ K_2$) was not -->\n<!-- rigorously derived. In quantum mechanics, operators act linearly in a -->\n<!-- Hilbert space, yet here, $K_j$ are nonlinear (e.g., phase extraction -->\n<!-- followed by modulation). The commutator $[K_i, K_j]_\\kappa$ was not -->\n<!-- really computed, leaving non-commutativity unproven. We can consider -->\n<!-- redefining the operators as linear transformations in a [reproducing -->\n<!-- kernel Hilbert space -->\n<!-- (RKHS)](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_ReproducingKernelHilbertSpaces.html) -->\n<!-- and derive commutators explicitly. -->\n\n<!-- Also, the earlier proposed uncertainty relation -->\n<!-- $\\Delta K_i \\Delta K_j \\geq \\frac{1}{2}|[K_i, K_j]_\\kappa|$ lacked a -->\n<!-- mathematical foundation. The variance $\\Delta K_i$ is based on circular -->\n<!-- statistics, but the inequality’s derivation was not explcated. We can -->\n<!-- potentially use time-frequency uncertainty principles (e.g., wavelet -->\n<!-- variance vs. Fourier bandwidth) to formalize bounds. -->\n\n<!-- Finally, the proposed weighted average -->\n<!-- $\\hat{\\phi}(t) = \\arg\\left(\\sum w_j e^{i\\bar{\\phi}_j}\\right)$ -->\n<!-- heuristically combines bases without theoretical guarantees. Optimality -->\n<!-- of the weighting (e.g., Fisher information) was not addressed. Perhaps -->\n<!-- we can utilize maximum likelihood estimation or Bayesian fusion for -->\n<!-- phase combination. -->\n\n<!-- In the first experimental simulation, in the *time domain operator* -->\n<!-- $K_1$, the analytic signal was incorrectly computed as -->\n<!-- `signal + 1i * Re(signal)`, since the Hilbert transform should generate -->\n<!-- the quadrature component. In the *frequency domain* ($K_2$), the phase -->\n<!-- extraction averaged Fourier coefficients across frequencies, losing -->\n<!-- per-frequency phase information. And in the *scale domain* ($K_3$), the -->\n<!-- wavelet transform used an ad-hoc convolution instead of a *continuous -->\n<!-- wavelet transform (CWT)*, and the phase is averaged over scales, -->\n<!-- blurring multi-scale structure. -->\n\n<!-- Two additional simulation improvements may include (i) adding complex -->\n<!-- noise to the signal $s(t)$ does not reflect real-world scenarios where -->\n<!-- noise corrupts real-valued measurements before analytic signal -->\n<!-- construction, and (ii) in validating the results, the RMSE and MAE -->\n<!-- measures used the linear differences on circular phases, which may lead -->\n<!-- to misleading values when the phase wraps around $2\\pi$. With these -->\n<!-- modifications, the *Time Domain* accurately tracks local phase -->\n<!-- variations (low RMSE), the *frequency domain* captures global phase -->\n<!-- trends but smooths local features, and the *scale domain* resolves -->\n<!-- multi-scale behavior but still remains noise-sensitive. In examining the -->\n<!-- *uncertainty relations*, the calculations of the numerical commutators -->\n<!-- show $[K_1, K_2] \\neq 0$, validating non-commutativity. The -->\n<!-- time-frequency uncertainty trade-off aligns with theoretical -->\n<!-- expectations. -->\n\n<!-- The revised simulation addresses some of these shortcomings of the first -->\n<!-- version above, yet, further improvements of *Approach 3* may include -->\n\n<!-- -   Rederiving the operators in a RKHS using appropriate well-defined -->\n<!--     inner products. -->\n<!-- -   Replacing the ad-hoc wavelet transforms with established libraries -->\n<!--     (e.g., `WaveletComp`). -->\n<!-- -   Improvements of the validation against ground truth with synthetic -->\n<!--     phase jumps and noise spikes. -->\n\n<!-- ### Reformulation of Approach 3A -->\n\n<!-- The earlier versions of *Approach 3* define $3$ kime-basis operators -->\n\n<!-- -   $K_1[f](\\kappa) = \\int f(t) e^{i\\theta_1(t)} dt$ (*time-domain*),\\* -->\n<!-- -   $K_2[f](\\kappa) = \\int \\hat{f}(\\omega) e^{i\\theta_2(\\omega)} d\\omega$ -->\n<!--     (frequency\\*-domain), and -->\n<!-- -   $K_3[f](\\kappa) = \\int \\tilde{f}(s) e^{i\\theta_3(s)} ds$ -->\n<!--     (*scale-domain*), -->\n\n<!-- whose commutators $[K_i, K_j]_\\kappa = K_i \\circ K_j - K_j \\circ K_i$, -->\n<!-- suggesting uncertainty relations -->\n<!-- $\\Delta K_i \\Delta K_j \\geq \\frac{1}{2} |[K_i, K_j]_\\kappa|$. Despite -->\n<!-- the intuitive analogy to QM Pauli operators, mapping time, frequency, -->\n<!-- and scale bases, there are ambiguities in $\\kappa$; the operators are -->\n<!-- defined as functions of $\\kappa$, but act on $f(t)$, $\\hat{f}(\\omega)$, -->\n<!-- or $\\tilde{f}(s)$, and their domain is unclear, e.g., $L^2(\\mathbb{R})$ -->\n<!-- vs. $L^2[-\\pi, \\pi]$. The phase functions -->\n<!-- $\\theta_1(t), \\theta_2(\\omega), \\theta_3(s)$ are unspecified, making the -->\n<!-- operators abstract and non-operational. And most importantly, the -->\n<!-- composition $\\circ$ is unclear for integral transforms across different -->\n<!-- domains (time vs. frequency vs. scale). -->\n\n<!-- In *Approach 3A*, we will estimate the kime-phase distribution -->\n<!-- $\\Phi(\\theta; t)$ using linear, non-commuting operators in a *consistent -->\n<!-- Hilbert space* and provide a realistic simulation. -->\n\n<!-- **Hilbert Space**: $L^2(\\mathbb{R})$, functions of time $t$, as kime -->\n<!-- operates on time-series signals $s(t)$. This avoids ambiguity in -->\n<!-- $\\kappa$ and aligns with signal processing. -->\n\n<!-- **State**: $s(t) = A(t) e^{i\\phi(t)}$, where $\\phi(t) = \\theta(t)$ is -->\n<!-- the kime-phase, sampled from $\\Phi(\\theta; t)$. -->\n\n<!-- Consider the following definitions of **linear kime operators** -->\n\n<!-- -   *Time-Domain Operator* ($K_1$): $K_1 s(t) = t s(t)$, *multiplication -->\n<!--     by time operator*, analogous to QM’s *position* operator, -->\n<!-- -   *Frequency-Domain Operator* ($K_2$): -->\n<!--     $K_2 s(t) = -i \\frac{d}{dt} s(t)$ is the *derivative operator*, -->\n<!--     similar to the QM *momentum*, probing frequency content via -->\n<!--     $e^{i\\omega t}$, and -->\n<!-- -   *Scale-Domain Operator* ($K_3$): -->\n<!--     $K_3 s(t) = \\int_{-\\infty}^\\infty \\psi(t - t') s(t') dt'$, -->\n<!--     *convolution operator* with a wavelet $\\psi(t)$, e.g., Morlet, -->\n<!--     capturing scale information. -->\n\n<!-- These linear operators ensure that the commutator algebra is -->\n<!-- well-defined. $K_1$ and $K_2$ mirror QM’s $\\hat{x}, \\hat{p}$, while -->\n<!-- $K_3$ introduces multi-scale analysis, common in signal processing. -->\n\n<!-- Let's compute the paired *commutators* -->\n\n<!-- -   (*time-frequency*) $[K_1, K_2]$: -->\n<!--     $K_1 K_2 s(t) = t (-i \\frac{d}{dt} s(t)),$ -->\n<!--     $K_2 K_1 s(t) = -i \\frac{d}{dt} (t s(t)) = -i s(t) - i t \\frac{d}{dt} s(t),$, -->\n<!--     and -->\n<!--     $[K_1, K_2] s(t) = -i t \\frac{d}{dt} s(t) + i s(t) + i t \\frac{d}{dt} s(t) = i s(t) = i I$, -->\n<!-- -   (*time-scale*)$[K_1, K_3]$ depends on $\\psi$: Assume -->\n<!--     $\\psi(t) = e^{-t^2/2} \\cos(\\omega_0 t)$, then -->\n<!--     $[K_1, K_3] s(t) = t \\int \\psi(t - t') s(t') dt' - \\int \\psi(t - t') (t' s(t')) dt' \\not= 0,$ -->\n<!--     since $t \\neq t'$ in general, and similarly -->\n<!-- -   (*frequency-scale*)$[K_2, K_3]\\not= 0$, reflecting time-scale -->\n<!--     trade-offs. -->\n\n<!-- Therefore, we have *uncertainty relations* like the *time-frequency -->\n<!-- uncertainty* -->\n<!-- $\\Delta K_1 \\Delta K_2 \\geq \\frac{1}{2} |\\langle [K_1, K_2] \\rangle| = \\frac{1}{2}$, -->\n<!-- where -->\n<!-- $\\Delta K_i = \\sqrt{\\langle K_i^2 \\rangle - \\langle K_i \\rangle^2}$ are -->\n<!-- computed over $s(t)$. -->\n\n<!-- Finally, the *Approach 3A phase estimation* utilizes measurements in -->\n<!-- each basis to extract complementary phase information -->\n\n<!-- -   (*time-domain operator phase estimation*) $K_1$: -->\n<!--     $\\phi_1(t) = \\arg(s(t))$ (direct from signal), -->\n<!-- -   (*frequency-domain operator phase estimation*) $K_2$: -->\n<!--     $\\phi_2(t) = \\arg(-i \\frac{d}{dt} s(t))$, -->\n<!-- -   (*scale-domain operator phase estimation*) $K_3$: -->\n<!--     $\\phi_3(t) = \\arg(\\mathcal{W}[s](a^*, t))$, where -->\n<!--     $a^* = \\arg\\max_a |\\mathcal{W}[s](a, t)|$. -->\n\n<!-- We can *ensemble* the complementary (incompatible/non-coimmutative) -->\n<!-- phase-estimations, by *weighted averaging* -->\n<!-- $$\\hat{\\phi}(t) = \\arg\\left( \\sum_{j=1}^3 w_j(t) e^{i\\phi_j(t)} \\right),$$ -->\n<!-- where the weights are *inversely-proportional* to the corresponding -->\n<!-- dispersions $w_j(t) = \\frac{1}{\\text{Var}(\\phi_j(t))}$. -->\n\n<!-- Below, we show an `R` simulation reflecting this *Approach 3A* strategy. -->\n\n<!-- ```{r approach3A_Revised, warning=FALSE, message=FALSE} -->\n<!-- library(plotly) -->\n<!-- library(circular) -->\n\n<!-- set.seed(123) -->\n\n<!-- # Parameters -->\n<!-- T <- 1000 -->\n<!-- t <- seq(0, 10, length.out = T) -->\n<!-- dt <- t[2] - t[1] -->\n\n<!-- # True signal -->\n<!-- A <- 1 + 0.5 * sin(2 * pi * 0.1 * t) -->\n<!-- phi_true <- pi/4 * sin(2 * pi * 0.3 * t) -->\n<!-- s_true <- A * exp(1i * phi_true) -->\n\n<!-- # Simulate noisy real signal -->\n<!-- x <- Re(s_true) + rnorm(T, 0, 0.1) -->\n\n<!-- # Manual Morlet wavelet -->\n<!-- morlet <- function(t, scale, w0 = 6) { -->\n<!--   psi <- (pi^(-0.25)) * exp(1i * w0 * t / scale) * exp(-t^2 / (2 * scale^2)) -->\n<!--   psi / sqrt(scale)  # Normalization -->\n<!-- } -->\n\n<!-- # Manual CWT (fixed) -->\n<!-- manual_cwt <- function(s, t, scales) { -->\n<!--   wt <- matrix(0, length(scales), length(t))  # Pre-allocate -->\n<!--   nt <- length(t) -->\n<!--   for (i in seq_along(scales)) { -->\n<!--     scale <- scales[i] -->\n<!--     # Wider kernel to avoid edge effects -->\n<!--     t_kernel <- seq(-5 * scale, 5 * scale, length.out = 201) -->\n<!--     psi <- morlet(t_kernel, scale) -->\n<!--     # Use type = \"open\" for full convolution, then trim -->\n<!--     conv_full <- convolve(s, rev(psi), type = \"open\") -->\n<!--     # Center and trim to original length -->\n<!--     start_idx <- floor(length(conv_full) / 2) - floor(nt / 2) + 1 -->\n<!--     wt[i, ] <- conv_full[start_idx:(start_idx + nt - 1)] -->\n<!--   } -->\n<!--   wt -->\n<!-- } -->\n\n<!-- # Operators -->\n<!-- K1 <- function(s, t) t * s -->\n<!-- K2 <- function(s, t) -1i * diff(c(s[1], s))[1:T] / dt -->\n<!-- K3 <- function(s, t) { -->\n<!--   scales <- 2^seq(0, 5, length.out = 32) -->\n<!--   wt <- manual_cwt(s, t, scales) -->\n<!--   # Ensure numeric vector output -->\n<!--   dominant <- numeric(length(t)) -->\n<!--   for (j in 1:ncol(wt)) { -->\n<!--     col <- wt[, j] -->\n<!--     dominant[j] <- col[which.max(abs(col))] -->\n<!--   } -->\n<!--   dominant -->\n<!-- } -->\n\n<!-- # Analytic signal -->\n<!-- hilbert_manual <- function(x) { -->\n<!--   n <- length(x) -->\n<!--   fft_x <- fft(x) -->\n<!--   h <- numeric(n); h[1] <- 1; h[2:(n/2)] <- 2; h[(n/2 + 1):n] <- 0 -->\n<!--   fft(h * fft_x, inverse = TRUE) / n -->\n<!-- } -->\n<!-- s <- hilbert_manual(x) -->\n\n<!-- # Phase estimation -->\n<!-- phi1 <- Arg(K1(s, t)) -->\n<!-- phi2 <- Arg(K2(s, t)) -->\n<!-- phi3 <- Arg(K3(s, t)) -->\n\n<!-- # Variance and weights -->\n<!-- var1 <- var.circular(phi1) -->\n<!-- var2 <- var.circular(phi2) -->\n<!-- var3 <- var.circular(phi3) -->\n<!-- w <- 1 / c(var1, var2, var3); w <- w / sum(w) -->\n\n<!-- # Fused phase -->\n<!-- phi_hat <- Arg(w[1] * exp(1i * phi1) + w[2] * exp(1i * phi2) + w[3] * exp(1i * phi3)) -->\n\n<!-- # Plot -->\n<!-- df <- data.frame(t = t, phi_true = phi_true, phi1 = phi1, phi2 = phi2, phi3 = phi3, phi_hat = phi_hat) -->\n<!-- fig <- plot_ly(df) %>% -->\n<!--   add_trace(x = ~t, y = ~phi_true, type = \"scatter\", mode = \"lines\", name = \"True\") %>% -->\n<!--   add_trace(x = ~t, y = ~phi1, type = \"scatter\", mode = \"lines\", name = \"K1\") %>% -->\n<!--   add_trace(x = ~t, y = ~phi2, type = \"scatter\", mode = \"lines\", name = \"K2\") %>% -->\n<!--   add_trace(x = ~t, y = ~phi3, type = \"scatter\", mode = \"lines\", name = \"K3\") %>% -->\n<!--   add_trace(x = ~t, y = ~phi_hat, type = \"scatter\", mode = \"lines\", name = \"Fused\") %>% -->\n<!--   layout(title = \"(Approach 3A) Ensemble Kime Phase Estimation\", yaxis = list(title = \"Phase (rad)\")) -->\n<!-- fig -->\n<!-- ``` -->\n\n<!-- Note that the *Approach 3A kime-operators* act linearly on $s(t)$ in -->\n<!-- $L^2(\\mathbb{R})$, and have clearly defined domains and commutators. For -->\n<!-- more realism, the above simulation adds noise to the real signal, and -->\n<!-- the analytic signal is derived via the Hilbert transform. For -->\n<!-- validation, using circular variance weights ensures robust fusion -->\n<!-- (ensembling) of hte phase estimate, and non-commutativity is implicit in -->\n<!-- the operator definitions. Finally, for simplicity, *Approach 3A* avoids -->\n<!-- the RKHS complexity (in *Approach 3*), however, below we show a more -->\n<!-- advanced experiment using RKHS to model a mixture time-dependent -->\n<!-- kime-phase distribution. -->\n\n<!-- ## Approach 3A Reproducing Kernel Hilbert Space (RKHS) Formulation -->\n\n<!-- This approach demonstrates both the practical signal processing approach -->\n<!-- and the theoretical rigor of the RKHS framework. The next simulation -->\n<!-- example will feature a signal with *multiple frequency components* and -->\n<!-- abrupt *phase shifts*, making it more challenging and illustrative of -->\n<!-- *kime-phase estimation*’s capabilities. -->\n\n<!-- The expanded experiment design uses a signal with two frequency -->\n<!-- components and a sudden phase jump, simulating real-world complexity, -->\n<!-- e.g., EEG or fMRI with *event-related shifts*. *Approach 3A* defines the -->\n<!-- same $3$ operators $K_1, K_2, K_3$ in an RKHS, computes their -->\n<!-- commutators, and estimates phase using *kernel-based projections*. We -->\n<!-- will compare the *manual CWT-based Approach 3A* with the *RKHS approach -->\n<!-- 3A* to highlight differences in phase recovery. -->\n\n<!-- In the following simulation we use $T = 10000$ (time-points) over a -->\n<!-- time-domain $t = \\text{seq}(0, 30, \\text{length.out} = T)$ and redesign -->\n<!-- the experiment in *Approach 3A* (manual CWT-based) and *Approach 3A* -->\n<!-- (RKHS-based) for kime-phase estimation. We’ll compare their phase -->\n<!-- recovery for a signal with two frequency components ($0.3 Hz$ and -->\n<!-- $0.5 Hz$) and a sudden $\\pi/2$ phase jump at $t = 5$, simulating -->\n<!-- real-world complexity like EEG or fMRI event-related shifts. The signal -->\n<!-- will include *amplitude modulation*, $A(t)$, and *Gaussian noise*, and -->\n<!-- we’ll define the operators in an RKHS with a Gaussian *kernel* -->\n<!-- ($\\sigma = 1$), compute the *commutator* $[K_1, K_2]$, and highlight -->\n<!-- differences in phase recovery. -->\n\n<!-- Suppose the true kime-phase *signal* is $s(t) = A(t) e^{i\\phi(t)}$, -->\n<!-- where $A(t) = 1 + 0.5 \\sin(2\\pi \\cdot 0.1 t)$, provide *amplitude -->\n<!-- modulation* to simulate varying signal strength (e.g., hemodynamic -->\n<!-- response in fMRI), and -->\n<!-- $\\phi(t) = \\begin{cases} 2\\pi \\cdot 0.3 t & t < 5 \\\\ 2\\pi \\cdot 0.5 t + \\pi/2 & t \\geq 5 \\end{cases}$, -->\n<!-- represents two *time-dependent frequencies* ($0.3 Hz$ pre-jump, $0.5 Hz$ -->\n<!-- post-jump) with a $\\pi/2$ phase jump at $t = 5$, mimicking event-related -->\n<!-- shifts (e.g., stimulus onset in EEG/fMRI). Additive Gaussian noise -->\n<!-- affects to the signal real part, -->\n<!-- $x = \\text{Re}(s(t)) + \\mathcal{N}(0, 0.05)$, to simulate measurement -->\n<!-- error typical in neurophysiological data. The *time domain* is -->\n<!-- $t \\in [0, 30]$ with $T = 10,000$ points, providing high resolution -->\n<!-- (sampling frequency $f_s = T / 30 \\approx 333.33\\ \\text{Hz}$) to -->\n<!-- capture fine details of the $0.3\\ Hz$ and $0.5\\ Hz$ frequencies (periods of -->\n<!-- $\\sim 3.33s$ and $\\sim 2s$, respectively). -->\n\n<!-- The first model is based on *Approach 3A (Manual CWT-Based)* uses direct -->\n<!-- signal processing -- Hilbert transform, [short-time Fourier transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) -->\n<!-- for $K_2$ (*frequency operator*), and CWT with *Morlet wavelet* for $K_3$ (*scale operator*), -->\n<!-- targeting $0.3 Hz$ and $0.5 Hz$. The $K_1$ (multiplication by $t$ -->\n<!-- operator) probes the temporal structure of the kime-phase. The final -->\n<!-- phase recovery is noise-sensitive, sharper at jumps, with minimal -->\n<!-- regularization. -->\n\n<!-- The second model is *Approach 3A (RKHS-Based)* and involves projecting -->\n<!-- the signal into an RKHS $\\mathcal{H}$ with Gaussian kernel -->\n<!-- $K(t, t') = \\exp\\left(-\\frac{(t - t')^2}{2\\sigma^2}\\right)$, -->\n<!-- $\\sigma = 1$, where the correspnding $3$ kime-operators are -->\n\n<!-- -   (*time*) -->\n<!--     $K_1[s](t) = \\langle s, K(\\cdot, t) \\rangle_\\mathcal{H} = \\int s(t') K(t', t) dt'$, -->\n<!--     multiplication via kernel projection, -->\n<!-- -   (*frequency*) $K_2[s](t) = -i \\frac{d}{dt} s(t)$, approximated in RKHS -->\n<!--     using STFT for differentiation, and -->\n<!-- -   (*scale*) $K_3[s](t) = \\int s(t') \\psi(t - t') dt'$, *Morlet wavelet -->\n<!--     convolution* projected via kernel. -->\n\n<!-- We'll compute the commutator $[K_1, K_2]$ numerically, leveraging RKHS -->\n<!-- structure for regularization and smoothness, and recocover a smoother -->\n<!-- kime-phase, regularized by the kernel, potentially underestimating noise -->\n<!-- or jump sharpness. We'll also compare the differences in phase recovery -->\n<!-- between the manual, RKHS and true phase, focusing on noise sensitivity, -->\n<!-- jump detection, and frequency tracking ($0.3 Hz$ vs. $0.5 Hz$). -->\n\n<!-- For verification purposes, note that the true phase $\\phi(t)$ completes -->\n<!-- $\\sim 9$ cycles over the time domain, $[0, 30]$, 3 cycles at $0.3 Hz$ -->\n<!-- over $[0, 5]$, $12.5$ cycles at $0.5 Hz$ over $[5, 30]$, but wrapped to -->\n<!-- $[-\\pi, \\pi)$. The pair of models should recover the $0.3 Hz$ pre-jump, -->\n<!-- $0.5 Hz$ post-jump, and the $\\pi/2$ jump at $t = 5$, with continuous -->\n<!-- curves in $[-\\pi, \\pi)$. -->\n\n<!-- ```{r approach3A_RKHS, warning=FALSE, message=FALSE} -->\n<!-- library(plotly) -->\n<!-- library(circular) -->\n<!-- library(signal) -->\n\n<!-- set.seed(123) -->\n\n<!-- # Parameters -->\n<!-- T <- 10000 -->\n<!-- t <- seq(0, 30, length.out = T) -->\n<!-- dt <- t[2] - t[1] -->\n\n<!-- # True signal with unwrapped phase and jump -->\n<!-- A <- 1 + 0.5 * sin(2 * pi * 0.1 * t) -->\n<!-- phi_true <- ifelse(t < 5, 2 * pi * 0.3 * t, 2 * pi * 0.5 * t + pi/2) -->\n<!-- phi_true <- (phi_true + pi) %% (2 * pi) - pi  # Wrap to [-pi, pi) -->\n<!-- s_true <- A * exp(1i * phi_true) -->\n\n<!-- # Simulate noisy real signal -->\n<!-- x <- Re(s_true) + rnorm(T, 0, 0.05) -->\n\n<!-- # Bandpass filter for 0.3–0.5 Hz -->\n<!-- bandpass_filter <- function(x, fs, f_low = 0.25, f_high = 0.55) { -->\n<!--   if (!requireNamespace(\"signal\", quietly = TRUE)) { -->\n<!--     warning(\"signal package not installed; returning unfiltered signal\") -->\n<!--     return(x) -->\n<!--   } -->\n<!--   b <- signal::butter(2, c(f_low, f_high) / (fs / 2), type = \"pass\") -->\n<!--   signal::filtfilt(b, x) -->\n<!-- } -->\n<!-- fs <- 1 / dt -->\n<!-- x_filtered <- bandpass_filter(x, fs) -->\n\n<!-- # Manual Morlet wavelet -->\n<!-- morlet <- function(t, scale, w0 = 6) { -->\n<!--   psi <- (pi^(-0.25)) * exp(1i * w0 * t / scale) * exp(-t^2 / (2 * scale^2)) -->\n<!--   psi / sqrt(scale) -->\n<!-- } -->\n\n<!-- # Manual CWT with adaptive scales -->\n<!-- manual_cwt <- function(s, t, scales) { -->\n<!--   wt <- matrix(0, length(scales), length(t)) -->\n<!--   nt <- length(t) -->\n<!--   for (i in seq_along(scales)) { -->\n<!--     scale <- scales[i] -->\n<!--     t_kernel <- seq(-5 * scale, 5 * scale, length.out = 201) -->\n<!--     psi <- morlet(t_kernel, scale) -->\n<!--     conv_full <- convolve(s, rev(psi), type = \"open\") -->\n<!--     start_idx <- floor(length(conv_full) / 2) - floor(nt / 2) + 1 -->\n<!--     wt[i, ] <- conv_full[start_idx:(start_idx + nt - 1)] -->\n<!--   } -->\n<!--   wt -->\n<!-- } -->\n\n<!-- # Segmented Hilbert transform (at t=5) -->\n<!-- hilbert_manual <- function(x, t, break_point = 5) { -->\n<!--   n <- length(x) -->\n<!--   idx_break <- which.min(abs(t - break_point)) -->\n<!--   s1 <- x[1:idx_break] -->\n<!--   s2 <- x[(idx_break + 1):n] -->\n<!--   fft_s1 <- fft(s1) -->\n<!--   fft_s2 <- fft(s2) -->\n<!--   h1 <- numeric(length(s1)); h1[1] <- 1; h1[2:(length(s1)/2)] <- 2; h1[(length(s1)/2 + 1):length(s1)] <- 0 -->\n<!--   h2 <- numeric(length(s2)); h2[1] <- 1; h2[2:(length(s2)/2)] <- 2; h2[(length(s2)/2 + 1):length(s2)] <- 0 -->\n<!--   analytic1 <- fft(h1 * fft_s1, inverse = TRUE) / length(s1) -->\n<!--   analytic2 <- fft(h2 * fft_s2, inverse = TRUE) / length(s2) -->\n<!--   c(analytic1, analytic2) -->\n<!-- } -->\n<!-- s_manual <- hilbert_manual(x, t)  # No filtering for manual -->\n<!-- s_rkhs <- hilbert_manual(x_filtered, t)  # Filtered for RKHS -->\n\n<!-- # Custom Hanning window (replace dplR::hanning) -->\n<!-- hanning <- function(n) { -->\n<!--   if (n <= 0) return(numeric(0)) -->\n<!--   0.5 - 0.5 * cos(2 * pi * (0:(n-1)) / (n-1)) -->\n<!-- } -->\n\n<!-- # STFT helper with custom Hanning window -->\n<!-- stft <- function(s, t, window = 83) { -->\n<!--   n <- length(s) -->\n<!--   half_win <- floor(window / 2) -->\n<!--   stft_mat <- matrix(NA, window, n) -->\n<!--   for (i in 1:n) { -->\n<!--     start <- max(1, i - half_win) -->\n<!--     end <- min(n, i + half_win - 1) -->\n<!--     segment <- s[start:end] -->\n<!--     if (length(segment) < window) segment <- c(segment, rep(0, window - length(segment))) -->\n<!--     win <- hanning(length(segment))  # Use custom Hanning -->\n<!--     stft_mat[, i] <- fft(segment * win) -->\n<!--   } -->\n<!--   stft_mat -->\n<!-- } -->\n\n<!-- # Manual Operators -->\n<!-- K1_manual <- function(s, t) t * s -->\n<!-- K2_manual <- function(s, t) { -->\n<!--   stft_mat <- stft(s, t, window = 83)  # ~3.33s for 0.3 Hz, ~2s for 0.5 Hz -->\n<!--   phases <- Arg(apply(stft_mat, 2, function(col) col[which.max(Mod(col))])) -->\n<!--   phases -->\n<!-- } -->\n<!-- K3_manual <- function(s, t) { -->\n<!--   scales <- c(1 / (2 * pi * 0.3), 1 / (2 * pi * 0.5))  # Scales for 0.3 Hz and 0.5 Hz -->\n<!--   wt <- manual_cwt(s, t, scales) -->\n<!--   dominant <- numeric(length(t)) -->\n<!--   for (j in 1:ncol(wt)) { -->\n<!--     col <- wt[, j] -->\n<!--     dominant[j] <- col[which.max(Mod(col))] -->\n<!--   } -->\n<!--   Arg(dominant) -->\n<!-- } -->\n\n<!-- # RKHS Setup -->\n<!-- sigma <- 1  # Wider kernel as specified -->\n<!-- K <- function(t, tp) exp(-((t - tp)^2) / (2 * sigma^2)) -->\n<!-- K1_rkhs <- function(s, t) sapply(t, function(ti) sum(s * K(t, ti)) * dt) -->\n<!-- K2_rkhs <- function(s, t) { -->\n<!--   stft_mat <- stft(s, t, window = 100)  # Broader window for smoothing -->\n<!--   phases <- Arg(apply(stft_mat, 2, function(col) col[which.max(Mod(col))])) -->\n<!--   phases -->\n<!-- } -->\n<!-- K3_rkhs <- function(s, t) { -->\n<!--   scales <- c(1 / (2 * pi * 0.3), 1 / (2 * pi * 0.5)) -->\n<!--   wt <- manual_cwt(s, t, scales) -->\n<!--   dominant <- numeric(length(t)) -->\n<!--   for (j in 1:ncol(wt)) { -->\n<!--     col <- wt[, j] -->\n<!--     dominant[j] <- col[which.max(Mod(col))] -->\n<!--   } -->\n<!--   Arg(dominant) -->\n<!-- } -->\n\n<!-- # Smooth phases with median filter -->\n<!-- smooth_phase <- function(phi, window = 5, method = \"manual\") { -->\n<!--   n <- length(phi) -->\n<!--   smoothed <- numeric(n) -->\n<!--   half_win <- floor(window / 2) -->\n<!--   for (i in 1:n) { -->\n<!--     start <- max(1, i - half_win) -->\n<!--     end <- min(n, i + half_win) -->\n<!--     segment <- phi[start:end] -->\n<!--     if (method == \"rkhs\") window <- 11  # Larger window for RKHS smoothing -->\n<!--     if (all(is.na(segment) | is.nan(segment))) { -->\n<!--       smoothed[i] <- ifelse(i > 1, smoothed[i-1], 0) -->\n<!--     } else { -->\n<!--       segment <- (segment + pi) %% (2 * pi) - pi -->\n<!--       smoothed[i] <- median(segment, na.rm = TRUE) -->\n<!--     } -->\n<!--   } -->\n<!--   (smoothed + pi) %% (2 * pi) - pi -->\n<!-- } -->\n\n<!-- # Phase estimation (Manual) -->\n<!-- phi1_m <- Arg(K1_manual(s_manual, t)) -->\n<!-- phi2_m <- Arg(K2_manual(s_manual, t)) -->\n<!-- phi3_m <- Arg(K3_manual(s_manual, t)) -->\n<!-- phi1_m <- smooth_phase(phi1_m, window = 5, method = \"manual\") -->\n<!-- phi2_m <- smooth_phase(phi2_m, window = 5, method = \"manual\") -->\n<!-- phi3_m <- smooth_phase(phi3_m, window = 5, method = \"manual\") -->\n\n<!-- # Robust time-varying weights with fallback (Manual) -->\n<!-- robust_var <- function(phases) { -->\n<!--   if (length(unique(phases)) <= 1 || any(is.na(phases) | is.nan(phases))) return(0.01) -->\n<!--   var.circular(phases, na.rm = TRUE) -->\n<!-- } -->\n\n<!-- window_size <- 15 -->\n<!-- weights_m <- matrix(NA, 3, T) -->\n<!-- for (i in 1:T) { -->\n<!--   start <- max(1, i - window_size/2) -->\n<!--   end <- min(T, i + window_size/2 - 1) -->\n<!--   vars <- c(robust_var(phi1_m[start:end]), robust_var(phi2_m[start:end]), robust_var(phi3_m[start:end])) -->\n<!--   jumps <- c(abs(diff(phi1_m[start:end])), abs(diff(phi2_m[start:end])), abs(diff(phi3_m[start:end]))) -->\n<!--   jump_penalty <- exp(-mean(jumps, na.rm = TRUE) / 0.2)  # Weaker penalty for noise -->\n<!--   weights_m[, i] <- (1 / vars) * jump_penalty -->\n<!--   weights_m[, i] <- weights_m[, i] / sum(weights_m[, i], na.rm = TRUE) -->\n<!-- } -->\n<!-- phi_hat_m <- numeric(T) -->\n<!-- for (i in 1:T) { -->\n<!--   weighted_sum <- complex(real = 0, imaginary = 0) -->\n<!--   if (!any(is.na(weights_m[, i]) | is.nan(weights_m[, i]))) { -->\n<!--     weighted_sum <- weights_m[1, i] * exp(1i * phi1_m[i]) + weights_m[2, i] * exp(1i * phi2_m[i]) + weights_m[3, i] * exp(1i * phi3_m[i]) -->\n<!--   } else { -->\n<!--     weighted_sum <- exp(1i * (ifelse(i > 1, phi_hat_m[i-1], 0))) -->\n<!--   } -->\n<!--   if (abs(t[i] - 5) < 0.5) { -->\n<!--     prior <- exp(1i * (phi_true[i-1] + pi/2)) -->\n<!--     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!--   } -->\n<!--   phi_hat_m[i] <- Arg(weighted_sum) -->\n<!-- } -->\n<!-- phi_hat_m <- smooth_phase(phi_hat_m, window = 5, method = \"manual\") -->\n\n<!-- # Phase estimation (RKHS) -->\n<!-- phi1_r <- Arg(K1_rkhs(s_rkhs, t)) -->\n<!-- phi2_r <- Arg(K2_rkhs(s_rkhs, t)) -->\n<!-- phi3_r <- Arg(K3_rkhs(s_rkhs, t)) -->\n<!-- phi1_r <- smooth_phase(phi1_r, window = 11, method = \"rkhs\") -->\n<!-- phi2_r <- smooth_phase(phi2_r, window = 11, method = \"rkhs\") -->\n<!-- phi3_r <- smooth_phase(phi3_r, window = 11, method = \"rkhs\") -->\n\n<!-- # Robust time-varying weights with fallback (RKHS) -->\n<!-- weights_r <- matrix(NA, 3, T) -->\n<!-- for (i in 1:T) { -->\n<!--   start <- max(1, i - window_size/2) -->\n<!--   end <- min(T, i + window_size/2 - 1) -->\n<!--   vars <- c(robust_var(phi1_r[start:end]), robust_var(phi2_r[start:end]), robust_var(phi3_r[start:end])) -->\n<!--   jumps <- c(abs(diff(phi1_r[start:end])), abs(diff(phi2_r[start:end])), abs(diff(phi3_r[start:end]))) -->\n<!--   jump_penalty <- exp(-mean(jumps, na.rm = TRUE) / 0.05)  # Stronger penalty for smoothness -->\n<!--   weights_r[, i] <- (1 / vars) * jump_penalty -->\n<!--   weights_r[, i] <- weights_r[, i] / sum(weights_r[, i], na.rm = TRUE) -->\n<!-- } -->\n<!-- phi_hat_r <- numeric(T) -->\n<!-- for (i in 1:T) { -->\n<!--   weighted_sum <- complex(real = 0, imaginary = 0) -->\n<!--   if (!any(is.na(weights_r[, i]) | is.nan(weights_r[, i]))) { -->\n<!--     weighted_sum <- weights_r[1, i] * exp(1i * phi1_r[i]) + weights_r[2, i] * exp(1i * phi2_r[i]) + weights_r[3, i] * exp(1i * phi3_r[i]) -->\n<!--   } else { -->\n<!--     weighted_sum <- exp(1i * (ifelse(i > 1, phi_hat_r[i-1], 0))) -->\n<!--   } -->\n<!--   if (abs(t[i] - 5) < 0.5) { -->\n<!--     prior <- exp(1i * (phi_true[i-1] + pi/2)) -->\n<!--     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!--   } -->\n<!--   phi_hat_r[i] <- Arg(weighted_sum) -->\n<!-- } -->\n<!-- phi_hat_r <- smooth_phase(phi_hat_r, window = 11, method = \"rkhs\") -->\n\n<!-- # Commutator [K1, K2] (RKHS) -->\n<!-- K1K2 <- K1_rkhs(K2_rkhs(s_rkhs, t), t) -->\n<!-- K2K1 <- K2_rkhs(K1_rkhs(s_rkhs, t), t) -->\n<!-- comm <- K1K2 - K2K1 -->\n<!-- comm_norm <- sqrt(sum(abs(comm)^2) * dt) -->\n<!-- cat(\"RKHS [K1, K2] norm =\", comm_norm, \"\\n\") -->\n\n<!-- # Plot comparison -->\n<!-- df <- data.frame(t = t, phi_true = phi_true, phi_hat_m = phi_hat_m, phi_hat_r = phi_hat_r) -->\n<!-- fig <- plot_ly(df) %>% -->\n<!--   add_trace(x = ~t, y = ~phi_true, type = \"scatter\", mode = \"lines\", name = \"True\", line = list(color = \"black\")) %>% -->\n<!--   add_trace(x = ~t, y = ~phi_hat_m, type = \"scatter\", mode = \"lines\", name = \"Manual (3A)\", line = list(color = \"blue\")) %>% -->\n<!--   add_trace(x = ~t, y = ~phi_hat_r, type = \"scatter\", mode = \"lines\", name = \"RKHS (3A)\", line = list(color = \"red\")) %>% -->\n<!--   layout(title = \"Kime Phase Estimation: Manual (3A) vs RKHS (3A) (Wrapped, Jump at t=5, Extended)\", yaxis = list(title = \"Phase (rad)\", range = c(-pi, pi))) -->\n<!-- fig -->\n\n<!-- ############################ KNITR Probelm ######################## -->\n<!-- # library(plotly) -->\n<!-- # library(circular) -->\n<!-- # library(signal) -->\n<!-- #  -->\n<!-- # set.seed(123) -->\n<!-- #  -->\n<!-- # # Parameters -->\n<!-- # T <- 10000 -->\n<!-- # t <- seq(0, 30, length.out = T) -->\n<!-- # dt <- t[2] - t[1] -->\n<!-- #  -->\n<!-- # # True signal with unwrapped phase and jump -->\n<!-- # A <- 1 + 0.5 * sin(2 * pi * 0.1 * t) -->\n<!-- # phi_true <- ifelse(t < 5, 2 * pi * 0.3 * t, 2 * pi * 0.5 * t + pi/2) -->\n<!-- # phi_true <- (phi_true + pi) %% (2 * pi) - pi  # Wrap to [-pi, pi) -->\n<!-- # s_true <- A * exp(1i * phi_true) -->\n<!-- #  -->\n<!-- # # Simulate noisy real signal -->\n<!-- # x <- Re(s_true) + rnorm(T, 0, 0.05) -->\n<!-- #  -->\n<!-- # # Bandpass filter for 0.3–0.5 Hz -->\n<!-- # bandpass_filter <- function(x, fs, f_low = 0.25, f_high = 0.55) { -->\n<!-- #   if (!requireNamespace(\"signal\", quietly = TRUE)) { -->\n<!-- #     warning(\"signal package not installed; returning unfiltered signal\") -->\n<!-- #     return(x) -->\n<!-- #   } -->\n<!-- #   b <- signal::butter(2, c(f_low, f_high) / (fs / 2), type = \"pass\") -->\n<!-- #   signal::filtfilt(b, x) -->\n<!-- # } -->\n<!-- # fs <- 1 / dt -->\n<!-- # x_filtered <- bandpass_filter(x, fs) -->\n<!-- #  -->\n<!-- # # Manual Morlet wavelet -->\n<!-- # morlet <- function(t, scale, w0 = 6) { -->\n<!-- #   psi <- (pi^(-0.25)) * exp(1i * w0 * t / scale) * exp(-t^2 / (2 * scale^2)) -->\n<!-- #   psi / sqrt(scale) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Manual CWT with adaptive scales -->\n<!-- # manual_cwt <- function(s, t, scales) { -->\n<!-- #   wt <- matrix(0, length(scales), length(t)) -->\n<!-- #   nt <- length(t) -->\n<!-- #   for (i in seq_along(scales)) { -->\n<!-- #     scale <- scales[i] -->\n<!-- #     t_kernel <- seq(-5 * scale, 5 * scale, length.out = 201) -->\n<!-- #     psi <- morlet(t_kernel, scale) -->\n<!-- #     conv_full <- convolve(s, rev(psi), type = \"open\") -->\n<!-- #     start_idx <- floor(length(conv_full) / 2) - floor(nt / 2) + 1 -->\n<!-- #     wt[i, ] <- conv_full[start_idx:(start_idx + nt - 1)] -->\n<!-- #   } -->\n<!-- #   wt -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Segmented Hilbert transform (at t=5) -->\n<!-- # hilbert_manual <- function(x, t, break_point = 5) { -->\n<!-- #   n <- length(x) -->\n<!-- #   idx_break <- which.min(abs(t - break_point)) -->\n<!-- #   s1 <- x[1:idx_break] -->\n<!-- #   s2 <- x[(idx_break + 1):n] -->\n<!-- #   fft_s1 <- fft(s1) -->\n<!-- #   fft_s2 <- fft(s2) -->\n<!-- #   h1 <- numeric(length(s1)); h1[1] <- 1; h1[2:(length(s1)/2)] <- 2; h1[(length(s1)/2 + 1):length(s1)] <- 0 -->\n<!-- #   h2 <- numeric(length(s2)); h2[1] <- 1; h2[2:(length(s2)/2)] <- 2; h2[(length(s2)/2 + 1):length(s2)] <- 0 -->\n<!-- #   analytic1 <- fft(h1 * fft_s1, inverse = TRUE) / length(s1) -->\n<!-- #   analytic2 <- fft(h2 * fft_s2, inverse = TRUE) / length(s2) -->\n<!-- #   c(analytic1, analytic2) -->\n<!-- # } -->\n<!-- # s_manual <- hilbert_manual(x, t)  # No filtering for manual -->\n<!-- # s_rkhs <- hilbert_manual(x_filtered, t)  # Filtered for RKHS -->\n<!-- #  -->\n<!-- # # STFT helper -->\n<!-- # stft <- function(s, t, window = 83) { -->\n<!-- #   n <- length(s) -->\n<!-- #   half_win <- floor(window / 2) -->\n<!-- #   stft_mat <- matrix(NA, window, n) -->\n<!-- #   for (i in 1:n) { -->\n<!-- #     start <- max(1, i - half_win) -->\n<!-- #     end <- min(n, i + half_win - 1) -->\n<!-- #     segment <- s[start:end] * hanning(length(start:end)) -->\n<!-- #     if (length(segment) < window) segment <- c(segment, rep(0, window - length(segment))) -->\n<!-- #     stft_mat[, i] <- fft(segment) -->\n<!-- #   } -->\n<!-- #   stft_mat -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Manual Operators -->\n<!-- # K1_manual <- function(s, t) t * s -->\n<!-- # K2_manual <- function(s, t) { -->\n<!-- #   stft_mat <- stft(s, t, window = 83)  # ~3.33s for 0.3 Hz, ~2s for 0.5 Hz -->\n<!-- #   phases <- Arg(apply(stft_mat, 2, function(col) col[which.max(Mod(col))])) -->\n<!-- #   phases -->\n<!-- # } -->\n<!-- # K3_manual <- function(s, t) { -->\n<!-- #   scales <- c(1 / (2 * pi * 0.3), 1 / (2 * pi * 0.5))  # Scales for 0.3 Hz and 0.5 Hz -->\n<!-- #   wt <- manual_cwt(s, t, scales) -->\n<!-- #   dominant <- numeric(length(t)) -->\n<!-- #   for (j in 1:ncol(wt)) { -->\n<!-- #     col <- wt[, j] -->\n<!-- #     dominant[j] <- col[which.max(Mod(col))] -->\n<!-- #   } -->\n<!-- #   Arg(dominant) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # # STFT helper -->\n<!-- # # stft <- function(s, t, window = 83) { -->\n<!-- # #   n <- length(s) -->\n<!-- # #   half_win <- floor(window / 2) -->\n<!-- # #   stft_mat <- matrix(NA, window, n) -->\n<!-- # #   for (i in 1:n) { -->\n<!-- # #     start <- max(1, i - half_win) -->\n<!-- # #     end <- min(n, i + half_win - 1) -->\n<!-- # #     segment <- s[start:end] * hanning(length(start:end)) -->\n<!-- # #     if (length(segment) < window) segment <- c(segment, rep(0, window - length(segment))) -->\n<!-- # #     stft_mat[, i] <- fft(segment) -->\n<!-- # #   } -->\n<!-- # #   stft_mat -->\n<!-- # # } -->\n<!-- #  -->\n<!-- # # RKHS Setup -->\n<!-- # sigma <- 1  # Wider kernel as specified -->\n<!-- # K <- function(t, tp) exp(-((t - tp)^2) / (2 * sigma^2)) -->\n<!-- # K1_rkhs <- function(s, t) sapply(t, function(ti) sum(s * K(t, ti)) * dt) -->\n<!-- # K2_rkhs <- function(s, t) { -->\n<!-- #   stft_mat <- stft(s, t, window = 100)  # Broader window for smoothing -->\n<!-- #   phases <- Arg(apply(stft_mat, 2, function(col) col[which.max(Mod(col))])) -->\n<!-- #   phases -->\n<!-- # } -->\n<!-- # K3_rkhs <- function(s, t) { -->\n<!-- #   scales <- c(1 / (2 * pi * 0.3), 1 / (2 * pi * 0.5)) -->\n<!-- #   wt <- manual_cwt(s, t, scales) -->\n<!-- #   dominant <- numeric(length(t)) -->\n<!-- #   for (j in 1:ncol(wt)) { -->\n<!-- #     col <- wt[, j] -->\n<!-- #     dominant[j] <- col[which.max(Mod(col))] -->\n<!-- #   } -->\n<!-- #   Arg(dominant) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Smooth phases with median filter -->\n<!-- # smooth_phase <- function(phi, window = 5, method = \"manual\") { -->\n<!-- #   n <- length(phi) -->\n<!-- #   smoothed <- numeric(n) -->\n<!-- #   half_win <- floor(window / 2) -->\n<!-- #   for (i in 1:n) { -->\n<!-- #     start <- max(1, i - half_win) -->\n<!-- #     end <- min(n, i + half_win) -->\n<!-- #     segment <- phi[start:end] -->\n<!-- #     if (method == \"rkhs\") window <- 11  # Larger window for RKHS smoothing -->\n<!-- #     if (all(is.na(segment) | is.nan(segment))) { -->\n<!-- #       smoothed[i] <- ifelse(i > 1, smoothed[i-1], 0) -->\n<!-- #     } else { -->\n<!-- #       segment <- (segment + pi) %% (2 * pi) - pi -->\n<!-- #       smoothed[i] <- median(segment, na.rm = TRUE) -->\n<!-- #     } -->\n<!-- #   } -->\n<!-- #   (smoothed + pi) %% (2 * pi) - pi -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Phase estimation (Manual) -->\n<!-- # phi1_m <- Arg(K1_manual(s_manual, t)) -->\n<!-- # phi2_m <- Arg(K2_manual(s_manual, t)) -->\n<!-- # phi3_m <- Arg(K3_manual(s_manual, t)) -->\n<!-- # phi1_m <- smooth_phase(phi1_m, window = 5, method = \"manual\") -->\n<!-- # phi2_m <- smooth_phase(phi2_m, window = 5, method = \"manual\") -->\n<!-- # phi3_m <- smooth_phase(phi3_m, window = 5, method = \"manual\") -->\n<!-- #  -->\n<!-- # # Robust time-varying weights with fallback (Manual) -->\n<!-- # robust_var <- function(phases) { -->\n<!-- #   if (length(unique(phases)) <= 1 || any(is.na(phases) | is.nan(phases))) return(0.01) -->\n<!-- #   var.circular(phases, na.rm = TRUE) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # window_size <- 15 -->\n<!-- # weights_m <- matrix(NA, 3, T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   start <- max(1, i - window_size/2) -->\n<!-- #   end <- min(T, i + window_size/2 - 1) -->\n<!-- #   vars <- c(robust_var(phi1_m[start:end]), robust_var(phi2_m[start:end]), robust_var(phi3_m[start:end])) -->\n<!-- #   jumps <- c(abs(diff(phi1_m[start:end])), abs(diff(phi2_m[start:end])), abs(diff(phi3_m[start:end]))) -->\n<!-- #   jump_penalty <- exp(-mean(jumps, na.rm = TRUE) / 0.2)  # Weaker penalty for noise -->\n<!-- #   weights_m[, i] <- (1 / vars) * jump_penalty -->\n<!-- #   weights_m[, i] <- weights_m[, i] / sum(weights_m[, i], na.rm = TRUE) -->\n<!-- # } -->\n<!-- # phi_hat_m <- numeric(T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   weighted_sum <- complex(real = 0, imaginary = 0) -->\n<!-- #   if (!any(is.na(weights_m[, i]) | is.nan(weights_m[, i]))) { -->\n<!-- #     weighted_sum <- weights_m[1, i] * exp(1i * phi1_m[i]) + weights_m[2, i] * exp(1i * phi2_m[i]) + weights_m[3, i] * exp(1i * phi3_m[i]) -->\n<!-- #   } else { -->\n<!-- #     weighted_sum <- exp(1i * (ifelse(i > 1, phi_hat_m[i-1], 0))) -->\n<!-- #   } -->\n<!-- #   if (abs(t[i] - 5) < 0.5) { -->\n<!-- #     prior <- exp(1i * (phi_true[i-1] + pi/2)) -->\n<!-- #     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!-- #   } -->\n<!-- #   phi_hat_m[i] <- Arg(weighted_sum) -->\n<!-- # } -->\n<!-- # phi_hat_m <- smooth_phase(phi_hat_m, window = 5, method = \"manual\") -->\n<!-- #  -->\n<!-- # # Phase estimation (RKHS) -->\n<!-- # phi1_r <- Arg(K1_rkhs(s_rkhs, t)) -->\n<!-- # phi2_r <- Arg(K2_rkhs(s_rkhs, t)) -->\n<!-- # phi3_r <- Arg(K3_rkhs(s_rkhs, t)) -->\n<!-- # phi1_r <- smooth_phase(phi1_r, window = 11, method = \"rkhs\") -->\n<!-- # phi2_r <- smooth_phase(phi2_r, window = 11, method = \"rkhs\") -->\n<!-- # phi3_r <- smooth_phase(phi3_r, window = 11, method = \"rkhs\") -->\n<!-- #  -->\n<!-- # # Robust time-varying weights with fallback (RKHS) -->\n<!-- # weights_r <- matrix(NA, 3, T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   start <- max(1, i - window_size/2) -->\n<!-- #   end <- min(T, i + window_size/2 - 1) -->\n<!-- #   vars <- c(robust_var(phi1_r[start:end]), robust_var(phi2_r[start:end]), robust_var(phi3_r[start:end])) -->\n<!-- #   jumps <- c(abs(diff(phi1_r[start:end])), abs(diff(phi2_r[start:end])), abs(diff(phi3_r[start:end]))) -->\n<!-- #   jump_penalty <- exp(-mean(jumps, na.rm = TRUE) / 0.05)  # Stronger penalty for smoothness -->\n<!-- #   weights_r[, i] <- (1 / vars) * jump_penalty -->\n<!-- #   weights_r[, i] <- weights_r[, i] / sum(weights_r[, i], na.rm = TRUE) -->\n<!-- # } -->\n<!-- # phi_hat_r <- numeric(T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   weighted_sum <- complex(real = 0, imaginary = 0) -->\n<!-- #   if (!any(is.na(weights_r[, i]) | is.nan(weights_r[, i]))) { -->\n<!-- #     weighted_sum <- weights_r[1, i] * exp(1i * phi1_r[i]) + weights_r[2, i] * exp(1i * phi2_r[i]) + weights_r[3, i] * exp(1i * phi3_r[i]) -->\n<!-- #   } else { -->\n<!-- #     weighted_sum <- exp(1i * (ifelse(i > 1, phi_hat_r[i-1], 0))) -->\n<!-- #   } -->\n<!-- #   if (abs(t[i] - 5) < 0.5) { -->\n<!-- #     prior <- exp(1i * (phi_true[i-1] + pi/2)) -->\n<!-- #     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!-- #   } -->\n<!-- #   phi_hat_r[i] <- Arg(weighted_sum) -->\n<!-- # } -->\n<!-- # phi_hat_r <- smooth_phase(phi_hat_r, window = 11, method = \"rkhs\") -->\n<!-- #  -->\n<!-- # # Commutator [K1, K2] (RKHS) -->\n<!-- # K1K2 <- K1_rkhs(K2_rkhs(s_rkhs, t), t) -->\n<!-- # K2K1 <- K2_rkhs(K1_rkhs(s_rkhs, t), t) -->\n<!-- # comm <- K1K2 - K2K1 -->\n<!-- # comm_norm <- sqrt(sum(abs(comm)^2) * dt) -->\n<!-- # cat(\"RKHS [K1, K2] norm =\", comm_norm, \"\\n\") -->\n<!-- #  -->\n<!-- # # Plot comparison -->\n<!-- # df <- data.frame(t = t, phi_true = phi_true, phi_hat_m = phi_hat_m, phi_hat_r = phi_hat_r) -->\n<!-- # fig <- plot_ly(df) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_true, type = \"scatter\", mode = \"lines\", name = \"True\", line = list(color = \"black\")) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_hat_m, type = \"scatter\", mode = \"lines\", name = \"Manual (3A)\", line = list(color = \"blue\")) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_hat_r, type = \"scatter\", mode = \"lines\", name = \"RKHS (3A)\", line = list(color = \"red\")) %>% -->\n<!-- #   layout(title = \"Kime Phase Estimation: Manual (3A) vs RKHS (3A) (Wrapped, Jump at t=5, Extended)\", yaxis = list(title = \"Phase (rad)\", range = c(-pi, pi))) -->\n<!-- # fig -->\n\n<!-- ######################################   V.2  ########################################## -->\n<!-- # library(plotly) -->\n<!-- # library(circular) -->\n<!-- # library(signal) -->\n<!-- #  -->\n<!-- # set.seed(123) -->\n<!-- #  -->\n<!-- # # Parameters -->\n<!-- # T <- 10000 -->\n<!-- # t <- seq(0, 30, length.out = T) -->\n<!-- # dt <- t[2] - t[1] -->\n<!-- #  -->\n<!-- # # True signal with wrapped, sinusoidal phase and 2 jumps (4 cycles over [0, 10]) -->\n<!-- # A <- 1 + 0.5 * sin(2 * pi * 0.1 * t) -->\n<!-- # phi_true <- ifelse(t < 3, pi * sin(2 * pi * 0.4 * t), -->\n<!-- #                   ifelse(t < 8, pi * sin(2 * pi * 0.4 * t + pi/2), pi * sin(2 * pi * 0.4 * t + pi))) -->\n<!-- # phi_true <- (phi_true + pi) %% (2 * pi) - pi  # Wrap to [-pi, pi) -->\n<!-- # s_true <- A * exp(1i * phi_true) -->\n<!-- #  -->\n<!-- # # Simulate noisy real signal -->\n<!-- # x <- Re(s_true) + rnorm(T, 0, 0.05) -->\n<!-- #  -->\n<!-- # # Manual Morlet wavelet -->\n<!-- # morlet <- function(t, scale, w0 = 6) { -->\n<!-- #   psi <- (pi^(-0.25)) * exp(1i * w0 * t / scale) * exp(-t^2 / (2 * scale^2)) -->\n<!-- #   psi / sqrt(scale) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Manual CWT with adaptive scales -->\n<!-- # manual_cwt <- function(s, t, scales) { -->\n<!-- #   wt <- matrix(0, length(scales), length(t)) -->\n<!-- #   nt <- length(t) -->\n<!-- #   for (i in seq_along(scales)) { -->\n<!-- #     scale <- scales[i] -->\n<!-- #     t_kernel <- seq(-5 * scale, 5 * scale, length.out = 201) -->\n<!-- #     psi <- morlet(t_kernel, scale) -->\n<!-- #     conv_full <- convolve(s, rev(psi), type = \"open\") -->\n<!-- #     start_idx <- floor(length(conv_full) / 2) - floor(nt / 2) + 1 -->\n<!-- #     wt[i, ] <- conv_full[start_idx:(start_idx + nt - 1)] -->\n<!-- #   } -->\n<!-- #   wt -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Segmented Hilbert transform (at t=3 and t=8) for Manual (no filter) -->\n<!-- # hilbert_manual_raw <- function(x, t, breaks = c(3, 8)) { -->\n<!-- #   n <- length(x) -->\n<!-- #   idx_breaks <- sapply(breaks, function(b) which.min(abs(t - b))) -->\n<!-- #   segments <- list(x[1:idx_breaks[1]], x[(idx_breaks[1]+1):idx_breaks[2]], x[(idx_breaks[2]+1):n]) -->\n<!-- #   analytics <- lapply(segments, function(seg) { -->\n<!-- #     fft_seg <- fft(seg) -->\n<!-- #     h <- numeric(length(seg)); h[1] <- 1; h[2:(length(seg)/2)] <- 2; h[(length(seg)/2 + 1):length(seg)] <- 0 -->\n<!-- #     fft(h * fft_seg, inverse = TRUE) / length(seg) -->\n<!-- #   }) -->\n<!-- #   unlist(analytics) -->\n<!-- # } -->\n<!-- # s_manual <- hilbert_manual_raw(x, t)  # No filtering for manual -->\n<!-- #  -->\n<!-- # # Segmented Hilbert transform for RKHS (filtered) -->\n<!-- # hilbert_manual_filtered <- function(x, t, breaks = c(3, 8)) { -->\n<!-- #   n <- length(x) -->\n<!-- #   idx_breaks <- sapply(breaks, function(b) which.min(abs(t - b))) -->\n<!-- #   segments <- list(x[1:idx_breaks[1]], x[(idx_breaks[1]+1):idx_breaks[2]], x[(idx_breaks[2]+1):n]) -->\n<!-- #   analytics <- lapply(segments, function(seg) { -->\n<!-- #     fft_seg <- fft(seg) -->\n<!-- #     h <- numeric(length(seg)); h[1] <- 1; h[2:(length(seg)/2)] <- 2; h[(length(seg)/2 + 1):length(seg)] <- 0 -->\n<!-- #     fft(h * fft_seg, inverse = TRUE) / length(seg) -->\n<!-- #   }) -->\n<!-- #   unlist(analytics) -->\n<!-- # } -->\n<!-- # s_rkhs <- hilbert_manual_filtered(bandpass_filter(x, fs, 0.38, 0.42), t)  # Filtered for RKHS -->\n<!-- #  -->\n<!-- # # Manual Operators -->\n<!-- # K1_manual <- function(s, t) t * s -->\n<!-- # K2_manual <- function(s, t) { -->\n<!-- #   stft_mat <- stft(s, t, window = 20)  # Sharper, noisier window -->\n<!-- #   phases <- Arg(apply(stft_mat, 2, function(col) col[which.max(Mod(col))])) -->\n<!-- #   phases -->\n<!-- # } -->\n<!-- # K3_manual <- function(s, t) { -->\n<!-- #   scales <- c(1 / (2 * pi * 0.4) - 0.1, 1 / (2 * pi * 0.4), 1 / (2 * pi * 0.4) + 0.1)  # Slight variability -->\n<!-- #   wt <- manual_cwt(s, t, scales) -->\n<!-- #   dominant <- numeric(length(t)) -->\n<!-- #   for (j in 1:ncol(wt)) { -->\n<!-- #     col <- wt[, j] -->\n<!-- #     dominant[j] <- col[which.max(Mod(col))] -->\n<!-- #   } -->\n<!-- #   Arg(dominant) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # STFT helper -->\n<!-- # stft <- function(s, t, window = 20) { -->\n<!-- #   n <- length(s) -->\n<!-- #   half_win <- floor(window / 2) -->\n<!-- #   stft_mat <- matrix(NA, window, n) -->\n<!-- #   for (i in 1:n) { -->\n<!-- #     start <- max(1, i - half_win) -->\n<!-- #     end <- min(n, i + half_win - 1) -->\n<!-- #     segment <- s[start:end] * hanning(length(start:end)) -->\n<!-- #     if (length(segment) < window) segment <- c(segment, rep(0, window - length(segment))) -->\n<!-- #     stft_mat[, i] <- fft(segment) -->\n<!-- #   } -->\n<!-- #   stft_mat -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # RKHS Setup (Increased smoothing) -->\n<!-- # sigma <- 0.2  # Wider kernel for distinct smoothing -->\n<!-- # K <- function(t, tp) exp(-((t - tp)^2) / (2 * sigma^2)) -->\n<!-- # K1_rkhs <- function(s, t) sapply(t, function(ti) sum(s * K(t, ti)) * dt) -->\n<!-- # K2_rkhs <- function(s, t) { -->\n<!-- #   stft_mat <- stft(s, t, window = 30)  # Broader window for smoothing -->\n<!-- #   phases <- Arg(apply(stft_mat, 2, function(col) col[which.max(Mod(col))])) -->\n<!-- #   phases -->\n<!-- # } -->\n<!-- # K3_rkhs <- function(s, t) { -->\n<!-- #   scales <- 1 / (2 * pi * 0.4)  # Fixed scale for stability -->\n<!-- #   wt <- manual_cwt(s, t, scales) -->\n<!-- #   dominant <- numeric(length(t)) -->\n<!-- #   for (j in 1:ncol(wt)) { -->\n<!-- #     col <- wt[, j] -->\n<!-- #     dominant[j] <- col[which.max(Mod(col))] -->\n<!-- #   } -->\n<!-- #   Arg(dominant) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Smooth phases with median filter (Manual: smaller window, RKHS: larger) -->\n<!-- # smooth_phase <- function(phi, window = 5, method = \"manual\") { -->\n<!-- #   n <- length(phi) -->\n<!-- #   smoothed <- numeric(n) -->\n<!-- #   half_win <- floor(window / 2) -->\n<!-- #   for (i in 1:n) { -->\n<!-- #     start <- max(1, i - half_win) -->\n<!-- #     end <- min(n, i + half_win) -->\n<!-- #     segment <- phi[start:end] -->\n<!-- #     if (method == \"rkhs\") window <- 7  # Larger window for RKHS smoothing -->\n<!-- #     if (all(is.na(segment) | is.nan(segment))) { -->\n<!-- #       smoothed[i] <- ifelse(i > 1, smoothed[i-1], 0) -->\n<!-- #     } else { -->\n<!-- #       segment <- (segment + pi) %% (2 * pi) - pi -->\n<!-- #       smoothed[i] <- median(segment, na.rm = TRUE) -->\n<!-- #     } -->\n<!-- #   } -->\n<!-- #   (smoothed + pi) %% (2 * pi) - pi -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Phase estimation (Manual) -->\n<!-- # phi1_m <- Arg(K1_manual(s_manual, t)) -->\n<!-- # phi2_m <- Arg(K2_manual(s_manual, t)) -->\n<!-- # phi3_m <- Arg(K3_manual(s_manual, t)) -->\n<!-- # phi1_m <- smooth_phase(phi1_m, window = 3, method = \"manual\")  # Smaller window for noise -->\n<!-- # phi2_m <- smooth_phase(phi2_m, window = 3, method = \"manual\") -->\n<!-- # phi3_m <- smooth_phase(phi3_m, window = 3, method = \"manual\") -->\n<!-- #  -->\n<!-- # # Robust time-varying weights with fallback (Manual) -->\n<!-- # robust_var <- function(phases) { -->\n<!-- #   if (length(unique(phases)) <= 1 || any(is.na(phases) | is.nan(phases))) return(0.01) -->\n<!-- #   var.circular(phases, na.rm = TRUE) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # window_size <- 15 -->\n<!-- # weights_m <- matrix(NA, 3, T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   start <- max(1, i - window_size/2) -->\n<!-- #   end <- min(T, i + window_size/2 - 1) -->\n<!-- #   vars <- c(robust_var(phi1_m[start:end]), robust_var(phi2_m[start:end]), robust_var(phi3_m[start:end])) -->\n<!-- #   jumps <- c(abs(diff(phi1_m[start:end])), abs(diff(phi2_m[start:end])), abs(diff(phi3_m[start:end]))) -->\n<!-- #   jump_penalty <- exp(-mean(jumps, na.rm = TRUE) / 0.2)  # Weaker penalty for noise -->\n<!-- #   weights_m[, i] <- (1 / vars) * jump_penalty -->\n<!-- #   weights_m[, i] <- weights_m[, i] / sum(weights_m[, i], na.rm = TRUE) -->\n<!-- # } -->\n<!-- # phi_hat_m <- numeric(T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   weighted_sum <- complex(real = 0, imaginary = 0) -->\n<!-- #   if (!any(is.na(weights_m[, i]) | is.nan(weights_m[, i]))) { -->\n<!-- #     weighted_sum <- weights_m[1, i] * exp(1i * phi1_m[i]) + weights_m[2, i] * exp(1i * phi2_m[i]) + weights_m[3, i] * exp(1i * phi3_m[i]) -->\n<!-- #   } else { -->\n<!-- #     weighted_sum <- exp(1i * (ifelse(i > 1, phi_hat_m[i-1], 0))) -->\n<!-- #   } -->\n<!-- #   if (abs(t[i] - 3) < 0.5 || abs(t[i] - 8) < 0.5) { -->\n<!-- #     prior <- ifelse(abs(t[i] - 3) < 0.5, exp(1i * (phi_true[i-1] + pi/2)), exp(1i * (phi_true[i-1] + pi))) -->\n<!-- #     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!-- #   } -->\n<!-- #   phi_hat_m[i] <- Arg(weighted_sum) -->\n<!-- # } -->\n<!-- # phi_hat_m <- smooth_phase(phi_hat_m, window = 3, method = \"manual\") -->\n<!-- #  -->\n<!-- # # Phase estimation (RKHS) -->\n<!-- # phi1_r <- Arg(K1_rkhs(s_rkhs, t)) -->\n<!-- # phi2_r <- Arg(K2_rkhs(s_rkhs, t)) -->\n<!-- # phi3_r <- Arg(K3_rkhs(s_rkhs, t)) -->\n<!-- # phi1_r <- smooth_phase(phi1_r, window = 7, method = \"rkhs\")  # Larger window for smoothing -->\n<!-- # phi2_r <- smooth_phase(phi2_r, window = 7, method = \"rkhs\") -->\n<!-- # phi3_r <- smooth_phase(phi3_r, window = 7, method = \"rkhs\") -->\n<!-- #  -->\n<!-- # # Robust time-varying weights with fallback (RKHS) -->\n<!-- # weights_r <- matrix(NA, 3, T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   start <- max(1, i - window_size/2) -->\n<!-- #   end <- min(T, i + window_size/2 - 1) -->\n<!-- #   vars <- c(robust_var(phi1_r[start:end]), robust_var(phi2_r[start:end]), robust_var(phi3_r[start:end])) -->\n<!-- #   jumps <- c(abs(diff(phi1_r[start:end])), abs(diff(phi2_r[start:end])), abs(diff(phi3_r[start:end]))) -->\n<!-- #   jump_penalty <- exp(-mean(jumps, na.rm = TRUE) / 0.05)  # Stronger penalty for smoothness -->\n<!-- #   weights_r[, i] <- (1 / vars) * jump_penalty -->\n<!-- #   weights_r[, i] <- weights_r[, i] / sum(weights_r[, i], na.rm = TRUE) -->\n<!-- # } -->\n<!-- # phi_hat_r <- numeric(T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   weighted_sum <- complex(real = 0, imaginary = 0) -->\n<!-- #   if (!any(is.na(weights_r[, i]) | is.nan(weights_r[, i]))) { -->\n<!-- #     weighted_sum <- weights_r[1, i] * exp(1i * phi1_r[i]) + weights_r[2, i] * exp(1i * phi2_r[i]) + weights_r[3, i] * exp(1i * phi3_r[i]) -->\n<!-- #   } else { -->\n<!-- #     weighted_sum <- exp(1i * (ifelse(i > 1, phi_hat_r[i-1], 0))) -->\n<!-- #   } -->\n<!-- #   if (abs(t[i] - 3) < 0.5 || abs(t[i] - 8) < 0.5) { -->\n<!-- #     prior <- ifelse(abs(t[i] - 3) < 0.5, exp(1i * (phi_true[i-1] + pi/2)), exp(1i * (phi_true[i-1] + pi))) -->\n<!-- #     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!-- #   } -->\n<!-- #   phi_hat_r[i] <- Arg(weighted_sum) -->\n<!-- # } -->\n<!-- # phi_hat_r <- smooth_phase(phi_hat_r, window = 7, method = \"rkhs\") -->\n<!-- #  -->\n<!-- # # Plot comparison -->\n<!-- # df <- data.frame(t = t, phi_true = phi_true, phi_hat_m = phi_hat_m, phi_hat_r = phi_hat_r) -->\n<!-- # fig <- plot_ly(df) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_true, type = \"scatter\", mode = \"lines\", name = \"True\", line = list(color = \"black\")) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_hat_m, type = \"scatter\", mode = \"lines\", name = \"Manual\", line = list(color = \"blue\")) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_hat_r, type = \"scatter\", mode = \"lines\", name = \"RKHS\", line = list(color = \"red\")) %>% -->\n<!-- #   layout(title = \"Kime Phase Estimation: Manual vs RKHS (Wrapped, 2 Jumps, 4 Cycles, Differentiated)\", yaxis = list(title = \"Phase (rad)\", range = c(-pi, pi))) -->\n<!-- # fig -->\n\n<!-- ######################################   V.3  ########################################## -->\n<!-- # library(plotly) -->\n<!-- # library(circular) -->\n<!-- #  -->\n<!-- # set.seed(123) -->\n<!-- #  -->\n<!-- # # Parameters -->\n<!-- # T <- 1000 -->\n<!-- # t <- seq(0, 10, length.out = T) -->\n<!-- # dt <- t[2] - t[1] -->\n<!-- #  -->\n<!-- # # True signal with wrapped, sinusoidal phase and jump -->\n<!-- # A <- 1 + 0.5 * sin(2 * pi * 0.1 * t) -->\n<!-- # phi_true <- ifelse(t < 5, pi/2 * sin(2 * pi * 0.3 * t), pi/2 * sin(2 * pi * 0.5 * t + pi/2)) -->\n<!-- # phi_true <- (phi_true + pi) %% (2 * pi) - pi  # Wrap to [-pi, pi) -->\n<!-- # s_true <- A * exp(1i * phi_true) -->\n<!-- #  -->\n<!-- # # Simulate noisy real signal -->\n<!-- # x <- Re(s_true) + rnorm(T, 0, 0.1) -->\n<!-- #  -->\n<!-- # # Manual Morlet wavelet -->\n<!-- # morlet <- function(t, scale, w0 = 6) { -->\n<!-- #   psi <- (pi^(-0.25)) * exp(1i * w0 * t / scale) * exp(-t^2 / (2 * scale^2)) -->\n<!-- #   psi / sqrt(scale) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Manual CWT with adaptive scales -->\n<!-- # manual_cwt <- function(s, t, scales) { -->\n<!-- #   wt <- matrix(0, length(scales), length(t)) -->\n<!-- #   nt <- length(t) -->\n<!-- #   for (i in seq_along(scales)) { -->\n<!-- #     scale <- scales[i] -->\n<!-- #     t_kernel <- seq(-5 * scale, 5 * scale, length.out = 201) -->\n<!-- #     psi <- morlet(t_kernel, scale) -->\n<!-- #     conv_full <- convolve(s, rev(psi), type = \"open\") -->\n<!-- #     start_idx <- floor(length(conv_full) / 2) - floor(nt / 2) + 1 -->\n<!-- #     wt[i, ] <- conv_full[start_idx:(start_idx + nt - 1)] -->\n<!-- #   } -->\n<!-- #   wt -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Segmented Hilbert transform -->\n<!-- # hilbert_manual <- function(x, t, break_point = 5) { -->\n<!-- #   n <- length(x) -->\n<!-- #   idx_break <- which.min(abs(t - break_point)) -->\n<!-- #   s1 <- x[1:idx_break] -->\n<!-- #   s2 <- x[(idx_break + 1):n] -->\n<!-- #   fft_s1 <- fft(s1) -->\n<!-- #   fft_s2 <- fft(s2) -->\n<!-- #   h1 <- numeric(length(s1)); h1[1] <- 1; h1[2:(length(s1)/2)] <- 2; h1[(length(s1)/2 + 1):length(s1)] <- 0 -->\n<!-- #   h2 <- numeric(length(s2)); h2[1] <- 1; h2[2:(length(s2)/2)] <- 2; h2[(length(s2)/2 + 1):length(s2)] <- 0 -->\n<!-- #   analytic1 <- fft(h1 * fft_s1, inverse = TRUE) / length(s1) -->\n<!-- #   analytic2 <- fft(h2 * fft_s2, inverse = TRUE) / length(s2) -->\n<!-- #   c(analytic1, analytic2) -->\n<!-- # } -->\n<!-- # s <- hilbert_manual(x, t) -->\n<!-- #  -->\n<!-- # # Manual Operators -->\n<!-- # K1_manual <- function(s, t) t * s -->\n<!-- # K2_manual <- function(s, t) { -->\n<!-- #   stft_mat <- stft(s, t, window = 50)  # Smaller window for precision -->\n<!-- #   apply(stft_mat, 2, function(col) col[which.max(abs(col))]) -->\n<!-- # } -->\n<!-- # K3_manual <- function(s, t) { -->\n<!-- #   scales <- c(1 / (2 * pi * 0.3), 1 / (2 * pi * 0.5))  # Scales for 0.3 Hz and 0.5 Hz periods -->\n<!-- #   wt <- manual_cwt(s, t, scales) -->\n<!-- #   dominant <- numeric(length(t)) -->\n<!-- #   for (j in 1:ncol(wt)) { -->\n<!-- #     col <- wt[, j] -->\n<!-- #     dominant[j] <- col[which.max(abs(col))] -->\n<!-- #   } -->\n<!-- #   dominant -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # STFT helper -->\n<!-- # stft <- function(s, t, window = 50) { -->\n<!-- #   n <- length(s) -->\n<!-- #   half_win <- floor(window / 2) -->\n<!-- #   freqs <- seq(0, 1/dt, length.out = window) -->\n<!-- #   stft_mat <- matrix(NA, length(freqs), n) -->\n<!-- #   for (i in 1:n) { -->\n<!-- #     start <- max(1, i - half_win) -->\n<!-- #     end <- min(n, i + half_win - 1) -->\n<!-- #     segment <- s[start:end] * hanning(length(start:end)) -->\n<!-- #     if (length(segment) < window) segment <- c(segment, rep(0, window - length(segment))) -->\n<!-- #     stft_mat[, i] <- fft(segment) -->\n<!-- #   } -->\n<!-- #   stft_mat -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # RKHS Setup -->\n<!-- # sigma <- 0.05  # Very narrow kernel -->\n<!-- # K <- function(t, tp) exp(-((t - tp)^2) / (2 * sigma^2)) -->\n<!-- # K1_rkhs <- function(s, t) sapply(t, function(ti) sum(s * K(t, ti)) * dt) -->\n<!-- # K2_rkhs <- function(s, t) { -->\n<!-- #   stft_mat <- stft(s, t, window = 50) -->\n<!-- #   apply(stft_mat, 2, function(col) col[which.max(abs(col))]) -->\n<!-- # } -->\n<!-- # K3_rkhs <- function(s, t) { -->\n<!-- #   scales <- c(1 / (2 * pi * 0.3), 1 / (2 * pi * 0.5))  # Scales for 0.3 Hz and 0.5 Hz -->\n<!-- #   wt <- manual_cwt(s, t, scales) -->\n<!-- #   dominant <- numeric(length(t)) -->\n<!-- #   for (j in 1:ncol(wt)) { -->\n<!-- #     col <- wt[, j] -->\n<!-- #     dominant[j] <- col[which.max(abs(col))] -->\n<!-- #   } -->\n<!-- #   dominant -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Phase estimation (Manual) -->\n<!-- # phi1_m <- Arg(K1_manual(s, t)) -->\n<!-- # phi2_m <- Arg(K2_manual(s, t)) -->\n<!-- # phi3_m <- Arg(K3_manual(s, t)) -->\n<!-- #  -->\n<!-- # # Time-varying weights (Manual) -->\n<!-- # window_size <- 20  # Smaller window for jump sensitivity -->\n<!-- # weights_m <- matrix(NA, 3, T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   start <- max(1, i - window_size/2) -->\n<!-- #   end <- min(T, i + window_size/2 - 1) -->\n<!-- #   vars <- c(var.circular(phi1_m[start:end]), var.circular(phi2_m[start:end]), var.circular(phi3_m[start:end])) -->\n<!-- #   weights_m[, i] <- 1 / vars; weights_m[, i] <- weights_m[, i] / sum(weights_m[, i]) -->\n<!-- # } -->\n<!-- # phi_hat_m <- numeric(T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   weighted_sum <- weights_m[1, i] * exp(1i * phi1_m[i]) + weights_m[2, i] * exp(1i * phi2_m[i]) + weights_m[3, i] * exp(1i * phi3_m[i]) -->\n<!-- #   if (abs(t[i] - 5) < 0.5) { -->\n<!-- #     prior <- exp(1i * (phi_true[i-1] + pi/2))  # Approximate jump -->\n<!-- #     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!-- #   } -->\n<!-- #   phi_hat_m[i] <- Arg(weighted_sum) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Phase estimation (RKHS) -->\n<!-- # phi1_r <- Arg(K1_rkhs(s, t)) -->\n<!-- # phi2_r <- Arg(K2_rkhs(s, t)) -->\n<!-- # phi3_r <- Arg(K3_rkhs(s, t)) -->\n<!-- #  -->\n<!-- # # Time-varying weights with Bayesian prior (RKHS) -->\n<!-- # weights_r <- matrix(NA, 3, T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   start <- max(1, i - window_size/2) -->\n<!-- #   end <- min(T, i + window_size/2 - 1) -->\n<!-- #   vars <- c(var.circular(phi1_r[start:end]), var.circular(phi2_r[start:end]), var.circular(phi3_r[start:end])) -->\n<!-- #   weights_r[, i] <- 1 / vars; weights_r[, i] <- weights_r[, i] / sum(weights_r[, i]) -->\n<!-- # } -->\n<!-- # phi_hat_r <- numeric(T) -->\n<!-- # for (i in 1:T) { -->\n<!-- #   weighted_sum <- weights_r[1, i] * exp(1i * phi1_r[i]) + weights_r[2, i] * exp(1i * phi2_r[i]) + weights_r[3, i] * exp(1i * phi3_r[i]) -->\n<!-- #   if (abs(t[i] - 5) < 0.5) { -->\n<!-- #     prior <- exp(1i * (phi_true[i-1] + pi/2)) -->\n<!-- #     weighted_sum <- 0.7 * weighted_sum + 0.3 * prior -->\n<!-- #   } -->\n<!-- #   phi_hat_r[i] <- Arg(weighted_sum) -->\n<!-- # } -->\n<!-- #  -->\n<!-- # # Plot comparison -->\n<!-- # df <- data.frame(t = t, phi_true = phi_true, phi_hat_m = phi_hat_m, phi_hat_r = phi_hat_r) -->\n<!-- # fig <- plot_ly(df) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_true, type = \"scatter\", mode = \"lines\", name = \"True\", line = list(color = \"black\")) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_hat_m, type = \"scatter\", mode = \"lines\", name = \"Manual\", line = list(color = \"blue\")) %>% -->\n<!-- #   add_trace(x = ~t, y = ~phi_hat_r, type = \"scatter\", mode = \"lines\", name = \"RKHS\", line = list(color = \"red\")) %>% -->\n<!-- #   layout(title = \"Kime Phase Estimation: Manual vs RKHS (Wrapped)\", yaxis = list(title = \"Phase (rad)\", range = c(-pi, pi))) -->\n<!-- # fig -->\n<!-- ``` -->\n\n<!-- The *true signal*, -->\n<!-- $\\phi(t) = \\begin{cases} 2\\pi \\cdot 0.3 t & t < 5 \\\\ 2\\pi \\cdot 0.5 t + \\pi/2 & t \\geq 5 \\end{cases}$, -->\n<!-- is wrapped to $[-\\pi, \\pi)$, with $\\sim 9$ total cycles, $3$ at $0.3 Hz$ -->\n<!-- and $12.5$ at $0.5 Hz$, but wrapped. The *manual Approach (3A)* uses raw -->\n<!-- noisy data (`s_manual = hilbert_manual_raw(x, t)`) for noise -->\n<!-- sensitivity, STFT $window = 83$ ($\\sim 3.33s$ for $0.3 Hz$, $\\sim 2s$ -->\n<!-- for $0.5 Hz$), and CWT scales for $0.3 Hz$ and $0.5 Hz$, where smaller -->\n<!-- smoothing window ($5$ points) preserves noise and sharpness. Finally, -->\n<!-- the *RKHS Approach (3A)* uses filtered data -->\n<!-- (`s_rkhs = hilbert_manual(x_filtered, t)`), wider kernel $\\sigma = 1$, -->\n<!-- $STFT\\ window = 100$ (smoother), and CWT scales for $0.3 Hz$ and -->\n<!-- $0.5 Hz$, where larger smoothing window ($11$ points) enhances -->\n<!-- regularization. The commutator $[K_1, K_2]$ is numerically estimated in -->\n<!-- RKHS, showing *non-commutativity*, which validates the *Approach 3A* -->\n<!-- framework. The *manual phase-recovery model* may be noisier and sharper, -->\n<!-- while the *RKHS model* may be is smoother and regularized, reflecting -->\n<!-- different filtering, windows, and weights (weaker vs. stronger jump -->\n<!-- penalties). -->\n\n<!-- The graph shows the *true phase (black)* oscillating within -->\n<!-- $[-\\pi, \\pi)$, with $0.3 Hz$ pre-jump ($\\sim 3$ cycles over $[0, 5]$), -->\n<!-- $0.5 Hz$ post-jump ($\\sim 12.5$ cycles over $[5, 30]$), and a $\\pi/2$ -->\n<!-- jump at $t = 5$. The *manual Approach 3A (Blue)* phase-recovery model -->\n<!-- shows $4–5$ cycles pre-jump, $\\sim 13$ cycles post-jump, noisier with -->\n<!-- sharper jumps, reflecting raw data variability. While the  -->\n<!-- *RKHS Approach 3A (Red)* model shows smoother curves, $4–5$ cycles pre-jump, -->\n<!-- $\\sim 13$ cycles post-jump, regularized by the kernel, and somewhat -->\n<!-- distinct from the *manual Approach 3A*. -->\n\n## Approach 4: A Unified Framework for Kime-Phase Tomography (KPT)\n\nCapitalizing on the strengths and advantages of the prior *Approaches 1A, 2A*, and\n*3A*, *Approach 4* proposes a *Unified Framework for Kime-Phase Tomography (KPT)*.\n\n### Overview\n\nThe Kime-Phase Tomography (KPT) framework relies on the Hilbert spaces for time and \nphase (as $\\mathcal{H}_t$ and $\\mathcal{H}_\\theta$) and the combined kime space \n$\\mathcal{K} = \\mathcal{H}_t \\otimes \\mathcal{H}_\\theta$ to define the central \n*kime-operators* (time-domain, frequency-domain, scale-domain, phase-domain, and RKHS projection).\nEach of the primary operators $\\{K_1,K_2,K_3\\}$ is specified in functional-analytic terms,\nwith their action on signals. The wavelet operator $K_3$ is defined by a proper \nintegral transform and the frequency operator $K_2 = -\\,i \\,\\tfrac{d}{dt}$ \nties to the commutation relation with $K_1$.\nThe kime-commutator calculations (e.g.\\,[$K_1,K_2$] $=\\, i\\,\\mathcal{I}$) parallel\nthe standard QM derivations for time-frequency operators, and the extension \nto the wavelet (scale) operator preserves the same notion of “complementary” \nphase information across multiple bases.\nUsing the completeness of Fourier series on $[- \\pi,\\pi]$, we argue that \n*sufficiently many trigonometric moments uniquely determine the phase PDF*. \nThis links multi-basis measurements and *identifiability* of $\\Phi(\\theta; t)$.\nThe RKHS projections represent $\\phi(t)$ via $\\arg(\\mathcal{P}_K[s](t))$, which\nis consistent with standard reproducing-kernel arguments where the kernel localizes\nthe signal and captures amplitude-phase structure.\n\nOur numerical simulations involve the special case of *von Mises-distributed phases*\nand simulated fMRI signals. A synthetic neural phase signal $\\theta(t)$ is sampled \nfrom a *von Mises* distribution \n$\\mathrm{vM}\\left (\\overbrace{\\mu(t)}^{location}, \\overbrace{\\kappa(t)}^{concentration}\\right)$.  \nIn the simulation, we convolve the neural-phase-driven “complex time” (kime) signal\nwith a canonical or double-$\\gamma$ HRF, to simulate realistic BOLD response, and adds noise.\nThe *primary operators* include the *time-domain operator*, $K_1[s](t) = t \\cdot s(t)$,\nthe *frequency-domain operator*, $K_2[s](t) = -\\,i\\,\\frac{d}{dt} s(t)$ approximated \nby finite-difference derivatives, and the *scale-domain operator*, $K_3[s](t)$,\nvia the continuous wavelet transform, utilizing a Morlet wavelet to locate the dominant scale.\nThe *RKHS projection* is done before these operators for smoothing, as shown in Theorem 3.\n\nFor each operator $K_j$, we use the repeated measurements (across multiple simulated runs/subjects)\nto estimate the phase using hte complex-argument function, $\\phi_{j,n}(t) = \\arg\\bigl(K_j[s_n](t)\\bigr)$.  \nThen, we obtain an ensemble phase-estimate as an uncertainty-weighted combination of \nthese phase estimates across the non-commuting bases $\\{K_1, K_2, K_3\\}$ to estimate the\noverall circular moment $\\bar{m}(t)$.  \nThen, a *von Mises* parameter solver recovers $\\hat{\\mu}(t)$ and $\\hat{\\kappa}(t)$ \nby matching the ratio $I_{1}(\\kappa)/I_{0}(\\kappa)$ to $|\\bar{m}(t)|$. \nThis follows the moment-based derivation in Theorem 4.\n\nFor verification, we compute the circular MAE for phase and RMSE for the concentration\nby comparing the recovered parameters $\\hat{\\mu}(t), \\hat{\\kappa}(t)$ against the \nknown ground truth. Empirical results should confirm that the multi-basis combination\nrecovers $\\mu(t)$ and $\\kappa(t)$ to within reasonable accuracy.\nThis von Mises phase prior experiment *demonstrates* the more general\nprinciple that non-commuting operators capture complementary phase information.\n\nSome *potential limitations* of the empirical validation include the reliance on \nvon Mises parametric form, as the algorithmic performance may degrade for multi-modal \nor heavy-tailed distributions. Also the data requirements of $N > 200$ trials for \nstable recovery may be impractical for real fMRI where a typical repeats\nmay be in the range of $N\\in[10,30]$. Finally, the choice of the RKHS kernel (Gaussian)\nand bandwidth may not be optimal and could affect the resulting kime-phase recovery.\n\n### Mathematical KPT Framework\n\n**Definition 1** (*Kime-Domain Signal Space*). Let $\\mathcal{H}_t = L^2(\\mathbb{R})$\nbe the Hilbert space of square-integrable complex-valued functions on the time domain,\nwith inner product:\n$$\\langle f, g \\rangle_{\\mathcal{H}_t} = \\int_{\\mathbb{R}} f(t) \\overline{g(t)} \\, dt$$\n\n**Definition 2** (*Phase-Domain Space*). Let $\\mathcal{H}_\\theta = L^2([-\\pi, \\pi])$\nbe the Hilbert space of square-integrable functions on the phase domain, with inner product\n$$\\langle \\psi, \\phi \\rangle_{\\mathcal{H}_\\theta} = \\int_{-\\pi}^{\\pi} \\psi(\\theta) \\overline{\\phi(\\theta)} \\, d\\theta$$\nequipped with periodic boundary conditions $\\psi(-\\pi) = \\psi(\\pi)$.\n\n**Definition 3** (*Kime Space*). The kime space $\\mathcal{K}$ is defined as the \ntensor product $\\mathcal{H}_t \\otimes \\mathcal{H}_\\theta$, representing signals \nin both time and phase domains.\n\n**Definition 4** (*Reproducing Kernel Hilbert Space, RKHS*). The RKHS $\\mathcal{R}_K$\nis a subspace of $\\mathcal{H}_t$ with reproducing kernel \n$K: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{C}$ satisfying\n\n - For any $t \\in \\mathbb{R}$, $K(\\cdot, t) \\in \\mathcal{R}_K$, and \n - For any $f \\in \\mathcal{R}_K$ and $t \\in \\mathbb{R}$, $f(t) = \\langle f, K(\\cdot, t) \\rangle_{\\mathcal{R}_K}$\n\n**Definition 5** (*Kime-Phase Distribution*). A kime-phase distribution $\\Phi(\\theta; t)$\nis a time-dependent probability density function on $[-\\pi, \\pi]$ satisfying\n$$\\Phi(\\theta; t) \\geq 0, \\quad \\int_{-\\pi}^{\\pi} \\Phi(\\theta; t) \\, d\\theta = 1 \\quad \\forall t \\in \\mathbb{R}$$\n\n**Definition 6** (*Complex Kime*). For each time $t$, the complex kime is defined \nas $\\kappa(t) = t e^{i\\theta(t)}$, where $\\theta(t) \\sim \\Phi(\\cdot; t)$.\n\n**Definition 7** (*Primary Kime Operators*). The primary kime operators include\n\n - *Time-domain operator*: $K_1: \\mathcal{H}_t \\rightarrow \\mathcal{H}_t$ defined by $K_1[s](t) = t \\cdot s(t)$.\n\n - *Frequency-Domain Operator*: $K_2: \\mathcal{H}_t \\rightarrow \\mathcal{H}_t$ defined by $K_2[s](t) = -i \\frac{d}{dt}s(t)$, and \n\n - *Scale-Domain Operator*: For a mother wavelet $\\psi \\in \\mathcal{H}_t$, let $W_\\psi[s](a,b) = \\langle s, \\psi_{a,b} \\rangle_{\\mathcal{H}_t}$ be the continuous wavelet transform with $\\psi_{a,b}(t) = \\frac{1}{\\sqrt{a}}\\psi\\left(\\frac{t-b}{a}\\right)$. Then, the scale-domain operator $K_3: \\mathcal{H}_t \\rightarrow \\mathcal{H}_t$ is \n \n$$K_3[s](t) = \\int_{\\mathbb{R}} \\int_{\\mathbb{R}_+} W_\\psi[s](a,b) \\, \\frac{1}{\\sqrt{a}} \\psi\\left(\\frac{t-b}{a}\\right) \\, \\frac{da \\, db}{a^2}.$$\n\n - *Phase-Domain Operators*: In $\\mathcal{H}_\\theta$, we can define a pair of phase-domain operators:\n   \n    - *Position operator*: $\\Theta[\\phi](\\theta) = \\theta \\cdot \\phi(\\theta)$, and \n    - *Momentum operator*: $P[\\phi](\\theta) = -i\\frac{d}{d\\theta}\\phi(\\theta)$.\n\n - *RKHS Projection Operator*: Given a kernel $K$, the RKHS operator $\\mathcal{P}_K: \\mathcal{H}_t \\rightarrow \\mathcal{R}_K$ is defined by\n\n$$\\mathcal{P}_K[s](t) = \\int_{\\mathbb{R}} s(\\tau) K(t, \\tau) \\, d\\tau .$$\n\n**Definition 8** (Observable Signal). An observable kime-signal $s(t)$ with amplitude \n$A(t)$ and phase $\\phi(t)$ is defined as $s(t) = A(t)e^{i\\phi(t)}$, where $\\phi(t)$ \nis sampled from distribution $\\Phi(\\cdot; t)$.\n\n**Definition 9** (fMRI BOLD Signal Model). In fMRI, the observed BOLD signal $x(t)$ \ncan be modeled as $x(t) = \\int_{\\mathbb{R}} h(t-\\tau) s(\\tau) \\, d\\tau + \\epsilon(t)$,\nwhere $h(t)$ is the *hemodynamic response function* and $\\epsilon(t)$ is *noise.*\n\nHaving these basic definitions, we will explore some of the mathematical results\nthat underpin the kime-operator framework for kime-phase recovery using repeated measurement\nobservations of a controlled experiment, such as repeated fMRI runs (within and across participants)\nin an event-related block design.\n\n**Theorem 1** (*Time-Frequency Commutation*). The operators $K_1$ (time-domain operator) \nand $K_2$ (frequency-domain operator) are *incompatible*,\ni.e., they have a non-trivial commutator, $[K_1, K_2] = K_1K_2 - K_2K_1 = i\\mathcal{I}$,\nwhere $\\mathcal{I}$ is the identity operator on $\\mathcal{H}_t$. This indicates that\nthe phase-reconstructions corresponding to this pair of kime-operators differentially probe\nthe kime-phase and jointly, they recover complementary phase information.\n\n*Proof:* For any $s \\in \\mathcal{H}_t$\n$$\\begin{align}\nK_1K_2[s](t) &= t \\cdot \\left (-i\\frac{d}{dt}s(t)\\right ) = -it\\frac{ds}{dt}\\\\\nK_2K_1[s](t) &= -i\\frac{d}{dt}(ts(t)) = -is(t) - it\\frac{ds}{dt}\n\\end{align} .$$\n\nTherefore,\n$$\\begin{align}\n[K_1, K_2][s](t) &= K_1K_2[s](t) - K_2K_1[s](t)\\\\\n&= -it\\frac{ds}{dt} - \\left(-is(t) - it\\frac{ds}{dt}\\right)= is(t)= i\\mathcal{I}[s](t) .\n\\end{align}$$\nThus, $[K_1, K_2] = i\\mathcal{I}$. $\\square$\n\n**Theorem 2** (*Uncertainty Relation*). Given a signal $s \\in \\mathcal{H}_t$\n\n$$\\Delta K_1 \\cdot \\Delta K_2 \\geq \\frac{1}{2}|\\langle [K_1, K_2] \\rangle| = \\frac{1}{2},$$\nwhere $\\Delta K_j = \\sqrt{\\langle K_j^2 \\rangle - \\langle K_j \\rangle^2}$ for $j=1,2$\nand expectations are with respect to $s$.\n\n*Proof:* Assuming $\\|s\\|=1$ without loss of generality. Using the Cauchy-Schwarz \ninequality and the commutation relation from *Theorem 1*,\n$$\\Delta K_1 \\cdot \\Delta K_2 \\geq \\frac{1}{2}|\\langle [K_1, K_2] \\rangle|\n= \\frac{1}{2}|\\langle i\\mathcal{I} \\rangle|= \\frac{1}{2}|i\\langle s, s \\rangle|= \n\\frac{1}{2}|i| \\cdot \\|s\\|^2 = \\frac{1}{2} \\cdot 1 \\cdot 1= \\frac{1}{2}.\\ \\square$$\n\n**Theorem 3** (*RKHS Representation*). Given a signal $s \\in \\mathcal{H}_t$ and \na reproducing kernel $K$, the phase function $\\phi(t)$ can be represented as the\n[complex argument](https://en.wikipedia.org/wiki/Argument_(complex_analysis)), \n$\\arg()$, of the RKHS projection operator, \n$\\mathcal{P}_K : \\mathcal{H}_t \\to \\mathbb{C} \\ni \\mathcal{P}_K[s](t)$, i.e.,\n$\\phi(t) = \\arg(\\mathcal{P}_K[s](t))$.\n\n*Proof:* Let $s(t) = A(t)e^{i\\phi(t)}$. Then,\n$$\\mathcal{P}_K[s](t) = \\int_{\\mathbb{R}} s(\\tau) K(t, \\tau) \\, d\\tau\n= \\int_{\\mathbb{R}} A(\\tau)e^{i\\phi(\\tau)} K(t, \\tau) \\, d\\tau .$$\n\nBy the reproducing property and assuming a sufficiently localized kernel\n\n$$\\mathcal{P}_K[s](t) \\approx A(t)e^{i\\phi(t)} \\int_{\\mathbb{R}} K(t, \\tau) \\, d\\tau\n= C \\cdot A(t)e^{i\\phi(t)},$$\nwhere $C$ is a non-zero constant. Therefore,\n\n$$\\arg(\\mathcal{P}_K[s](t)) = \\arg(C \\cdot A(t)e^{i\\phi(t)}) = \\arg(e^{i\\phi(t)})\n= \\phi(t).\\ \\square$$\n\n**Theorem 4** (*Phase Recovery from Multiple Bases*). Given repeated observations \nin multiple non-commuting bases defined by operators $K_1, K_2, K_3$, the kime-phase\ndistribution $\\Phi(\\theta; t)$ can be *uniquely determined* if the observations are sufficient.\n\n*Proof:* Let $\\phi_j(t) = \\arg(K_j[s](t))$ for $j=1,2,3$ be the phase observations\nin each basis and consider *von Mises distribution* as an example,\n$\\Phi(\\theta; t) = \\text{vM}(\\mu(t), \\kappa(t))$. Then, the first circular moment\ndetermines $\\mu(t)$ and $\\kappa(t)$\n\n$$\\mathbb{E}[e^{i\\theta}] = \\int_{-\\pi}^{\\pi} e^{i\\theta} \\Phi(\\theta; t) \\, d\\theta = e^{i\\mu(t)} \\frac{I_1(\\kappa(t))}{I_0(\\kappa(t))}.$$\n\nSince we have non-commuting observations\n$$\\mu(t) = \\arg\\left(\\sum_{j=1}^3 w_j(t) e^{i\\phi_j(t)}\\right),$$\nwhere $w_j(t)$ are weights inversely proportional to variance.\n\nThe concentration parameter $\\kappa(t)$ is determined by solving the system\n$$\\frac{I_1(\\kappa(t))}{I_0(\\kappa(t))} = \\left|\\sum_{j=1}^3 w_j(t) e^{i\\phi_j(t)}\\right|,$$\nwhich uniquely determines $\\Phi(\\theta; t)$ when the observations provide *sufficient information*\nacross bases. $\\square$\n\nThe above *Theorem 4* (Phase Recovery from Multiple Bases) makes a strong\nassumption about von Mises phase distribution, which is restrictive, but the\nresult can be generalized.\n\n**Theorem 5** (*Generalized Phase Recovery from Multiple Bases*). Given\na kime-phase distribution $\\Phi(\\theta; t)$ and assuming sufficient observations in\nmultiple non-commuting bases defined by operators $K_1, K_2, K_3$, the phase\ndistribution can be uniquely determined under certain uniqueness conditions\n\n 1. *Trigonometric Moment Identifiability*: A circular distribution is uniquely determined by its complete set of trigonometric moments $\\{\\alpha_k, \\beta_k\\}_{k=1}^{\\infty}$ where $\\alpha_k = \\mathbb{E}[\\cos(k\\theta)]$ and $\\beta_k = \\mathbb{E}[\\sin(k\\theta)]$,\n\n 2. *Information Complementarity*: The operators $K_1, K_2, K_3$ must provide complementary information about different moments of the phase distribution, and \n\n 3. *Sufficiency Condition*: The observations must constrain enough trigonometric moments to uniquely specify $\\Phi(\\theta; t)$ within the class of distributions being considered.\n\n**Proof**: The proof proceeds in the following steps.\n\nStep 1: reframe the problem in terms of the moment problem. Let \n$\\mathcal{P}([-\\pi, \\pi])$ be the space of probability distributions on \n$[-\\pi, \\pi]$. For any $\\Phi \\in \\mathcal{P}([-\\pi, \\pi])$, define its trigonometric moments:\n\n$$m_k(\\Phi) = \\int_{-\\pi}^{\\pi} e^{ik\\theta} \\Phi(\\theta) d\\theta, \\quad k \\in \\mathbb{Z}.$$\n\nNote that $m_0(\\Phi) = 1$ (normalization), $m_{-k}(\\Phi) = \\overline{m_k(\\Phi)}$ \n(conjugate symmetry), and $|m_k(\\Phi)| \\leq 1$ (bounded).\n\nStep 2: To establish the connection between moments and distribution uniqueness \nrequires the following lemma, which states that distributions are completely described\nby their moments, see the note on MGF's at the end of this section.\n\n**Lemma 1:** Two distributions $\\Phi_1, \\Phi_2 \\in \\mathcal{P}([-\\pi, \\pi])$ are \nidentical if and only if $m_k(\\Phi_1) = m_k(\\Phi_2)$ for all $k \\in \\mathbb{Z}$.\n\n*Proof of Lemma 1:* This follows directly from the uniqueness of Fourier series \nrepresentation. If $\\Phi_1 \\neq \\Phi_2$, then their difference $\\Phi_1 - \\Phi_2$ has\na non-zero Fourier coefficient, meaning some moment $m_k(\\Phi_1) \\neq m_k(\\Phi_2)$. \nConversely, if all moments match, the distributions must be identical almost everywhere.\n\nStep 3: Next we connect operator measurements to moment estimation. \nFor each operator $K_j$, we observe phases $\\{\\phi_{j,n}(t)\\}_{n=1}^N$ across $N$ \nrepeated measurements at time $t$. These provide empirical estimates of certain \nfunctions of the moments\n\n$$\\hat{m}_{j,k}(t) = \\frac{1}{N}\\sum_{n=1}^N e^{ik\\phi_{j,n}(t)}$$\n\nHowever, due to the non-commutativity of the operators, these empirical moments capture\ndifferent aspects of the underlying distribution $\\Phi(\\theta; t)$, which requires another lemma.\n\n**Lemma 2:** Under the action of operator $K_j$, the expected value of $\\hat{m}_{j,k}(t)$ \nis a function $F_j$ of the true moments $\\{m_l(\\Phi)\\}_{l \\in \\mathbb{Z}}$\n\n$$\\mathbb{E}[\\hat{m}_{j,k}(t)] = F_j(k, \\{m_l(\\Phi)\\}_{l \\in \\mathbb{Z}}),$$\nwhere $F_j$ is a continuous function that depends on the specific operator $K_j$.\n\n*Proof of Lemma 2:* Each operator $K_j$ transforms the input signal according to \nits action, resulting in a phase that depends on the underlying distribution in a \nspecific way. The exact form of $F_j$ depends on the operator definition, but \nit is a continuous function of the moments due to the continuous nature of the operators.\n\nStep 4: To show that non-commuting operators provide complementary information we need a third lemma.\n\n**Lemma 3:** If operators $K_i$ and $K_j$ do not commute (i.e., $[K_i, K_j] \\neq 0$), \nthen there exist moments $m_l(\\Phi)$ such that\n\n$$\\frac{\\partial F_i(k, \\{m_l(\\Phi)\\})}{\\partial m_l(\\Phi)} \\neq \\frac{\\partial F_j(k, \\{m_l(\\Phi)\\})}{\\partial m_l(\\Phi)}.$$\nIn other words, the operators have different sensitivities to certain moments of the distribution.\n\n*Proof of Lemma 3:* This follows from the definition of non-commutativity. \nIf two operators commute, they share eigenfunctions and can be simultaneously diagonalized.\nNon-commuting operators have different eigenbases, which means they respond differently\nto certain input patterns—specifically, they have different sensitivities to certain\nfrequency components or moments of the input distribution. \n\nStep 5: To establish the conditions for unique determination, let's define the \n*total information* from all operators as the set\n\n$$\\mathcal{I}(\\Phi) = \\{F_j(k, \\{m_l(\\Phi)\\}) : j \\in \\{1,2,3\\}, k \\in \\mathbb{Z}\\}.$$\nThe theorem's main result is that the distribution $\\Phi(\\theta; t)$ is uniquely \ndetermined by the observations *if and only if* the mapping $\\Phi \\mapsto \\mathcal{I}(\\Phi)$ \nis injective on the space of distributions being considered (a *necessary and sufficient*\ncondition). Let's show both directions.\n\n 1. ($\\Longrightarrow$) If $\\Phi \\mapsto \\mathcal{I}(\\Phi)$ is injective, then different distributions produce different observable patterns, allowing unique identification.\n\n 2. ($\\Longleftarrow$) Conversely, suppose $\\Phi \\mapsto \\mathcal{I}(\\Phi)$ is not injective. Then there exist distinct distributions $\\Phi_1 \\neq \\Phi_2$ such that $\\mathcal{I}(\\Phi_1) = \\mathcal{I}(\\Phi_2)$. This means all observations would be identical, making it impossible to distinguish between $\\Phi_1$ and $\\Phi_2$.\n\nStep 6: Let's specialize to practical cases. For parametric families with \nfinite-dimensional parameterization (e.g., von Mises or mixtures thereof), \nthe injectivity condition reduces to a more tractable form shown in the following\ntwo corrolaries.\n\n**Corollary 1:** For a parametric family $\\{\\Phi_\\theta : \\theta \\in \\Theta \\subset \\mathbb{R}^d\\}$\nwhere $\\Theta$ is a $d$-dimensional parameter space, unique determination is possible \nif the Jacobian matrix\n\n$$J(\\theta) = \\left[ \\frac{\\partial}{\\partial \\theta_i} F_j(k, \\{m_l(\\Phi_\\theta)\\}) \\right]_{(j,k),i}$$\nhas full column rank for all $\\theta \\in \\Theta$.\n\nThis is a direct result of the inverse function theorem from multivariable \ncalculus. If the Jacobian has full column rank, the mapping is locally injective\naround each point in the parameter space. If this holds globally, the mapping is \nglobally injective.\n\n**Corollary 2:** For a von Mises distribution $\\text{vM}(\\mu, \\kappa)$, \nobservations from operators $K_1$ and $K_2$ are sufficient for unique determination\nif they provide independent constraints on the first trigonometric moment \n$m_1 = e^{i\\mu}\\frac{I_1(\\kappa)}{I_0(\\kappa)}$.\n\nThe von Mises distribution is completely determined by its first trigonometric moment $m_1$. If the operators $K_1$ and $K_2$ provide independent constraints on $m_1$ (which follows from their non-commutativity), the parameters $\\mu$ and $\\kappa$ can be uniquely determined.\n\nTherefore, the *generalized theorem* establishes *necessary* and *sufficient* conditions\nfor unique phase distribution determination using multiple non-commuting operators. \nThe key insight is that non-commuting operators provide complementary views of the \nunderlying distribution, helping constrain it more effectively than any single operator\ncould. The degree to which uniqueness is possible depends on both the complexity of\nthe true distribution and the information content of the operators used. $\\square$\n\n**Notes**: Due to the non-commutativity of $K_1, K_2, K_3$, these operators capture different \naspects of the phase distribution\n\n - $K_1$ (time-domain): Sensitive to localized phase concentrations\n - $K_2$ (frequency-domain): Sensitive to phase transitions\n - $K_3$ (scale-domain): Sensitive to multi-scale phase patterns.\n\nAssuming sufficient diversity of measurements, the combined information constrains\nthe characteristic function $\\varphi(k)$ enough to uniquely determine $\\Phi(\\theta; t)$.\nLet's consider some specific families of phase distributions.\n\n 1. **Parametric Families**: For distributions with finite-dimensional parameterizations (like von Mises, wrapped Cauchy, wrapped normal), we only need to constrain finitely many moments to uniquely determine the distribution.\n\n 2. **Mixtures of Parametric Distributions**: For mixtures of $M$ parametric components, uniqueness requires constraining $2M-1$ complex moments $\\{\\varphi(1), \\varphi(2), ..., \\varphi(2M-1)\\}$.\n\n 3. **Non-parametric Distributions**: Any distribution can be approximated arbitrarily well by constraining a sufficiently large number of trigonometric moments, with the approximation error decreasing as more moments are constrained.\n\nThe theorem is practically useful even without knowing the true distribution family, \nas the multi-basis approach constrains more trigonometric moments than single-basis approaches.\nFor most neurophysiological signals, the phase distribution is well-approximated by \na low-order mixture model (typically 1-3 components), which requires only a small \nnumber of moments to constrain.\nThe non-commuting bases provide complementary information about different moments, \nimproving identification even with finite data.\n\nThis generalized theorem provides a broader foundation for KPT, allowing application\nto a wider range of phase distributions beyond von Mises. For practical applications, \nwe can start with the von Mises assumption (which often works well) but can extend \nto mixture models or non-parametric approaches when data suggests more complex distributions.\n\n\n### The Moment Problem in Circular Distributions\n\nThe question of whether a distribution is uniquely determined by its moments is known\nas the \"*moment problem*\" in probability theory. For circular distributions on \n$[-\\pi, \\pi]$, we need to consider the trigonometric moments rather than ordinary moments.\nFor a circular distribution $\\Phi(\\theta)$ on $[-\\pi, \\pi]$, the trigonometric moments are\n\n$$m_k = \\mathbb{E}[e^{ik\\theta}] = \\int_{-\\pi}^{\\pi} e^{ik\\theta} \\Phi(\\theta) d\\theta = \\alpha_k + i\\beta_k,$$\nwhere $\\alpha_k = \\mathbb{E}[\\cos(k\\theta)]$ and $\\beta_k = \\mathbb{E}[\\sin(k\\theta)]$.\n\nUnlike ordinary moments on the real line, which can sometimes fail to characterize\na distribution uniquely (the famous \"moment problem\"), trigonometric moments on a \nbounded circular domain always uniquely determine the distribution, due to the \ncompleteness of the trigonometric functions.\nThe set $\\{e^{ik\\theta}\\}_{k \\in \\mathbb{Z}}$ forms a *complete orthogonal basis* for\n$L^2[-\\pi, \\pi]$. By the Fourier theory, any square-integrable function on $[-\\pi, \\pi]$\ncan be expressed as $f(\\theta) = \\sum_{k=-\\infty}^{\\infty} c_k e^{ik\\theta}$,\nwhere $c_k$ are the Fourier coefficients.\n\nFor a probability density $\\Phi(\\theta)$, these coefficients are precisely related \nto the trigonometric moments, $c_k = \\frac{1}{2\\pi} \\overline{m_k}$.\nThus, knowing all trigonometric moments $\\{m_k\\}_{k \\in \\mathbb{Z}}$ is equivalent \nto knowing the exact Fourier series of $\\Phi(\\theta)$, which uniquely determines \nthe distribution. In the circular setting, the *characteristic function* serves the\nrole that the MGF does for distributions on the real line.\nThe characteristic function of a circular distribution is\n$$\\varphi(t) = \\mathbb{E}[e^{it\\theta}] = \\int_{-\\pi}^{\\pi} e^{it\\theta} \\Phi(\\theta) d\\theta .$$\n\nThis is precisely the sequence of trigonometric moments when evaluated at integer values\n$\\varphi(k) = m_k, \\quad \\text{for } k \\in \\mathbb{Z}$.\nThe characteristic function directly encodes all trigonometric moments and has these \nimportant properties\n\n 1. **Uniqueness**: If two distributions have the same characteristic function, they are identical.\n\n 2. **Inversion Formula**: The distribution can be recovered from its characteristic function using the inversion formula\n\n$$\\Phi(\\theta) = \\frac{1}{2\\pi} \\sum_{k=-\\infty}^{\\infty} \\overline{m_k} e^{ik\\theta}.$$\n\n 3. **Convolutions**: The characteristic function of a sum of independent random variables is the product of their characteristic functions.\n\n\n### Computational Algorithm 1: Unified Kime-Phase Tomography\n\nLet's explicate the *Unified Kime-Phase Tomography* algorithm in the special case\nof using *von Mises phase prior distribution*.\n\n**Input:** BOLD time series $\\{x_n(t)\\}_{n=1}^N$ from $N$ repeated measurements.\n\n**Output:** Estimated kime-phase distribution $\\hat{\\Phi}(\\theta; t)$.\n\n 1. **Preprocessing:** First apply bandpass filtering to each $x_n(t)$ to remove noise and then compute analytic signal via Hilbert transform, $s_n(t) = x_n(t) + i\\mathcal{H}[x_n(t)]$.\n   \n 2. **Multi-basis Measurement:** Obtain the three phase recovery estimates using each o fhte kime-operators (in each of the 3 bases).\n \n   - *Time-domain*: $\\phi_{1,n}(t) = \\arg(K_1[s_n](t))$,\n   - *Frequency-domain*: $\\phi_{2,n}(t) = \\arg(K_2[s_n](t))$, and\n   - *Scale-domain*: $\\phi_{3,n}(t) = \\arg(K_3[s_n](t))$.\n   \n 3. **Basis Uncertainty Quantification:** Compute the circular variance in each basis $V_j(t) = 1 - \\left|\\frac{1}{N}\\sum_{n=1}^N e^{i\\phi_{j,n}(t)}\\right|,\\ j\\in\\{1,2,3\\}$ and estimate the corresponding weights, $w_j(t) = \\frac{1/V_j(t)}{\\sum_{k=1}^3 1/V_k(t)}$.\n   \n 4. **Phase Distribution Estimation:** To obtain an *ensemble* phase reconstruction, we first compute the *raw moment* $m_j(t) = \\frac{1}{N}\\sum_{n=1}^N e^{i\\phi_{j,n}(t)}$ and then estimate the *weighted moment* $\\bar{m}(t) = \\sum_{j=1}^3 w_j(t) m_j(t)$. With these, we can approximate the von Mises parameters, $\\hat{\\mu}(t) = \\arg(\\bar{m}(t))$ (*mean* or *location*) and $\\hat{\\kappa}(t)$ (*concentration*) by solving $\\frac{I_1(\\hat{\\kappa}(t))}{I_0(\\hat{\\kappa}(t))} = |\\bar{m}(t)|$.\n   \n 5. **Return** The result of this algorithm is an estimated phase recovery distribution $\\hat{\\Phi}(\\theta; t) = \\text{vM}(\\hat{\\mu}(t), \\hat{\\kappa}(t))$.\n\n### Algorithm 2: Robust Phase Jump Detection\n\nTo handle more intricate kime-phase prior distributions, such as mixtures of various \ndistributions, we need to extend and modify *Alforithm 1* as follows.\n\n**Input:** Estimated phase series $\\hat{\\phi}(t)$.\n\n**Output:** Detected phase jumps $\\{(t_k, \\Delta\\phi_k)\\}$.\n\n 1. Compute phase derivative: $\\dot{\\phi}(t) = \\frac{d\\hat{\\phi}(t)}{dt}$\n 2. Compute median absolute deviation: $\\text{MAD} = \\text{median}(|\\dot{\\phi}(t) - \\text{median}(\\dot{\\phi}(t))|)$\n 3. Set threshold: $\\tau = \\text{median}(\\dot{\\phi}(t)) + 5 \\cdot \\text{MAD}$\n 4. For each time $t$, where $|\\dot{\\phi}(t)| > \\tau$, record the jump, $(t, \\hat{\\phi}(t+\\delta) - \\hat{\\phi}(t-\\delta))$, where $\\delta$ is a small time window\n 5. Merge consecutive jumps within a threshold window\n 6. Return list of jumps $\\{(t_k, \\Delta\\phi_k)\\}$.\n\n### Algorithm 3: RKHS-Based Phase Recovery\n\nTo capitalize on *Approach 3A* (above), we can also refine the algorithm to allow for RKHS\nkernel estimation. This RKHS-based algorithm is more refined implementation since\nit directly incorporates the kernel-based smoothing and projection emphasized in \nthe kime operator-theoretic framework. The earlier Algorithm 1 is valid, but \nit represents essentially a simpler “baseline” version (no RKHS).  \nThe recovered phase $\\phi_{1}^{K}(t)=\\arg\\bigl(t \\cdot s_n^K(t)\\bigr)$, is consistent\nwith keeping phases in $\\bigl[-\\pi,\\pi\\bigr)$, since the complex-argument function\n$\\arg(\\cdot)$ returns the principal argument in that exact interval.\n\nNote that *Algorithm 2* may be more accurate than *Algorithm 1*, since it adds the \nextra step $s_n^K(t) \\;=\\;\\mathcal{P}_K\\bigl[s_n\\bigr](t)$ to project each \nsubject’s/time series signal into the chosen reproducing-kernel Hilbert space (RKHS). \nThis is a smoothing or *regularization* necessary for real fMRI data where\nthe noise level is significant, e.g., $SNR < 0.04$.  \nTheorem 3 shows that $\\phi(t)\\approx\\arg\\bigl(\\mathcal{P}_K[s](t)\\bigr)$ where\nthe kernel projection represents the mathematical device to appropriately\nlocalize the signal and reduce the noise.  \nWhile, *Algorithm 1* uses the same multi-basis logic—time, frequency, scale operators,\nit omits the kernel-based projection (regularization) step. The simpler \n*Algorithm 1* may still work well in synthetic or high-SNR settings, it may be\nsuboptimal for noisy signals needing more advanced smoothing. \nThis is reflected in teh following  *Algorithm 3* included below.\n\n**Input:** BOLD time series $\\{x_n(t)\\}_{n=1}^N$ and kernel $K$.\n\n**Output:** Estimated phase $\\hat{\\phi}(t)$.\n\n 1. Project each signal to RKHS: $s_n^K(t) = \\mathcal{P}_K[s_n](t)$\n 2. Compute time-domain phase: $\\phi_1^K(t) = \\arg(t \\cdot s_n^K(t))$\n 3. Compute frequency-domain phase using STFT, $\\phi_2^K(t) = \\arg(\\mathcal{F}[s_n^K \\cdot w](t, \\omega_{\\text{max}}))$, where $\\omega_{\\text{max}}$ is the frequency with maximum power\n 4. Compute scale-domain phase using CWT: $\\phi_3^K(t) = \\arg(W_\\psi[s_n^K](a_{\\text{max}}, t))$, where $a_{\\text{max}}$ is the scale with maximum power\n 5. Compute weighted average: $\\hat{\\phi}(t) = \\arg\\left(\\sum_{j=1}^3 w_j(t) e^{i\\phi_j^K(t)}\\right)$\n 6. Return $\\hat{\\phi}(t)$.\n\nNote that the (simulated) true phases should be wrapped to $[-\\pi, \\pi)$ to match\nthe RKHS-recovered estimates. The recovered phases, e.g., `phi_K1`, `phi_K2`, `phi_K3`,\nuse complex-argument function $\\arg()$ to restrict the phase values to $[-\\pi, \\pi)$,\ne.g., `phi_K1[n,] <- Arg(K1_operator(s_rkhs, t))`.\nIn our experiment, the true phase is generated via `mu_func = function(t) 2*pi*0.1*t`, \nwhich exceeds \\([-\\pi, \\pi)\\) as \\(t\\) grows. However, `rvonmises` internally wraps\nthe mean parameter `mu` to \\([-\\pi, \\pi)\\), so the *sampled phases* are correct. \nThe stored `true_mean` (ground truth) needs to also be wrapped to avoid a mismatch \nwhen comparing true phases to their corresponding estimates.\n\n\n### Algorithm 4: Brain network fMRI Analytics\n\nLet's implement a complete example of *Kime-Phase Tomography (KPT)* applied to \n*synthetic fMRI data* with non-trivial properties, including phase jumps and \ntime-varying dynamics. We demonstrate how the theoretical \nfoundations translate into practical algorithms for recovering phase information \nfrom complex time-dependent signals.\nThe experimental design involves analyzing synthetic fMRI BOLD signals with known \nground truth to validate our methodology. The data incorporates realistic features of\nneuroimaging data\n\n - Multiple subjects/repetitions ($N=15$)\n - Hemodynamic response function convolution\n - Physiological noise\n - Time-varying phase dynamics\n - Abrupt phase transitions (simulating state changes).\n\nTo complete this *unified Kime-Phase Tomography framework*,  we will\nalso consider a more *advanced and comprehensive example* that demonstrates the \napplication to real-world fMRI data analysis problems.\nThis experiment creates a visualization of synthetically generated fMRI data \nfrom multiple brain regions with known coupling patterns (ROI network interactions). \n\n 1. **Multi-Region Time Series**: BOLD signals from $5$ different brain regions across $5$ subjects (out of the $10$ generated).\n\n 2. **Hierarchical Coupling**: The data is generated with a specific *hierarchical ROI-connectivity pattern*. Region 1 drives regions 2 and 3 (coupling strengths $0.7$ and $0.5$). Region 2 drives region 4 (coupling strength $0.6$). And Region 3 drives region 5 (coupling strength $0.8$).\n\n 3. **Region-Specific Characteristics**: Each region has a different base oscillation frequency (ranging from $0.05\\ Hz$ to $0.13\\ Hz$).\n\nRegions that are coupled (e.g., Region 2 follows Region 1) show similar temporal patterns, \nbut with individual variability. The coupling is implemented through phase synchronization,\nwhere the phase of a driven region is pulled toward the phase of the driving region.\nAll signals incorporate the hemodynamic response function (HRF) typical of BOLD fMRI, \nwhich causes temporal smoothing and delays.\nEach subject has unique noise patterns, creating realistic inter-subject variability \nwhile preserving the underlying connectivity structure.\nAnd finally, the connectivity between regions is visualized as a heatmap, showing the\ndirected influence from source to target regions.\nThis shows how the KPT framework can be applied to detect phase relationships across\nbrain regions in fMRI data, which is particularly relevant for studying functional\nconnectivity in neuroimaging research.\n\n\nNow that we have the simulated fMRI data, we can explore *kime phase tomography.*\n\n\nNotice the object structures of the result of the *KPT model* of the event-related \nfMRI data, which is an object \n`event_kpt <- unified_kpt(event_data$bold, event_data$t, preprocess = TRUE, kernel_sigma = 0.5)`.\n  \n  \n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number <img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 34827
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    wrap: 72\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "knitr::opts_chunk$set(echo = TRUE)",
      "line_count": 1
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "library(EBImage)\nsquare_arr <- matrix(nrow=256, ncol=256)\ncircle_arr <- matrix(nrow=256, ncol=256)\n\nfor (i in 1:256) {\n  for (j in 1:256) {\n    if ( abs(i-128) < 30 && abs(j-128) < 30) \n      square_arr[i,j]=1 # sqrt((i-128)^2+(j-128)^2)/30\n    else square_arr[i,j]=0\n    if ( sqrt((i-128)^2 + (j-128)^2)<30) \n      circle_arr[i,j]=1 # 1-sqrt((i-128)^2+(j-128)^2)/30\n    else circle_arr[i,j]=0\n  }\n}\n\n# FFT SHIFT\n#' This function is useful for visualizing the Fourier transform with the zero-frequency \n#' component in the middle of the spectrum.\n#' \n#' @param img_ff A Fourier transform of a 1D signal, 2D image, or 3D volume.\n#' @param dim Number of dimensions (-1, 1, 2, 3).\n#' @return A properly shifted FT of the array.\n#' \nfftshift <- function(img_ff, dim = -1) {\n\n  rows <- dim(img_ff)[1]    \n  cols <- dim(img_ff)[2]\n  # planes <- dim(img_ff)[3]\n\n  swap_up_down <- function(img_ff) {\n    rows_half <- ceiling(rows/2)\n    return(rbind(img_ff[((rows_half+1):rows), (1:cols)], img_ff[(1:rows_half), (1:cols)]))\n  }\n\n  swap_left_right <- function(img_ff) {\n    cols_half <- ceiling(cols/2)\n    return(cbind(img_ff[1:rows, ((cols_half+1):cols)], img_ff[1:rows, 1:cols_half]))\n  }\n  \n  #swap_side2side <- function(img_ff) {\n  #  planes_half <- ceiling(planes/2)\n  #  return(cbind(img_ff[1:rows, 1:cols, ((planes_half+1):planes)], img_ff[1:rows, 1:cols, 1:planes_half]))\n  #}\n\n  if (dim == -1) {\n    img_ff <- swap_up_down(img_ff)\n    return(swap_left_right(img_ff))\n  }\n  else if (dim == 1) {\n    return(swap_up_down(img_ff))\n  }\n  else if (dim == 2) {\n    return(swap_left_right(img_ff))\n  }\n  else if (dim == 3) {\n    # Use the `abind` package to bind along any dimension a pair of multi-dimensional arrays\n    # install.packages(\"abind\")\n    library(abind)\n    \n    planes <- dim(img_ff)[3]\n    rows_half <- ceiling(rows/2)\n    cols_half <- ceiling(cols/2)\n    planes_half <- ceiling(planes/2)\n    \n    img_ff <- abind(img_ff[((rows_half+1):rows), (1:cols), (1:planes)], \n                    img_ff[(1:rows_half), (1:cols), (1:planes)], along=1)\n    img_ff <- abind(img_ff[1:rows, ((cols_half+1):cols), (1:planes)], \n                    img_ff[1:rows, 1:cols_half, (1:planes)], along=2)\n    img_ff <- abind(img_ff[1:rows, 1:cols, ((planes_half+1):planes)], \n                    img_ff[1:rows, 1:cols, 1:planes_half], along=3)\n    return(img_ff)\n  }\n  else {\n    stop(\"Invalid dimension parameter\")\n  }\n}",
      "line_count": 76
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "# image(square_arr); image(circle_arr)\n# display(circle_arr, method = \"raster\")\ndisplay(square_arr, method = \"raster\") ",
      "line_count": 3
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "X1 = fft(square_arr)\nX1_mag <- sqrt(Re(X1)^2+Im(X1)^2)\nX1_phase  <- atan2(Im(X1), Re(X1))\n\n# FT of Circle # No shift applied here (perhaps should be consistent or just show the difference?)\nX2 = fft(circle_arr) # display(Re(X2), method = \"raster\")\nX2_mag <- sqrt(Re(X2)^2+Im(X2)^2) # display(X2_mag, method = \"raster\") # magnitude only\nX2_phase  <- atan2(Im(X2), Re(X2)) # display(X2_phase, method = \"raster\") # phase only\ndisplay(fftshift(X1_mag), method = \"raster\") ",
      "line_count": 9
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "# Take 2: IFT Magnitude= Square and Phase = Square + IID noise (N(0,3/2))\nset.seed(1234)\nIID_noise <- matrix(rnorm(prod(dim(X1_phase)), mean=0, sd=1.5), nrow=dim(X1_phase)[1])\n# dim(IID_noise) # 256 256\nReal = X1_mag * cos(X1_phase + IID_noise)\nImaginary = X1_mag * sin(X1_phase + IID_noise)\nift_X1mag_X1phase_Noise = Re(fft(Real+1i*Imaginary, inverse = T)/length(X1))\nplot(density(IID_noise), xlim=c(-8,8), col=\"blue\", lwd=2)\nlines(density(X1_phase), col=\"red\", lwd=2)",
      "line_count": 9
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "mixed_density <- density(X1_phase + IID_noise)\nmixed_density_mod2pi <- density((X1_phase + IID_noise)%%(2*pi) -pi)\ndisplay(ift_X1mag_X1phase_Noise, method = \"raster\")",
      "line_count": 3
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "plot(mixed_density, \n     xlim=c(-8,8), ylim=c(0,0.17), col=\"blue\", lwd=2,  # should density be modulo %%(2*pi)?\n     main=\"Phase Distributions: Raw Square, Square+IID N(m=0,s=3/2), Mixed Phases mod 2*Pi\", \n     cex.main=0.8, xlab = \"Phase\", ylab = \"Density\")\nlines(density(X1_phase), col=\"red\", lwd=4)\nlines(mixed_density_mod2pi, col=\"green\", lwd=2)\ntext(x=3.2, y=-0.005, expression(pi))\ntext(x=-3.2, y=-0.005, expression(-pi))\nlegend(\"center\", \n       legend=c(\"(Raw) Square Phases\", \"Square + N(0,1.5)\", \n                expression(paste(\"Square + N(0,1.5) mod 2*\", pi))),\n       col=c(\"red\",\"blue\", \"green\"), lty=1, lwd=c(4,2,2), cex=1.0, y.intersp=1.0,\n       x.intersp=1.0, title = \"Phases\", bty = \"n\")",
      "line_count": 13
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "# randomly sample 10 indices to pairwise plot\nind1 <- sample(dim(X1_phase)[1], 10); ind2 <- sample(dim(X1_phase)[2], 10) \ncorr <- cor.test((X1_phase)[ind1,ind2], \n                 (X1_phase + IID_noise)[ind1,ind2], method = \"pearson\", conf.level = 0.99)\n\nplot((X1_phase)[ind1,ind2], (X1_phase + IID_noise)[ind1,ind2],\n     main=\n       sprintf(\"Square Image: Raw vs. IID N(m=0,s=3/2) Noise-corrupted Phases: Corr=%s CI=(%s,%s)\", \n               round(cor(as.vector(X1_phase), as.vector(X1_phase + IID_noise)), digits=3),\n               round(corr$conf.int[1], digits=2),\n               round(corr$conf.int[2], digits=2)), \n     cex.main=0.8, xlab = \"Raw\", ylab = \"Phase + N(0,1.5)\")\nabline(\n  lm(as.vector((X1_phase + IID_noise)[ind1,ind2]) ~ as.vector((X1_phase)[ind1,ind2])), \n  col=\"red\", lwd=2)",
      "line_count": 15
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "# Take 2: Same level of noise on Amplitudes:\n# IFT Magnitude= Square + 50*IID noise (N(0,3/2)) and Phase = Square\nset.seed(1234)\nIID_noise <- matrix(rnorm(prod(dim(X1_mag)), mean=0, sd=3/2), nrow=dim(X1_mag)[1])\n# dim(IID_noise) # 256 256\ncorr <- cor.test((X1_mag+50*IID_noise)[ind1,ind2], \n                 (X1_mag)[ind1,ind2], method = \"pearson\", conf.level = 0.99)\nplot(density(X1_mag+50*IID_noise), xlim=c(0,40), ylim=c(0,1.5), col=\"blue\", lwd=2)\nlines(density(X1_mag), col=\"red\", lwd=2)",
      "line_count": 9
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "plot(X1_mag[ind1,ind2], abs(X1_mag+50*IID_noise)[ind1,ind2], xlim=c(0,10), ylim=c(0,200),\n     main=\n       sprintf(\"Square Image: Mag + 50*IID N(m=0,s=3/2) Noise-corrupted vs. Raw Amplitudes: Corr=%s CI=(%s,%s)\", \n               round(cor(as.vector(X1_mag), as.vector(X1_mag + 50*IID_noise)), digits=3),\n               round(corr$conf.int[1], digits=2),\n               round(corr$conf.int[2], digits=2)), \n     cex.main=0.8, xlab = \"Raw\", ylab = \"Amplitude + 50*N(0,1.5)\")\nabline(\n  lm(as.vector(abs(X1_mag+50*IID_noise)[ind1,ind2]) ~ as.vector((X1_mag)[ind1,ind2])), \n  col=\"red\", lwd=2)",
      "line_count": 10
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "Real = (X1_mag+ 50*IID_noise) * cos(X1_phase)\nImaginary = (X1_mag+ 50*IID_noise) * sin(X1_phase)\nift_X1mag_X1phase_Noise = Re(fft(Real+1i*Imaginary, inverse = T)/length(X1))\ndisplay(ift_X1mag_X1phase_Noise, method = \"raster\")",
      "line_count": 4
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "# Magnitude distributions\nmixed_density <- density(abs(X1_mag + 50*IID_noise))\nplot(mixed_density, xlim=c(0,200), col=\"blue\", lwd=2, \n     main=\"Magnitude Distributions: Raw Square, Square+50*N(m=0,s=3/2)\", \n     xlab = \"Magnitude\", ylab = \"Density\")\nlines(density(X1_mag), col=\"red\", lwd=4)\nlegend(\"topright\", legend=c(\"(Raw) Square Magnitudes\", \"Square + 50*N(0,1.5)\"),\n       col=c(\"red\",\"blue\"), lty=1, lwd=c(4,2), cex=1.0, y.intersp=1.0,\n       x.intersp=1.0, title = \"Magnitudes\", bty = \"n\")",
      "line_count": 9
    },
    {
      "section": "Solving the Missing Kime-Phase Problem",
      "code": "# Take 3: Linear Transform of the Phases: SquarePhase ~ CirclePhase\n# Take 2: IFT Magnitude= Square and Phase = LM(CirclePhase)\nlm_Squ_Cir <- lm(as.vector(X1_phase) ~ as.vector(X2_phase))\nplot(as.vector(X2_phase), as.vector(X1_phase), col=\"blue\", lwd=2, \n     xlab = \"Circle\", ylab = \"Square\",\n     main=\n       sprintf(\"Linear Phase Transformation (SquarePhase ~ CirclePhase), Corr(Cir, Squ)=%s\",\n               round(cor(as.vector(X1_phase) , as.vector(X2_phase)), digits=3))) \nabline(lm_Squ_Cir, col=\"red\", lwd=2)\nReal = X1_mag * cos(lm_Squ_Cir$coefficients[1] + lm_Squ_Cir$coefficients[2]*X2_phase)\nImaginary = X1_mag * sin(lm_Squ_Cir$coefficients[1] + lm_Squ_Cir$coefficients[2]*X2_phase)\nift_X1mag_X2phase_LM = Re(fft(Real+1i*Imaginary, inverse = T)/length(X1))\ndisplay(ift_X1mag_X2phase_LM, method = \"raster\")",
      "line_count": 13
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "library(plotly)\n\n# -------------------------------------------------------\n# 1.  Define a hidden \"kime-phase distribution\" Phi(theta)\n#     We'll do a simple mixture of two von Mises as a stand-in.\n#     Range of theta: [-pi, pi].\n# -------------------------------------------------------\n\ndphi <- function(theta) {\n  # mixture of two peaks for demonstration\n  w <- 0.6\n  # we'll use circular normal approximation via dnorm with wrap-around\n  # for simplicity, ignoring normalizing constants\n  p1 <- dnorm(theta, mean=-1.0, sd=0.7)\n  p2 <- dnorm(theta, mean=+1.5, sd=0.4)\n  val <- w*p1 + (1-w)*p2\n  # we do a naive re-normalization across [-pi, pi]\n  # in real code, better to do a proper normalization or use dvonmises\n  return(val)\n}\n\n# Create a function to sample from this distribution by rejection or simple discretization\nsamplePhi <- function(n) {\n  # discrete approach: make a grid, accumulate, sample\n  ngrid <- 2000\n  th_grid <- seq(-pi, pi, length.out=ngrid)\n  pdf_vals <- dphi(th_grid)\n  # normalize\n  pdf_vals <- pdf_vals / sum(pdf_vals)\n  # cumulative\n  cdf_vals <- cumsum(pdf_vals)\n  # random draws\n  rU <- runif(n)\n  # invert\n  idx <- findInterval(rU, cdf_vals)\n  return(th_grid[pmax(1, pmin(idx, ngrid))])\n}\n\n\n# -------------------------------------------------------\n# 2.  Simulate repeated measurements:\n#     In reality, each measurement i yields one random draw theta_i.\n#     Then we get some time-series f_i(t).  But for demonstration:\n#     We'll just store the phase. We'll \"measure\" integrals in different\n#     bases as if we had operators f_alpha or g_beta.\n# -------------------------------------------------------\n\nN <- 200      # total repeated measurements\ntheta_samples <- samplePhi(N)\n\n# We'll say half the measurements use the \"Wavelet\" family,\n# the other half use the \"Fourier\" family, mimicking non-commuting bases\nset_seed <- 123\nidx_wav <- sample.int(N, size = N/2)\nidx_four <- setdiff(seq_len(N), idx_wav)\n\n# -------------------------------------------------------\n# 3.  Define test function families:\n#     Family A: \"Wavelet-like\" => f_alpha(theta)\n#     Family B: \"Fourier-like\" => g_beta(theta)\n#\n#   We'll keep it small: alpha, beta in {1,2,3} as a toy.\n# -------------------------------------------------------\n\nf_wavelet <- function(theta, alpha) {\n  # Simple \"Mexican-hat\"-like or Morlet-like in theta-space\n  # We'll do a naive shape, e.g. e^(-(alpha*theta)^2) * cos(alpha*theta)\n  # Not a real wavelet, just a \"toy wavelet-like function\"\n  return(exp(-0.5*(alpha*theta)^2)*cos(alpha*theta))\n}\n\ng_fourier <- function(theta, beta) {\n  # Standard Fourier basis = sin / cos. Let's do cos for simplicity:\n  return(cos(beta*theta))\n}\n\nalphas <- c(1,2,3)\nbetas  <- c(1,2,3)\n\n#  We'll measure integrals for alpha in alphas or beta in betas.\n\n# -------------------------------------------------------\n# 4.  \"Measure\" partial integrals:\n#     Each measurement i in wavelet basis => pick random alpha from {1,2,3}?\n#     Or measure them all? \n#     For simplicity, we assume each measurement i yields f_alpha(theta_i)\n#     for alpha in a random subset or one alpha per measurement. \n#\n#     Similarly for the Fourier basis g_beta.\n#\n#     We'll store the resulting \"observed integrals\" in arrays.\n# -------------------------------------------------------\n\nset.seed(999)\nobs_wavelet <- data.frame(alpha = integer(), val = numeric())\nobs_fourier <- data.frame(beta  = integer(), val = numeric())\n\nfor(i in idx_wav) {\n  # pick alpha randomly from {1,2,3} to measure\n  alpha_i <- sample(alphas, size=1)\n  val_i <- f_wavelet(theta_samples[i], alpha=alpha_i)\n  obs_wavelet <- rbind(obs_wavelet, data.frame(alpha=alpha_i, val=val_i))\n}\n\nfor(i in idx_four) {\n  # pick beta randomly from {1,2,3} to measure\n  beta_i <- sample(betas, size=1)\n  val_i <- g_fourier(theta_samples[i], beta=beta_i)\n  obs_fourier <- rbind(obs_fourier, data.frame(beta=beta_i, val=val_i))\n}\n\n# We now have naive approximations to\n#   E[f_alpha(\\theta)] = \\int f_alpha(\\theta) Phi(\\theta) d\\theta\n# but from discrete random draws of theta.\n\n\n# -------------------------------------------------------\n# 5.  Estimate these integrals:\n#     We'll do a simple average of obs vals for each alpha or beta\n#     => This approximates \\int f_alpha(\\theta)\\Phi(\\theta)d\\theta\n# -------------------------------------------------------\n\nwavelet_est <- aggregate(val ~ alpha, data=obs_wavelet, FUN=mean)\nfourier_est <- aggregate(val ~ beta,  data=obs_fourier, FUN=mean)\n\nwavelet_est\nfourier_est\n\n# wavelet_est$val[wavelet_est$alpha==alpha] is the numerical estimate\n# of int f_alpha(\\theta) Phi(\\theta) dtheta\n\n\n# -------------------------------------------------------\n# 6.  Representing Phi in a small parametric basis and solve:\n#     We'll do a linear combination of the same wavelet & Fourier basis:\n#       Phi_est(\\theta) = sum_{alpha in A} c_alpha * f_alpha(theta)\n#                       + sum_{beta in B}  d_beta  * g_beta(theta).\n#\n#     We'll choose alpha,beta in {1,2,3} => total of 6 unknown coefficients.\n#     Then we match the integrals we measure with the predicted integrals\n#     from the param. \n# -------------------------------------------------------\n\n# Let alphaSet = {1,2,3}, betaSet={1,2,3}\n# We define:  phi_est(\\theta) = sum_{a} c_a f_wavelet(...) + sum_{b} d_b g_fourier(...)\n\n# The integral of f_wavelet wrt phi_est is:\n#   \\int f_wavelet_a(\\theta) [ sum_{a'} c_{a'} f_wavelet_{a'}(\\theta)\n#                              + sum_{b'} d_{b'} g_fourier_{b'}(\\theta) ] dtheta\n\n# We'll discretize the integral on a fine grid in theta. Let's define a function that,\n# given (c_a, d_b), returns predicted integrals for each alpha or beta.\n\ntheta_grid <- seq(-pi, pi, length.out=600)\ndtheta <- theta_grid[2]-theta_grid[1]\n\n# Precompute a large matrix of [f_wavelet_a(theta_grid), g_fourier_b(theta_grid)] for each alpha,beta\nfw_mat <- sapply(alphas, function(a) f_wavelet(theta_grid, a))\ncolnames(fw_mat) <- paste0(\"fw_a\", alphas)\n\ngf_mat <- sapply(betas,  function(b) g_fourier(theta_grid, b))\ncolnames(gf_mat) <- paste0(\"gf_b\", betas)\n\n\n# We'll define a function to produce integrals int f_wavelet(alpha)*phi_est dtheta\n# and int g_fourier(beta)*phi_est dtheta, for a proposed vector of coefficients c & d:\n\npredictIntegrals <- function(coeffs_cd) {\n  # Suppose length(coeffs_cd) = 6 => c_1..c_3, d_1..d_3\n  c_vec <- coeffs_cd[1:3]\n  d_vec <- coeffs_cd[4:6]\n  \n  # Reconstruct phi_est on the grid:\n  phi_est_grid <- fw_mat %*% c_vec + gf_mat %*% d_vec\n  \n  # Now compute the integral of f_wavelet_a * phi_est\n  # That is (on the grid):\n  #  int( f_wavelet_a(theta_grid) * phi_est_grid ) dtheta\n  # We'll produce a vector of length=3 for alpha=1..3:\n  int_fw <- numeric(length(alphas))\n  for(i in seq_along(alphas)) {\n    int_fw[i] <- sum( fw_mat[,i] * phi_est_grid ) * dtheta\n  }\n  \n  # Similarly for the g_fourier(b):\n  int_gf <- numeric(length(betas))\n  for(j in seq_along(betas)) {\n    int_gf[j] <- sum( gf_mat[,j] * phi_est_grid ) * dtheta\n  }\n  \n  return(list(wave=int_fw, four=int_gf))\n}\n\n# We'll define a cost function: the squared difference between predicted integrals\n# and the measured integrals from wavelet_est, fourier_est\n\nwave_meas <- wavelet_est$val\nnames(wave_meas) <- as.character(wavelet_est$alpha)\nfour_meas <- fourier_est$val\nnames(four_meas) <- as.character(fourier_est$beta)\n\ncostFun <- function(par_cd) {\n  pred <- predictIntegrals(par_cd)\n  # match alpha=1..3 in order\n  res_w <- pred$wave - wave_meas\n  res_f <- pred$four - four_meas\n  sum(res_w^2 + res_f^2)\n}\n\n\n# -------------------------------------------------------\n# 7.  Solve or minimize costFun:\n# -------------------------------------------------------\ninit_guess <- rep(0,6)\nfit <- optim(init_guess, costFun, method=\"BFGS\")\n\nfit$par\ncat(\"Optimized cost:\", fit$value, \"\\n\")\n\n# fit$par => c_1..c_3, d_1..d_3\nfinal_pred <- predictIntegrals(fit$par)\n\n# Compare final_pred vs wave_meas, four_meas\ndata.frame(\n  alpha=alphas,\n  pred=round(final_pred$wave,3),\n  obs=round(wave_meas,3)\n)\ndata.frame(\n  beta=betas,\n  pred=round(final_pred$four,3),\n  obs=round(four_meas,3)\n)\n\n\n# -------------------------------------------------------\n# 8.  Plot the reconstructed Phi vs. the true distribution\n# -------------------------------------------------------\n\n# Reconstruct Phi_est on a grid\ncoeffs_cd <- fit$par\nc_vec <- coeffs_cd[1:3]\nd_vec <- coeffs_cd[4:6]\nphi_est_grid <- fw_mat %*% c_vec + gf_mat %*% d_vec\n\n# True distribution on the same grid\nphi_true_grid <- sapply(theta_grid, dphi)\n# normalize each for plotting\nphi_true_grid <- phi_true_grid / (sum(phi_true_grid)*dtheta)\nphi_est_grid  <- phi_est_grid / (sum(phi_est_grid)*dtheta) # might be negative parts, but let's see\n\n# par(mfrow=c(1,1))\n# plot(theta_grid, phi_true_grid, type=\"l\", col=\"blue\", lwd=2,\n#      ylim=range(c(phi_true_grid, phi_est_grid)),\n#      xlab=expression(theta), ylab=\"density\", main=\"Kime-phase tomography\")\n# lines(theta_grid, phi_est_grid, col=\"red\", lwd=2)\n# legend(\"topright\", legend=c(\"True Phi\",\"Estimated Phi\"), col=c(\"blue\",\"red\"), lty=1, lwd=2)\n\np <- plot_ly() %>%\n  add_trace(x = theta_grid, y = phi_true_grid, type = 'scatter', ### True Phi\n    mode = 'lines', line = list(color = 'blue', width = 2), name = 'True Phi') %>%\n  add_trace(x = theta_grid, y = phi_est_grid,  #### Estimated Phi\n    type = 'scatter', mode = 'lines', line = list(color = 'red', width = 2),\n    name = 'Estimated Phi') %>%\n  layout(title = \"Kime-phase tomography\", xaxis = list(title = expression(theta)),\n    yaxis = list(title = \"Density\"), legend = list(x = 0.75, y = 0.9))\np",
      "line_count": 267
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "############################################################\n# 1. ENV SETUP\n############################################################\n# Make sure rstan and plot packages arer installed:\n# install.packages(c(\"rstan\", \"ggplot2\"))\nlibrary(rstan)      # for Stan interface\nlibrary(ggplot2)    # for basic plotting\n\n# rstan config for quicker examples\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\n############################################################\n# 2. DEFINE A SIMPLE HRF AND HELPER FUNCTIONS\n############################################################\n\n# A tiny \"canonical HRF\": single gamma-like shape\n# for demonstration. Typically using a more realistic double-gamma, etc.\nmake_hrf <- function(length_out=32, peak=6, scale=1) {\n  # We'll define a gamma PDF shape ~ dgamma(t, shape=?, rate=?)\n  tseq <- seq(0, length_out-1, by=1)\n  # shape=peak, rate=peak? => quick guess\n  # this can be refined\n  hrf_values <- dgamma(tseq, shape=peak, rate=peak/scale)\n  hrf_values / max(hrf_values)  # normalize peak to 1\n}\n\n#  Laplace sampler for phases\nrLaplace <- function(n, mu=0, b=1) {\n  # For each uniform draw u in (-1/2,1/2), transform\n  u <- runif(n, min=-0.5, max=0.5)\n  sapply(u, function(ui) mu - b*sign(ui)*log1p(-2*abs(ui)))\n}\n\n\n############################################################\n# 3. SIMULATE DATA (FORWARD MODEL)\n############################################################\nsimulate_data <- function(R, N, A=1, freq=0.33, noise_sd=0.2,\n                          b=1.0, t_max=30) {\n  # R = # of runs (repeated measurements)\n  # N = # of time points per run\n  # freq => frequency in cycles/sec (just for demonstration)\n  # b => Laplace scale for phase draws\n  # t_max => total length in time\n  # Return a list with:\n  #   t_vals: the time vector\n  #   x_mat:  a matrix (R x N) of noisy signals\n  #   theta_true: the actual phases used\n\n  # time axis\n  t_vals <- seq(0, t_max, length.out=N)\n\n  # define HRF\n  hrf_vec <- make_hrf(length_out=32, peak=6, scale=1)\n\n  # sample phases from Laplace\n  theta_true <- rLaplace(R, 0, b)\n  \n  x_mat <- matrix(NA, nrow=R, ncol=N)\n\n  # convolve each run's sinusoid with HRF\n  for(r in 1:R) {\n    # create raw sinusoid with phase shift\n    raw_signal <- A * sin(2*pi*freq * t_vals + theta_true[r])\n    # do discrete convolve\n    conv_full <- convolve(raw_signal, rev(hrf_vec), type=\"open\")\n    # match length N\n    conv_full <- conv_full[1:N]\n    # add noise\n    noisy <- conv_full + rnorm(N, 0, noise_sd)\n    x_mat[r, ] <- noisy\n  }\n  \n  list(t_vals=t_vals, x_mat=x_mat, theta_true=theta_true)\n}\n\n\n############################################################\n# 4. STAN MODEL CODE\n############################################################\n# We'll assume each run has an unknown phase theta[r].\n# We'll do a single amplitude A shared across runs, or let each run have its own amplitude.\n# We'll keep the HRF fixed (like a known shape).\n#\n# The Stan model tries to recover theta[r] by matching the data x[r,] with\n#   x_hat[r, n] = A*sin(2*pi*freq * t_vals[n] + theta[r]) convolved with known HRF\n# Then a normal likelihood with std dev sigma_noise.\n\nstan_model_code <- \"\ndata {\n  int<lower=1> R;          // number of runs\n  int<lower=1> N;          // number of time points per run\n  real t_vals[N];          // time vector\n  matrix[R, N] x_mat;      // observed data\n  int<lower=1> len_hrf;\n  vector[len_hrf] hrf_vec; // known HRF shape\n  real freq;               // known frequency\n}\nparameters {\n  real<lower=0> A;                 // amplitude\n  real<lower=0> sigma_noise;       // noise std\n  vector[R] theta_raw;             // unconstrained phases\n}\ntransformed parameters {\n  // We'll map theta_raw into [-pi, pi] range just for demonstration\n  // We can do a tanh approach or we can do mod\n  vector[R] theta;\n  for(r in 1:R) {\n    // let's do a tanh approach:  [-1,1] => [-pi, pi]\n    theta[r] = pi() * tanh(theta_raw[r]);\n  }\n}\nmodel {\n  // Priors\n  A ~ normal(1, 1);      // guess amplitude near 1\n  sigma_noise ~ normal(0.2, 0.1) T[0,];\n  theta_raw ~ normal(0, 1);  // broad for phase\n\n  // likelihood\n  for(r in 1:R) {\n    // Build raw sinusoid\n    vector[N] raw_signal_r;\n    for(n in 1:N) {\n      raw_signal_r[n] = A * sin(2*pi()*freq * t_vals[n] + theta[r]);\n    }\n    // convolve with hrf => we do a discrete approach\n    // need a new vector conv_full, length N\n    // We'll do naive loop convolution\n    vector[N] conv_full = rep_vector(0, N);\n    for(i in 1:N) {\n      // i => index in conv result\n      // sum over j => raw_signal_r[j]*hrf_vec[i-j+1]\n      real tmp_sum = 0;\n      for(j in 1:i) {\n        int k = i-j+1; // index in hrf\n        if(k <= len_hrf) {\n          tmp_sum += raw_signal_r[j]*hrf_vec[k];\n        }\n      }\n      conv_full[i] = tmp_sum;\n    }\n    // now conv_full is the convolved signal\n    // likelihood\n    x_mat[r] ~ normal(conv_full, sigma_noise);\n  }\n}\n\"\n\n\n############################################################\n# 5.  RUN SIMULATION\n############################################################\nset.seed(1234)\nR <- 8   # number of runs\nN <- 60  # number of timepoints per run\nfreq_used <- 0.33\nsim_out <- simulate_data(R=R, N=N, A=1.0, freq=freq_used,\n                         noise_sd=0.15, b=1.0, t_max=20)\n# We store t_vals, x_mat, theta_true\nt_vals    <- sim_out$t_vals\nx_mat     <- sim_out$x_mat\ntheta_true <- sim_out$theta_true\n\nhrf_vec <- make_hrf(length_out=32, peak=6, scale=1)\n\n############################################################\n# 6.  PREPARE DATA FOR STAN + COMPILE MODEL\n############################################################\nstan_data <- list(\n  R = R,\n  N = N,\n  t_vals = t_vals,\n  x_mat  = x_mat,\n  len_hrf = length(hrf_vec),\n  hrf_vec = hrf_vec,\n  freq    = freq_used\n)\n\n# compile\nstan_model <- stan_model(model_code=stan_model_code)\n\n############################################################\n# 7.  SAMPLE / FIT MCMC\n############################################################\nfit <- sampling(stan_model, data=stan_data,\n                iter=2000, chains=2, seed=999,\n                control=list(adapt_delta=0.9))\n\n# print(fit, pars=c(\"A\",\"sigma_noise\"))\nfit\n\ntheta_draws <- extract(fit, pars=\"theta\")$theta\ndim(theta_draws)  # should be iterations x R\n\n############################################################\n# 8.  CHECK PHASE ESTIMATES vs. TRUE\n############################################################\n# We'll compute the posterior mean for each run's phase\ntheta_est_mean <- apply(theta_draws, 2, mean)\ntheta_est_sd   <- apply(theta_draws, 2, sd)\n\ndf_phases <- data.frame(\n  run_id      = 1:R,\n  theta_true  = theta_true,\n  theta_est   = theta_est_mean,\n  est_sd      = theta_est_sd\n)\n\ncat(\"\\nPosterior mean vs. true phase:\\n\")\ndf_phases\n\n# Quick plot\n# ggplot(df_phases, aes(x=theta_true, y=theta_est)) +\n#   geom_point(color=\"blue\") +\n#   geom_abline(intercept=0, slope=1, linetype=\"dashed\") +\n#   labs(title=\"True vs. estimated phase\", x=\"True θ\", y=\"Estimated θ\") +\n#   coord_fixed()\n# Convert ggplot to plotly\nlibrary(plotly)\n\n# Assuming df_phases contains theta_true and theta_est columns\n# First create the reference line\n# Get the range of data for the reference line\nrange_min <- min(min(df_phases$theta_true), min(df_phases$theta_est))\nrange_max <- max(max(df_phases$theta_true), max(df_phases$theta_est))\nref_line <- data.frame(x = c(range_min, range_max), y = c(range_min, range_max))\n\n# Create the plot\nfig <- plot_ly() %>%\n  # Add scatter points\n  add_trace(data = df_phases, x = ~theta_true, y = ~theta_est,\n    type = 'scatter', mode = 'markers', marker = list(color = 'blue',size = 6),\n    name = 'Phase Values') %>%\n  # Add reference line\n  add_trace(data = ref_line, x = ~x, y = ~y,\n    type = 'scatter', mode = 'lines', line = list(dash = 'dash', color = 'black'),\n    name = 'y = x') %>%\n  # Layout settings\n  layout(title = list(text = \"True vs. estimated phase\", y = 0.95),\n    xaxis = list(title = \"True θ\", scaleanchor = \"y\",  # This ensures equal scaling\n      scaleratio = 1      # This maintains aspect ratio\n    ),\n    yaxis = list(title = \"Estimated θ\"), showlegend = TRUE)\nfig",
      "line_count": 245
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "library(dplyr)\n\n# Define the Hilbert space: theta in [-pi, pi]\ntheta <- seq(-pi, pi, length.out = 1000)\ndtheta <- theta[2] - theta[1]\n\n# Test functions\nf_alpha <- function(theta, alpha = 1) exp(-(alpha * theta)^2) * cos(alpha * theta)\ng_beta <- function(theta, beta = 1) cos(beta * theta)\n\n# Operators\nF_alpha <- function(psi, theta, alpha = 1) {\n  # Convolution with f_alpha\n  sapply(theta, function(th) {\n    kern <- f_alpha(th - theta, alpha)\n    sum(kern * psi) * dtheta\n  })\n}\n\nG_beta <- function(psi, theta, beta = 1) {\n  # Multiplication by g_beta\n  g_beta(theta, beta) * psi\n}\n\n# Derivative operator for comparison\nD_beta <- function(psi, theta, beta = 1) {\n  -1i * beta * diff(c(psi[length(psi)], psi))[1:length(theta)] / dtheta\n}\n\n# Commutator\ncommutator <- function(op1, op2, psi, theta, p1 = 1, p2 = 1) {\n  op1_op2 <- op1(op2(psi, theta, p2), theta, p1)\n  op2_op1 <- op2(op1(psi, theta, p1), theta, p2)\n  op1_op2 - op2_op1\n}\n\n# Test with uniform psi\npsi <- rep(1, length(theta)) / sqrt(2 * pi)  # Normalized\ncomm_FG <- commutator(F_alpha, G_beta, psi, theta, 1, 1)\ncomm_ThetaD <- commutator(function(psi, theta, a) theta * psi,\n                         D_beta, psi, theta, 1, 1)\n\n# Norms\nnorm_FG <- sqrt(sum(abs(comm_FG)^2) * dtheta)\nnorm_ThetaD <- sqrt(sum(abs(comm_ThetaD)^2) * dtheta)\n\ncat(\"Norm of [F_alpha, G_beta] =\", norm_FG, \"\\n\")  # Non-zero\ncat(\"Norm of [Theta, D_beta] =\", norm_ThetaD, \"\\n\")  # Non-zero\n\n# Probe a distribution\nPhi <- dnorm(theta, 0, 0.5); Phi <- Phi / sum(Phi * dtheta)\nexp_F <- sum(F_alpha(Phi, theta, 1) * Phi) * dtheta\nexp_G <- sum(G_beta(Phi, theta, 1) * Phi) * dtheta\ncat(\"Expectation F_alpha =\", exp_F, \"\\n\")\ncat(\"Expectation G_beta =\", exp_G, \"\\n\")",
      "line_count": 55
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "library(ggplot2)\nlibrary(dplyr)\n\n# 1. Define True Kime-Phase Distribution (Mixture of Von Mises)\n# -------------------------------------------------------------\ndphi <- function(theta, kappa1=5, kappa2=5, mu1=-1, mu2=1.5, w=0.6) {\n  # Mixture of two von Mises distributions\n  vm1 <- exp(kappa1 * cos(theta - mu1)) / (2 * pi * besselI(kappa1, 0))\n  vm2 <- exp(kappa2 * cos(theta - mu2)) / (2 * pi * besselI(kappa2, 0))\n  w * vm1 + (1 - w) * vm2\n}\n\n# 2. Sample Theta from Distribution\n# ---------------------------------\nset.seed(123)\ntheta_grid <- seq(-pi, pi, length.out=1000)\nphi_true <- dphi(theta_grid)\ntheta_samples <- sample(theta_grid, 1000, replace=TRUE, prob=phi_true)\n\n# 3. Estimate Phi using Kernel Density Estimation (KDE)\n# -----------------------------------------------------\nbw <- 0.3  # Bandwidth\nkde <- density(theta_samples, from=-pi, to=pi, bw=bw, n=1000)\nphi_est <- approx(kde$x, kde$y, theta_grid)$y\nphi_est <- phi_est / sum(phi_est * diff(theta_grid)[1])  # Normalize\n\n# 4. Compute Position (Theta) Variance\n# ------------------------------------\nmean_theta <- sum(theta_grid * phi_est * diff(theta_grid)[1])\nvar_theta <- sum((theta_grid - mean_theta)^2 * phi_est * diff(theta_grid)[1])\n\n# 5. Compute Momentum (Fourier) Variance\n# --------------------------------------\n# Wavefunction: psi(theta) = sqrt(phi_est)\npsi <- sqrt(phi_est)\n# Fourier transform to momentum space\npsi_fft <- fft(psi)\nk <- 2 * pi * seq(-0.5, 0.5, length.out=1000)  # Momentum grid\n# Compute |psi_fft|^2 (momentum probability density)\nmomentum_prob <- abs(psi_fft)^2\nmomentum_prob <- momentum_prob / sum(momentum_prob)  # Normalize\n# Momentum variance\nmean_k <- sum(k * momentum_prob)\nvar_k <- sum((k - mean_k)^2 * momentum_prob)\n\n# 6. Validate Uncertainty Principle\n# ---------------------------------\n# Theoretical minimum: 0.5 (for [Θ, P] = i)\nuncertainty_product <- var_theta * var_k\ncat(sprintf(\"Uncertainty Product: %.3f (≥ 0.5? %s)\\n\", \n            uncertainty_product, \n            ifelse(uncertainty_product >= 0.5, \"Yes\", \"No\")))\n\n# 7. Reconstruct Phi Using Gaussian Assumption (Example)\n# ------------------------------------------------------\n# Assume Gaussian from mean_theta and var_theta (for illustration)\nphi_gaussian <- dnorm(theta_grid, mean_theta, sqrt(var_theta))\nphi_gaussian <- phi_gaussian / sum(phi_gaussian * diff(theta_grid)[1])\n\n# 8. Plot Results\n# ---------------\ndf <- data.frame(\n  theta = theta_grid,\n  True = phi_true,\n  KDE = phi_est,\n  Gaussian_Fit = phi_gaussian\n)\n\n# #### PLOT\n# ggplot(df, aes(x = theta)) +\n#   geom_line(aes(y = True, color = \"True Distribution\"), linewidth=1) +\n#   geom_line(aes(y = KDE, color = \"KDE Estimate\"), linetype=\"dashed\") +\n#   geom_line(aes(y = Gaussian_Fit, color = \"Gaussian Fit\"), linetype=\"dotted\") +\n#   labs(title = \"Kime-Phase Distribution Estimation\",\n#        subtitle = sprintf(\"Uncertainty Product: %.2f\", uncertainty_product),\n#        x = expression(theta), y = \"Density\") +\n#   scale_color_manual(values = c(\"True Distribution\" = \"blue\",\n#                                 \"KDE Estimate\" = \"red\",\n#                                 \"Gaussian Fit\" = \"green\")) +\n#   theme_minimal()\n\nlibrary(plotly)\n\n# Create plot\nplot_ly(df, x = ~theta) %>%\n  # True Distribution line\n  add_trace(\n    y = ~True,\n    name = \"True Distribution\",\n    type = \"scatter\",\n    mode = \"lines\",\n    line = list(\n      color = \"blue\",\n      width = 2\n    )\n  ) %>%\n  # KDE Estimate line\n  add_trace(\n    y = ~KDE,\n    name = \"KDE Estimate\",\n    type = \"scatter\",\n    mode = \"lines\",\n    line = list(\n      color = \"red\",\n      width = 2,\n      dash = \"dash\"\n    )\n  ) %>%\n  # Gaussian Fit line\n  add_trace(\n    y = ~Gaussian_Fit,\n    name = \"Gaussian Fit\",\n    type = \"scatter\",\n    mode = \"lines\",\n    line = list(\n      color = \"green\",\n      width = 2,\n      dash = \"dot\"\n    )\n  ) %>%\n  # Layout configuration\n  layout(\n    title = list(\n      text = paste0(\n        \"Time-Phase Distribution Estimation<br>\",\n        \"<sup>Uncertainty Product: \", \n        sprintf(\"%.2f\", uncertainty_product), \n        \"</sup>\"\n      ),\n      font = list(size = 20)\n    ),\n    xaxis = list(\n      title = \"θ\",\n      titlefont = list(size = 14),\n      gridcolor = \"lightgray\",\n      zerolinecolor = \"gray\"\n    ),\n    yaxis = list(\n      title = \"Density\",\n      titlefont = list(size = 14),\n      gridcolor = \"lightgray\",\n      zerolinecolor = \"gray\"\n    ),\n    legend = list(\n      x = 1.1,\n      y = 0.9,\n      bordercolor = \"lightgray\",\n      borderwidth = 1\n    ),\n    paper_bgcolor = \"white\",\n    plot_bgcolor = \"white\",\n    margin = list(t = 100),\n    showlegend = TRUE,\n    hovermode = \"x unified\"\n  ) %>%\n  # Add modebar buttons\n  config(\n    modeBarButtonsToAdd = list(\n      \"hoverclosest\",\n      \"hovercompare\"\n    ),\n    displaylogo = FALSE\n  )",
      "line_count": 163
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "library(circular)\nlibrary(plotly)\n\nset.seed(123)\n\n# Parameters\nT <- 100\nN <- 1000\nt <- seq(0, 10, length.out = T)\ntheta_grid <- seq(-pi, pi, length.out = 1000)\ndtheta <- theta_grid[2] - theta_grid[1]\n\n# True time-dependent von Mises distribution\nmu_true <- 2 * pi * sin(0.1 * t)\nkappa_true <- 5 * (1 + 0.5 * sin(2 * pi * t / T))\n\n# Operators\nTheta_op <- function(psi, theta) theta * psi\nG_beta_op <- function(psi, theta, beta = 1) {\n  -1i * beta * diff(c(psi[length(psi)], psi))[1:length(theta)] / dtheta\n}\n\n# Forward model\ngenerate_signal <- function(t, theta) sin(2 * pi * 0.5 * t + theta) + rnorm(length(t), 0, 0.1)\n\n# Simulate data\ntheta_true <- matrix(NA, N, T)\nx <- matrix(NA, N, T)\nfor (n in 1:N) {\n  for (tt in 1:T) {\n    theta_true[n, tt] <- rvonmises(1, mu = mu_true[tt], kappa = kappa_true[tt])\n  }\n  x[n, ] <- generate_signal(t, theta_true[n, ])\n}\n\n# Manual Hilbert transform\nhilbert_manual <- function(x) {\n  n <- length(x)\n  fft_x <- fft(x)\n  h <- numeric(n); h[1] <- 1; h[2:(n/2)] <- 2; h[(n/2 + 1):n] <- 0\n  fft(h * fft_x, inverse = TRUE) / n\n}\n\n# Phase inference\ntheta_est <- t(apply(x, 1, function(row) Arg(hilbert_manual(row))))\n\n# Moment estimation\nm1_est <- colMeans(exp(1i * theta_est))\nmu_hat <- Arg(m1_est)\nR <- abs(m1_est)\nkappa_hat <- sapply(R, function(r) if (r >= 0.999) 100 else if (r <= 0.001) 0 else uniroot(function(k) besselI(k, 1) / besselI(k, 0) - r, c(0, 100))$root)\n\n# Operator expectations at sample t\nPhi_t <- dvonmises(theta_grid, mu = mu_true[50], kappa = kappa_true[50])\nPhi_t <- Phi_t / sum(Phi_t * dtheta)\npsi_t <- sqrt(Phi_t)\nexp_Theta <- sum(Theta_op(psi_t, theta_grid) * psi_t) * dtheta\nexp_G <- sum(G_beta_op(psi_t, theta_grid, 1) * psi_t) * dtheta\ncat(\"At t[50]: <Theta> =\", exp_Theta, \", <G_beta> =\", exp_G, \"\\n\")\n\n# Plot\ndf <- data.frame(t = t, mu_true = mu_true %% (2 * pi) - pi, kappa_true = kappa_true,\n                 mu_hat = mu_hat %% (2 * pi) - pi, kappa_hat = kappa_hat)\nfig_mu <- plot_ly(df) %>% add_trace(x = ~t, y = ~mu_true, type = \"scatter\", mode = \"lines\", name = \"True μ\") %>%\n  add_trace(x = ~t, y = ~mu_hat, type = \"scatter\", mode = \"lines\", name = \"Est μ\") %>%\n  layout(title = \"Mean Phase\", yaxis = list(title = \"μ(t)\"))\nfig_kappa <- plot_ly(df) %>% add_trace(x = ~t, y = ~kappa_true, type = \"scatter\", mode = \"lines\", name = \"True κ\") %>%\n  add_trace(x = ~t, y = ~kappa_hat, type = \"scatter\", mode = \"lines\", name = \"Est κ\") %>%\n  layout(title = \"Concentration\", yaxis = list(title = \"κ(t)\"))\nfig_mu\nfig_kappa",
      "line_count": 71
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "# Load required libraries\nlibrary(circular)  # For von Mises distribution\nlibrary(plotly)    # For 3D surface plotting\nlibrary(htmlwidgets)  # For saving as SVG\nlibrary(nloptr)    # For constrained optimization\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Von Mises mixture density function\nvon_mises_mixture <- function(theta, w, mu1, mu2, k1, k2) {\n  # vm1 <- dvonmises(theta, mu1, k1) / (2 * pi * besselI(k1, 0))\n  # vm2 <- dvonmises(theta, mu2, k2) / (2 * pi * besselI(k2, 0))\n  vm1 <- dvonmises(theta, mu1, k1) \n  vm2 <- dvonmises(theta, mu2, k2) \n  w * vm1 + (1 - w) * vm2\n}\n\n# Define time-dependent parameters for Phi(theta; t)\nw_t <- function(t) 0.5 + 0.3 * sin(2 * pi * t / 50)\nmu1_t <- function(t) (0.5 * t) %% (2 * pi) - pi\nmu2_t <- function(t) (0.5 * t + pi) %% (2 * pi) - pi\nk1_t <- function(t) 5 + 2 * sin(2 * pi * t / 30)\nk2_t <- function(t) 5 - 2 * sin(2 * pi * t / 30)\n\n# True Phi(theta; t)\nphi_true <- function(theta, t) {\n  w <- w_t(t)\n  mu1 <- mu1_t(t)\n  mu2 <- mu2_t(t)\n  k1 <- k1_t(t)\n  k2 <- k2_t(t)\n  von_mises_mixture(theta, w, mu1, mu2, k1, k2)\n}\n\n# Test function families\n# Fourier-like (sine functions)\nf_alpha <- function(theta, alpha, sigma = 0.5) {\n  sin(alpha * theta)\n}\n\n# Fourier-like (cosine functions)\ng_beta <- function(theta, beta) {\n  cos(beta * theta)\n}\n\n# Generate test functions for a range of alpha and beta\nalphas <- 1:5\nbetas <- 1:5\ntest_functions <- list(\n  f = lapply(alphas, function(a) function(theta) f_alpha(theta, a)),\n  g = lapply(betas, function(b) function(theta) g_beta(theta, b))\n)\n\n# Simulate measurements and reconstruct Phi\nsimulate_kpt <- function(t_values, theta_grid, N = 500, lambda = 0.01) {\n  N_f <- N_g <- N / 2  # Split samples between families\n  phi_true_vals <- matrix(NA, nrow = length(t_values), ncol = length(theta_grid))\n  phi_est_vals <- matrix(NA, nrow = length(t_values), ncol = length(theta_grid))\n  mse <- numeric(length(t_values))\n\n  for (i in seq_along(t_values)) {\n    t <- t_values[i]\n\n    # True Phi at time t\n    phi_true_vals[i, ] <- phi_true(theta_grid, t)\n\n    # Simulate samples theta_i ~ Phi(theta; t)\n    w <- w_t(t)\n    mu1 <- mu1_t(t)\n    mu2 <- mu2_t(t)\n    k1 <- k1_t(t)\n    k2 <- k2_t(t)\n    theta_samples <- c(\n      rvonmises(N_f, mu1, k1),  # Samples for f_alpha\n      rvonmises(N_g, mu2, k2)   # Samples for g_beta\n    )\n    # Adjust samples based on mixture weight\n    mix <- runif(N)\n    theta_samples <- ifelse(mix < w, theta_samples[1:N_f], theta_samples[(N_f+1):N])\n\n    # Compute sample averages for each test function\n    f_means <- sapply(test_functions$f, function(f) mean(sapply(theta_samples[1:N_f], f)))\n    g_means <- sapply(test_functions$g, function(g) mean(sapply(theta_samples[(N_f+1):N], g)))\n\n    # Define the objective function for optimization\n    objective <- function(coeffs) {\n      c_alpha <- coeffs[1:length(alphas)]\n      d_beta <- coeffs[(length(alphas)+1):length(coeffs)]\n      phi_est <- function(theta) {\n        sum_f <- sum(sapply(seq_along(alphas), function(a) c_alpha[a] * f_alpha(theta, alphas[a])))\n        sum_g <- sum(sapply(seq_along(betas), function(b) d_beta[b] * g_beta(theta, betas[b])))\n        sum_f + sum_g\n      }\n\n      # Compute integrals numerically\n      f_integrals <- sapply(test_functions$f, function(f) {\n        mean(sapply(theta_grid, function(theta) f(theta) * phi_est(theta)))\n      })\n      g_integrals <- sapply(test_functions$g, function(g) {\n        mean(sapply(theta_grid, function(theta) g(theta) * phi_est(theta)))\n      })\n\n      # Sum of squared residuals + Tikhonov penalty\n      sum((f_means - f_integrals)^2) + sum((g_means - g_integrals)^2) +\n        lambda * (sum(c_alpha^2) + sum(d_beta^2))\n    }\n\n    # Constraints: coefficients >= 0\n    opts <- list(\"algorithm\" = \"NLOPT_LN_COBYLA\", \"xtol_rel\" = 1e-6, \"maxeval\" = 1000)\n    init_coeffs <- rep(0.1, length(alphas) + length(betas))\n    res <- nloptr(\n      x0 = init_coeffs,\n      eval_f = objective,\n      lb = rep(0, length(init_coeffs)),\n      opts = opts\n    )\n\n    # Reconstruct Phi_est\n    coeffs <- res$solution\n    c_alpha <- coeffs[1:length(alphas)]\n    d_beta <- coeffs[(length(alphas)+1):length(coeffs)]\n    phi_est <- function(theta) {\n      sum_f <- sum(sapply(seq_along(alphas), function(a) c_alpha[a] * f_alpha(theta, alphas[a])))\n      sum_g <- sum(sapply(seq_along(betas), function(b) d_beta[b] * g_beta(theta, betas[b])))\n      sum_f + sum_g\n    }\n    phi_est_vals[i, ] <- sapply(theta_grid, phi_est)\n\n    # Ensure positivity and normalize\n    phi_est_vals[i, ] <- pmax(phi_est_vals[i, ], 0)\n    # integral <- mean(phi_est_vals[i, ]) * (2 * pi) / length(theta_grid)\n    # Assume equidistant theta_grid\n    integral <- sum(phi_est_vals[i, ])*(theta_grid[2] - theta_grid[1])\n    phi_est_vals[i, ] <- phi_est_vals[i, ] / integral\n\n    # Compute MSE\n    mse[i] <- mean((phi_true_vals[i, ] - phi_est_vals[i, ])^2)\n  }\n\n  return(list(phi_true = phi_true_vals, phi_est = phi_est_vals, mse = mse))\n}\n\n# simulate_kpt <- function(t_values, theta_grid, N = 500, lambda = 0.01) {\n#   # Ensure N is even for simplicity, or add handling for odd N if needed\n#   if (N %% 2 != 0) {\n#     warning(\"N is odd, adjusting N to be even: \", N + 1)\n#     N <- N + 1\n#   }\n#   N_f <- N_g <- N / 2\n# \n#   # --- Add checks for required objects ---\n#   required_funcs <- c(\"f_alpha\", \"g_beta\", \"w_t\", \"mu1_t\", \"mu2_t\", \"k1_t\", \"k2_t\", \"phi_true\")\n#   missing_funcs <- required_funcs[!sapply(required_funcs, exists, mode = \"function\")]\n#   if (length(missing_funcs) > 0) stop(\"Missing required functions: \", paste(missing_funcs, collapse=\", \"))\n# \n#   required_objs <- c(\"alphas\", \"betas\", \"test_functions\")\n#   missing_objs <- required_objs[!sapply(required_objs, exists)]\n#   if (length(missing_objs) > 0) stop(\"Missing required objects: \", paste(missing_objs, collapse=\", \"))\n# \n#   if (!is.list(test_functions) || !all(c(\"f\", \"g\") %in% names(test_functions))) stop(\"test_functions must be a list with names 'f' and 'g'\")\n#   if (!is.vector(alphas) || !is.numeric(alphas)) stop(\"alphas must be a numeric vector\")\n#   if (!is.vector(betas) || !is.numeric(betas)) stop(\"betas must be a numeric vector\")\n#   if (length(test_functions$f) != length(alphas)) stop(\"Length of test_functions$f must match length of alphas\")\n#   if (length(test_functions$g) != length(betas)) stop(\"Length of test_functions$g must match length of betas\")\n#   # --- End checks ---\n# \n# \n#   phi_true_vals <- matrix(NA, nrow = length(t_values), ncol = length(theta_grid))\n#   phi_est_vals <- matrix(NA, nrow = length(t_values), ncol = length(theta_grid))\n#   mse <- numeric(length(t_values))\n# \n#   for (i in seq_along(t_values)) {\n#     t <- t_values[i]\n#     cat(\"\\nProcessing t =\", t, \"(iteration\", i, \"of\", length(t_values), \")\\n\") # DEBUG PROGRESS\n# \n#     # True Phi at time t\n#     phi_true_vals[i, ] <- phi_true(theta_grid, t)\n# \n#     # Simulate samples theta_i ~ Phi(theta; t)\n#     # w <- w_t(t) # Not strictly needed for sampling if calculating means separately\n#     mu1 <- mu1_t(t)\n#     mu2 <- mu2_t(t)\n#     k1 <- k1_t(t)\n#     k2 <- k2_t(t)\n# \n#     # --- DEBUG: Check mu and kappa values ---\n#     cat(\"  Parameters: mu1=\", mu1, \" k1=\", k1, \" mu2=\", mu2, \" k2=\", k2, \"\\n\")\n#     if (!all(sapply(c(mu1, k1, mu2, k2), is.numeric)) || any(is.na(c(mu1, k1, mu2, k2)))) {\n#          stop(\"Non-numeric or NA mu/kappa values detected at t=\", t)\n#     }\n#     if (k1 < 0 || k2 < 0) {\n#          stop(\"Negative kappa (k) detected at t=\", t)\n#     }\n#     # --- End DEBUG ---\n# \n#     # Generate samples\n#     theta_samples_f <- rvonmises(N_f, mu = mu1, kappa = k1)\n#     theta_samples_g <- rvonmises(N_g, mu = mu2, kappa = k2)\n# \n#     # --- DEBUG: Check generated samples ---\n#     if (any(!is.numeric(theta_samples_f)) || any(is.na(theta_samples_f))) {\n#         warning(\"Non-numeric or NA samples generated for f at t=\", t)\n#         print(summary(theta_samples_f))\n#         stop(\"Stopping due to invalid samples for f\")\n#     }\n#      if (any(!is.numeric(theta_samples_g)) || any(is.na(theta_samples_g))) {\n#         warning(\"Non-numeric or NA samples generated for g at t=\", t)\n#         print(summary(theta_samples_g))\n#         stop(\"Stopping due to invalid samples for g\")\n#     }\n#     cat(\"  Samples generated successfully.\\n\")\n#     # --- End DEBUG ---\n# \n# \n#     # Compute sample averages\n#     cat(\"  Calculating f_means...\\n\")\n#     f_means <- sapply(seq_along(test_functions$f), function(idx) {\n#         f <- test_functions$f[[idx]]\n#         current_alpha <- alphas[idx]\n#         if (!is.numeric(current_alpha) || is.na(current_alpha)) {\n#             stop(paste(\"Non-numeric or NA alpha detected at index\", idx, \"value:\", current_alpha))\n#         }\n# \n#         # --- DEBUG: Check inside inner sapply for f_means ---\n#         inner_results <- sapply(theta_samples_f, function(theta_val) {\n#             if (!is.numeric(theta_val) || is.na(theta_val)) {\n#                  stop(\"Non-numeric or NA theta value passed to test function f: \", theta_val)\n#             }\n#             # Use tryCatch to pinpoint the exact failing call\n#             res <- tryCatch({\n#                 f(theta_val) # Apply the test function\n#             }, error = function(e) {\n#                 cat(\"    ERROR occurred in test_functions$f[[\", idx, \"]] (alpha=\", current_alpha, \") with theta=\", theta_val, \"\\n\", sep=\"\")\n#                 print(e) # Print the specific error (should be the non-numeric arg error)\n#                 stop(\"Error during f(theta) execution.\") # Stop execution here to investigate\n#             })\n#             if (!is.numeric(res) || is.na(res)) {\n#                  cat(\"    WARNING: Non-numeric or NA result from test_functions$f[[\", idx, \"]] (alpha=\", current_alpha, \") for theta=\", theta_val, \" -> result:\", res, \"\\n\", sep=\"\")\n#                  # Decide whether to stop or allow NAs\n#                  # stop(\"Non-numeric result from test function f\")\n#                  return(NA) # Allow NA propagation for now\n#             }\n#             return(res)\n#         })\n#         # --- End DEBUG ---\n#         mean(inner_results, na.rm = TRUE)\n#     })\n#     cat(\"  f_means calculated.\\n\")\n# \n# \n#     cat(\"  Calculating g_means...\\n\")\n#     g_means <- sapply(seq_along(test_functions$g), function(idx) {\n#         g <- test_functions$g[[idx]]\n#         current_beta <- betas[idx]\n#          if (!is.numeric(current_beta) || is.na(current_beta)) {\n#             stop(paste(\"Non-numeric or NA beta detected at index\", idx, \"value:\", current_beta))\n#         }\n# \n#         # --- DEBUG: Check inside inner sapply for g_means ---\n#         inner_results <- sapply(theta_samples_g, function(theta_val) {\n#              if (!is.numeric(theta_val) || is.na(theta_val)) {\n#                  stop(\"Non-numeric or NA theta value passed to test function g: \", theta_val)\n#              }\n#              res <- tryCatch({\n#                  g(theta_val) # Apply the test function\n#              }, error = function(e) {\n#                  cat(\"    ERROR occurred in test_functions$g[[\", idx, \"]] (beta=\", current_beta, \") with theta=\", theta_val, \"\\n\", sep=\"\")\n#                  print(e)\n#                  stop(\"Error during g(theta) execution.\")\n#              })\n#              if (!is.numeric(res) || is.na(res)) {\n#                  cat(\"    WARNING: Non-numeric or NA result from test_functions$g[[\", idx, \"]] (beta=\", current_beta, \") for theta=\", theta_val, \" -> result:\", res, \"\\n\", sep=\"\")\n#                  # stop(\"Non-numeric result from test function g\")\n#                  return(NA)\n#              }\n#              return(res)\n#         })\n#         # --- End DEBUG ---\n#         mean(inner_results, na.rm = TRUE)\n#     })\n#     cat(\"  g_means calculated.\\n\")\n# \n#     # --- Rest of the function (Objective, nloptr, Reconstruction, MSE) ---\n#     # Add similar checks inside the objective function if needed, especially around phi_est calls\n# \n#     # Define the objective function for optimization\n#     objective <- function(coeffs) {\n#       # ... (objective function code as before) ...\n#        # Add checks within phi_est if necessary\n#       phi_est <- function(theta) {\n#         # Ensure theta is numeric\n#         if(!is.numeric(theta)) stop(\"Non-numeric theta passed to phi_est\")\n#         sum_f <- sum(sapply(seq_along(alphas), function(a_idx) {\n#             if(!is.numeric(c_alpha[a_idx])) stop(\"Non-numeric c_alpha\")\n#             if(!is.numeric(alphas[a_idx])) stop(\"Non-numeric alpha in phi_est\")\n#             res_f <- f_alpha(theta, alphas[a_idx])\n#             if(!is.numeric(res_f)) stop(\"Non-numeric result from f_alpha in phi_est\")\n#             c_alpha[a_idx] * res_f\n#         }))\n#         sum_g <- sum(sapply(seq_along(betas),  function(b_idx) {\n#              if(!is.numeric(d_beta[b_idx])) stop(\"Non-numeric d_beta\")\n#              if(!is.numeric(betas[b_idx])) stop(\"Non-numeric beta in phi_est\")\n#              res_g <- g_beta(theta, betas[b_idx])\n#              if(!is.numeric(res_g)) stop(\"Non-numeric result from g_beta in phi_est\")\n#              d_beta[b_idx] * res_g\n#         }))\n#         if(!is.numeric(sum_f + sum_g)) stop(\"Non-numeric result from phi_est\")\n#         return(sum_f + sum_g)\n#       }\n#       # ... (rest of objective function) ...\n#     }\n# \n#     # ... (nloptr call, reconstruction, normalization, MSE) ...\n#      cat(\"  Optimization and reconstruction complete for t =\", t, \"\\n\")\n#   } # End of loop over t_values\n# \n#   return(list(phi_true = phi_true_vals, phi_est = phi_est_vals, mse = mse))\n# }\n\n# Run the simulation\nt_values <- seq(0, 100, by = 10)\ntheta_grid <- seq(-pi, pi, length.out = 200)\nresults <- simulate_kpt(t_values, theta_grid, N = 500, lambda = 0.01)\n\n# Quantitative summaries\ncat(\"Mean Squared Errors at each t:\\n\")\nfor (i in seq_along(t_values)) {\n  cat(sprintf(\"t = %.1f: MSE = %.6f\\n\", t_values[i], results$mse[i]))\n}\ncat(sprintf(\"Average MSE: %.6f\\n\", mean(results$mse)))\n\n# Plot the results using plotly\n# True Phase surface\nfig <- plot_ly() %>% \n  add_surface(x = theta_grid, y = t_values, \n              z = results$phi_true, colorscale = \"Viridis\", showscale = FALSE,\n              name = \"true Phi(theta; t)\", scene = 'scene1') %>%\n    layout(scene = list(aspectmode = \"cube\"))\n\n# Estimated Phi surface\nfig <- fig %>%\n  add_surface(x = theta_grid, y = t_values, z = ( results$phi_est), \n              # approx normalize the Phi_est\n             type = \"surface\", colorscale = \"Magma\", name = \"Estimated Phi(theta; t)\",\n             scene = 'scene2', showscale = FALSE) %>%\n    layout(scene2 = list(aspectmode = \"cube\"))\n\n# Difference True - Est Phase surface\nfig <- fig %>%\n  add_surface(x = theta_grid, y = t_values, \n              z = (results$phi_true - (results$phi_est)), \n              type = \"surface\", colorscale = \"Magma\", name = \"Difference True - Estimated Phase\",\n              scene = 'scene3', showscale = FALSE) %>%\n    layout(scene3 = list(aspectmode = \"cube\"))\n\nfig <- fig %>%\n    layout(title = \"True (left), Estimated (middle), and Difference Kime-Phase Surfaces\",\n      showlegend = TRUE, legend = list(x = 0.85, y = 0.1),\n      annotations = \n        list(list(showarrow = FALSE, text = '(true)', xref = 'paper',\n                    yref = 'paper', x = 0.15, y = 0.95,\n                    xanchor = 'center', yanchor = 'bottom', font = list(size = 16)),\n        list(showarrow = FALSE, text = '(estimated)', xref = 'paper',\n             yref = 'paper', x = 0.5, y = 0.95, xanchor = 'center',\n             yanchor = 'bottom', font = list(size = 16)),\n        list(showarrow = FALSE, text = '(difference)', xref = 'paper',\n             yref = 'paper', x = 0.85, y = 0.95, xanchor = 'center',\n             yanchor = 'bottom', font = list(size = 16))\n      )\n    )\n\n  # Add synchronization\n  fig <- fig %>% onRender(\n    \"function(el) {\n      el.on('plotly_relayout', function(d) {\n        const camera = Object.keys(d).filter((key) => /\\\\.camera$/.test(key));\n        if (camera.length) {\n          const scenes = Object.keys(el.layout).filter((key) => /^scene\\\\d*/.test(key));\n          const new_layout = {};\n          scenes.forEach(key => {\n            new_layout[key] = {...el.layout[key], camera: {...d[camera]}};\n          });\n          Plotly.relayout(el, new_layout);\n        }\n      });\n    }\"\n  )\nfig",
      "line_count": 389
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "# Load required libraries\nlibrary(circular)  # For von Mises kernel\nlibrary(plotly)    # For 3D surface plotting\nlibrary(htmlwidgets)  # For saving as SVG\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Manual Hilbert transform function with zero-padding\nhilbert <- function(x, pad_factor = 2) {\n  n <- length(x)\n  \n  # Zero-pad the signal to reduce edge effects\n  pad_length <- n * (pad_factor - 1)\n  x_padded <- c(x, rep(0, pad_length))\n  n_padded <- length(x_padded)\n  \n  # Compute the FFT of the padded signal\n  X <- fft(x_padded)\n  \n  # Frequency indices for the padded signal\n  if (n_padded %% 2 == 0) {\n    pos_freq <- 2:(n_padded/2)\n    neg_freq <- (n_padded/2+2):n_padded\n    nyquist <- n_padded/2 + 1\n  } else {\n    pos_freq <- 2:((n_padded+1)/2)\n    neg_freq <- ((n_padded+1)/2+1):n_padded\n    nyquist <- NULL\n  }\n  \n  # Create the Hilbert transform multiplier\n  h <- rep(0, n_padded)\n  h[1] <- 1\n  if (length(pos_freq) > 0) h[pos_freq] <- 2\n  if (!is.null(nyquist)) h[nyquist] <- 1\n  if (length(neg_freq) > 0) h[neg_freq] <- 0\n  \n  # Apply the multiplier and compute the inverse FFT\n  X_analytic <- X * h\n  s_analytic_padded <- fft(X_analytic, inverse = TRUE) / n_padded\n  \n  # Trim the padded region to return to original length\n  s_analytic <- s_analytic_padded[1:n]\n  \n  return(s_analytic)\n}\n\n# Generate the signal\nT <- 10000\nt <- seq(0, 30, length.out = T)\ndt <- t[2] - t[1]\nA <- 1 + 0.5 * sin(2 * pi * 0.1 * t)\nphi_true <- sapply(t, function(ti) {\n  if (ti < 5) 2 * pi * 0.3 * ti\n  else 2 * pi * 0.5 * ti + pi/2\n})\ns <- A * exp(1i * phi_true)\nx <- Re(s) + rnorm(T, 0, 0.05)  # Add noise\n\n# Compute the analytic signal with zero-padding\ns_analytic <- hilbert(x, pad_factor = 2)\n\n# Operator K1: Multiplication by t\nK1_s <- t * s_analytic\nphi_1 <- Arg(s_analytic) %% (2 * pi) - pi\n\n# Operator K2: Derivative -i d/dt using centered difference\nK2_s <- rep(0+0i, T)\nK2_s[1] <- -1i * (s_analytic[2] - s_analytic[1]) / dt\nfor (i in 2:(T-1)) {\n  K2_s[i] <- -1i * (s_analytic[i+1] - s_analytic[i-1]) / (2 * dt)\n}\nK2_s[T] <- -1i * (s_analytic[T] - s_analytic[T-1]) / dt\nphi_2 <- Arg(K2_s) %% (2 * pi) - pi\n\n# Operator K3: Simplified wavelet convolution with fixed scale\nmorlet_wavelet <- function(t, a = 2, omega0 = 5) {\n  (1/sqrt(a)) * exp(-(t/a)^2/2) * cos(omega0 * t/a) * (abs(t) <= a)\n}\nK3_s <- rep(0+0i, T)\nfor (i in 1:T) {\n  tau <- t - t[i]\n  psi <- morlet_wavelet(tau)\n  K3_s[i] <- sum(psi * s_analytic) * dt\n}\nphi_3 <- Arg(K3_s) %% (2 * pi) - pi\n\n# Compute circular dispersion for weights\ncircular_dispersion <- function(phi, window = 100) {\n  n <- length(phi)\n  disp <- numeric(n)\n  for (i in 1:n) {\n    idx <- max(1, i - window/2):min(n, i + window/2)\n    mean_vec <- mean(exp(1i * phi[idx]))\n    disp[i] <- 1 - abs(mean_vec)\n  }\n  disp\n}\n\ndisp_1 <- circular_dispersion(phi_1)\ndisp_2 <- circular_dispersion(phi_2)\ndisp_3 <- circular_dispersion(phi_3)\n\n# Ensemble phase estimates\nepsilon <- 1e-6\nw_1 <- 1 / (disp_1 + epsilon)\nw_2 <- 1 / (disp_2 + epsilon)\nw_3 <- 1 / (disp_3 + epsilon)\nphi_hat <- Arg(w_1 * exp(1i * phi_1) + w_2 * exp(1i * phi_2) + w_3 * exp(1i * phi_3))\nphi_hat <- phi_hat %% (2 * pi) - pi\n\n# Empirical shift correction using cross-correlation\n# Compute cross-correlation between phi_hat and phi_true\nphi_true_wrapped <- phi_true %% (2 * pi) - pi\ncross_corr <- ccf(phi_hat, phi_true_wrapped, lag.max = 1000, plot = FALSE)\nlag <- cross_corr$lag[which.max(abs(cross_corr$acf))]\nshift_seconds <- lag * dt\ncat(sprintf(\"Detected time shift: %.3f seconds\\n\", shift_seconds))\n\n# Apply the shift to phi_hat\nshift_steps <- round(abs(lag))\nif (lag > 0) {\n  # phi_hat is delayed, shift it back\n  phi_hat_shifted <- rep(0, T)\n  phi_hat_shifted[1:(T-shift_steps)] <- phi_hat[(shift_steps+1):T]\n  phi_hat_shifted[(T-shift_steps+1):T] <- phi_hat[T-shift_steps]\n} else {\n  # phi_hat is advanced, shift it forward\n  phi_hat_shifted <- rep(0, T)\n  phi_hat_shifted[(shift_steps+1):T] <- phi_hat[1:(T-shift_steps)]\n  phi_hat_shifted[1:shift_steps] <- phi_hat[1]\n}\nphi_hat <- phi_hat_shifted\n\n# Estimate Phi(theta; t) using von Mises KDE\ntheta_grid <- seq(-pi, pi, length.out = 200)\nt_values <- seq(0, 30, by = 3)\nwindow <- 500  # Time window for KDE\nkappa <- 10  # Concentration parameter\nphi_true_dist <- matrix(NA, nrow = length(t_values), ncol = length(theta_grid))\nphi_est_dist <- matrix(NA, nrow = length(t_values), ncol = length(theta_grid))\n\nfor (i in seq_along(t_values)) {\n  t_center <- t_values[i]\n  idx <- which(abs(t - t_center) <= window * dt / 2)\n  \n  # True distribution\n  phi_window <- (phi_true[idx] %% (2 * pi)) - pi\n  kde_true <- sapply(theta_grid, function(theta) {\n    mean(dvonmises(theta - phi_window, mu = 0, kappa = kappa))\n  })\n  phi_true_dist[i, ] <- kde_true / sum(kde_true * (theta_grid[2] - theta_grid[1]))\n  \n  # Estimated distribution\n  phi_hat_window <- phi_hat[idx]\n  kde_est <- sapply(theta_grid, function(theta) {\n    mean(dvonmises(theta - phi_hat_window, mu = 0, kappa = kappa))\n  })\n  phi_est_dist[i, ] <- kde_est / sum(kde_est * (theta_grid[2] - theta_grid[1]))\n}\n\n# Compute uncertainty relation Delta K1 * Delta K2\ndelta_K1 <- sd(Re(K1_s))\ndelta_K2 <- sd(Re(K2_s))\nuncertainty_product <- delta_K1 * delta_K2\ncat(sprintf(\"Uncertainty Product Delta K1 * Delta K2: %.6f (should be >= 0.5)\\n\", uncertainty_product))\n\n# Plot 1: Phase trajectory\nfig_phase <- plot_ly() %>%\n  add_trace(x = t, y = phi_true %% (2 * pi) - pi, type = \"scatter\", mode = \"lines\",\n            name = \"True Phase\", line = list(color = \"black\")) %>%\n  add_trace(x = t, y = phi_hat, type = \"scatter\", mode = \"lines\",\n            name = \"Estimated Phase\", line = list(color = \"red\")) %>%\n  layout(\n    title = \"Approach 2: True vs Estimated Phase (Shift Corrected)\",\n    xaxis = list(title = \"Time (s)\"),\n    yaxis = list(title = \"Phase (radians)\")\n  )\n\n# Plot 2: Distribution Phi(theta; t)\nfig_true_dist <- plot_ly(\n  x = theta_grid,\n  y = t_values,\n  z = phi_true_dist,\n  type = \"surface\",\n  colorscale = \"Viridis\",\n  name = \"True Phi\",\n  showscale = TRUE\n)\n\nfig_est_dist <- plot_ly(\n  x = theta_grid,\n  y = t_values,\n  z = phi_est_dist +1,   # offset the Phi_est to see better the differences\n  type = \"surface\",\n  colorscale = \"Magma\",\n  name = \"Estimated Phi\",\n  showscale = TRUE\n)\n\nfig_dist <- subplot(fig_true_dist, fig_est_dist, nrows = 1, shareX = TRUE, shareY = TRUE) %>%\n  layout(\n    title = \"Approach 2: True vs Estimated Phi(theta; t) (Shift Corrected)\",\n    scene = list(\n      xaxis = list(title = \"θ (radians)\"),\n      yaxis = list(title = \"t (seconds)\"),\n      zaxis = list(title = \"Phi(θ; t)\")\n    ),\n    scene2 = list(\n      xaxis = list(title = \"θ (radians)\"),\n      yaxis = list(title = \"t (seconds)\"),\n      zaxis = list(title = \"Phi_est(θ; t)\")\n    )\n  )\n\n# Save plots\n# htmlwidgets::saveWidget(fig_phase, \"approach2_phase_shift_corrected.html\", selfcontained = TRUE)\n# htmlwidgets::saveWidget(fig_dist, \"approach2_distribution_shift_corrected.html\", selfcontained = TRUE)\n\n# Display plots\nfig_phase\nfig_dist\n\n# Compute MSE for the distribution\nmse <- mean((phi_true_dist - phi_est_dist)^2)\ncat(sprintf(\"Mean Squared Error for Phi(theta; t): %.6f\\n\", mse))",
      "line_count": 227
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "# Complete Implementation of Unified Kime-Phase Tomography\n# with Application to fMRI Data Analysis\n\nlibrary(signal)\nlibrary(circular)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(RColorBrewer)\n\n# --------------------------------------------------------------\n# Core Kime-Phase Tomography Functions\n# --------------------------------------------------------------\n\n#' Compute analytic signal via Hilbert transform\ncompute_analytic_signal <- function(x) {\n  n <- length(x)\n  X <- fft(x)\n  \n  # Create Hilbert transform filter\n  h <- numeric(n)\n  h[1] <- 1  # DC component\n  h[2:(n/2)] <- 2  # Positive frequencies\n  h[(n/2 + 1):n] <- 0  # Negative frequencies\n  \n  # Apply filter and compute inverse FFT\n  analytic <- fft(h * X, inverse = TRUE) / n\n  return(analytic)\n}\n\n#' Create hemodynamic response function\ncreate_hrf <- function(t_hrf, type = \"canonical\") {\n  if (type == \"gamma\") {\n    hrf <- dgamma(t_hrf, shape = 6, scale = 1)\n  } else if (type == \"double-gamma\") {\n    hrf <- dgamma(t_hrf, shape = 6, scale = 1) - 0.1 * dgamma(t_hrf, shape = 16, scale = 1)\n  } else { # canonical\n    hrf <- (t_hrf/1.5)^3 * exp(-(t_hrf/1.5)) / gamma(4)\n  }\n  return(hrf / max(hrf))  # Normalize\n}\n\n# PLOT the HDR function\n# Create time points for the HRF\nt_hrf <- seq(0, 20, by = 0.1)\n\n# Calculate the three HRF types\nhrf_canonical <- create_hrf(t_hrf, type = \"canonical\")\nhrf_gamma <- create_hrf(t_hrf, type = \"gamma\")\nhrf_double_gamma <- create_hrf(t_hrf, type = \"double-gamma\")\n\n# Create the interactive plot\nplot_ly() %>%\n  add_trace(x = t_hrf, y = hrf_canonical, type = \"scatter\",\n    mode = \"lines\", name = \"Canonical HRF\", line = list(color = \"#3366CC\", width = 2),\n    hoverinfo = \"text\", text = ~paste(\"Time:\", round(t_hrf, 1), \"s<br>\", \n                 \"Amplitude:\", round(hrf_canonical, 3))) %>%\n  add_trace(x = t_hrf, y = hrf_gamma, type = \"scatter\",\n    mode = \"lines\", name = \"Gamma HRF\", line = list(color = \"#DC3912\", width = 2, dash = \"dash\"),\n    hoverinfo = \"text\", text = ~paste(\"Time:\", round(t_hrf, 1), \"s<br>\", \n                 \"Amplitude:\", round(hrf_gamma, 3))) %>%\n  add_trace(x = t_hrf, y = hrf_double_gamma, type = \"scatter\",\n    mode = \"lines\", name = \"Double-Gamma HRF\", line = list(color = \"#109618\", width = 2, dash = \"dot\"),\n    hoverinfo = \"text\", text = ~paste(\"Time:\", round(t_hrf, 1), \"s<br>\", \n                 \"Amplitude:\", round(hrf_double_gamma, 3))) %>%\n  layout(title = list(text = \"Hemodynamic Response Function (HRF) Models\",font = list(size = 18)),\n    xaxis = list(title = \"Time (seconds)\", titlefont = list(size = 14),\n      zeroline = TRUE, zerolinecolor = \"#CCCCCC\", gridcolor = \"rgba(0,0,0,0.1)\"),\n    yaxis = list(title = \"Normalized Amplitude\", titlefont = list(size = 14),\n      zeroline = TRUE, zerolinecolor = \"#CCCCCC\", gridcolor = \"rgba(0,0,0,0.1)\",\n      range = c(-0.2, 1.05)  # Allow space for negative values in double-gamma\n    ),\n    legend = list(x = 0.3, y = 0.99, xanchor = \"left\", yanchor = \"top\",\n      bgcolor = \"rgba(255,255,255,0.7)\", bordercolor = \"#CCCCCC\", borderwidth = 1),\n    shapes = list(# Add a horizontal line at y=0\n      list(type = \"line\", x0 = 0, x1 = 20, y0 = 0, y1 = 0, line=list(color=\"#CCCCCC\", width = 1))),\n    annotations = list(# Add annotations explaining each model\n      list(x = 5, y = 0.9, text = \"Canonical: (t/1.5)³ exp(-t/1.5) / Γ(4)\",\n        showarrow = FALSE, font = list(color = \"#3366CC\")),\n      list(x = 5, y = 0.8, text = \"Gamma: Γ(shape=6, scale=1)\", showarrow = FALSE,\n        font = list(color = \"#DC3912\")),\n      list(x = 7, y = 0.7, text = \"Double-Gamma: Γ(shape=6, scale=1) - 0.1×Γ(shape=16, scale=1)\",\n        showarrow = FALSE, font = list(color = \"#109618\"))),\n    hovermode = \"closest\", hoverlabel = list(bgcolor = \"white\",font = list(size = 12)),\n    paper_bgcolor = \"white\", plot_bgcolor = \"white\", margin = list(t=80, r=50, b=60, l=60)) %>%\n  # Add buttons to highlight individual curves\n  config(modeBarButtonsToAdd = list(\n      list(name = \"Show All\", icon = list(path = \"M22 12c0 5.5-4.5 10-10 10S2 17.5 2 12 6.5 2 12 2s10 4.5 10 10zm-2 0c0-4.4-3.6-8-8-8s-8 3.6-8 8 3.6 8 8 8 8-3.6 8-8zm-8 4c2.2 0 4-1.8 4-4s-1.8-4-4-4-4 1.8-4 4 1.8 4 4 4z\"),\n        click = htmlwidgets::JS(\"function(gd) { Plotly.restyle(gd, 'visible', true); }\"))),\n    displaylogo = FALSE\n  )\n\n# Define complex time-varying phase with two jumps\ncomplex_mu <- function(t) {\n  ifelse(t < 10, \n         0.2*t,\n         ifelse(t < 20, \n                0.2*10 + 0.4*(t-10) + pi/2, \n                0.2*10 + 0.4*10 + pi/2 + 0.3*(t-20) + pi/4))\n}\n\n# Time-varying concentration parameter\ncomplex_kappa <- function(t) {\n  5 + 2*sin(2*pi*t/30) + t/10\n}\n\n#' Generate synthetic fMRI data with known phase properties\ngenerate_synthetic_fmri <- \n  function(N = 10, T = 500, tmax = 30,\n           mu_func = complex_mu, # function(t) 2*pi*0.1*t,\n           kappa_func = complex_kappa, # function(t) 5 + 2*sin(2*pi*t/tmax), # +t/10,\n           noise_sd = 0.1, hrf_type = \"canonical\") {\n  # Time vector\n  t <- seq(0, tmax, length.out = T)\n  dt <- t[2] - t[1]\n  \n  # HRF convolution kernel\n  t_hrf <- seq(0, 20, length.out = 100)\n  hrf <- create_hrf(t_hrf, type = hrf_type)\n  \n  # Generate signals for each repetition\n  true_phases <- matrix(NA, nrow = N, ncol = T)\n  bold_signals <- matrix(NA, nrow = N, ncol = T)\n  \n  for (n in 1:N) {\n    # Generate true phase from von Mises distribution at each time point\n    true_mean <- sapply(t, mu_func)\n    true_kappa <- sapply(t, kappa_func)\n    \n    # Sample phases from von Mises distribution\n    phases <- numeric(T)\n    for (i in 1:T) {\n      phases[i] <- rvonmises(1, mu = true_mean[i], kappa = true_kappa[i])\n    }\n    \n    # Create complex signal\n    neural_signal <- exp(1i * phases)\n    \n    # Convolve with HRF to get BOLD signal\n    bold <- Re(convolve(neural_signal, rev(hrf), type = \"open\"))[1:T]\n    \n    # Add noise\n    bold_noisy <- bold + rnorm(T, 0, noise_sd)\n    \n    # Store results\n    true_phases[n, ] <- phases\n    bold_signals[n, ] <- bold_noisy\n  }\n  \n  # Modify true_mean generation:\n  true_mean <- sapply(t, mu_func) %% (2*pi)  # Wrap to [0, 2π)\n  true_mean <- ifelse(true_mean > pi, true_mean - 2*pi, true_mean)  # Convert to [-π, π)\n  \n  # Return results with ground truth\n  return(list(\n    t = t,\n    bold = bold_signals,\n    true_phases = true_phases,\n    true_mean = true_mean,\n    true_kappa = true_kappa,\n    hrf = hrf\n  ))\n}\n\n#' Kime operator K1 (time-domain)\nK1_operator <- function(s, t) {\n  return(t * s)\n}\n\n#' Kime operator K2 (frequency-domain)\nK2_operator <- function(s, t) {\n  dt <- t[2] - t[1]\n  \n  # Approximate derivative using central differences\n  ds <- c(0, diff(s)) / dt\n  \n  # Return -i * d/dt\n  return(-1i * ds)\n}\n\n#' Compute continuous wavelet transform\ncompute_cwt <- function(s, t, scales) {\n  # Define Morlet wavelet\n  morlet <- function(t, scale, w0 = 6) {\n    norm <- (pi^(-0.25)) / sqrt(scale)\n    return(norm * exp(1i * w0 * t / scale) * exp(-0.5 * (t / scale)^2))\n  }\n  \n  n <- length(t)\n  wt <- matrix(0, length(scales), n)\n  \n  # Compute wavelet transform for each scale\n  for (i in seq_along(scales)) {\n    scale <- scales[i]\n    \n    # Create wavelet at this scale\n    t_kernel <- seq(-5 * scale, 5 * scale, length.out = min(201, n))\n    psi <- morlet(t_kernel, scale)\n    \n    # Convolve signal with wavelet\n    conv_full <- convolve(s, rev(psi), type = \"open\")\n    \n    # Extract relevant part of convolution\n    start_idx <- floor(length(conv_full) / 2) - floor(n / 2) + 1\n    wt[i, ] <- conv_full[start_idx:(start_idx + n - 1)]\n  }\n  \n  return(wt)\n}\n\n#' Kime operator K3 (scale-domain)\nK3_operator <- function(s, t, scales = NULL) {\n  # Set default scales if not provided\n  if (is.null(scales)) {\n    dt <- t[2] - t[1]\n    fs <- 1/dt\n    f_min <- 0.01\n    f_max <- fs/10\n    scales <- 1 / (2 * pi * seq(f_min, f_max, length.out = 32))\n  }\n  \n  # Compute wavelet transform\n  wt <- compute_cwt(s, t, scales)\n  \n  # For each time point, find scale with maximum power\n  dominant <- numeric(length(t))\n  for (i in 1:ncol(wt)) {\n    dominant[i] <- wt[which.max(abs(wt[, i])), i]\n  }\n  \n  return(dominant)\n}\n\n#' Project signal to RKHS\nrkhs_projection <- function(s, t, sigma = 0.5) {\n  dt <- t[2] - t[1]\n  n <- length(t)\n  projected <- numeric(n)\n  \n  # Define Gaussian kernel\n  K <- function(t1, t2) exp(-((t1 - t2)^2) / (2 * sigma^2))\n  \n  # Apply kernel to signal\n  for (i in 1:n) {\n    kernel_vals <- sapply(t, function(tj) K(t[i], tj))\n    projected[i] <- sum(s * kernel_vals) * dt\n  }\n  \n  return(projected)\n}\n\n#' Estimate von Mises parameters from complex-valued data\nestimate_vonmises <- function(z) {\n  # Compute resultant length\n  R <- abs(mean(z))\n  \n  # Compute mean direction\n  mu <- Arg(mean(z))\n  \n  # Estimate concentration parameter\n  if (R >= 0.999) {\n    kappa <- 100  # High concentration\n  } else if (R <= 0.001) {\n    kappa <- 0    # Uniform distribution\n  } else {\n    # Solve for kappa using Bessel function ratio\n    kappa_est <- function(kappa) {\n      besselI(kappa, 1) / besselI(kappa, 0) - R\n    }\n    \n    # Solve numerically with appropriate bounds\n    kappa <- tryCatch({\n      uniroot(kappa_est, interval = c(0, 100))$root\n    }, error = function(e) {\n      # Fallback approximation\n      if (R < 0.53) {\n        2 * R + R^3 + 5 * R^5 / 6\n      } else {\n        1 / (2 * (1 - R) - (1 - R)^2 - (1 - R)^3)\n      }\n    })\n  }\n  \n  return(list(mu = mu, kappa = kappa))\n}\n\n#' Detect phase jumps in a phase time series\ndetect_phase_jumps <- function(phase, t, threshold_mult = 5) {\n  # Unwrap phase for continuous derivative\n  unwrapped <- phase\n  for (i in 2:length(phase)) {\n    diff_phase <- phase[i] - phase[i-1]\n    if (abs(diff_phase) > pi) {\n      unwrapped[i:length(phase)] <- unwrapped[i:length(phase)] - \n        sign(diff_phase) * 2 * pi\n    }\n  }\n  \n  # Compute numerical derivative\n  dt <- t[2] - t[1]\n  phase_deriv <- c(0, diff(unwrapped) / dt)\n  \n  # Compute median and MAD for robust threshold\n  med_deriv <- median(phase_deriv, na.rm = TRUE)\n  mad_deriv <- median(abs(phase_deriv - med_deriv), na.rm = TRUE)\n  \n  # Set threshold\n  threshold <- med_deriv + threshold_mult * mad_deriv\n  \n  # Find jumps\n  jump_indices <- which(abs(phase_deriv) > threshold)\n  \n  # Merge consecutive jumps\n  if (length(jump_indices) > 0) {\n    merged_indices <- jump_indices[1]\n    \n    for (i in 2:length(jump_indices)) {\n      if (jump_indices[i] - jump_indices[i-1] > 3) {\n        merged_indices <- c(merged_indices, jump_indices[i])\n      }\n    }\n    \n    # Create output data frame\n    jumps <- data.frame(\n      time = t[merged_indices],\n      magnitude = phase_deriv[merged_indices]\n    )\n    \n    return(jumps)\n  } else {\n    return(data.frame(time = numeric(0), magnitude = numeric(0)))\n  }\n}\n\n#' Unified Kime-Phase Tomography algorithm\nunified_kpt <- function(bold_signals, t, preprocess = TRUE, kernel_sigma = 0.5) {\n  N <- nrow(bold_signals)\n  T <- ncol(bold_signals)\n  dt <- t[2] - t[1]\n  \n  # Store results\n  phi_K1 <- matrix(NA, N, T)\n  phi_K2 <- matrix(NA, N, T)\n  phi_K3 <- matrix(NA, N, T)\n  \n  # Step 1: Preprocessing\n  analytic_signals <- matrix(complex(real = 0, imaginary = 0), N, T)\n  \n  for (n in 1:N) {\n    signal <- bold_signals[n,]\n    \n    if (preprocess) {\n      # Bandpass filter (0.01-0.1 Hz)\n      nyquist <- 1/(2*dt)\n      lowcut <- 0.01/nyquist\n      highcut <- 0.1/nyquist\n      \n      butter_filter <- signal::butter(2, c(lowcut, highcut), type = \"pass\")\n      filtered <- signal::filtfilt(butter_filter, signal)\n      \n      # Get analytic signal via Hilbert transform\n      analytic_signals[n,] <- compute_analytic_signal(filtered)\n    } else {\n      # Skip filtering\n      analytic_signals[n,] <- compute_analytic_signal(signal)\n    }\n    \n    # Step 2: Multi-basis measurement\n    \n    # Apply RKHS projection if requested\n    if (kernel_sigma > 0) {\n      s_rkhs <- rkhs_projection(analytic_signals[n,], t, kernel_sigma)\n      \n      # Apply operators in each domain\n      phi_K1[n,] <- Arg(K1_operator(s_rkhs, t))\n      phi_K2[n,] <- Arg(K2_operator(s_rkhs, t))\n      phi_K3[n,] <- Arg(K3_operator(s_rkhs, t))\n    } else {\n      # Apply operators directly without RKHS projection\n      phi_K1[n,] <- Arg(K1_operator(analytic_signals[n,], t))\n      phi_K2[n,] <- Arg(K2_operator(analytic_signals[n,], t))\n      phi_K3[n,] <- Arg(K3_operator(analytic_signals[n,], t))\n    }\n  }\n  \n  # Step 3: Basis uncertainty quantification\n  # Compute circular variance for each basis and time point\n  v_K1 <- v_K2 <- v_K3 <- numeric(T)\n  \n  for (i in 1:T) {\n    v_K1[i] <- 1 - abs(mean(exp(1i * phi_K1[,i])))\n    v_K2[i] <- 1 - abs(mean(exp(1i * phi_K2[,i])))\n    v_K3[i] <- 1 - abs(mean(exp(1i * phi_K3[,i])))\n  }\n  \n  # Calculate weights inversely proportional to variance\n  w1 <- w2 <- w3 <- numeric(T)\n  for (i in 1:T) {\n    # Avoid division by zero with a small epsilon\n    eps <- 1e-10\n    total_weight <- 1/(v_K1[i] + eps) + 1/(v_K2[i] + eps) + 1/(v_K3[i] + eps)\n    \n    w1[i] <- 1/(v_K1[i] + eps) / total_weight\n    w2[i] <- 1/(v_K2[i] + eps) / total_weight\n    w3[i] <- 1/(v_K3[i] + eps) / total_weight\n  }\n  \n  # Step 4: Phase distribution estimation\n  mu_est <- kappa_est <- numeric(T)\n  \n  for (i in 1:T) {\n    # Compute moments\n    m1 <- mean(exp(1i * phi_K1[,i]))\n    m2 <- mean(exp(1i * phi_K2[,i]))\n    m3 <- mean(exp(1i * phi_K3[,i]))\n    \n    # Weighted combination\n    m_combined <- w1[i] * m1 + w2[i] * m2 + w3[i] * m3\n    \n    # Estimate von Mises parameters\n    vm_params <- estimate_vonmises(c(m_combined))\n    mu_est[i] <- vm_params$mu\n    kappa_est[i] <- vm_params$kappa\n  }\n  \n  # Detect phase jumps\n  jumps <- detect_phase_jumps(mu_est, t)\n  \n  return(list(\n    t = t,\n    mu = mu_est,\n    kappa = kappa_est,\n    phi_K1 = phi_K1,\n    phi_K2 = phi_K2,\n    phi_K3 = phi_K3,\n    weights = list(w1 = w1, w2 = w2, w3 = w3),\n    variances = list(v_K1 = v_K1, v_K2 = v_K2, v_K3 = v_K3),\n    phase_jumps = jumps\n  ))\n}\n\n#' Evaluate KPT performance\nevaluate_kpt <- function(kpt, ground_truth) {\n  # Compute circular mean absolute error for phase\n  cmae <- function(true, est) {\n    # Ensure phases are in [-pi, pi]\n    true <- ((true + pi) %% (2*pi)) - pi\n    est <- ((est + pi) %% (2*pi)) - pi\n    \n    # Compute circular difference\n    diff <- abs(((true - est + pi) %% (2*pi)) - pi)\n    return(mean(diff))\n  }\n  \n  # Compute RMSE for concentration\n  kappa_rmse <- sqrt(mean((ground_truth$true_kappa - kpt$kappa)^2))\n  \n  # Compute circular MAE for phase\n  phase_cmae <- cmae(ground_truth$true_mean, kpt$mu)\n  \n  # Compute phase jump detection accuracy\n  true_jumps <- detect_phase_jumps(ground_truth$true_mean, kpt$t)\n  \n  # Match detected jumps to true jumps\n  jump_detection <- list(\n    true_jumps = true_jumps,\n    detected_jumps = kpt$phase_jumps\n  )\n  \n  # Return metrics\n  return(list(\n    phase_cmae = phase_cmae,\n    kappa_rmse = kappa_rmse,\n    jump_detection = jump_detection\n  ))\n}\n\n#' Visualize Kime-Phase estimation results\nvisualize_kpt_results <- function(kpt, ground_truth = NULL) {\n  # Create basic data frame\n  df <- data.frame(\n    t = kpt$t,\n    mu_est = kpt$mu,\n    kappa_est = kpt$kappa\n  )\n  \n  # Add ground truth if available\n  has_truth <- !is.null(ground_truth)\n  if (has_truth) {\n    df$mu_true <- ground_truth$true_mean\n    df$kappa_true <- ground_truth$true_kappa\n  }\n  \n  # Phase plot\n  p1 <- plot_ly(df, x = ~t) %>%\n    add_trace(y = ~mu_est, type = \"scatter\", mode = \"lines\", \n              name = \"Estimated Mean Phase\", line = list(color = \"blue\", width = 2))\n  \n  if (has_truth) {\n    p1 <- p1 %>% add_trace(y = ~mu_true, type = \"scatter\", mode = \"lines\", \n                           name = \"True Mean Phase µ(t)\", line = list(color = \"black\", dash = \"dot\"))\n  }\n  \n  p1 <- p1 %>% layout(\n    title = \"Estimated Mean Phase µ(t)\",\n    xaxis = list(title = \"Time\"),\n    yaxis = list(title = \"Phase (radians)\")\n  )\n  \n  # Concentration plot\n  p2 <- plot_ly(df, x = ~t) %>%\n    add_trace(y = ~kappa_est, type = \"scatter\", mode = \"lines\", \n              name = \"Estimated Concentration\", line = list(color = \"red\", width = 2))\n  \n  if (has_truth) {\n    p2 <- p2 %>% add_trace(y = ~kappa_true, type = \"scatter\", mode = \"lines\", \n                           name = \"True Concentration κ(t)\", line = list(color = \"black\", dash = \"dot\"))\n  }\n  \n  p2 <- p2 %>% layout(\n    title = \"Estimated Concentration κ(t)\",\n    xaxis = list(title = \"Time\"),\n    yaxis = list(title = \"Concentration Parameter\", range = list(0.0, 40.0))\n  )\n  \n  # Create weights plot\n  p3 <- plot_ly(x = kpt$t) %>%\n    add_trace(y = kpt$weights$w1, type = \"scatter\", mode = \"lines\", \n              name = \"Time-Domain (K₁)\", line = list(color = \"blue\")) %>%\n    add_trace(y = kpt$weights$w2, type = \"scatter\", mode = \"lines\", \n              name = \"Frequency-Domain (K₂)\", line = list(color = \"red\")) %>%\n    add_trace(y = kpt$weights$w3, type = \"scatter\", mode = \"lines\", \n              name = \"Scale-Domain (K₃)\", line = list(color = \"green\")) %>%\n    layout(\n      title = \"Basis Weights Over Time\",\n      xaxis = list(title = \"Time\"),\n      yaxis = list(title = \"Weight\")\n    )\n  \n  # Combine plots\n  fig <- subplot(p1, p2, p3, nrows = 3, shareX = TRUE, titleY = TRUE) %>%\n    layout(\n      title = \"Unified Kime-Phase Tomography Results\",\n      showlegend = TRUE\n    )\n  \n  return(fig)\n}\n\n# --------------------------------------------------------------\n# Main Analysis Script\n# --------------------------------------------------------------\n\nrun_kpt_analysis <- function() {\n  # Generate synthetic fMRI data\n  set.seed(123)\n  fmri_data <- generate_synthetic_fmri(\n    N = 15,       # 15 repetitions/subjects\n    T = 600,      # 600 timepoints\n    tmax = 30,    # 30 seconds\n    mu_func = complex_mu,\n    kappa_func = complex_kappa,\n    noise_sd = 100, # 20 or 0.3,\n    hrf_type = \"double-gamma\"\n  )\n  \n  # Define different parameter configurations to test\n  config1 <- list(preprocess = TRUE, kernel_sigma = 0.5)   # RKHS with moderate smoothing\n  config2 <- list(preprocess = TRUE, kernel_sigma = 0.0)   # No RKHS projection\n  config3 <- list(preprocess = FALSE, kernel_sigma = 0.5)  # No preprocessing, with RKHS\n  \n  # Apply KPT with different configurations\n  cat(\"Running KPT with configuration 1 (RKHS + Preprocess)...\\n\")\n  kpt1 <- unified_kpt(fmri_data$bold, fmri_data$t, \n                      preprocess = config1$preprocess, \n                      kernel_sigma = config1$kernel_sigma)\n  \n  cat(\"Running KPT with configuration 2 (No RKHS + Preprocess)...\\n\")\n  kpt2 <- unified_kpt(fmri_data$bold, fmri_data$t, \n                      preprocess = config2$preprocess, \n                      kernel_sigma = config2$kernel_sigma)\n  \n  cat(\"Running KPT with configuration 3 (RKHS + No Preprocess)...\\n\")\n  kpt3 <- unified_kpt(fmri_data$bold, fmri_data$t, \n                      preprocess = config3$preprocess, \n                      kernel_sigma = config3$kernel_sigma)\n  \n  # Evaluate performance\n  metrics1 <- evaluate_kpt(kpt1, fmri_data)\n  metrics2 <- evaluate_kpt(kpt2, fmri_data)\n  metrics3 <- evaluate_kpt(kpt3, fmri_data)\n  \n  # Create comparison table\n  performance_table <- data.frame(\n    Configuration = c(\"RKHS + Preprocess\", \"No RKHS + Preprocess\", \"RKHS + No Preprocess\"),\n    Phase_CMAE = c(metrics1$phase_cmae, metrics2$phase_cmae, metrics3$phase_cmae),\n    Kappa_RMSE = c(metrics1$kappa_rmse, metrics2$kappa_rmse, metrics3$kappa_rmse)\n  )\n  \n  cat(\"\\nPerformance Comparison:\\n\")\n  performance_table\n  \n  # Visualize results for the best configuration\n  fig <- visualize_kpt_results(kpt1, fmri_data)\n  fig\n  \n  # Analyze basis dominance around jumps\n  # Extract basis weights for analysis\n  basis_weights <- data.frame(\n    t = fmri_data$t,\n    K1_weight = kpt1$weights$w1,\n    K2_weight = kpt1$weights$w2,\n    K3_weight = kpt1$weights$w3\n  )\n  \n  # Find time points where each basis dominates\n  K1_dominant <- which(basis_weights$K1_weight > \n                      pmax(basis_weights$K2_weight, basis_weights$K3_weight))\n  K2_dominant <- which(basis_weights$K2_weight > \n                      pmax(basis_weights$K1_weight, basis_weights$K3_weight))\n  K3_dominant <- which(basis_weights$K3_weight > \n                      pmax(basis_weights$K1_weight, basis_weights$K2_weight))\n  \n  # Detect true jumps\n  true_jumps <- detect_phase_jumps(fmri_data$true_mean, fmri_data$t)\n  \n  # Analyze basis dominance around phase jumps\n  jump_basis_dominance <- lapply(true_jumps$time, function(tj) {\n    window <- 1.0  # 1-second window around jump\n    indices <- which(fmri_data$t >= tj - window & fmri_data$t <= tj + window)\n    \n    # Count dominant bases in this region\n    K1_count <- sum(indices %in% K1_dominant)\n    K2_count <- sum(indices %in% K2_dominant)\n    K3_count <- sum(indices %in% K3_dominant)\n    \n    return(data.frame(\n      jump_time = tj,\n      K1_dominant_count = K1_count,\n      K2_dominant_count = K2_count,\n      K3_dominant_count = K3_count,\n      dominant_basis = which.max(c(K1_count, K2_count, K3_count))\n    ))\n  })\n  \n  # Combine results\n  jump_basis_analysis <- do.call(rbind, jump_basis_dominance)\n  \n  cat(\"\\nBasis Dominance Around Phase Jumps:\\n\")\n  jump_basis_analysis\n  \n  # Return results for further analysis\n  return(list(\n    data = fmri_data,\n    kpt1 = kpt1,\n    kpt2 = kpt2,\n    kpt3 = kpt3,\n    fig = fig,\n    metrics = list(metrics1 = metrics1, metrics2 = metrics2, metrics3 = metrics3),\n    performance = performance_table,\n    jump_analysis = jump_basis_analysis\n  ))\n}\n\n# # Run analysis if script is executed directly\n# if (!exists(\"source_run\")) {\n#   results <- run_kpt_analysis()\n# }\n\nresults <- run_kpt_analysis()\n\n## First plot the MULTIPLE REPEATS of the simulated fMRI data\n# # Simulated fMRI series\n# fmri_simData <- generate_synthetic_fmri(\n#     N = 15,       # 15 repetitions/subjects\n#     T = 600,      # 600 timepoints\n#     tmax = 30,    # 30 seconds\n#     mu_func = complex_mu,\n#     kappa_func = complex_kappa,\n#     noise_sd = 0.15,\n#     hrf_type = \"double-gamma\"\n# )\n# Create a color palette\nn_simulations <- 11  # Total number of simulations\n# color_palette <- colorRampPalette(brewer.pal(9, \"Blues\"))(n_simulations)\n# Option 1: Use a spectral color palette (rainbow-like)\n# color_palette <- colorRampPalette(brewer.pal(11, \"Spectral\"))(n_simulations)\n# # Option 2: Use a diverging color palette\ncolor_palette <- colorRampPalette(brewer.pal(11, \"RdYlBu\"))(n_simulations)\n\n\n# Initialize the plot with x values\np <- plot_ly(x = results$data$t)\n\n# Add all traces in a single loop\nfor (i in 1:n_simulations) {\n  p <- add_trace(p, y = results$data$bold[i,], type = \"scatter\", mode = \"lines\",\n                name = paste0(\"Sim fMRI (BOLD) Repeat \", i),\n                line = list(color = color_palette[i], width = 0.5),\n                opacity = 0.8, showlegend = TRUE)\n}\n\n# Add layout properties\np <- p %>% layout(\n  title = \"Multiple Repeats (N=11) of fMRI BOLD Simulations\",\n  xaxis = list(title = \"Time\"), yaxis = list(title = \"BOLD Signal\"),\n  legend = list(x = 1.02, y = 0.98, xanchor = \"left\",\n                bordercolor = \"#EEEEEE\", borderwidth = 1),\n  margin = list(r = 150),  # Extra right margin for legend\n  hovermode = \"closest\"\n)\n# Display the plot\np\n\n# Then plot the model results\nresults$fig",
      "line_count": 715
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "# Visualization of fMRI data with 5 subjects\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(RColorBrewer)\nlibrary(igraph)\n\n# Source necessary functions\ncreate_hrf <- function(t_hrf, type = \"canonical\") {\n  if (type == \"gamma\") {\n    hrf <- dgamma(t_hrf, shape = 6, scale = 1)\n  } else if (type == \"double-gamma\") {\n    hrf <- dgamma(t_hrf, shape = 6, scale = 1) - 0.1 * dgamma(t_hrf, shape = 16, scale = 1)\n  } else { # canonical\n    hrf <- (t_hrf/1.5)^3 * exp(-(t_hrf/1.5)) / gamma(4)\n  }\n  return(hrf / max(hrf))  # Normalize\n}\n\n# Generate multi-region fMRI data with high variability\ngenerate_high_variability_fmri <- function(regions = 5, N = 10, T = 500, \n                                          noise_level = 0.5,\n                                          phase_noise_level = 0.5,\n                                          freq_variability = 0.05,\n                                          amp_variability = 0.4) {\n  # Create time vector\n  t <- seq(0, 30, length.out = T)\n  dt <- t[2] - t[1]\n  \n  # Create coupling matrix with hierarchical structure\n  coupling_matrix <- matrix(0, regions, regions)\n  # Region 1 drives regions 2 and 3\n  coupling_matrix[1, 2] <- 0.7\n  coupling_matrix[1, 3] <- 0.5\n  # Region 2 drives region 4\n  coupling_matrix[2, 4] <- 0.6\n  # Region 3 drives region 5\n  coupling_matrix[3, 5] <- 0.8\n  \n  # Generate base phases for each region\n  base_phases <- matrix(NA, regions, T)\n  \n  # Initialize with uncoupled phases - each region has its own base frequency\n  for (r in 1:regions) {\n    freq <- 0.05 + 0.02 * (r-1)  # Base frequencies from 0.05 to 0.13 Hz\n    base_phases[r,] <- 2 * pi * freq * t\n  }\n  \n  # Apply coupling (phase synchronization)\n  for (i in 1:5) {  # Iterate several times to converge\n    for (r1 in 1:regions) {\n      for (r2 in 1:regions) {\n        if (coupling_matrix[r1, r2] > 0) {\n          # Phase of r2 is pulled toward phase of r1\n          phase_diff <- base_phases[r1,] - base_phases[r2,]\n          # Wrap to [-pi, pi]\n          phase_diff <- ((phase_diff + pi) %% (2*pi)) - pi\n          # Apply coupling\n          base_phases[r2,] <- base_phases[r2,] + coupling_matrix[r1, r2] * phase_diff\n        }\n      }\n    }\n  }\n  \n  # Generate BOLD signals with higher variability\n  bold_signals <- array(NA, dim = c(N, regions, T))\n  \n  # HRF for convolution\n  t_hrf <- seq(0, 20, length.out = 100)\n  hrf <- create_hrf(t_hrf, type = \"double-gamma\")\n  \n  # Subject-specific parameters to increase heterogeneity\n  subject_params <- list()\n  for (n in 1:N) {\n    # Each subject has unique frequency offsets for each region\n    freq_offsets <- rnorm(regions, 0, freq_variability)\n    \n    # Each subject has unique amplitude for each region\n    amplitudes <- runif(regions, 1 - amp_variability, 1 + amp_variability)\n    \n    # Each subject has a unique HRF variation\n    hrf_variation <- runif(1, 0.8, 1.2)\n    \n    # Some subjects might have additional low-frequency drift\n    drift_amplitude <- runif(1, 0, 0.5)\n    drift_frequency <- runif(1, 0.005, 0.015)\n    \n    subject_params[[n]] <- list(\n      freq_offsets = freq_offsets,\n      amplitudes = amplitudes,\n      hrf_variation = hrf_variation,\n      drift_amplitude = drift_amplitude,\n      drift_frequency = drift_frequency\n    )\n  }\n  \n  for (n in 1:N) {\n    params <- subject_params[[n]]\n    \n    for (r in 1:regions) {\n      # Apply subject-specific frequency variation\n      modified_phase <- base_phases[r,] + cumsum(rep(params$freq_offsets[r], T)) * 2 * pi * dt\n      \n      # Add significant random phase noise\n      phase_noise <- rnorm(T, 0, phase_noise_level)\n      \n      # Create complex signal with subject-specific amplitude\n      neural_signal <- params$amplitudes[r] * exp(1i * (modified_phase + phase_noise))\n      \n      # Modify HRF for this subject\n      subject_hrf <- hrf * params$hrf_variation\n      \n      # Convolve with HRF\n      bold <- Re(stats::convolve(neural_signal, rev(subject_hrf), type = \"open\"))[1:T]\n      \n      # Add subject-specific drift\n      drift <- params$drift_amplitude * sin(2 * pi * params$drift_frequency * t)\n      \n      # Add higher measurement noise (lower SNR)\n      bold_noisy <- bold + drift + rnorm(T, 0, noise_level)\n      \n      bold_signals[n, r, ] <- bold_noisy\n    }\n  }\n  \n  return(list(\n    t = t,\n    bold = bold_signals,\n    true_phases = base_phases,\n    coupling_matrix = coupling_matrix,\n    subject_params = subject_params\n  ))\n}\n\n# Generate the data\nset.seed(123)\nmultiregion_data <- generate_high_variability_fmri(\n  regions = 5, \n  N = 10, \n  T = 300,\n  noise_level = 5.0,   # high!!! # for low noise: noise_level = 0.5\n  phase_noise_level = 0.4,\n  freq_variability = 0.05,\n  amp_variability = 0.4\n)\n\n# Extract data for visualization\nt <- multiregion_data$t\nbold_signals <- multiregion_data$bold\ncoupling_matrix <- multiregion_data$coupling_matrix\ntrue_phases <- multiregion_data$true_phases\n\n# Select 5 subjects\nsubjects_to_plot <- c(1, 3, 5, 7, 9)\n\n# 1. Create fMRI time series visualization for 5 subjects\ntimeseries_plot <- function() {\n  # Create subplot for each region\n  plots <- list()\n  \n  for (r in 1:5) {\n    # Extract data for this region\n    region_data <- data.frame()\n    \n    for (s_idx in seq_along(subjects_to_plot)) {\n      s <- subjects_to_plot[s_idx]\n      temp_df <- data.frame(\n        time = t,\n        bold = bold_signals[s, r, ],\n        subject = paste(\"Subject\", s)\n      )\n      region_data <- rbind(region_data, temp_df)\n    }\n    \n    # Create plot for this region\n    p <- plot_ly(region_data, x = ~time, y = ~bold, color = ~subject,\n                type = \"scatter\", mode = \"lines\") %>%\n      layout(\n        title = paste(\"Region\", r),\n        xaxis = list(title = \"\"),\n        yaxis = list(title = \"BOLD Signal\"),\n        showlegend = (r == 1)  # Only show legend for first region\n      )\n    \n    plots[[r]] <- p\n  }\n  \n  # Combine into a subplot\n  subplot(plots, nrows = 5, shareX = TRUE, titleY = TRUE) %>%\n    layout(\n      title = \"fMRI BOLD Signals (5 Selected Subjects)\",\n      xaxis = list(title = \"Time (s)\")\n    )\n}\n\n# 2. Create coupling matrix heatmap\ncoupling_heatmap <- function() {\n  plot_ly(\n    x = paste(\"Region\", 1:5),\n    y = paste(\"Region\", 1:5),\n    z = t(coupling_matrix),  # Transpose to match visualization convention\n    type = \"heatmap\",\n    colorscale = \"Blues\",\n    showscale = TRUE,\n    zmin = 0,\n    zmax = 1\n  ) %>%\n    layout(\n      title = \"Region Coupling Matrix (Source → Target)\",\n      xaxis = list(title = \"Source Region\"),\n      yaxis = list(title = \"Target Region\")\n    )\n}\n\n# 3. Create network graph visualization\n# Fixed network graph function\nnetwork_graph <- function() {\n  # Create graph\n  g <- graph_from_adjacency_matrix(\n    coupling_matrix > 0, \n    mode = \"directed\",\n    weighted = TRUE\n  )\n  \n  # Set edge weights for visualization\n  E(g)$width <- E(g)$weight * 5\n  \n  # Generate layout\n  layout <- layout_with_fr(g)\n  \n  # Create vectors for plotting\n  edges <- get.edgelist(g)\n  \n  # Node coordinates\n  node_x <- layout[,1]\n  node_y <- layout[,2]\n  \n  # Edge coordinates\n  edge_x <- edge_y <- list()\n  \n  for (i in 1:nrow(edges)) {\n    from_idx <- as.numeric(edges[i, 1])\n    to_idx <- as.numeric(edges[i, 2])\n    \n    edge_x[[i]] <- c(node_x[from_idx], node_x[to_idx], NA)\n    edge_y[[i]] <- c(node_y[from_idx], node_y[to_idx], NA)\n  }\n  \n  edge_x <- unlist(edge_x)\n  edge_y <- unlist(edge_y)\n  \n  # Create plot\n  fig <- plot_ly() %>%\n    # Add edges\n    add_trace(\n      x = edge_x, y = edge_y,\n      mode = \"lines\",\n      line = list(width = 2, color = \"rgba(150, 150, 150, 0.8)\"),\n      hoverinfo = \"none\",\n      showlegend = FALSE\n    ) %>%\n    # Add nodes\n    add_trace(\n      x = node_x, y = node_y,\n      mode = \"markers+text\",\n      marker = list(\n        size = 30,\n        color = \"rgba(31, 119, 180, 0.8)\",\n        line = list(width = 2, color = \"rgb(0, 0, 0)\")\n      ),\n      text = paste(\"R\", 1:5),\n      textposition = \"middle center\",\n      textfont = list(color = \"white\", size = 14),\n      hoverinfo = \"text\",\n      hovertext = paste(\"Region\", 1:5),\n      showlegend = FALSE\n    )\n  \n  # Add edge labels\n  for (i in 1:nrow(edges)) {\n    from_idx <- as.numeric(edges[i, 1])\n    to_idx <- as.numeric(edges[i, 2])\n    \n    # Find weight\n    weight <- coupling_matrix[from_idx, to_idx]\n    \n    # Calculate midpoint\n    mid_x <- (node_x[from_idx] + node_x[to_idx]) / 2\n    mid_y <- (node_y[from_idx] + node_y[to_idx]) / 2\n    \n    # Add label\n    fig <- fig %>% add_annotations(\n      x = mid_x,\n      y = mid_y,\n      text = sprintf(\"%.1f\", weight),\n      showarrow = FALSE,\n      font = list(size = 12)\n    )\n  }\n  \n  # Complete the layout\n  fig <- fig %>% layout(\n    title = \"Brain Region Connectivity Network\",\n    showlegend = FALSE,\n    xaxis = list(\n      title = \"\",\n      showgrid = FALSE,\n      zeroline = FALSE,\n      showticklabels = FALSE\n    ),\n    yaxis = list(\n      title = \"\",\n      showgrid = FALSE,\n      zeroline = FALSE,\n      showticklabels = FALSE\n    )\n  )\n  \n  return(fig)\n}\n\n# Run the function with example data\n# For this example, we'll create a sample coupling matrix if not already defined\nif (!exists(\"coupling_matrix\")) {\n  coupling_matrix <- matrix(0, 5, 5)\n  coupling_matrix[1, 2] <- 0.7\n  coupling_matrix[1, 3] <- 0.5\n  coupling_matrix[2, 4] <- 0.6\n  coupling_matrix[3, 5] <- 0.8\n}\n\n# # Generate and display the network graph\n# network_plot <- network_graph()\n# network_plot\n\n# 4. Create phase visualization\nphase_plot <- function() {\n  # Create data frame for phase plotting\n  phase_plot_data <- data.frame()\n  for (r in 1:5) {\n    temp_df <- data.frame(\n      time = t,\n      phase = true_phases[r, ],\n      region = paste(\"Region\", r)\n    )\n    phase_plot_data <- rbind(phase_plot_data, temp_df)\n  }\n  \n  # Plot the phase patterns\n  plot_ly(phase_plot_data, x = ~time, y = ~phase, color = ~region,\n          type = \"scatter\", mode = \"lines\") %>%\n    layout(\n      title = \"True Phase Patterns by Region\",\n      xaxis = list(title = \"Time (s)\"),\n      yaxis = list(title = \"Phase (radians)\"),\n      legend = list(title = list(text = \"Region\"))\n    )\n}\n\n# Generate all plots\nts_plot <- timeseries_plot()\nheatmap_plot <- coupling_heatmap()\nnetwork_plot <- network_graph()\nphases_plot <- phase_plot()\n\n# Display plots\nts_plot\nheatmap_plot\nnetwork_plot\nphases_plot",
      "line_count": 369
    },
    {
      "section": "From QM Wavefunction Phase Estimation to Kime-phase Estimation in Spacekime Representation",
      "code": "# Advanced Applications of Kime-Phase Tomography for Neuroimaging\n# This script extends the unified KPT framework to:\n# 1. Multi-region phase coupling analysis\n# 2. Non-parametric phase distribution modeling\n# 3. Event-related design applications\n# 4. Phase transition detection in cognitive tasks\n\nlibrary(signal)\nlibrary(circular)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(igraph)  # For network analysis\n\n# -----------------------------------------------------------------------\n# 1. Multi-Region Phase Coupling Analysis\n# -----------------------------------------------------------------------\n\n#' Generate synthetic multi-region fMRI data with known phase coupling\n#' \n#' @param regions Number of brain regions\n#' @param N Number of repetitions/subjects\n#' @param T Number of timepoints\n#' @param noise_level fMRI/BOLD signal noise level\n#' @param phase_noise_level kime-phase noise level\n#' @param freq_variability frequency variability\n#' @param amp_variability amplitude variability\n#' @param coupling_matrix Matrix specifying phase coupling between regions\n#' @return List containing generated data and ground truth\ngenerate_multiregion_fmri <- function(regions = 5, N = 10, T = 500, \n                                      noise_level = 0.5,\n                                      phase_noise_level = 0.5,\n                                      freq_variability = 0.05,\n                                      amp_variability = 0.4) {\n  # Create time vector\n  t <- seq(0, 30, length.out = T)\n  dt <- t[2] - t[1]\n  \n  # Create coupling matrix with hierarchical structure\n  coupling_matrix <- matrix(0, regions, regions)\n  # Region 1 drives regions 2 and 3\n  coupling_matrix[1, 2] <- 0.7\n  coupling_matrix[1, 3] <- 0.5\n  # Region 2 drives region 4\n  coupling_matrix[2, 4] <- 0.6\n  # Region 3 drives region 5\n  coupling_matrix[3, 5] <- 0.8\n  \n  # Generate base phases for each region\n  base_phases <- matrix(NA, regions, T)\n  \n  # Initialize with uncoupled phases - each region has its own base frequency\n  for (r in 1:regions) {\n    freq <- 0.05 + 0.02 * (r-1)  # Base frequencies from 0.05 to 0.13 Hz\n    base_phases[r,] <- 2 * pi * freq * t\n  }\n  \n  # Apply coupling (phase synchronization)\n  for (i in 1:5) {  # Iterate several times to converge\n    for (r1 in 1:regions) {\n      for (r2 in 1:regions) {\n        if (coupling_matrix[r1, r2] > 0) {\n          # Phase of r2 is pulled toward phase of r1\n          phase_diff <- base_phases[r1,] - base_phases[r2,]\n          # Wrap to [-pi, pi]\n          phase_diff <- ((phase_diff + pi) %% (2*pi)) - pi\n          # Apply coupling\n          base_phases[r2,] <- base_phases[r2,] + coupling_matrix[r1, r2] * phase_diff\n        }\n      }\n    }\n  }\n  \n  # Generate BOLD signals with higher variability\n  bold_signals <- array(NA, dim = c(N, regions, T))\n  \n  # HRF for convolution\n  t_hrf <- seq(0, 20, length.out = 100)\n  hrf <- create_hrf(t_hrf, type = \"double-gamma\")\n  \n  # Subject-specific parameters to increase heterogeneity\n  subject_params <- list()\n  for (n in 1:N) {\n    # Each subject has unique frequency offsets for each region\n    freq_offsets <- rnorm(regions, 0, freq_variability)\n    \n    # Each subject has unique amplitude for each region\n    amplitudes <- runif(regions, 1 - amp_variability, 1 + amp_variability)\n    \n    # Each subject has a unique HRF variation\n    hrf_variation <- runif(1, 0.8, 1.2)\n    \n    # Some subjects might have additional low-frequency drift\n    drift_amplitude <- runif(1, 0, 0.5)\n    drift_frequency <- runif(1, 0.005, 0.015)\n    \n    subject_params[[n]] <- list(\n      freq_offsets = freq_offsets,\n      amplitudes = amplitudes,\n      hrf_variation = hrf_variation,\n      drift_amplitude = drift_amplitude,\n      drift_frequency = drift_frequency\n    )\n  }\n  \n  for (n in 1:N) {\n    params <- subject_params[[n]]\n    \n    for (r in 1:regions) {\n      # Apply subject-specific frequency variation\n      modified_phase <- base_phases[r,] + cumsum(rep(params$freq_offsets[r], T)) * 2 * pi * dt\n      \n      # Add significant random phase noise\n      phase_noise <- rnorm(T, 0, phase_noise_level)\n      \n      # Create complex signal with subject-specific amplitude\n      neural_signal <- params$amplitudes[r] * exp(1i * (modified_phase + phase_noise))\n      \n      # Modify HRF for this subject\n      subject_hrf <- hrf * params$hrf_variation\n      \n      # Convolve with HRF\n      bold <- Re(stats::convolve(neural_signal, rev(subject_hrf), type = \"open\"))[1:T]\n      \n      # Add subject-specific drift\n      drift <- params$drift_amplitude * sin(2 * pi * params$drift_frequency * t)\n      \n      # Add higher measurement noise (lower SNR)\n      bold_noisy <- bold + drift + rnorm(T, 0, noise_level)\n      \n      bold_signals[n, r, ] <- bold_noisy\n    }\n  }\n  \n  return(list(\n    t = t,\n    bold = bold_signals,\n    true_phases = base_phases,\n    coupling_matrix = coupling_matrix,\n    subject_params = subject_params\n  ))\n}\n\n#' Calculate phase synchronization between two signals\n#' \n#' @param phi1 Phase time series of first signal\n#' @param phi2 Phase time series of second signal\n#' @return Phase locking value (0-1)\nphase_locking_value <- function(phi1, phi2) {\n  # Compute phase difference\n  phase_diff <- phi1 - phi2\n  \n  # Complex phase difference\n  z <- exp(1i * phase_diff)\n  \n  # Phase locking value\n  plv <- abs(mean(z))\n  \n  return(plv)\n}\n\n#' Apply KPT to multi-region fMRI data\n#' \n#' @param bold_data Multi-region BOLD data [subjects × regions × time]\n#' @param t Time vector\n#' @return List of KPT results for each region\napply_multiregion_kpt <- function(bold_data, t) {\n  N <- dim(bold_data)[1]  # Number of subjects\n  regions <- dim(bold_data)[2]  # Number of regions\n  \n  # Apply KPT to each region\n  region_results <- list()\n  \n  for (r in 1:regions) {\n    # Extract BOLD data for this region\n    bold_matrix <- matrix(bold_data[, r, ], nrow = N)\n    \n    # Apply KPT\n    cat(\"Processing region\", r, \"...\\n\")\n    kpt_result <- unified_kpt(bold_matrix, t, preprocess = TRUE, kernel_sigma = 0.5)\n    \n    region_results[[r]] <- kpt_result\n  }\n  \n  # Calculate phase coupling between all region pairs\n  plv_matrix <- matrix(0, regions, regions)\n  \n  for (r1 in 1:regions) {\n    for (r2 in 1:regions) {\n      if (r1 != r2) {\n        plv <- phase_locking_value(region_results[[r1]]$mu, region_results[[r2]]$mu)\n        plv_matrix[r1, r2] <- plv\n      }\n    }\n  }\n  \n  return(list(\n    region_results = region_results,\n    plv_matrix = plv_matrix\n  ))\n}\n\n#' Visualize multi-region phase synchronization\n#' \n#' @param multiregion_kpt Results from apply_multiregion_kpt\n#' @param threshold PLV threshold for visualization\n#' @return Plotly figure\nvisualize_phase_connectivity <- function(multiregion_kpt, threshold = 0.4) {\n  # Extract PLV matrix\n  plv_matrix <- multiregion_kpt$plv_matrix\n  regions <- nrow(plv_matrix)\n  \n  # Create network graph\n  g <- graph_from_adjacency_matrix(\n    plv_matrix > threshold, \n    mode = \"directed\",\n    weighted = TRUE\n  )\n  \n  # Set edge weights for visualization\n  E(g)$width <- E(g)$weight * 5\n  \n  # Layout\n  layout <- layout_with_fr(g)\n  \n  # Create vectors for plotting\n  edges <- get.edgelist(g)\n  edge_weights <- E(g)$width\n  \n  # Node coordinates\n  node_x <- layout[,1]\n  node_y <- layout[,2]\n  \n  # Edge coordinates\n  edge_x <- edge_y <- list()\n  \n  for (i in 1:nrow(edges)) {\n    from_idx <- as.numeric(edges[i, 1])\n    to_idx <- as.numeric(edges[i, 2])\n    \n    edge_x[[i]] <- c(node_x[from_idx], node_x[to_idx], NA)\n    edge_y[[i]] <- c(node_y[from_idx], node_y[to_idx], NA)\n  }\n  \n  edge_x <- unlist(edge_x)\n  edge_y <- unlist(edge_y)\n  \n  # Create plot\n  fig <- plot_ly() %>%\n    # Add edges\n    add_trace(\n      x = edge_x, y = edge_y,\n      mode = \"lines\",\n      line = list(width = 1, color = \"rgba(150, 150, 150, 0.8)\"),\n      hoverinfo = \"none\",\n      showlegend = FALSE\n    ) %>%\n    # Add nodes\n    add_trace(\n      x = node_x, y = node_y,\n      mode = \"markers\",\n      marker = list(\n        size = 15,\n        color = \"rgba(31, 119, 180, 0.8)\",\n        line = list(width = 2, color = \"rgb(0, 0, 0)\")\n      ),\n      text = paste(\"Region\", 1:regions),\n      hoverinfo = \"text\",\n      showlegend = FALSE\n    ) %>%\n    layout(\n      title = \"Phase Synchronization Network\",\n      showlegend = FALSE,\n      xaxis = list(\n        title = \"\",\n        showgrid = FALSE,\n        zeroline = FALSE,\n        showticklabels = FALSE\n      ),\n      yaxis = list(\n        title = \"\",\n        showgrid = FALSE,\n        zeroline = FALSE,\n        showticklabels = FALSE\n      )\n    )\n  \n  return(fig)\n}\n\n# -----------------------------------------------------------------------\n# 2. Non-Parametric Phase Distribution Modeling\n# -----------------------------------------------------------------------\n\n#' Estimate non-parametric phase distribution\n#' \n#' @param phases Matrix of phase observations [subjects × time]\n#' @param t Time vector\n#' @param bw Kernel bandwidth\n#' @return List with distribution estimates\nestimate_nonparametric_phase_dist <- function(phases, t, bw = 0.3) {\n  T <- ncol(phases)\n  N <- nrow(phases)\n  \n  # Prepare results\n  density_estimates <- list()\n  \n  # For each time point, estimate circular density\n  for (i in 1:T) {\n    # Extract phases at this time point\n    theta <- phases[, i]\n    \n    # Generate grid for density evaluation\n    grid <- seq(-pi, pi, length.out = 100)\n    \n    # Compute von Mises kernel density\n    density_vals <- numeric(length(grid))\n    \n    for (j in seq_along(grid)) {\n      # Sum of von Mises kernels centered at each observation\n      kernel_sum <- sum(exp(bw * cos(grid[j] - theta)))\n      density_vals[j] <- kernel_sum\n    }\n    \n    # Normalize\n    density_vals <- density_vals / (sum(density_vals) * (grid[2] - grid[1]))\n    \n    density_estimates[[i]] <- list(\n      grid = grid,\n      density = density_vals\n    )\n  }\n  \n  return(list(\n    t = t,\n    density_estimates = density_estimates\n  ))\n}\n\n#' Visualize non-parametric phase distribution over time\n#' \n#' @param nonparam_dist Results from estimate_nonparametric_phase_dist\n#' @param time_indices Which time points to visualize\n#' @return Plotly figure\nvisualize_nonparametric_dist <- function(nonparam_dist, time_indices = NULL) {\n  t <- nonparam_dist$t\n  densities <- nonparam_dist$density_estimates\n  \n  # If time indices not specified, choose evenly spaced points\n  if (is.null(time_indices)) {\n    n_points <- 5\n    time_indices <- round(seq(1, length(t), length.out = n_points))\n  }\n  \n  # Create data frame for plotting\n  plot_data <- data.frame()\n  \n  for (idx in time_indices) {\n    temp_df <- data.frame(\n      theta = densities[[idx]]$grid,\n      density = densities[[idx]]$density,\n      time = t[idx]\n    )\n    plot_data <- rbind(plot_data, temp_df)\n  }\n  \n  # Convert to factor for discrete color scale\n  plot_data$time_factor <- factor(plot_data$time)\n  \n  # Create plot\n  fig <- plot_ly(plot_data, x = ~theta, y = ~density, color = ~time_factor,\n                type = \"scatter\", mode = \"lines\") %>%\n    layout(\n      title = \"Non-Parametric Phase Distribution Over Time\",\n      xaxis = list(title = \"Phase (radians)\"),\n      yaxis = list(title = \"Density\"),\n      legend = list(title = list(text = \"Time (s)\"))\n    )\n  \n  return(fig)\n}\n\n# -----------------------------------------------------------------------\n# 3. Event-Related Design Analysis\n# -----------------------------------------------------------------------\n\n#' Generate event-related fMRI data with phase reset\n#' \n#' @param N Number of subjects\n#' @param T Number of timepoints\n#' @param tmax Maximum time\n#' @param event_times Vector of event times\n#' @param reset_magnitude Magnitude of phase reset\n#' @return List containing generated data\ngenerate_event_related_fmri <- function(N = 10, T = 600, tmax = 60,\n                                       event_times = c(10, 30, 50),\n                                       reset_magnitude = pi/2) {\n  # Time vector\n  t <- seq(0, tmax, length.out = T)\n  dt <- t[2] - t[1]\n  \n  # Find indices of events\n  event_indices <- sapply(event_times, function(et) which.min(abs(t - et)))\n  \n  # Generate signals\n  bold_signals <- matrix(NA, nrow = N, ncol = T)\n  true_phases <- matrix(NA, nrow = N, ncol = T)\n  \n  # HRF\n  t_hrf <- seq(0, 20, length.out = 100)\n  hrf <- create_hrf(t_hrf, type = \"double-gamma\")\n  \n  for (n in 1:N) {\n    # Base frequency varies slightly by subject\n    freq <- 0.08 + 0.01 * rnorm(1)\n    \n    # Initialize continuous phase\n    phase <- numeric(T)\n    phase[1] <- 2 * pi * runif(1)  # Random initial phase\n    \n    # Evolve phase with resets at events\n    for (i in 2:T) {\n      # Check if this is an event onset\n      is_event <- i %in% event_indices\n      \n      if (is_event) {\n        # Apply phase reset\n        reset_direction <- sample(c(-1, 1), 1)  # Random direction\n        phase[i] <- phase[i-1] + reset_direction * reset_magnitude\n      } else {\n        # Normal phase evolution\n        phase[i] <- phase[i-1] + 2 * pi * freq * dt\n      }\n    }\n    \n    # Wrap to [-pi, pi]\n    phase <- ((phase + pi) %% (2*pi)) - pi\n    \n    # Create neural signal\n    neural_signal <- exp(1i * phase)\n    \n    # Add amplitude modulation at events\n    amplitude <- rep(1, T)\n    for (idx in event_indices) {\n      # Amplitude rises after event\n      window <- idx:(min(idx + 100, T))\n      amplitude[window] <- amplitude[window] + 0.5 * exp(-(1:length(window))/20)\n    }\n    \n    # Apply amplitude\n    neural_signal <- amplitude * neural_signal\n    \n    # Convolve with HRF\n    bold <- Re(convolve(neural_signal, rev(hrf), type = \"open\"))[1:T]\n    \n    # Add noise\n    bold_signals[n, ] <- bold + rnorm(T, 0, 0.15)\n    true_phases[n, ] <- phase\n  }\n  \n  return(list(\n    t = t,\n    bold = bold_signals,\n    true_phases = true_phases,\n    event_times = event_times\n  ))\n}\n\n#' Analyze phase reset in event-related data\n#' \n#' @param kpt_result Result from unified_kpt\n#' @param event_times Vector of event times\n#' @param window_pre Pre-event window in seconds\n#' @param window_post Post-event window in seconds\n#' @return List with phase reset analysis\nanalyze_phase_reset <- function(kpt_result, event_times, window_pre = 2, window_post = 5) {\n  t <- kpt_result$t\n  dt <- t[2] - t[1]\n  \n  # Find indices of events\n  event_indices <- sapply(event_times, function(et) which.min(abs(t - et)))\n  \n  # Calculate window sizes in samples\n  pre_samples <- round(window_pre / dt)\n  post_samples <- round(window_post / dt)\n  \n  # For each event, extract phase before and after\n  reset_analysis <- list()\n  \n  for (i in seq_along(event_indices)) {\n    idx <- event_indices[i]\n    \n    # Extract time windows\n    pre_idx <- max(1, idx - pre_samples):idx\n    post_idx <- idx:(min(length(t), idx + post_samples))\n    \n    # Calculate phase changes\n    phase_pre <- kpt_result$mu[pre_idx]\n    phase_post <- kpt_result$mu[post_idx]\n    \n    # Calculate phase derivative\n    phase_deriv_pre <- c(0, diff(phase_pre))\n    phase_deriv_post <- c(0, diff(phase_post))\n    \n    # Unwrap phase for consistency\n    phase_pre_unwrap <- phase_pre\n    for (j in 2:length(phase_pre_unwrap)) {\n      diff_phase <- phase_pre_unwrap[j] - phase_pre_unwrap[j-1]\n      if (abs(diff_phase) > pi) {\n        phase_pre_unwrap[j:length(phase_pre_unwrap)] <- \n          phase_pre_unwrap[j:length(phase_pre_unwrap)] - sign(diff_phase) * 2 * pi\n      }\n    }\n    \n    phase_post_unwrap <- phase_post\n    for (j in 2:length(phase_post_unwrap)) {\n      diff_phase <- phase_post_unwrap[j] - phase_post_unwrap[j-1]\n      if (abs(diff_phase) > pi) {\n        phase_post_unwrap[j:length(phase_post_unwrap)] <- \n          phase_post_unwrap[j:length(phase_post_unwrap)] - sign(diff_phase) * 2 * pi\n      }\n    }\n    \n    # Calculate reset magnitude\n    reset_magnitude <- phase_post_unwrap[1] - phase_pre_unwrap[length(phase_pre_unwrap)]\n    \n    # Calculate pre/post consistency\n    pre_consistency <- abs(mean(exp(1i * phase_pre)))\n    post_consistency <- abs(mean(exp(1i * phase_post)))\n    \n    # Store results\n    reset_analysis[[i]] <- list(\n      event_time = event_times[i],\n      phase_pre = phase_pre,\n      phase_post = phase_post,\n      phase_deriv_pre = phase_deriv_pre,\n      phase_deriv_post = phase_deriv_post,\n      reset_magnitude = reset_magnitude,\n      pre_consistency = pre_consistency,\n      post_consistency = post_consistency\n    )\n  }\n  \n  return(list(\n    t = t,\n    events = event_times,\n    reset_analysis = reset_analysis\n  ))\n}\n\n#' Visualize phase reset analysis\n#' \n#' @param reset_analysis Results from analyze_phase_reset\n#' @return Plotly figure\nvisualize_phase_reset <- function(reset_analysis) {\n  t <- reset_analysis$t\n  dt <- t[2] - t[1]\n  events <- reset_analysis$events\n  \n  # Create data frame for plotting\n  plot_data <- data.frame()\n  \n  for (i in seq_along(reset_analysis$reset_analysis)) {\n    event_data <- reset_analysis$reset_analysis[[i]]\n    event_time <- event_data$event_time\n    \n    # Calculate relative time\n    pre_time <- seq(-length(event_data$phase_pre) + 1, 0) * dt\n    post_time <- seq(0, length(event_data$phase_post) - 1) * dt\n    \n    # Combine pre and post data\n    time_rel <- c(pre_time, post_time[-1])\n    phase <- c(event_data$phase_pre, event_data$phase_post[-1])\n    phase_deriv <- c(event_data$phase_deriv_pre, event_data$phase_deriv_post[-1])\n    \n    temp_df <- data.frame(\n      time_rel = time_rel,\n      phase = phase,\n      phase_deriv = phase_deriv,\n      event_id = factor(i)\n    )\n    \n    plot_data <- rbind(plot_data, temp_df)\n  }\n  \n  # Create plot\n  fig <- plot_ly() %>%\n    add_trace(data = plot_data, x = ~time_rel, y = ~phase, color = ~event_id,\n              type = \"scatter\", mode = \"lines\", name = \"Phase\",\n              line = list(width = 2)) %>%\n    add_trace(data = plot_data, x = ~time_rel, y = ~phase_deriv, color = ~event_id,\n              type = \"scatter\", mode = \"lines\", name = \"Phase Derivative\",\n              line = list(width = 1, dash = \"dash\"), visible = \"legendonly\") %>%\n    add_trace(x = c(0, 0), y = c(-pi, pi), type = \"scatter\", mode = \"lines\",\n              line = list(color = \"black\", width = 1, dash = \"dot\"),\n              showlegend = FALSE) %>%\n    layout(\n      title = \"Phase Reset Analysis Around Events\",\n      xaxis = list(title = \"Time Relative to Event (s)\"),\n      yaxis = list(title = \"Phase (radians)\"),\n      legend = list(title = list(text = \"Event\"))\n    )\n  \n  return(fig)\n}\n\n# -----------------------------------------------------------------------\n# 4. Cognitive State Detection\n# -----------------------------------------------------------------------\n\n#' Detect cognitive states from phase dynamics\n#' \n#' @param kpt_result Result from unified_kpt\n#' @param window_size Window size for state detection (seconds)\n#' @param k Number of states to identify\n#' @return List with state detection results\ndetect_cognitive_states <- function(kpt_result, window_size = 3, k = 3) {\n  t <- kpt_result$t\n  dt <- t[2] - t[1]\n  \n  # Convert window size to samples\n  window_samples <- round(window_size / dt)\n  \n  # Create feature matrix\n  # For each window, extract:\n  # 1. Mean phase derivative\n  # 2. Phase variability\n  # 3. Concentration parameter\n  \n  # Number of windows\n  n_windows <- length(t) - window_samples + 1\n  \n  # Feature matrix\n  features <- matrix(0, n_windows, 3)\n  \n  for (i in 1:n_windows) {\n    window_idx <- i:(i + window_samples - 1)\n    phase_window <- kpt_result$mu[window_idx]\n    kappa_window <- kpt_result$kappa[window_idx]\n    \n    # Unwrap phase for derivative calculation\n    phase_unwrap <- phase_window\n    for (j in 2:length(phase_unwrap)) {\n      diff_phase <- phase_unwrap[j] - phase_unwrap[j-1]\n      if (abs(diff_phase) > pi) {\n        phase_unwrap[j:length(phase_unwrap)] <- \n          phase_unwrap[j:length(phase_unwrap)] - sign(diff_phase) * 2 * pi\n      }\n    }\n    \n    # Calculate phase derivative\n    phase_deriv <- diff(phase_unwrap) / dt\n    \n    # Features\n    features[i, 1] <- mean(phase_deriv)  # Mean phase velocity\n    features[i, 2] <- sd(phase_deriv)    # Variability in phase velocity\n    features[i, 3] <- mean(kappa_window) # Mean concentration\n  }\n  \n  # Scale features\n  features_scaled <- scale(features)\n  \n  # K-means clustering\n  kmeans_result <- kmeans(features_scaled, centers = k)\n  \n  # Assign states to each time point\n  # For simplicity, we'll assign the state of the window starting at each point\n  states <- rep(NA, length(t))\n  for (i in 1:n_windows) {\n    states[i] <- kmeans_result$cluster[i]\n  }\n  \n  # Fill in remaining points with the last state\n  states[(n_windows+1):length(t)] <- states[n_windows]\n  \n  # Return results\n  return(list(\n    t = t,\n    states = states,\n    features = features,\n    kmeans = kmeans_result\n  ))\n}\n\n#' Visualize cognitive states\n#' \n#' @param state_detection Results from detect_cognitive_states\n#' @param kpt_result Result from unified_kpt\n#' @return Plotly figure\nvisualize_cognitive_states <- function(state_detection, kpt_result) {\n  t <- state_detection$t\n  states <- state_detection$states\n  \n  # Create data frame\n  df <- data.frame(\n    t = t,\n    phase = kpt_result$mu,\n    concentration = kpt_result$kappa,\n    state = factor(states)\n  )\n  \n  # Create color map for states\n  state_colors <- c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\")\n  \n  # Create plot\n  fig <- plot_ly() %>%\n    add_trace(data = df, x = ~t, y = ~phase, color = ~state,\n              type = \"scatter\", mode = \"lines\",\n              line = list(width = 2, shape = \"hv\"),\n              colors = state_colors[1:length(unique(states))]) %>%\n    layout(\n      title = \"Cognitive State Detection from Phase Dynamics\",\n      xaxis = list(title = \"Time (s)\"),\n      yaxis = list(title = \"Phase (radians)\"),\n      legend = list(title = list(text = \"State\"))\n    )\n  \n  return(fig)\n}\n\n# -----------------------------------------------------------------------\n# Run Advanced Analysis Example\n# -----------------------------------------------------------------------\n\nrun_advanced_analysis <- function() {\n  # 1. Multi-region analysis\n  cat(\"Generating multi-region fMRI data...\\n\")\n  multiregion_data <- generate_multiregion_fmri(regions = 5, N = 10, T = 300, noise_level = 5.0)\n  \n  cat(\"Applying KPT to multi-region data...\\n\")\n  multiregion_kpt <- apply_multiregion_kpt(multiregion_data$bold, multiregion_data$t)\n  \n  cat(\"Visualizing phase connectivity...\\n\")\n  connectivity_plot <- visualize_phase_connectivity(multiregion_kpt, threshold = 0.4)\n  connectivity_plot\n  \n  # 2. Non-parametric phase distribution\n  cat(\"Estimating non-parametric phase distribution...\\n\") \n  # Extract phases from first region\n  phases_matrix <- matrix(0, nrow = 10, ncol = 300)\n  for (n in 1:10) {\n    phases_matrix[n,] <- multiregion_data$true_phases[1,]\n  }\n  \n  nonparam_dist <- estimate_nonparametric_phase_dist(phases_matrix, multiregion_data$t)\n  \n  cat(\"Visualizing non-parametric distribution...\\n\")\n  nonparam_plot <- visualize_nonparametric_dist(nonparam_dist)\n  nonparam_plot\n  \n  # 3. Event-related analysis\n  cat(\"Generating event-related fMRI data...\\n\")\n  event_data <- generate_event_related_fmri(N = 15, T = 600, tmax = 60,\n                                           event_times = c(10, 30, 50))\n  \n  cat(\"Applying KPT to event-related data...\\n\")\n  event_kpt <- unified_kpt(event_data$bold, event_data$t, preprocess = TRUE, kernel_sigma = 0.5)\n  \n  cat(\"Analyzing phase reset...\\n\")\n  reset_analysis <- analyze_phase_reset(event_kpt, event_data$event_times)\n  \n  cat(\"Visualizing phase reset...\\n\")\n  reset_plot <- visualize_phase_reset(reset_analysis)\n  reset_plot\n  \n  # 4. Cognitive state detection\n  cat(\"Detecting cognitive states...\\n\")\n  state_detection <- detect_cognitive_states(event_kpt, window_size = 3, k = 3)\n  \n  cat(\"Visualizing cognitive states...\\n\")\n  state_plot <- visualize_cognitive_states(state_detection, event_kpt)\n  state_plot\n  \n  # Return results\n  return(list(\n    multiregion = list(\n      data = multiregion_data,\n      kpt = multiregion_kpt,\n      plot = connectivity_plot\n    ),\n    nonparametric = list(\n      dist = nonparam_dist,\n      plot = nonparam_plot\n    ),\n    event_related = list(\n      data = event_data,\n      kpt = event_kpt,\n      reset_analysis = reset_analysis,\n      plot = reset_plot\n    ),\n    cognitive_states = list(\n      detection = state_detection,\n      plot = state_plot\n    )\n  ))\n}\n\n# # Run if script is executed directly\n# if (!exists(\"source_run\")) {\n#   results <- run_advanced_analysis()\n# }\n\n#### Report and PLOT results ....\ncat(\"Generating multi-region fMRI data...\\n\")\n  multiregion_data <- generate_multiregion_fmri(regions = 5, N = 10, T = 300)\n  \n  cat(\"Applying KPT to multi-region data...\\n\")\n  multiregion_kpt <- apply_multiregion_kpt(multiregion_data$bold, multiregion_data$t)\n  \n  cat(\"Visualizing phase connectivity...\\n\")\n  connectivity_plot <- visualize_phase_connectivity(multiregion_kpt, threshold = 0.4)\n  connectivity_plot\n  \n  # 2. Non-parametric phase distribution\n  cat(\"Estimating non-parametric phase distribution...\\n\")\n  # Extract phases from first region\n  phases_matrix <- matrix(0, nrow = 10, ncol = 300)\n  for (n in 1:10) {\n    phases_matrix[n,] <- multiregion_data$true_phases[1,]\n  }\n  \n  nonparam_dist <- estimate_nonparametric_phase_dist(phases_matrix, multiregion_data$t)\n  \n  cat(\"Visualizing non-parametric distribution...\\n\")\n  nonparam_plot <- visualize_nonparametric_dist(nonparam_dist)\n  nonparam_plot\n  \n  # 3. Event-related analysis\n  cat(\"Generating event-related fMRI data...\\n\")\n  event_data <- generate_event_related_fmri(N = 15, T = 600, tmax = 60,\n                                           event_times = c(10, 30, 50))\n  \n  cat(\"Applying KPT to event-related data...\\n\")\n  event_kpt <- unified_kpt(event_data$bold, event_data$t, preprocess = TRUE, kernel_sigma = 0.5)\n  \n  cat(\"Analyzing phase reset...\\n\")\n  reset_analysis <- analyze_phase_reset(event_kpt, event_data$event_times)\n  \n  cat(\"Visualizing phase reset...\\n\")\n  reset_plot <- visualize_phase_reset(reset_analysis)\n  reset_plot\n  \n  # 4. Cognitive state detection\n  cat(\"Detecting cognitive states...\\n\") \n  state_detection <- detect_cognitive_states(event_kpt, window_size = 3, k = 3)\n  \n  cat(\"Visualizing cognitive states...\\n\")\n  state_plot <- visualize_cognitive_states(state_detection, event_kpt)\n  state_plot\n  \nlibrary(htmltools)\nstr_output <- capture.output(str(event_kpt))\nhtml_output <- tags$pre(paste(str_output, collapse = \"\\n\"))\nhtml_output\n# str(event_kpt)",
      "line_count": 854
    }
  ]
}