{
  "metadata": {
    "created_at": "2025-05-15T17:01:01.211721",
    "total_sections": 20,
    "total_code_chunks": 10,
    "total_tables": 2,
    "r_libraries": [
      "DiagrammeR",
      "animation",
      "circular",
      "kableExtra",
      "plotly",
      "tidyr"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"Spacekime Analytics (Time Complexity and Inferential Uncertainty)\"\nauthor: \"SOCR Team (Yueyang Shen, Jun Chen, Ivo Dinov)\"\ndate: \"`r format(Sys.time(),'%m/%d/%Y')`\"\noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: TCIU_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    code_folding: hide\n  pdf_document:\n    toc: true\n    toc_depth: '3'\n  word_document:\n    toc: true\n    toc_depth: '3'\nalways_allow_html: true",
      "word_count": 55
    },
    {
      "title": "Spacekime - Rate of Change and Wirtinger calculus",
      "content": "The domain of the kime variable ($k$) is the complex plane parameterized by pairs of Descartes Cartesian coordinates, conjugate-pairs coordinates, or polar coordinates:\n\n$$\\mathbb{C \\cong}\\mathbb{R}^{2} = \\left\\{ z = (\\ x\\ ,y\\ )\\mid\\ x,\\ y\\mathbb{\\in R\\ } \\right\\} \\cong$$ $$\\left\\{ \\left( \\ z\\ ,\\overline{z}\\  \\right)\\mid\\ z\\mathbb{\\in C,\\ }z = x + iy,\\ \\ \\overline{z}\\  = x - iy,\\ \\ x,y\\mathbb{\\in R\\ } \\right\\} \\cong$$\n\n$$\\{\\ k = r\\ e^{i\\varphi} = r\\left( \\cos\\varphi + i\\sin\\varphi \\right)\\  \\mid \\ r \\geq 0,\\  - \\pi \\leq \\varphi < \\pi\\ \\}.$$\n\nThe *Wirtinger derivative* of a continuously differentiable function ($f$) of kime ($k$), $f(k)$, and its conjugate are defined as *first-order* linear partial differential operators:\n\nIn Cartesian coordinates:\n\n$f'(z) = \\frac{\\partial f(z)}{\\partial z} = \\frac{1}{2}\\left( \\frac{\\partial f}{\\partial x} - i\\frac{\\partial f}{\\partial y} \\right)$ and $f'\\left( \\overline{z} \\right) = \\frac{\\partial f\\left( \\overline{z} \\right)}{\\partial\\overline{z}} = \\frac{1}{2}\\left( \\frac{\\partial f}{\\partial x} + i\\frac{\\partial f}{\\partial y} \\right).$\n\nIn Conjugate-pair basis: $df = \\partial f + \\overline{\\partial}f = \\frac{\\partial f}{\\partial z}dz + \\frac{\\partial f}{\\partial\\overline{z}}d\\overline{z}.$\n\nIn Polar kime coordinates:\n\n$$f'(k) = \\frac{\\partial f(k)}{\\partial k} = \\frac{1}{2}\\left( \\cos\\varphi\\frac{\\partial f}{\\partial r} - \\frac{1}{r}\\sin\\varphi\\frac{\\partial f}{\\partial\\varphi} - i\\left( \\sin\\varphi\\frac{\\partial f}{\\partial r} + \\frac{1}{r}\\cos\\varphi\\frac{\\partial f}{\\partial\\varphi} \\right) \\right) = \\frac{e^{- i\\varphi}}{2}\\left( \\frac{\\partial f}{\\partial r} - \\frac{i}{r}\\ \\frac{\\partial f}{\\partial\\varphi} \\right)$$\n\nand\n\n$$f'\\left( \\overline{k} \\right) = \\frac{\\partial f\\left( \\overline{k} \\right)}{\\partial\\overline{k}} = \\frac{1}{2}\\left( \\cos\\varphi\\frac{\\partial f}{\\partial r} - \\frac{1}{r}\\sin\\varphi\\frac{\\partial f}{\\partial\\varphi} + i\\left( \\sin\\varphi\\frac{\\partial f}{\\partial r} + \\frac{1}{r}\\cos\\varphi\\frac{\\partial f}{\\partial\\varphi} \\right) \\right) = \\frac{e^{i\\varphi}}{2}\\left( \\frac{\\partial f}{\\partial r} + \\frac{i}{r}\\ \\frac{\\partial f}{\\partial\\varphi} \\right).$$\n\n*Notes*:\n\n-   The derivatives in terms of the polar coordinates are obtained by transforming the Cartesian complex variable $z = (x,y)$ into the complex-time (kime) variable $k = (r,\\varphi)$ using polar transformations:\n\n$$\\left \\| \\begin{matrix} x = r\\ \\cos\\varphi \\\\ y = r\\ \\sin\\varphi \\\\ \\end{matrix} \\right . $$\n$$\\left \\| \\begin{matrix} r = \\sqrt{x^{2} + y^{2}} \\\\ \\varphi = \\arctan\\left( \\frac{y}{x} \\right) = \\arctan{(y,x)} \\\\ \\end{matrix} \\right. $$\n\n$$\\left\\| \\begin{matrix} \\frac{\\partial}{\\partial x} = \\cos\\varphi\\frac{\\partial}{\\partial r} - \\frac{1}{r}\\ \\sin\\varphi\\frac{\\partial}{\\partial\\varphi} \\\\ \\frac{\\partial}{\\partial y} = \\sin\\varphi\\frac{\\partial}{\\partial r} + \\frac{1}{r}\\ \\cos\\varphi\\frac{\\partial}{\\partial\\varphi} \\\\ \\end{matrix}\n\\right. $$ \nsee Korn GA, Korn TM. [Mathematical handbook for scientists and engineers: definitions, theorems, and formulas for reference and review, Courier Corporation; 2000](https://books.google.com/books/about/Mathematical_Handbook_for_Scientists_and.html?id=8OZqAAAAMAAJ).\n\n-   Using the chain-rule of differentiation, we can derive the Cartesian coordinate derivatives by transforming the conjugate-pairs basis\n\n$$(x,y) \\rightarrow \\left( \\frac{1}{2}\\left( z + \\overline{z} \\right),\\ \\ \\ \\frac{1}{2i}\\left( z - \\overline{z} \\right) \\right),$$\n\n$$\\frac{\\partial}{\\partial z} = \\frac{\\partial}{\\partial x}\\frac{\\partial x}{\\partial z} + \\frac{\\partial}{\\partial y}\\frac{\\partial y}{\\partial z}.$$\n\nTherefore, $\\frac{\\partial x}{\\partial z} = \\frac{1}{2}\\frac{\\partial\\left( z + \\overline{z} \\right)}{\\partial z} = \\frac{1}{2}$ and $\\frac{\\partial y}{\\partial z} = \\frac{1}{2i}\\frac{\\partial\\left( z - \\overline{z} \\right)}{\\partial z} = \\frac{1}{2i} = - \\frac{i}{2}.$\n\nSimilarly,\n\n$\\frac{\\partial x}{\\partial\\overline{z}} = \\frac{1}{2}\\frac{\\partial\\left( z + \\overline{z} \\right)}{\\partial\\overline{z}} = \\frac{1}{2}$ and $\\frac{\\partial y}{\\partial\\overline{z}} = \\frac{1}{2i}\\frac{\\partial\\left( z - \\overline{z} \\right)}{\\partial\\overline{z}} = - \\frac{1}{2i} = \\frac{i}{2}$.\n\nThis explains the Cartesian coordinate derivatives:\n\n$f'(z) = \\frac{\\partial f(z)}{\\partial z} = \\frac{1}{2}\\left( \\frac{\\partial f}{\\partial x} - i\\frac{\\partial f}{\\partial y} \\right)$ and $f'\\left( \\overline{z} \\right) = \\frac{\\partial f\\left( \\overline{z} \\right)}{\\partial\\overline{z}} = \\frac{1}{2}\\left( \\frac{\\partial f}{\\partial x} + i\\frac{\\partial f}{\\partial y} \\right)$.\n\nBelow, we present the core principles of *Wirtinger* differentiation} and *integration*:\n\n-   Complex conjugation ($\\overline{z}\\mathbb{\\in C}$) for $z = (x + iy\\mathbb{) \\in C}$ is defined by $\\overline{z} = x - iy$, so that the square norm of $z$ is: $z\\overline{z} = (x + iy)(x - iy) = x^{2} + y^{2} - ixy + ixy = x^{2} + y^{2} = \\left\\| z \\right\\|^{2}$. Solving for $x$ and $y$, in terms of $z$ and $\\overline{z}$ we get:\n\n$$\\left| \\begin{matrix}\nx = \\frac{1}{2}\\left( z + \\overline{z} \\right) \\\\\ny = \\frac{1}{2i}\\left( z - \\overline{z} \\right) \\\\\n\\end{matrix}\\ . \\right.\\ $$\n\n-   We can effectively change the variables: $(x,y) \\rightarrow (z,\\overline{z})$. Thus, all complex functions $f\\mathbb{:C \\rightarrow C}$ can be thought of as $f = f(x,y)$ or as $f = f(z,\\overline{z})$.\n\n*Wirtinger differentiation*: The Wirtinger derivative of $f$, $df_{z}$ is an $\\mathbb{R}$-linear operator on the tangent space $T_{z}\\mathbb{C \\cong C}$, i.e., $df_{z}$ is a differential 1-form on $\\mathbb{C}$. However, any such $\\mathbb{R}$-linear operator ($A$) on $C$ can be uniquely *decomposed* as $A = B + C$, where $B$ is its complex-linear part\\* (i.e., $B(iz) = iBz$), and $C$ is its *complex-antilinear part* (i.e., $C(iz) = - iCz$). The reverse (*composition*) mapping is $Bz = \\frac{1}{2}\\left( Az - iA(iz) \\right)$ and $Cz = \\frac{1}{2}\\left( Az + iA(iz) \\right)$.\n\nFor the Wirtinger derivative, this duality of the decomposition of $\\mathbb{R}$-linear operators characterizes the conjugate partial differential operators $\\partial$ and $\\overline{\\partial}$. That is, for all differentiable complex functions $f\\mathbb{:C \\rightarrow C}$, the derivative can be uniquely decomposed as $\\mathbf{d}\\mathbf{f}_{\\mathbf{z}}\\mathbf{= \\partial f +}\\overline{\\mathbf{\\partial}}\\mathbf{f}$, where $\\partial$ is its *complex-linear part* ($\\partial iz = i\\partial z$), and $\\overline{\\partial}$ is its complex-antilinear part\\* ($\\overline{\\partial}iz = - i\\overline{\\partial}z$).\n\nApplying the operators $\\frac{\\mathbf{\\partial}}{\\mathbf{\\partial z}}$ and $\\frac{\\mathbf{\\partial}}{\\mathbf{\\partial}\\overline{\\mathbf{z}}}$ to the identify function ($z \\rightarrow z = x + iy$) and its complex-conjugate ($z \\rightarrow \\overline{z} = x - iy$) yields the natural derivatives: $dz = dx + idy$ and $d\\overline{z} = dx - idy$. For each point in $\\mathbb{C}$, $\\{ dz,d\\overline{z}\\}$ represents a conjugate-pair basis for the $\\mathbb{C}$ cotangent space, with a dual basis of the partial differential operators:\n\n$$\\left\\{ \\frac{\\partial}{\\partial z}\\ ,\\ \\frac{\\partial}{\\partial\\overline{z}} \\right\\}\\ .$$\n\nThus, for any smooth complex functions $f\\mathbb{:C \\rightarrow C}$,\n\n$$df = \\partial f + \\overline{\\partial}f = \\frac{\\partial f}{\\partial z}dz + \\frac{\\partial f}{\\partial\\overline{z}}d\\overline{z}\\ .$$\n\n## Wirtinger calculus\n\n-   The *path-integral* is the simplest way to integrate a complex function $f\\mathbb{:C \\rightarrow C}$ on a specific path connecting $z_{a}\\mathbb{\\in C}$ to $z_{b}\\mathbb{\\in C}$. Generalizing Riemann sums:\n\n$$\\lim_{\\left| z_{i + 1} - z_{i} \\right| \\rightarrow 0}{\\sum_{i = 1}^{n - 1}\\left( f(z_{i})(z_{i + 1} - z_{i}) \\right)} \\cong \\oint_{z_{a}}^{z_{b}}{f\\left( z_{i} \\right)dz}\\ .$$\n\nThis assumes the path is a polygonal arc joining $z_{a}$ to $z_{b}$, via $z_{1} = z_{a},\\ z_{2},\\ z_{3},\\ ,\\ z_{n} = z_{b}$, and we integrate the piecewise constant function $f(z_{i})$ on the arc joining $z_{i} \\rightarrow z_{i + 1}$. Clearly the path $z_{a} \\rightarrow z_{b}$ needs to be defined and the limit of the generalized Riemann sums, as $n \\rightarrow \\infty$, will yield a complex number representing the Wirtinger integral of the function over the path. Similarly, we can extend the classical area integrals, indefinite integral, and Laplacian:\n\n-   Definite area integral: for $\\Omega \\subseteq \\mathbb{C}$, $\\int_{\\Omega}^{}{f(z)dzd\\overline{z}}$.\n\n-   Indefinite integral: $\\int_{}^{}{f(z)dzd\\overline{z}}$, $df = \\frac{\\partial f}{\\partial z}dz + \\frac{\\partial f}{\\partial\\overline{z}}d\\overline{z}$,\n\n-   The Laplacian in terms of conjugate pair coordinates is $\\nabla^{2}f \\equiv \\mathrm{\\Delta}f = 4\\frac{\\partial}{dz}\\frac{\\partial f}{d\\overline{z}} = 4\\frac{\\partial}{d\\overline{z}}\\frac{\\partial f}{dz}\\ .$\n\nMore details about Wirtinger calculus of differentiation and integration are provided later.",
      "word_count": 1064
    },
    {
      "title": "Chapter 3 Appendix",
      "content": "## kime-magnitude-time-derivatives\n\nIn general, the classical concepts of derivative and rate of change are well defined for a kime-process $f(t,\\varphi)$ with respect to the (positive) real variable time ($t \\equiv r = |\\kappa| = |r\\ e^{i\\varphi}|$). Depending on the context, we interchangeably use $\\varphi, \\phi, \\psi, \\theta$ and other lowercase Greek letters to represent the kime-phases. The smooth *time rate of change* of the process $f$ is explicated as the classical partial derivative, $\\frac{\\partial f}{\\partial r}$.\n\nAbove we showed that the partial derivatives of a kime-process in Cartesian kime coordinates are also well defined, using Wirtinger differentiation. However, in the polar representation, this strategy may not apply for the second variable (kime-phase), $\\varphi$, which could be interpreted as a distribution. In essence, $\\varphi$ represents an unobservable random sampling variable from a symmetric distribution compactly-supported on $\\lbrack - \\pi,\\pi)$, or a periodic distribution.\n\n## Kime-Phase Derivatives\n\nAbove we showed that in polar kime coordinates, analytic functions can be differentiated as follows\n\n$$f'(k) = \\frac{\\partial f(k)}{\\partial k} = \\frac{1}{2}\\left( \\cos\\varphi\\frac{\\partial f}{\\partial r} - \\frac{1}{r}\\sin\\varphi\\frac{\\partial f}{\\partial\\varphi} - i\\left( \\sin\\varphi\\frac{\\partial f}{\\partial r} + \\frac{1}{r}\\cos\\varphi\\frac{\\partial f}{\\partial\\varphi} \\right) \\right) = \\frac{e^{- i\\varphi}}{2}\\left( \\frac{\\partial f}{\\partial r} - \\frac{i}{r}\\ \\frac{\\partial f}{\\partial\\varphi} \\right)$$\n\nand\n\n$$f'\\left( \\overline{k} \\right) = \\frac{\\partial f\\left( \\overline{k} \\right)}{\\partial\\overline{k}} = \\frac{1}{2}\\left( \\cos\\varphi\\frac{\\partial f}{\\partial r} - \\frac{1}{r}\\sin\\varphi\\frac{\\partial f}{\\partial\\varphi} + i\\left( \\sin\\varphi\\frac{\\partial f}{\\partial r} + \\frac{1}{r}\\cos\\varphi\\frac{\\partial f}{\\partial\\varphi} \\right) \\right) = \\frac{e^{i\\varphi}}{2}\\left( \\frac{\\partial f}{\\partial r} + \\frac{i}{r}\\ \\frac{\\partial f}{\\partial\\varphi} \\right).$$\n\nThe problem with computing and interpreting $f'(k)$, and in particular $\\frac{\\partial f}{\\partial\\varphi}$, is that $f(r,\\varphi)$ may *not be analytic*. This is especially interesting since one plausible spacekime interpretation of the enigmatic kime-phase is that $\\varphi$ indexes the intrinsic stochastic sampling from some symmetric distribution, which corresponds to repeated sampling, or multiple IID random observations. This suggests that $\\varphi$ does not vary smoothly, but is rather a chaotic quantity (random variable) following some symmetric distribution $\\Phi_{}$, supported over $\\lbrack - \\pi,\\pi)$. In other words, the \\*kime-phase change} quantity $\\partial\\varphi$ may be stochastic! Hence, we need to investigate approaches to:\n\n-   Define *paths* through the kime domain,\n-   Compute (in a measure-theoretic probabilistic sense) *changes* in the kime-phase,\n-   Estimate *probability likelihoods* on regular *intervals* $(a,b) \\subseteq \\lbrack - \\pi,\\pi)$ as well as over (generally measurable) \\*Borel sets} $B \\subseteq \\lbrack - \\pi,\\pi)$, which are not necessarily contiguous intervals,\n-   Ultimately explicate the process *partial derivative* with respect to $\\varphi$, $\\frac{\\partial f}{\\partial\\varphi}$,\n-   Define kime-space probability measures, $p_{\\kappa}$, that facilitate modeling random experiments involving observing repeated samples from controlled experiments (reflecting the kime-phase distribution) across a range of time-points (kime-magnitudes).\n\nNote that the differentiability of $f(k)$ is not in question for the Cartesian representation of the kime domain, as Wirtinger derivatives explicate the multivalued differentiation of a multi-variable function, e.g., $f'(k),\\ \\forall\\kappa \\equiv (x,y\\mathbb{) \\in C}$. Wirtinger differentiation provides a robust formulation even in the polar coordinate representation of the complex domain.\n\nThe problem arises in the spacekime-interpretation where we try to tie the analytical math formalism with the computational algorithms and the quantum-physics/statistical/experimental interpretation of the kime-phase as an intrinsically stochastic repeated sampling variability.\n\nThe following well-known lemma suggests that we can quantify the \\*distribution of the kime-phase changes} in the case when we merge the math formalism, computational modeling, and the statistical/experimental interpretation of the stochastic kime-phases.\n\nWe'll use the standard convention where capital letters ($\\Gamma,\\Phi,\\Theta$) denote *random variables* and their lower-case counterparts ($\\gamma,\\phi,\\theta$) denote the corresponding values of these quantities. Let's denote by $P \\equiv \\Pr$ the probability functions with proper subindices indicating the specific corresponding random process.\n\n*Lemma*: Suppose $\\phi,\\ \\theta \\sim F_{\\mathcal{D}}$ are IIDs associated with some distribution $\\mathcal{D}$, with CDF $F_{\\mathcal{D}}$ and PDF $f_{\\mathcal{D}}$. Then, the distribution of their difference $\\gamma = \\phi - \\theta \\sim F_{\\Gamma}$ is effectively the autocorrelation function\n\n$$f_{\\Gamma}(\\gamma) = \\ \\ \\int_{- \\infty}^{\\infty}{f_{\\mathcal{D}}(\\phi)f_{\\mathcal{D}}(\\phi - \\gamma)d\\phi}\\ .$$\n\n*Proof*: This lemma quantifies the *phase-change distribution*, not the actual phase-distribution. We'll start with the most general case of two *independent* random variables, $\\Phi\\bot\\Theta$, following potentially different distributions, $\\Phi \\sim F_{\\Phi}$, $\\Theta \\sim F_{\\Theta}$.\n\nLet's compute the cumulative distribution of the difference random variable $\\Gamma = \\Phi - \\Theta$. In practice, $\\Gamma$ corresponds to the instantaneous change in the kime-phase between two random repeated experiments. Note that due to the exchangeability property associated with IID sampling {See SOCR \\href{https://www.overleaf.com/project/609571e13cd1ae42d5628747}{DL\\_NN\\_Sufficiency\\_Invariance\\_Exchangeability\\_Paper} on Overleaf}, the distribution of $\\Gamma = \\Phi - \\Theta$ is invariant with respond to the sequential order at which we record the pair of random observations. More specifically, if the random sample of kime-phases is $\\{\\phi_{1},\\ \\phi_{2},\\ \\phi_{3},\\ \\cdots,\\ \\phi_{n},\\ \\cdots\\}$, then for any index permutation $\\pi( \\cdot )$, the distributions of\n\n$$\\Gamma_{1} = \\Phi_{1} - \\Phi_{2},\\ \\ \\Gamma_{1}' = \\Phi_{\\pi(1)} - \\Phi_{\\pi(2)}$$\n\nare the same, i.e., $\\Gamma_{1},\\ \\Gamma_{1}' \\sim F_{\\Gamma}$. Hence, the *phase-change distribution*, $F_{\\Gamma}$, is symmetric.\n\nLet's compute the CDF of $\\Gamma$, the difference between any two kime-phases,\n\n$$F_{\\Gamma}(\\gamma) \\equiv P_{\\Gamma}(\\Gamma \\leq \\gamma) = P_{\\Gamma}(\\Phi - \\Theta \\leq \\gamma) = P_{\\Phi}(\\Phi \\leq \\gamma + \\Theta)\\ .$$\n\nJointly, $(\\phi,\\ \\theta) \\sim f_{(\\Phi,\\Theta)}$ and\n\n$$\\underbrace{\\ \\ F_{\\Gamma}(\\gamma)\\ }_{difference\\ CDF} = \nP_{\\Phi}(\\Phi \\leq \\gamma + \\Theta) = \\int_{- \\infty}^{\\infty}{\\int_{- \\infty}^{\\gamma + \\theta}\n{\\underbrace{f_{\\Phi,\\Theta}(\\phi,\\ \\theta)}_{joint\\ PDF}\\ d\\phi d\\theta}}\\ \\  \\Longrightarrow$$\n\n$$\\underbrace{f_{\\Gamma}(\\gamma)}_{difference\\ PDF} = \n\\frac{d}{d\\gamma}F_{\\Gamma}(\\gamma) = \\frac{d}{d\\gamma}\\int_{- \\infty}^{\\infty}{\\int_{- \\infty}^{\\gamma + \\theta}\n{\\underbrace{f_{\\Phi,\\Theta}(\\phi,\\ \\theta)}_{joint\\ PDF}\\ d\\phi d\\theta}}\\ .$$\n\nFor a fixed $\\theta_{o}$, we'll make a variable substitution $\\phi = \\nu + \\theta_{o}$, so that $d\\phi = d\\nu$. Hence,\n\n$$f_{\\Gamma}(\\gamma) = \\int_{- \\infty}^{\\infty}{\\left( \\frac{d}{d\\gamma}\\int_{- \\infty}^{\\gamma}{f_{\\Phi,\\Theta}(\\nu + \\theta,\\ \\theta)\\ d\\nu} \\right)d\\theta}\\ .$$\n\nIntegrals are functions of their (lower and upper) limits and for any function $g(\\phi,\\theta_{o})$,\n\n$$\\frac{d}{d\\gamma}\\int_{l(\\gamma)}^{h(\\gamma)}{g\\left( \\phi,\\ \\theta_{o} \\right)\\ d\\phi} = g\\left( h(\\gamma),\\ \\theta_{o} \\right)h'(\\gamma) - g\\left( l(\\gamma),\\ \\theta_{o} \\right)l'(\\gamma).$$\n\nIn particular, $\\frac{d}{d\\gamma}\\int_{- \\infty}^{\\gamma}{g\\left( \\phi,\\ \\theta_{o} \\right)\\ d\\phi} = g\\left( \\gamma,\\ \\theta_{o} \\right)$.\n\nTherefore, for any $\\theta$,\n\n$$f_{\\Gamma}(\\gamma) = \\int_{- \\infty}^{\\infty}{\\underbrace{\\left( \\frac{d}{d\\gamma}\\int_{- \\infty}^{\\gamma}{f_{\\Phi,\\Theta}(\\nu + \\theta,\\ \\theta)\\ d\\nu} \\right)}_{\\theta\\ is\\ fixed}\\ d\\theta} = \\int_{- \\infty}^{\\infty}{f_{\\Phi,\\Theta}(\\gamma + \\theta,\\ \\theta)d\\theta}\\ .$$\n\nThis derivation of the density of the kime-phase difference is rather general, $f_{\\Gamma}(\\gamma) = f_{\\Phi,\\Theta}(\\gamma + \\theta,\\ \\theta)d\\theta$. It works for any bivariate process, whether or not $\\phi,\\theta$ are independent, correlated, or follow the same distribution.\n\nIn the special case when $\\phi,\\ \\theta \\sim F_{\\mathcal{D}}$ are IIDs, we can apply another change of variables transformation, $\\gamma + \\theta = \\phi$, to get $d\\theta = d\\phi$ and\n\n$$f_{\\Gamma}(\\gamma) = \\int_{- \\infty}^{\\infty}{f_{\\Phi,\\Theta}(\\phi,\\phi - \\gamma)d\\phi}\\ \\ \n\\underbrace{\\ \\  = \\ \\ }_{\\phi\\bot\\theta}\\ \\ \\int_{- \\infty}^{\\infty}{f_{\\Phi}(\\phi)\\ \nf_{\\Theta}(\\phi - \\gamma)\\ d\\phi} \\underbrace{\\ \\  = \\ \\ }_{IIDs}\\ \n\\ \\int_{- \\infty}^{\\infty}{f_{\\mathcal{D}}(\\phi)\\ f_{\\mathcal{D}}(\\phi - \\gamma)\\ d\\phi}\\ .$$\n\nIn the most general case, we would like to compute the measure (i.e., size) of any Borel subset of the support $\\lbrack - \\pi,\\pi)$ of the distribution $\\mathcal{D}$. This will help with answering questions (2) and (3) from the above list. Of course, for any finite or countable Borel set, $B = \\left\\{ \\phi_{i} \\right\\}_{1}^{\\alpha}\\mathcal{\\sim D}$, it's measure will be trivial, $\\mu(B) = 0$. However, many regular (e.g., contiguous) subsets $B' \\subseteq \\lbrack - \\pi,\\pi)$ will be non-trivial, i.e., $0 < f_{\\mathcal{D}}\\left( B' \\right) \\leq 1$.\n\nRecall that the entire *observable universe* is finite, since the age of the universe is $\\sim 13.8$ billion years. Due to the continuously faster universal expansion, from any point in spacetime the *radius* and the full *diameter* of the entire visible universe are 46 billion and 93 billion light-years, respectively, see [article 1](https://www.nature.com/articles/415374a) and [article 2](https://doi.org/10.1038%2Fscientificamerican0305-36). Hence, the \\*amount of observable data is always finite}. In statistical data-science terms, the number of observations and sample-sizes are always finite. However, theoretically, we can model them as infinitely increasing, as in the first two laws of probability theory, the [central limit theorem (CLT)](https://doi.org/10.1080/10691898.2008.11889560) and the [law of large numbers (LLN)](https://doi.org/10.1080/10691898.2009.11889499).\n\nNow, let's assume that we have a sequence of randomly sampled kime-phases $\\left\\{ \\phi_{i} \\right\\}_{1}^{\\alpha}\\mathcal{\\sim D}$, where $\\alpha < \\infty$. In other words, assume we have observed a finite number of repeated measurements corresponding to multiple observations (corresponding to different kime-phase directions) acquired/recorded at the same *time* under identical experimental conditions. The phase-dispersion is the *variability observed in the recorded measurements*, e.g., $\\sigma_{(\\phi - \\theta)}$, is directly related to the distribution of the kime-phase differences, $F_{\\Gamma} = F_{(\\Phi - \\Theta)}$.\n\nNote that changes in spacetime (jointly spacetime or separately space and time alone), always permit analytic calculations, since the presence of significant intrinsic spatiotemporal correlations induce smooth process dynamics over 4D Minkowski spacetime. However, this analyticity may be broken in the 5D spacekime, especially in the kime-phase dimension, since kime-phase indexing of observations in inherently stochastic, rather than smooth or analytic.\n\nIn our special bivariate case above, we can assume $\\phi \\equiv \\phi_{i},\\ \\ \\theta \\equiv \\phi_{j} \\sim F_{\\mathcal{D}}$ are a pair of IIDs associated with an *a priori* kime-phase distribution $\\mathcal{D}$ and corresponding to identical experimental conditions. For simplicity, we are only considering a pair of fixed kime-phase indices $1 \\leq i < j$. Since *both kime-phase distributions coincide*, $\\Phi \\equiv \\Theta \\equiv \\mathcal{D}$, the above Lemma shows that the PDF $f_{\\Gamma}(\\gamma)$ of their difference ($\\gamma = \\phi - \\ \\theta \\equiv \\phi_{i} - \\phi_{j}$) is the *autocorrelation function*\n\n$$f_{\\Gamma}(\\gamma) = \\int_{- \\infty}^{\\infty}{f_{\\Phi}(\\phi)f_{\\Theta}(\\phi - \\gamma)d\\phi} = \\int_{- \\infty}^{\\infty}{f_{\\mathcal{D}}(\\phi)f_{\\mathcal{D}}\n\\left( \\underbrace{\\phi - \\gamma}_{\\theta} \\right)d\\phi} = \n\\mathbb{E}(\\phi,\\ \\theta) = \\underbrace{R_{phase}(\\phi_{i},\\phi_{j})}_{autocorrelation}\\ .$$\n\n## Problems & Questions\n\n-   What if the kime-phase distribution is \\*infinitely supported}? How can we interpret the phase distribution $\\Phi$ over the finite bounded interval $- \\pi \\leq \\phi < \\pi$?\n\n-   Does it make sense to interpret the phase distribution in terms of spherical coordinates as *helical*, instead of *circular* distribution?\n\n-   Can we account for both *compactly supported* and *infinitely supported* (univariate) distributions by applying *renormalization* [adjusting for self-interaction feedback](https://en.wikipedia.org/wiki/Renormalization), or a [*regularization*](https://en.wikipedia.org/wiki/Regularization_(physics)). Clearly there are many different (injective or bijective) transformations that map the domain of the reals to $\\lbrack - \\pi,\\pi)$. For instance,\n\n$$T_{o}:( - \\pi,\\pi)\\mathbb{\\overbrace{\\longrightarrow}^{bijective} R},\\ \\ \n      T_{o}(x) = \\frac{x}{\\sqrt{\\pi^{2} - x^{2}}},$$\n\n$$T_{1}:\\left\\lbrack - \\frac{\\pi}{\\sqrt{2}},\\frac{\\pi}{\\sqrt{2}} \\right)\\mathbb{\\rightarrow R},\\ \\ T_{1}(x) = {cotan}\\left( \\arccos\\frac{x}{\\sqrt{\\pi^{2} - x^{2}}} \\right), $$ see [Wolfram Alpha](https://www.wolframalpha.com/input?i=cotan+%5C%28arccos%5C%28x%5C%2F%5C%28Sqrt%5C%28Pi%5C%5E2-x%5C%5E2%5C%29%5C%29%5C%29), [cotan(arccos(x/(Sqrt(Pi\\^{}2-x\\^{}2))))](https://wolframalpha.com),\n\n$$T_{2}\\mathbb{:R \\rightarrow \\lbrack -}\\pi,\\pi),\\ \\ T_{2}(x) = 2\\arctan(x),$$ one-to-one injective, see [Wolfram Alpha, 2\\*arctan(x)](https://www.wolframalpha.com/input?i=2*arctan+%28x%29),\n\n$$T_{3}\\mathbb{:R \\rightarrow}\\lbrack - \\pi,\\pi),\\ T_{3}(x) = \\ \\ \\frac{2\\pi}{1 + e^{x}} - \\pi,$$ one-to-one injective, see [Wolfram Alpha, (2\\*Pi)/(1+exp(x))-Pi](https://www.wolframalpha.com/input?i=%282*Pi%29%2F%281%2Bexp%28x%29%29+-Pi).\n\nDoes it make sense to denote the *density function of the kime-phase differences* $\\gamma = \\phi - \\theta$, by $f_{\\Gamma}(\\gamma) = f_{\\Gamma}(\\phi_{i} - \\phi_{j})$ or by just by $f_{\\Gamma}(\\gamma) = f_{\\Gamma}(i - j)$? In other words, can we drop the explicit phase reference and denote the density of the kime-phase differences simply by $f_{\\Gamma}(\\gamma)\\mathbb{= E}\\left( \\phi(i),\\phi(j) \\right)\\mathbb{\\equiv E}(i,j) = R_{phase}(i,j)$?\n\n\\item\n\nAt a given kime, $k = te^{i\\varphi}$, we only have an analytic representation of the *distribution* of the kime-phase difference, *not* an explicit analytic *function* estimate of the actual kime-phase change or its derivative (as the kime-phase is random). In this situation, how can we define the partial derivative (or a measure-theoretic/probabilistic rate of change) of the original process of interest, $f'(k)$? Specifically, even though we know how to compute $\\frac{\\partial f}{\\partial r}$, we also need to estimate $\\frac{\\partial f}{\\partial\\varphi}$ but only having the probability *density function of the kime-phase differences* $\\Delta\\varphi = \\varphi_{i} - \\varphi_{j}$, $1 \\leq i < j$. In other words, we can work with the distribution\\* of $\\Delta\\varphi$, but can't estimate a specific *value* for this rate of change, $\\Delta\\varphi$.\n\n-   Formulate the partial derivate, $\\frac{\\partial f(\\kappa(r,\\varphi))}{\\partial\\varphi}$ as a [*Radon-Nikodym derivative*](https://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem#Radon%E2%80%93Nikodym_derivative) with respect to the kime-phase distribution, $\\varphi \\sim \\Phi$ or the distribution of the phase-change, $\\Delta\\varphi = \\varphi_{i} - \\varphi_{j} \\sim \\Gamma$, as derived above.\n\n## Analyticity, Measurability and Observability\n\nAbove discussed the notion of [analyticity](https://en.wikipedia.org/wiki/Analytic_function), which reflects functions $f:\\mathbb{C}\\to \\mathbb{C}$ whose Taylor series expansion about $z_o$ converges to the functional value $f(z)$ in some neighborhood $\\forall z\\in \\mathcal{N}_{z_o}\\subseteq \\mathbb{C}$.\n\nThe (physical) *measurability* property is associated with precise ability to track the value of some physical observable of a quantum system. This typically involves some kind of apparatus for measuring specific properties so that during the process of acquiring this information, the system survives the measurement after the measurement process is complete *and* an immediate follow up measurement of the same quantity would yield the same outcome, or at least a highly stable result. In reality, few experiments yield such measurements where the system being experimentally studied is physically unchanged, not modified, or preserved by the measurement process itself. Consider an extreme example of a *Geiger counter measurement* counting the number of positrons, or a more simple *photo-detector counting the number of photons* in a single mode cavity field. The tracked particles (photons or positrons) counted (tracked) by the device (photo-detector or Geiger counter) are actually absorbed to create a pulse, current, or an electrical impulse. While we count (track) the number of particles in the field, we transform the signal (particle dynamics) into some form of electrical data. After the measurement is completed, the state of the measurable cavity field is left in a base vacuum state $|0\\rangle$, which has no *memory* of the state of the field before the experiment took place and we recorded the digital measurement. However, we may be able redesign the experiment by using the observed outcome measurements (reflecting the number of particles detected in the cavity during a fixed time period) and every time we then measure the number of particles in the cavity, we always maintain the same level of particles in the cavity, $n$. With such feedback loop, the experimental procedure can maintain the cavity field in a constant state of $n$ particles, i.e., we can assign the state $|n\\rangle$ to the cavity field.\n\nThe [Von Neumann Measurement Postulate](https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics) reflects the *state of the system immediately after a measurement is made.*\n\n-   Suppose we are measuring a characteristic $Q$ for a system in state $|\\psi\\rangle$ and observe a result $q_n$. Then, the state of the system immediately after the measurement is $|q_n\\rangle$.\n\nThe Von Neumann measurement postulate may be expressed using projection operators $\\hat{P}_n = |q_n\\rangle\\langle q_n|$, where the state of the system after the measurement yields the result $q_n$ is\n\n$$\\frac{\\hat{P}_n|\\psi\\rangle}{\\sqrt{\\langle \\psi |\\hat{P}_n|\\psi\\rangle}}=\n\\frac{\\hat{P}_n|\\psi\\rangle}{\\sqrt{|\\langle {q}_n|\\psi\\rangle |^2}}\\ .$$\n\nNote that the denominator normalizes the state after the measurement to unity.\n\nThis postulate suggests that after performing a measurement and observing a particular outcome, if we immediately repeat the experiment and obtain another measurement we can expect to observe the same result, indicating the system is in the associated *eigenstate.* The fact that the outcome scalar value is stable upon *repeated measurement* indicating that the system is consistent and reliably yields the same instantaneous value after a measurement.\n\nIn *statistical terms*, the Von Neumann measurement postulate parallels the notion of repeated measurements corresponding to multiple (large) samples from the kime-phase distribution $\\{\\phi_i\\}_i \\sim \\Phi[-\\pi,\\pi)$. All measurable properties represent *pooled sample statistics*, such as the sample arithmetic average, which has an expected value equal to the phase distribution mean,\n\n$$\\mathbb{E}\\left (\\frac{1}{n}\\sum_{i=1}^n {\\phi_i}\\right )=\\mu_{\\Phi}\\ .$$\n\nIn [TCIU Chapter 6 (Kime-Phases Circular distribution)](https://www.socr.umich.edu/TCIU/HTMLs/Chapter6_Kime_Phases_Circular.html), the agreement between observed analyticity and Von Neumann measurability of real observables and the stochastic quantum mechanical nature of the enigmatic kime-phases.\n\nThis connection between quantum mechanical and relativistic properties is rooted in the fact that physical observable processes rely on *phase-aggregators* to pool quantum fluctuations. This process effectively *smooths* and *denoises* the actual quanta characteristics and yields highly polished classical physical measurements. This duality between quanta (single experimental data point) and relativistic observables (sample/distribution driven evidence) is related to the effort to define a kime operator whose eigenvalues are observable scalar measures of kime-phase aggregators (e.g., sample phase mean) and their corresponding eigenvectors are the state-vectors (e.g., system experimental states).\n\nLet's expand on the notions of kime-phase observability, operator eigenspectra, and kime operator. We'll attempt to draw direct parallels between the quantum physics (statistical mechanics) and data science (statistical inference). Starting with a physical question (e.g., computing the momentum of a particle) or a statistical inference problem (e.g., estimate a population mean), we can run as experiment to collect data, which can be used to calculate the expectation value $X$, a *physical observable* (energy, momentum, spin, position, etc.) or another statistic (dispersion, range, IQR, 5th percentile, CDF, etc.)\n\n$$ \\mathbb E(X)= \\sum_i x_i\\,p(x_i)\\ ,$$ where $\\{x_i\\}_i$ are the possible outcome values and $\\{p(x_i)\\}_i$ are their corresponding probabilities of those outcomes. Indeed, if the outcomes are continuous, the expectation is naturally formulated in terms of an integral.\n\nWe can associate a vector $|x_i\\rangle$ to each of these outcome states $x_i$ and since we assume IID sampling (random observations), we can consider orthonormal states, $\\langle x_i | x_j\\rangle =\\delta_{ij}$. As quantum mechanics and matrix algebra (linear models) are *linear*, a superposition state of outcomes (solutions) $\\{|x_i\\rangle \\}_{i}$ represents another a valid outcome solution $|\\phi \\rangle =\\sum_i {\\alpha_i |x_i\\rangle }$. By the Born rule postulate, the interpretation of the superposition state is extends the probability of observing a base state $|x_i\\rangle$, $p(x_i)=|\\alpha_i|^2$, where composite state $|\\phi \\rangle$ is normalized, $$\\sum_i {\\alpha_i^2}=1\\ .$$\n\nBy orthonormality and using Dirac notation, $$\\alpha_i = \\langle x_i|\\psi \\rangle,\\ \\alpha_i^* = \\langle \\psi|x_i \\rangle,\\ \\forall i\\ .$$ By linearity, the *expectation value* of $X$ is\n\n$$\\begin{align}\n\\mathbb E(X) &= \\sum_i {|\\alpha_i |^2 x_i} = \\sum_i {\\alpha_i^* \\alpha_i  x_i}  \n= \\sum_i {\\langle \\psi|x_i\\rangle \\langle x_i|\\psi\\rangle x_i } \\\\\n&=\\langle \\psi|\\left( \\underbrace{\\sum_i |x_i\\rangle\\langle x_i|x_i}_{Operator,\\ \\hat{X}}\\right)|\\psi\\rangle\n\\equiv \\langle \\psi| \\hat{X} |\\psi\\rangle \n\\end{align} \\ .$$\n\nMy the [spectral theorem](https://en.wikipedia.org/wiki/Spectral_theorem), any self-adjoint (Hermitian) matrix $\\hat{X}$ can be written as $$\\hat{X} = \\sum_i {\\overbrace{\\underbrace{|\\lambda_i\\rangle}_{eigenvectors} \\langle \\lambda_i|}^{Projection\\ along\\ |\\lambda_i\\rangle} \\underbrace{\\lambda_i}_{eigenvalues}}\\ .$$ The eigenvectors $\\{|\\lambda_i\\rangle\\}_i$ form an orthonormal basis suggesting that only linear combinations of these eigenvectors of $\\hat{X}$ give the observable outcomes of experimental measurements. For instance, all vectors $\\gamma$ orthogonal to $|\\lambda_i\\rangle$ will be annihilated. When a state $|\\gamma\\rangle \\perp |\\lambda_i\\rangle,\\ \\forall i$, the state can't be written as a linear combination of the eigenvectors and it does not contribute to the expectation value.\n\nThe expectation value of an operator $\\hat {\\mathcal{K}}$ corresponds to the average outcome of many repeated measurements of the observable associated with that operator,\n\n$$\\langle\\hat {\\mathcal{K}}\\rangle_{\\psi} \\equiv \\langle\\hat {\\mathcal{K}}\\rangle\n=\\langle\\psi| \\hat {\\mathcal{K}}|\\psi\\rangle =\n\\overbrace{\\sum_{\\alpha}{ \\underbrace{\\nu_{\\alpha}\\ }_{obs.value}\\ \n\\underbrace{\\ |\\langle \\psi |\\phi_{\\alpha}\\rangle|^2}_{(transition)probability}}}\n^{weighted-average-of-outcomes }\\ ,$$ where $\\{|\\phi_{\\alpha}\\rangle\\}_{\\alpha}$ is a complete set of eigenvectors for the observable operator $\\hat {\\mathcal{K}}$, i.e., $\\hat {\\mathcal{K}}|\\phi_{\\alpha}\\rangle = \\nu_{\\alpha}| \\phi_{\\alpha}\\rangle$.\n\nIn other words, if an observable quantity of a system is measured many times, the average or `expectation value` of these measurements is given by the corresponding operator $\\hat {\\mathcal{K}}$ acting on the state of the system, $|\\psi\\rangle$. The operator expectation is important because it ties the physical interpretation (the observable) with the mathematical representation of the quantum state (the operator). Each observable is associated with a specific self-adjoint (Hermitian) operator, which ensures that the observable scalar values (the eigenvalues of the operator) are *real*.\n\nThe expectation value of the kime-phase operator $\\hat {\\mathcal{K}}_{\\varphi}$ reflects the average of the kime-phase distribution, $\\varphi\\sim\\Phi[-\\pi,\\pi)$. For most kime-phase distributions, $\\langle\\hat {\\mathcal{K}}\\rangle_{\\psi}=\\mu_{\\Phi}=0$, reflecting the symmetry f the phase distribution. The expectation value of the operator $\\hat {\\mathcal{K}}_{\\varphi}$ in a quantum state described by a normalized wave function $\\psi(x)$ is given by the integral:\n\n$$\\langle\\hat {\\mathcal{K}}\\rangle_{\\psi}=\\int_{-\\pi}^{\\pi}{\\psi^*(x)\\ \\hat {\\mathcal{K}}\\ \\psi(x)\\ dx}\\ .$$\n\nThe wave function $\\psi(x)$ encodes all the possible kime-phase states the system could be in and their corresponding probabilities. Each value $x$ in the domain of the wave function is a possible kime-phase observation drawn from the phase distribution $\\Phi$. The complex-valued wavefunction portrays the probability amplitudes correlating with these potential measurements. Bounded operators acting on these phase state vectors help draw random kime-phases or measure the observable phases.\n\n*Caution*: In the simulation below, to emphasize the differences between different observable processes, we are *artificially offsetting (shifting) the random samples*, and their corresponding mean trajectories (observable patterns). These offsets are both in the vertical (*space*) dimension as well as in the radial (*phase*) space. These offsets are are completely artificial and just intended to enhance the interpretation of the kime-phase sampling process by avoiding significant overlaps of points and observable kime-dynamic trends.\n\n\nIn this simulation, the $3$ *spatial dimensions* $(x,y,z)\\in\\mathbb{R}^3$ are \ncompressed into $1D$ along the vertical axis $z\\in\\mathbb{R}^1$, the *radial displacement* represents the time dynamics, $t\\in \\mathbb{R}^+$, and the angular scatters of three different processes, representing repeated measurements from $3$ different circular distributions\nat a fixed spatiotemporal location, are shown in different colors. Mean-dynamics\nacross time of the three different time-varying distributions are shown as smooth curves color-coded to reflect the color of their corresponding process distributions, samples, \nand sampling-distributions.\n\n## Repeated Measures Observations and Analytics\n\nData from *repeated measurements* are acquired in time-dynamic experiments tracking univariate or multivariate processes repeatedly over time. Hence, repeated measurement data are tracked over multiple points in time. The simple situation when a single response variable is measured over time (univariate case) is easier to describe the challenges, but the mode general multivariate response is more realistic in many data science applications.\n\nIn general, the responses over time may be heavily temporally correlated, yet still corrupted by noise modeled by the kime-phase distribution. Note that individual observations collected at points in time close together are likely to be more similar to one another compared to observations collected at distant times. However, even at the same time point, the phase distribution controls the level of expected dispersion/variability between multiple measurements obtained under identical experimental conditions (at the fixed point in time). Classical multivariate analysis treats observations collected at different time points as (potentially correlated) different variables.\n\n### Classical repeated measurement analytics (ANOVA)\n\nConsider a [completely randomized block design experiment](https://link.springer.com/referenceworkentry/10.1007/978-0-387-32833-1_344) to determine the effects of 5 surgical treatments on cardiovascular health outcomes where a group of $35$ *patients* grouped into each of the $5$ *treatment groups* of sizes $\\{7, 6, 9, 5, 8\\}$. the health outcomes of every *patient* $\\{p_j\\}_{j=1}^{35}$ are recorded at $10$ consecutive time points, $\\{t_k\\}_{k=0}^9$ following one of the available $5$ clinical treatment regimens, $\\{R_i\\}_{i=1}^5$. Note that the *treatment group sizes* are $\\{|R_1|=7,|R_2|=6,|R_3|=9,|R_4|=5,|R_5|=8 \\}$.\n\nThere are many alternative strategies to analyze data from such repeated sample experiment. [analysis of variance (ANOVA)](https://en.wikipedia.org/wiki/Analysis_of_variance) and [multivatiate analysis of variance (MANOVA)](https://en.wikipedia.org/wiki/Multivariate_analysis_of_variance) represent a pair of common parametric techniques.\n\nLet's make the following notations:\n\n-   $Y_{ijk}\\equiv Y_{R_i,p_j, t_k}$ be the clinical health outcome corresponding to treatment regimen $i$ for patient $j$ at time $t_k$,\n-   $a =5$, the number of treatment regimens,\n-   $n_i=|R_i|$, the number of replicates of treatment regimen $i$,\n-   $N=\\sum_i^a{n_i}$, the total number of experimental units/measurements, and\n-   $t = 10$, the number of time observations.\n\nANOVA provides a parametric statistical test (using the Fisher's F distribution) to study interactions between *treatment regimens* and *time*, using the *main effects* of *treatments* and *time*. MANOVA facilitates testing for significant *interaction effects* between treatment regimens and time, and for *main effects* of treatment regimen;\n\nThe ANOVA (linear) model $$Y_{ijk}=\\mu+\\alpha_i+\\beta_{j(i)}+\\tau_k+(\\alpha \\tau)_{ik} + \\epsilon_{ijk},$$ assumes that the health outcome data $Y_{ijk}$ for the $i^{th}$ treatment regimen $R_i$ for the $j^{th}$ patient $p_j$ at $k^{th}$ time point $t_k$ is a mixture of some (yet unknown) overall *mean* response $\\mu$, plus some the *treatment effect* $\\alpha_i$, the effect of the *patient within that treatment regimen* $\\beta_{j(i)}$, the effect of *time* $\\tau_k$, the effect of the *interaction* between treatment and time $(\\alpha \\tau)_{ik}$, and a residual error $\\epsilon_{ijk}\\sim N(0,\\sigma_{\\epsilon}^2)$.\n\nThe explicit ANOVA assumptions include:\n\n-   The *residual errors* $\\epsilon_{ijk}\\sim N(0,\\sigma_{\\epsilon}^2)$ are independently sampled from a normal distribution with mean $0$ and variance $\\sigma_{\\epsilon}^2$,\n-   The *individual patient effects* $\\beta_{j(i)}$ are also independently sampled from a normal distribution with mean 0 and variance $\\sigma_{\\beta}^2$,\n-   The *effect of time* does not depend on the patient; that is, there is *no* time-by-patient interaction. This assumption protects against the health result dependence on the patient, which implies that we could not predict much for new patients.\n\nThis *ANOVA mixed-effects model* includes a *random effect of the patient* and *fixed effects for treatment regimen and time*. The Fisher's F-test is used to carry ANOVA using the following variance decomposition summary.\n\nwhere,\n\n-   $a$ is the number of treatments,\n-   $N$ is the total number of all experimental units, and\n-   $t$ is the number of time points.\n\nSum of Squares $SS$ formulas are given below:\n\n$$\\begin{array}{lll}SS_{total}& =& \\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\sum_{k=1}^{t}Y^2_{ijk}-Nt\\bar{y}^2_{...}\\\\SS_{treat} &= &t\\sum_{i=1}^{a}n_i\\bar{y}^2_{i..} - Nt\\bar{y}^2_{...}\\\\SS_{error(a)}& =& t\\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\bar{y}^2_{ij.} - t\\sum_{i=1}^{a}n_i\\bar{y}^2_{i..}\\\\SS_{time}& =& N\\sum_{k=1}^{t}\\bar{y}^2_{..k}-Nt\\bar{y}^2_{...}\\\\SS_{\\text{treat x time}} &=& \\sum_{i=1}^{a}\\sum_{k=1}^{t}n_i\\bar{y}^2_{i.k} - Nt\\bar{y}^2_{...}-SS_{treat} -SS_{time}\\end{array}\\ .$$\n\nThe statistical inference based on the ANOVA (variance decomposition) model includes:\n\n-   Tests for interaction between *treatment-regimen* and *time*, $H_o\\colon (\\alpha\\tau)_{ik} = 0$ (no interaction), versus an alternative, $H_a\\colon (\\alpha\\tau)_{ik} \\not= 0$. $H_o$ may bean rejected at a predefined palse-positive level $\\alpha$ if\n\n$$F = \\overbrace{\\frac{MS_{\\text{treat x time}}}{MS_{error(b)}}}^{obs.\\ test\\ statistic} > \n\\overbrace{F_{\\underbrace{(a-1)(t-1)}_{df_1}, \\underbrace{(N-a)(t-1)}_{df_2}, \n \\underbrace{\\alpha}_{signif.}}}^{F-distr.\\ critical\\ value}$$\n\n-   Test for the *main effects of treatment-regimen and time*. As an example, consider the effects of treatment based on $H_o\\colon \\alpha_1 = \\alpha_2 = \\dots = \\alpha_a = 0$. To test this null hypothesis, we use another F-test between the Mean Square for Treatment and the Mean Square for Error (a):\n\n$$F = \\dfrac{MS_{treat}}{MS_{error(a)}} > F_{a-1, N-a, a}\\ .$$\n\n-   Test for the effect of *time*; $H_o\\colon \\tau_1 = \\tau_2 = \\dots = \\tau_t = 0$ using\n\n$$F = \\frac{MS_{time}}{MS_{error(b)}} > F_{t-1, (N-a)(t-1), \\alpha}\\ .$$\n\n-   Multivariate MANOVA test aggregates the observations over *time* from the same patient $j$ receiving treatment-regimen $i$ into an observed vector\n\n$$\\mathbf{Y}_{ij} = \\left(\\begin{array}{c}Y_{ij1}\\\\ Y_{ij2} \\\\ \\vdots\\\\ Y_{ijt}\\end{array}\\right)\\ .$$\n\nEffectively, we treat the data at different time-points as different variables or features. One-way MANOVA assumptions include:\n\n-   Patients receiving treatment $i$ have common mean (health outcome) response vector, $\\mu_i$,\n-   All patients share a common variance-covariance matrix, $\\Sigma$,\n-   Data from different patients are independently sampled, and\n-   Data are presumed to be multivariate normally distributed, $\\mathcal{N}(\\mu,\\Sigma)$.\n\nMANOVA *inference* includes:\n\n-   Test that the mean vector of observations over time *does not* depend on the treatment-regimen, using [Hotelling's T-square test](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution),\n-   Test that the main effect of the treatment-regimen is trivial (not-significant),\n-   Test for *interaction* between *treatment* and *time* using a new data vector for each observation representing patient $j$ receiving treatment-regimen $i$. This data vector represents the different in the data between time 2 and time 1, the time 3 and time 2, and so on:\n\n$$\\mathbf{Z}_{ij} = \\left(\\begin{array}{c}Z_{ij1}\\\\ Z_{ij2} \\\\ \\vdots \\\\ Z_{ij, t-1}\\end{array}\\right) = \\left(\\begin{array}{c}Y_{ij2}-Y_{ij1}\\\\ Y_{ij3}-Y_{ij2} \\\\ \\vdots \\\\Y_{ijt}-Y_{ij,t-1}\\end{array}\\right)\\ .$$\n\nThe (random) vector $\\mathbf{Z}_{ij}$ is a function of the random data and we can compute the expected population mean for treatment $i$, $E(\\mathbf{Z}_{ij}) = \\boldsymbol{\\mu}_{Z_i}$. The MANOVA test on these $\\mathbf{Z}_{ij}$'s is based the null hypothesis $H_o\\colon \\boldsymbol{\\mu}_{Z_1} = \\boldsymbol{\\mu}_{Z_2} = \\dots = \\boldsymbol{\\mu}_{Z_a}$.\n\n### Variance decomposition\n\nThe key factor exploited by linear models such as ANOVA/MANOVA is the powerful *analytical decomposition of the observed process variation*, which quantifies the type and amount of complementary (orthogonal) dispersion observed in data samples. Recall that $N=\\sum_i^a{n_i}$ and\n\n$$\\bar{y}_{...} = \\frac{1}{Nt}\n\\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\sum_{k=1}^{t} {Y_{ijk}}, $$\n\n$$SS_{total} = \\frac{1}{n-1}\n\\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\sum_{k=1}^{t} {(Y^2_{ijk}- \\bar{y})^2} =\n\\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\sum_{k=1}^{t}Y^2_{ijk}-Nt\\bar{y}^2_{...}\\ .$$\n\nThen we can expand the total variance as a sum of complementary factors\n\n$$\\begin{array}{lll}\n\\underbrace{SS_{total}}_{\\text{Overall Obs. Variance}} & =& \\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\sum_{k=1}^{t}Y^2_{ijk}-Nt\\bar{y}^2_{...}\\\\\n \\underbrace{SS_{treat}}_{\\text{Obs. Var. due to Treatment}} &= &\n t\\sum_{i=1}^{a}n_i\\bar{y}^2_{i..} - Nt\\bar{y}^2_{...} \\\\\n \\underbrace{SS_{time}}_{\\text{Obs. Var. due to Time}} & =&\n N\\sum_{k=1}^{t}\\bar{y}^2_{..k}-Nt\\bar{y}^2_{...} \\\\\n \\underbrace{SS_{treat \\times time}}_{\\text{Obs. Var. due to Treat*Time Interaction}} &=& \n \\sum_{i=1}^{a}\\sum_{k=1}^{t}n_i\\bar{y}^2_{i.k} - Nt\\bar{y}^2_{...}-SS_{treat} -SS_{time} \\\\\n \\underbrace{SS_{error(a)}}_{\\text{Residual Var. (unaccounted by model)}} & =&\n t\\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\bar{y}^2_{ij.} -\n t\\sum_{i=1}^{a}n_i\\bar{y}^2_{i..}\\end{array}\\ .$$\n\n$$SS_{total} = \\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\sum_{k=1}^{t} Y^2_{ijk}-Nt\\bar{y}^2_{...} = \\\\\n\\underbrace{t\\sum_{i=1}^{a}n_i\\bar{y}^2_{i..} - Nt\\bar{y}^2_{...}}_{SS_{treat}} + \n\\underbrace{N\\sum_{k=1}^{t}\\bar{y}^2_{..k}-Nt\\bar{y}^2_{...}}_{SS_{time}} +\\\\ \n\\underbrace{\\sum_{i=1}^{a}\\sum_{k=1}^{t}n_i\\bar{y}^2_{i.k} - Nt\\bar{y}^2_{...}-SS_{treat} -SS_{time}}_{SS_{{treat \\times time}}} +\\\\\n\\underbrace{t\\sum_{i=1}^{a}\\sum_{j=1}^{n_i}\\bar{y}^2_{ij.} - t\\sum_{i=1}^{a}n_i\\bar{y}^2_{i..}}_{SS_{error(a)}}\\ .$$\n\n### fMRI Example of Variance Decomposition\n\nTo draw the connection between observed variability decomposition and complex-time we will consider a [study of single-subject task-based fMRI activation across multiple sessions](https://doi.org/10.1016/j.neuroimage.2016.10.024). In this study, the researchers proposed a *within-subject variance decomposition* of a task-based fMRI using a large number of repeated measures, $500$ trials of three different subjects undergoing $100$ functional scans in $9-10$ different sessions. In this case, the *observed within-subject variance* was segregated into $4$ primary components - *variance across-sessions*, *variance across-runs within a session*, *variance across-blocks within a run*, and *residual measurement* (model-unaccounted error).\n\nWithin-subject variance decomposition:\n\n-   *Across-sessions variance*, $\\sigma^2_{session}$, variance associated with entering and exiting the scanner on the same or different days),\n-   *Across-runs variance*, $\\sigma^2_{run}$, where $run$ represents a continuous scanning period containing several stimulation blocks or task,\n-   *Across-blocks variance*, $\\sigma^2_{block}$, where $block$ represents individual contiguous occurrences of the stimulus or task in a $run$, and\n-   *Residual error* (modeling variance), $\\sigma^2_{error}$, capturing the remaining within-subject variance not attributable to any of the other three factors.\n\nThat study reported that across $16$ cortical networks, $\\sigma^2_{block}$ contributions to within-subject variance were larger in *high-order cognitive networks* compared to *somatosensory brain networks*. In addition, $\\sigma^2_{block} \\gg \\sigma^2_{session}$ in *higher-order cognitive networks associated with emotion and interoception roles*.\n\nThis spatiotemporal distribution factorization of the total observed within-subject variance illustrates the importance of identifying *dominant* variability components, which subsequently may guide prospective fMRI study-designs and data acquisition protocols.\n\nAll functional MRI *runs* had the same organization of blocks as shown in the diagram below. <!-- An initial 30 s rest period was followed by five repetitions of the following sequence of blocks: task block (20 s) and rest block (40 s). An additional 10 s of rest were added at the end of each functional run. This resulted in 340 s runs.  -->\n\n\nSuppose the complex-valued fMRI signal is $Y_{session=i,run=j,block=k}$ and we fit a linear model decomposing the *effect estimates*\n\n$$\\underbrace{\\hat{\\beta}_{i(j(k))}}_{effect\\\\ estimate}=\\underbrace{\\alpha}_{overall\\\\ mean} + \n\\underbrace{\\theta_k}_{session\\\\ effect} +\n\\underbrace{\\xi_{j(k)}}_{run\\\\effect} + \n\\underbrace{\\eta_{i(j(k))}}_{block\\\\ effect} + \n\\underbrace{\\varepsilon_{i(j(k))}}_{measurement\\\\ error } \\ .$$\n\nThis partitioning of the effects is in terms of the indices $block=i$, $run=j$, and $day=k$, where parentheses $()$ indicate the *nesting structure* between consecutive levels. We *assume* Gaussian distributions for all *random effects* and the residual error,\n\n$$\\theta_k\\sim N(0,\\sigma^2_{session}),\\ \\ \n\\xi_{j(k)}\\sim N(0,\\sigma^2_{run}),\\ \\ \n\\eta_{i(j(k))}\\sim N(0,\\sigma^2_{block}),\\ \\ \n\\varepsilon_{i(j(k))}\\sim N(0,\\sigma^2_{i})\\ .$$\n\nThen, the *variance decomposition* of the effect estimate model is\n\n$$Var(\\hat{\\beta}_{i(j(k))}) = \\sigma^2_{session} + \\sigma^2_{run} +\n\\sigma^2_{block} + \\sigma^2_{error}\\ .$$\n\n*Question*: *Where do the random effects (and their estimates) come from?*\n\n*Answer*: As shown in [this article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2880445/), we start with a *general linear model (GLM)* $Y = X\\beta + \\varepsilon$ representing the observed *fMRI response (intensity)* $y$ at each voxel as a linear combination of some explanatory variables, columns in the *design matrix* $X$. Each column in $X$ corresponds to one effect that can be experimentally manipulated or that may confound the observed outcome $y$.\n\nThe GLM expresses the *response* variable $y$ as a mixture of *explanatory variables* and some residual error term $\\varepsilon\\sim N(0,\\Sigma)$. fMRI pre-processing protocols may filter the data with a convolution or residual forming matrix $S$, leading to a generalized linear model that includes *intrinsic serial correlations* and *applied extrinsic filtering*. Different choices of $S$ correspond to different estimation schemes\n\n$$\\underbrace{Y}_{observed\\\\ fMRI} = \\overbrace{X}^{design\\\\ matrix} \\underbrace{\\beta}_{effects\\\\ vector} + \\overbrace{\\varepsilon}^{residual\\\\ error}\\ .$$\n\n$$S=\\begin{cases} \n      \\Sigma^{-\\frac{1}{2}}, & whitening \\\\\n      1, & none \\\\\n      1-X_oX_o^+, & adjustment \n   \\end{cases}\\ .$$\n\n$$SY = SX\\beta + S\\varepsilon, \\ \\ V=S\\Sigma S',\\ \\ \n\\underbrace{\\hat{\\beta}=(SX)^+ SY}_{\\text {random effect estimates}},\\ \\ \n\\frac{c'\\hat{\\beta}}{\\sqrt{\\hat{V}(c'\\hat{\\beta})}}\\sim t_{df},$$\n\n$$\\hat{V}(c'\\hat{\\beta})=\\hat{\\sigma}^2 c'(SX)^+ V((SX)^+)',\\\\\nR=I-SX(SX)^+, \\ \\ \\hat{\\sigma}^2=\\frac{Y'RY}{trace(RV)}, \ndf=\\frac{trace(RV)^2}{trace(RVRV)}\\ .$$\n\nThe *effect parameter estimates* are obtained using [least squares](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html) based on the matrix-pseudo-inverse of the filtered design matrix, denoted by $^+$. The most general *effect of interest* is specified by a vector of contrast weights $c$ that give a weighted sum or compound of parameter estimates, referred to as a *contrast.* However, often we use unitary contrasts corresponding to a single variable in the design matrix $X$, e.g., $c=(1,0,\\cdots,0)$ corresponds to the effect of the first covariate (first column in $X$). Once the effects are estimated (we have computed the LS estimates of $\\hat{\\beta}_c$, we can compute the $T_c\\sim t_{df}$ statistic to assess the statistical significance of the effect.\n\n### Representation of the Two-tier fMRI Protocol as a General Mixed Effects Model\n\nIn essence, fMRI analyses typically involve *two phases*:\n\n - *Level-1* analysis at the *individual subject level.* For each subject $1\\leq m\\leq N$ and each space location (3D voxel) $v$, model the BOLD fMRI signal using a first level linear model, $Y_m=X_m \\beta_m + \\varepsilon_m$, where $Y_m$ is a vector of length $T$ capturing the entire BOLD time series at voxel $v$ for participant $m$, $X_m$ is a $T\\times P_m$ *design matrix* specified by the experimenter, $\\beta_m$ is a column vector of regression coefficients (effects for for each of the explanatory variables (*EV*s) in the design matrix, and $\\varepsilon_m\\sim N(0,\\sigma_m^2 V)$ is a column vector of residual errors. This assumes we have only one run per subject.\n - *Level-2* analysis is based on the estimated $\\beta_m$ coefficients for each subject. Level-2 analysis fits a second *population-level model* for the effects, $\\beta_m=X_{gm} \\beta_g +\\varepsilon_{gm}$, where, $X_{gm}$ is the design matrix for the *group*, $1\\leq g\\leq G$, indicating the phenotype for each subject ($m$), e.g., presence or absence of a disease or any other clinical trait, $\\beta_g$ represents the population-level effects for each of the explanatory variables corresponding to the effects $\\beta_m$, the residual errors $\\varepsilon_{gm}\\sim N(0,R_g)$, and $R_g$ is the variance-covariance matrix of the group-level errors.\n\n$$({\\text{First-level analysis}})\\ \n \\underbrace{Y_m=X_m \\beta_m + \\varepsilon_m}_{Individual\\ Subject\\ Model}, \\ \\underbrace{Y_m=\\{Y_{m,t=0},Y_{m,t=1},\\cdots, Y_{m,t=T}\\}}_{fMRI\\ series\\ signal},\\ \\forall \\ (subject)\\ m,\\\\\n ({\\text{Second-level analysis}})\\ \\ \n \\underbrace{\\beta_m=X_{gm} \\beta_g +\\varepsilon_{gm}}_{Population\\ Model}, \\ \\  \\underbrace{\\beta_g=\\{\\beta_{mo},\\beta_{m1},\\cdots, \\beta_{mG}\\}}_{subject-level-1\\ regression\\ coefficients\\\\ (effects)\\ \\forall EV\\ column\\ in\\ X},\\ \\forall g\\in G\\ (group)\\ .$$\n\nThis two-level fMRI analysis can also be represented into a *General Linear Mixed Effects Model*:\n\n$$Y_m=\\underbrace{\\beta_o+X_{m1} \\beta_1+\\cdots+X_{mp} \\beta_p}_{fixed-effects}+\n\\underbrace{\\beta_{mo}+Z_{m1} \\beta_{m1}+\\cdots + Z_{mq} \\beta_{mq}}_{random-effects}\n+\\varepsilon_m\\ , \\forall \\ (subject)\\ m .$$\n\nAgain, $Y_m=\\{Y_{m,t=0},Y_{m,t=1},\\cdots, Y_{m,t=T}\\}$ is a column vector of observed fMRI time-series for individual $m$ at a fixed 3D voxel location, $\\{X_{mj}\\}$ is the vector of the $p$ regressors included in the linear (LME) model, $\\{\\beta_o,\\cdots, \\beta_p\\}$ are the *fixed-effect regression coefficients*, which are identical across all subjects, the column vectors $\\{Z_{mq}\\}_q$ are the *random effect regressors* with corresponding *random effects* coefficients $\\{\\beta_{mq}\\}_q\\sim N(0,\\Psi)$, and $q$ is the *total number of random effects* included in the model. The *random effects* capture the variability across subjects for each of the regressors $\\{Z_{mq}\\}_q$, $\\Psi$ is a $(q+1)\\times (q+1)$ variance-covariance matrix, and the $n_m\\times 1$ vector of *within-group errors* $\\varepsilon_m=\\{\\varepsilon_{m1},\\cdots,\\varepsilon_{mn_m}\\}\\sim N(0,\\sigma^2 V)$, where $\\sigma^2$ and $V$ are the *variance* and the *correlation matrix.*\n\nThe two-tier fMRI analysis can be formulated into a generalized linear mixed-effects model as follows. For simplicity of notation, assume that the first level design matrix for subject $m$, $X_m$, has only four columns representing an intercept and $3$ covariates, $EV_1, EV_2, EV_3$, and there are $T$ temporal volumes in the 4D fMRI spatio-temporal data\n\n$$X_m=\\begin{pmatrix}\n1 & EV_{11} & EV_{21}  & EV_{31} \\\\\n1 & EV_{12} & EV_{22}  & EV_{32} \\\\\n\\vdots & \\vdots & \\vdots  & \\vdots \\\\\n1 & EV_{1T} & EV_{2T}  & EV_{3T}\n\\end{pmatrix}\\equiv [X_{mo}\\ X_{m1}\\ X_{m2}\\ X_{m3}\\ ]\\ ,$$\n\nwhere $\\forall m,\\ X_{mj},\\ 0\\leq j \\leq 3$ is a column vector of ones (for the intercept, $j=0$), or $EV_j, \\ 1\\leq j \\leq 3$. The vector of regression coefficients corresponding to each explanatory variable is\n\n$$\\beta_m=\\begin{pmatrix}\n \\beta_{mo}  \\\\\n \\beta_{m1} \\\\\n \\beta_{m2}  \\\\\n \\beta_{m3} \n\\end{pmatrix} ,$$\n\nSuppose that in the second-level analysis we want to estimate the *average population-wide effect* of each EV for the whole group. This corresponds to using hte following design matrix\n\n$$X_g=\\begin{pmatrix}\n1 & 0 & 0  & 0 \\\\\n0 & 1 & 0  & 0 \\\\\n0 & 0 & 1  & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\\equiv I_{4\\times 4}\\ .$$\n\nNext we plug this the second-level population model $\\beta_m=X_{gm} \\beta_g +\\varepsilon_{gm}$ into the first-level model\n\n$$\\overbrace{Y_m=X_m \\beta_m +\\varepsilon_{m}}^{First \\ Level\\ (individual)}\\\\\n=X_m \\underbrace{\\left ( X_{gm} \\beta_g +\\varepsilon_{gm}\\right )}_{\nSecond\\ Level\\ (population)} +\\varepsilon_{m}=\\\\\n[X_{mo}\\ X_{m1}\\ X_{m2}\\ X_{m3}\\ ] \\begin{pmatrix}\n1 & 0 & 0  & 0 \\\\\n0 & 1 & 0  & 0 \\\\\n0 & 0 & 1  & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} \n\\begin{pmatrix}\n \\beta_{go}  \\\\\n \\beta_{g1} \\\\\n \\beta_{g2}  \\\\\n \\beta_{g3} \n\\end{pmatrix} +\n[X_{mo}\\ X_{m1}\\ X_{m2}\\ X_{m3}\\ ] \\begin{pmatrix}\n \\varepsilon_{gmo}  \\\\\n \\varepsilon_{gm1} \\\\\n \\varepsilon_{gm2}  \\\\\n \\varepsilon_{gm3} \n\\end{pmatrix} +\\varepsilon_{m}=\\\\\n\\underbrace{\\underbrace{X_{mo}\\beta_{go}+X_{m1}\\beta_{g1}+X_{m2}\\beta_{g2}+X_{m3}\\beta_{g3}}_{fixed\\ effects}+\n\\underbrace{X_{mo}\\varepsilon_{gmo}+X_{m1}\\varepsilon_{gm1}+X_{m2}\\varepsilon_{gm2}+\nX_{m3}\\varepsilon_{gm3}}_{random\\ effects}+ \n\\underbrace{\\varepsilon_m}_{residual\\ errors}}_{General\\ LME} \\ ,$$ where $\\varepsilon_m\\sim N(0,\\sigma^2V)$, $\\varepsilon_g\\sim N(0,R_g)$, and $R_g$ is the *variance-covariance* matrix of the group-level errors.\n\n### Relation to Complex-Time Phase (kime-phase)\n\nThe *intrinsic process error* $\\varepsilon_m\\sim N(0,\\sigma^2V)$ reflects classical *variability in the underlying process distribution*. The more subtle *error-tensor* $\\varepsilon_g\\sim N(0,R_g)$, where $\\varepsilon_m\\sim N(0,\\sigma^2V)$, depends on the *variance-covariance* matrix of the group-level errors $R_g$ and can be interpreted as *repeated-measures variability due to the kime-phase distribution*. Hence, the kime-phases can be used to model or track *repeated-measure variations*. Recall this kime-phase simulation we discussed earlier demonstrating alternative kime-phase distributions coupled with corresponding *mean-phase-aggregators* (solid radial lines) representing real observable (deterministic, not stochastic) measurable quantities. The latter avoid the intrinsic kime-phase distribution noise due to random sampling through the phase-space distribution $\\Phi_{[-\\pi,\\pi))}$ when taking real measurements depending *de facto* on some phase-aggregating kernels that smooth the *intractable* (unobservable) and *intrinsically-random* kime-phases.",
      "word_count": 6061
    },
    {
      "title": "Existence, Testability, and Falsifiability of Kime-Phase Existence and Spacekime Representation",
      "content": "In 1915, Albert Einstein proposed the [general theory of relativity (GTR)](https://en.wikipedia.org/wiki/General_relativity). The following year, in 1916, he proposed [the following three tests to validate or falsify GTR](https://en.wikipedia.org/wiki/Tests_of_general_relativity). [Some of these GTR hypotheses were confirmed using data acquired by 1919](http://germanhistorydocs.ghi-dc.org/pdf/eng/EDU_Einstein_ENGLISH.pdf). \n\n - Can GTR explain the *anomalous precession* of the [perihelion of Mercury](https://en.wikipedia.org/wiki/Mercury_(planet))? \n - Confirm the GTR prediction that [light is bent in gravitational fields](https://en.wikipedia.org/wiki/Gravitational_lens) using Solar eclipses to track deflection of light;\n - Confirm the GTR model of [light gravitational redshift](https://en.wikipedia.org/wiki/Gravitational_redshift). \n\nSimilarly, to test, affirm, or reject the complex-time model, i.e., the *existence of kime-phases* and the interpretation of the spacekime representation of repeated longitudinal measurements, we need to propose *testable hypotheses* that can be used to determine the validity of the spacekime representation. Ideally, we need to express such hypotheses as *if-then statements* indicating that under certain *conditions* or actions (investigator controlled, *independent variables*, IVs), specific *outcomes* or results (*dependent variables*, DVs) are to be expected. We may need to propose experiments that can be carried to prove or disprove the existence of kime-phases, with substantial reproducibility.\n\nIn essence, there may be viable *physics experiments* or *statistical-analytical experiments* that can be formulated to test the spacekime representation.\n\nFor instance, potential *physics experiments* to consider include:\n\n - [Non-local Quantum Entanglement](https://en.wikipedia.org/wiki/Quantum_entanglemen), predicted by quantum mechanics where the outcomes measurements made in distant locations can be correlated with each other even when the measurements are truly independent and information-sharing between the outcomes require speeds exceeding the speed of light. This is related to the [1935 paper of Einstein, Podolsky and Rosen (EPR)](https://en.wikipedia.org/wiki/Einstein%E2%80%93Podolsky%E2%80%93Rosen_paradox). This also relates to [John Bell's inequalities](https://en.wikipedia.org/wiki/Bell%27s_inequality) and the limits imposed by [local hidden variable theory](https://en.wikipedia.org/wiki/Hidden_variable_theory).\n - [Experiments involving *virtual particles*](https://en.wikipedia.org/wiki/Virtual_particle), [quantum fluctuations](https://en.wikipedia.org/wiki/Quantum_fluctuation) and [Richard Feynman's Quantum Electrodynamics (QED)](https://en.wikipedia.org/wiki/Quantum_electrodynamics). These temporary random dynamics of energy in vacuum are related to the [Werner Heisenberg's uncertainty principle](https://en.wikipedia.org/wiki/Uncertainty_principle). Such sporadic instantaneous random fluctuations in observable electric and magnetic fields disturb controlled environmental conditions and have similar enigmatic characteristics as the kime-phases. The probability distributions of *virtual particles* and *kime-phases* can be modeled, simulated, and tracked, but *not predicted.*\n - [Particle-wave duality experiments](https://en.wikipedia.org/wiki/Wave%E2%80%93particle_duality) indicating that at the quantum world, the universe has both particle and wave characteristics.\n\nOn the other hand, potential *statistical and analytical experiments* to test the validity of the *kime-phase* as a stochastic representation of repeated sampling may rely on tests confirming or disproving scientific inference improvements based on the spacekime-model compared to classical approaches. For example:\n\n - *Model-based statistical* regression or classification predictions using specific longitudinal datasets, or alternatively,\n - *Model-free ML/AI* clustering, forecasting, or decision-making based on time-varying processes. See [this article on Model-based and Model-free Machine Learning Techniques for Diagnostic Prediction and Classification](https://www.nature.com/articles/s41598-018-24783-4).\n\nIn [quantum superposition](https://en.wikipedia.org/wiki/Quantum_superposition) *particles* are also *distributions* always maintaining simultaneously all possible basis states (realities) until the moment they’re sampled, drawn, measured, or detected, i.e., observed as scalars, vectors, or tensors. Similarly in statistics, data science, and AI, we often collect, sample, simulate, or model *data as proxy of a physical phenomenon* we are studying. The final data-driven *scientific inference about the phenomenon* is expected to be reproducible, consistent, unbiased, accurate, and reliable, reflecting the process distribution properties despite the fact that all *data-driven inference is sample evidence-based*, whereas the underlying *process distributions are generalized functions*. This *statistical analytic-duality* resembles the [particle-wave duality](https://en.wikipedia.org/wiki/Wave%E2%80%93particle_duality) in physics.\n\nIn applied sciences, *sample statistics*, such as the sample mean, variance, percentiles, range, inter-quartile range, etc., are always well-defined. Their values are not known at any given time until the (finite) sample $\\{x_i\\}_{i=1}^n$ is collected (observations are recorded). For instance the *sample mean statistic* is the *arithmetic average* of all observations, $\\bar{x}_n=\\frac{1}{n}\\sum_{i=1}^n {x_i}$, yet, not knowing the value of the sample-mean prior to collecting the data is simply due to lack of evidence, since we have a closed form expression of how to obtain the *sample average statistic* estimating the population (physical system's) *expected mean response*, $\\mu_X\\ \\underbrace{\\leftarrow}_{{n\\to\\infty}}\\ \\bar{x}_n$. Quantum systems interact in ways that can be explained with superpositions of different discrete base-states, and quantum system measurements yield statistical results corresponding to any one of the possible states appearing at random. \n\nSimilarly, *analytical systems* interact in ways that can be explained with superpositions of different, discrete, and finite samples, and analytical system measurements yield statistics corresponding to any one of the specific sample-statistic outcomes, which vary between samples and experiments. However, the first two fundamental laws of probability theory governing all statistical inference ([CLT](https://doi.org/10.1080/10691898.2008.11889560) and [LLN](https://doi.org/10.1080/10691898.2009.11889499)) yield that this *between sample variability of sample-statistics* is expected to decrease rapidly with the increases of the sample size. For instance, IID samples $\\{x_i\\}_{i=1}^n\\sim \\mathcal{D}(\\mu_X,\\sigma_X^2)$, drawn from most *nice distributions* $\\mathcal{D}$ with mean $\\mu_X$ and variance $\\sigma_X^2$, yield *sample-means* $\\bar{x}_n$ whose [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution) converges (in distribution) to a [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), an [asymptotic limiting distribution](https://en.wikipedia.org/wiki/Asymptotic_distribution) with rapidly decaying variance as the sample size increases\n\n$$\\underbrace{\\bar{x}_n }_{Sampling\\\\ distirbution}\\ \\ \\  \\overbrace{\\underbrace{\\longrightarrow}_{n\\to\\infty}}^{Convergence\\\\ in\\ distribution} \\underbrace{\\mathcal{N}\\left(\\mu_X,\\frac{\\sigma_X^2}{n}\\right)}_{Asymptotic\\\\ distribution}\\ .$$\n\n## Dual Kime-Phase Distribution and Spatiotemporal State Sampling\n\nLet's try to explicate the  duality between sampling the kime-phase distribution and the complementary sampling of the process state space (spatiotemporal sampling).\n\nSuppose the process $X\\sim F_X$, where $F_X$ is an arbitrary distribution and let $\\sigma^2=Var(X)$. If $\\{X_1,\\cdots ,X_n\\}\\sim F_X$ is an IID sample the CLT suggests that under mild assumptions $\\bar{X}\\longrightarrow N(\\mu,\\sigma^2/n)$ as $n\\to\\infty$. In *repeated spatiotemporal sampling*, $\\forall\\ n$, we \nobserve $K$ (longitudinal) sample means $\\{\\bar{X}_{n1},\\cdots, \\bar{X}_{nK}\\}$, where \n$\\forall 1\\leq k\\leq K$ each IID sample $\\{X_{1k},\\cdots ,X_{nk}\\}\\sim F_X$ (across the kime-phase space) allows us to compute the $k^{th}$ sample mean $\\bar{X}_{nk}$. \n\nObserve that computing $\\bar{X}_{nk}$ from the $k^{th}$ sample $\\{X_{1k},\\cdots ,X_{nk}\\}$\nis identical to directly sampling $\\bar{X}_{nk}$ from the distribution $F_{\\bar{X}_n}$. Let's reflect on this set up which clearly involves $2$ independent sampling strategies.\n\n - *Kime-phase sampling*, where the *spatiotemporal index* $k=k_o$ is fixed and we sample through the kime-phase distribution, $\\{X_{1k_o},\\cdots , X_{nk_o}\\}$.\n - The complementary *spatiotemporal sampling* reflects the varying $k$ index, where the kime-phase variability is annihilated by using a *kime-phase aggregator*, in this case sample-averages, to generate a sample $\\{\\bar{X}_{n1},\\cdots , \\bar{X}_{nK} \\}$.\n\nNote that the distribution $F_{\\bar{X}_n}$ need not be Normal, it may be, but in general it won't be Normal. For instance suppose our spatiotemporal sampling scheme involves exponential distribution, i.e., \n$$\\{X_{1k_o},\\cdots , X_{nk_o}\\}\\sim f_X\\equiv Exp\\left (\\underbrace{\\lambda}_{rate=\\frac{1}{scale}}\\right)\\equiv\\Gamma\n\\left (\\underbrace{1}_{shape},\\underbrace{\\lambda}_{rate=\\frac{const.}{scale}} \\right )$$\nand $\\sum_{i=1}^n{X_{ik_o}}\\sim \\Gamma\\left(n,\\lambda\\right )$,\n\n$$f_{Exp{(\\lambda)}}(x)=\\lambda e^{-\\lambda x}\\ \\ ; \\ \\mathbb{E}(X_{Exp{(\\lambda)}})=\\frac{1}{\\lambda} \\ \\ ; \\\nVar(X_{Exp{(\\lambda)}})= \\frac{1}{\\lambda^2}\\ ; \\\\ \nf_{\\Gamma(shape=\\alpha,scale=\\lambda)}(x)=\\frac{x^{\\alpha-1}\\lambda^{\\alpha}}{\\Gamma(\\alpha)}e^{-\\lambda x}\\ \\ ; \\ \n\\mathbb{E}(X_{\\Gamma(\\alpha,\\lambda)})=\\alpha\\lambda \\ \\ ; \\\nVar(X_{\\Gamma(\\alpha,\\lambda)})= \\alpha\\lambda^2\\ \\ .$$\n\nBased on this sample, the sampling distribution of $\\bar{X}_{nk_o}\\equiv \\frac{1}{n}\\sum_{i=1}^n{X_{ik_o}}\\sim Exp\\left (\\lambda\\right )\\equiv\\Gamma\\left(n,n\\lambda\\right )$, since\na sum of exponential IID $Exp(\\lambda)$ variables is gamma distributed, \n$\\sum_{i=1}^n{X_{ik_o}}\\sim \\Gamma\\left(n,\\lambda\\right )$ and scaling a random variable $Y=cX$ yields $F_Y(x)=\\frac{1}{c}f_X(\\frac{x}{c})$, in our case the \nconstant multiplier $c=\\frac{1}{n}$, and $Y=\\frac{1}{n}\\sum_{i=1}^n{X_i}$.\n\nOf course, as $n\\to\\infty$, $\\Gamma\\left(n,\\frac{\\gamma}{n}\\right )\\to N(0,1)$, yet\nfor any fixed $n$, the distribution is similar, but not identical to normal. \n\nPrior to data acquisition, $\\bar{X}_{nk_o}$ is a random variable, once the observed data values are plugged in, its a constant. Hence, the *sample mean random variable* \n$\\bar{X}_{nk_o}$ based on $\\{X_{1k_o},\\cdots , X_{nk_o}\\}\\sim F_X$, and the random variable $Y\\sim F_{\\bar{X}_{nk_o}}$ represent exactly the same random variable. \nIn other words, drawing $K$ samples of IID observations $\\{X_{1k_o},\\cdots , X_{nk_o}\\}\\sim F_X$ and computing $\\bar{X}_{nk_o}=\\frac{1}{n}\\sum_{i=1}^n{X_{ik_o}}$\nis equivalent to drawing $K$ samples directly from $F_{\\bar{X}_{nk_o}}$. \n\n## Stochastic Processes\n\nBelow is an example of a 3D [Brownian motion/Wiener process](https://en.wikipedia.org/wiki/Stochastic_process) as an example of a [stochastic random walk process](https://en.wikipedia.org/wiki/Stochastic_process).\nIn this example, the Wiener process is intentionally disturbed by random Poisson noise, which leads to occasional rapid and stochastic disruptions.\n\n\nIn our spacekime representation, we effectively have a (repeated measurement) spatiotemporal process including 3D spatial Gaussian model which is dynamic in time. In other words, the 3D [Gaussian Process](https://socr.umich.edu/DSPA2/DSPA2_notes/13_FunctionOptimization.html#57_Bayesian_Optimization) with a *mean vector* and a *variance-covariance matrix tensor* both dependent on time, $\\mu=\\mu(t)=(\\mu_x(t),\\mu_y(t),\\mu_z(t))'$ and $\\Sigma=\\Sigma(t)=\\left ( \\begin{array}{ccc} \\Sigma_{xx}(t) & \\Sigma_{xy}(t) & \\Sigma_{xz}(t)\\\\ \\Sigma_{yx}(t) & \\Sigma_{yy}(t) & \\Sigma_{yz}(t) \\\\ \\Sigma_{zx}(t) & \\Sigma_{zy}(t) & \\Sigma_{zz}(t) \\end{array}\\right )$.\n\nThe process distribution is $\\mathcal{D}(x,y,z,t)$ is specified by $\\mu=\\mu(t);\\ \\Sigma=\\Sigma(t)$. Given a spatial location, e.g., brain voxel, we the distribution probability density function at $(x,y,z)\\in\\mathbb{R}^3$ depends on the time localization, $t\\in \\mathbb{R}^+$. Actual repeated sample observations will draw phases from the phase distribution, $\\{\\phi_i\\}_i\\in\\Phi_{[-\\pi,\\pi)}$, which are associated with the *fixed spatiotemporal location* $(x,y,z,t)\\in\\mathbb{R}^3\\times \\mathbb{R}^+$. The repeated spatiotemporal samples are \n$$\\left\\{\\left (\\underbrace{x_i}_{x(\\phi_i)}, \\underbrace{y_i}_{y(\\phi_i)}, \\underbrace{z_i}_{z(\\phi_i)},\n\\underbrace{t_i}_{t(\\phi_i)}\\right )\\right\\}_i\\in\\mathbb{R}^3\\times \\mathbb{R}^+\\ .$$\n\nWhen the *mean vector* and the *variance-covariance matrix* vary with time $t$, proper inference may require use of [Wiener processes (Brownian motion)](https://en.wikipedia.org/wiki/Wiener_process), [Ito calculus](https://en.wikipedia.org/wiki/It%C3%B4_calculus), [Heston models](https://en.wikipedia.org/wiki/Heston_model), or\n[stochastic differential equation models](https://en.wikipedia.org/wiki/Stochastic_differential_equation). \n\n### Stochastic differential equations (SDEs)\n\nSDEs are generally expressed in a *differential* or *integral* forms.\n\n$$(\\text{Differential Form})\\ \\mathrm{d} X_t = \\mu(X_t,t)\\, \\mathrm{d} t + \\sigma(X_t,t)\\, \\mathrm{d} B_t ,$$ \n\nwhere $B$ is a time-varying Wiener process, $X$ is the (time-dependent) position of the system in its state space, and the spatiotemporally-dependent $\\mu$ and $\\sigma$ represent the process *mean* and *standard deviation.* This equation should be interpreted as an informal way of expressing the corresponding integral equation\n\n$$(\\text{Integral Form})\\ X_{t+s} - X_{t} = \\int_t^{t+s} \\mu(X_u,u) \\mathrm{d} u + \\int_t^{t+s} \\sigma(X_u,u)\\, \\mathrm{d} B_u \\ .$$ \n\nThe integral form of SDEs characterize the behavior of continuous time stochastic processes $X_t$ as the sum of an ordinary \n[Lebesgue integral](https://en.wikipedia.org/wiki/Lebesgue_integral) and \nanother [Ito integral](https://en.wikipedia.org/wiki/It%C3%B4_calculus). \n\n#### Ito Calculus\nThe Itô stochastic integral represents a generalization of the classical [Riemann–Stieltjes integral] where the integrand $H_s$ and the integrator $dX_s$ are time-dynamic stochastic processes\n\n$$Y_t = \\int_0^t {H_s\\ dX_s}\\ .$$\n\nThe *integrant* $H_s$ is a locally square-integrable process *non-anticipating *, i.e., *adapted*, to the *filtration* generated by a Brownian motion integrator $X_s$. A *filtration* $\\{\\mathcal{F}_t\\}_{t\\ge 0}$ on a *probability space* $(\\Omega,\\mathcal{F}, P)$ is a collection of sub-sigma-algebras on $\\mathcal{F}$ such that $\\mathcal{F}_s\\subseteq\\mathcal{F}_t$, $\\forall s\\le t$. In other words, the filtration $\\mathcal{F}_t$ represents the set of events observable by time $t$. A *probability space* enriched with a *filtration* forms a *filtered probability space* $(\\Omega,\\mathcal{F},\\{\\mathcal{F}_t\\}_{t\\ge 0},P)$.\n\nThe sigma-algebra $\\sigma(\\cdot)$ is generated by a collection of subsets of hte state space $\\Omega$. Filtration lead to defining *adapted processes* -- a stochastic process process $X$ is adapted when $X_t$ is an $\\mathcal{F}_t$-measurable random variable for each time $t\\ge 0$, i.e., we can observe the value $X_t=x_t$ at time $t$. The smallest filtration with respect to which the stochastic process is adapted is called the *natural filtration* of $X$, $\\mathcal{F}^X_t=\\sigma\\left(X_s\\colon s\\le t\\right)$. The measurability constraints on the process implies that the process can be considered as a map $X:{\\mathbb R}_+\\times\\Omega\\rightarrow{\\mathbb R}$, \n${\\mathbb R}_+\\times\\Omega\\ni (t,\\omega)\\longrightarrow X_t(\\omega) \\in {\\mathbb R}$. This makes the process $X_s$ is $\\mathcal{F}_t$-measurable, i.e., its distribution is computable. Having an increasing filtration $\\{F_t\\}_0^{\\infty}$, the stochastic process $\\{X_t\\}_{t}$ is adapted when each $\\{X_t\\}$ is $\\{F_t\\}$-measurable.\n\nThe stochastic Ito integral can be written in different (short-hand) forms:\n\n$$Y_t = \\int_0^t H\\ dX \\equiv \\int_0^t {H_s}\\ dX_s,$$\n\nThe time-parameterized stochastic process $Y_t$ can also be expressed as \n$Y = H \\cdot X$ or as a *differential form* $dY = H\\ dX$, or even $Y − Y_o = H\\cdot X$ relative to an underlying filtered probability space $(\\Omega ,{\\mathcal {F}},({\\mathcal {F}}_{t})_{t\\geq 0},P )$, where the $\\sigma$-algebra $\\mathcal {F}_{t}$ represents the information over time $s\\lt t$. The process $X$ is adapted \nwhen $X_t$ is $\\mathcal {F}_{t}$-measurable. \n\nAs the earlier 3D Wiener process experiment shows, increments of a Wiener process \nare independent, and for Gaussian processes, the increments are expected to be normally distributed. As a time-varying function, the process\n$\\mu=\\mu(t)$ is interpreted as *drift* or *trend*, whereas the time-varying standard deviation $\\sigma=\\sigma(t)$ is referred to as *diffusion*. \n\nSometimes, *stochastic processes* $X_t$, also be called *diffusion processes*, satisfies the [Markov property](https://en.wikipedia.org/wiki/It%C3%B4_diffusion#The_Markov_property). Independent of all past behavior prior to the current time $t$, over a tiny time interval of length $|[t,t+s]|=\\delta$, the stochastic process \n$X_t$ changes its value according to distribution with *expected value* $\\mu(X_t, t)\\delta$ and *variance* $\\sigma(X_t, t)^2\\delta$.\n\nThe *Markov property* specifies that the *conditional expectation* and *conditional probability* of the future of a Wiener process path given its present and its past. Denote the path over a *time interval* $[t_1, t_2],\\ t_1 \\lt t_2$) by $X_{[t_1,t_2]}$. Also denote, the time interval prior to time $t_1$ by\n$[0, t_1]$, the past information about the process path by $X_{[0,t_1]}$, and\nthe present information by $X_{t_1}$. Then, the Markov property \nabout the future process trajectory beyond $t_1$ requires that\nthat the distribution of $X_{[t_1,t_2]}$ conditional on the past, $[0, t_1]$,\nis the same as the distribution conditional on the present, $X_{t_1}$. \nThat is, the distribution conditional on $X_{[0,t_1]}$ is the same as the distribution solely conditional on $X_{t_1}$. \n\n### The Heston model\n\nThe Heston model is an example of a SDE describing the *evolution of volatility of a time-process*, such as *valuation of an asset*. The stochastic Heston SDE models the volatility of temporally-dynamic asset valuations. \n\nLet $S_t$ represent the the valuation (price) of the asset, the Heston SDE model\n\n$$dS_t=\\mu S_t\\ dt + {\\sqrt {\\nu_t}}S_t\\,dW_t^S\\ ,$$\n\nwhere $\\nu_t$ is the variance at time $t$, which is the Feller square-root or [Cox–Ingersoll–Ross (CIR) model](https://en.wikipedia.org/wiki/Cox%E2%80%93Ingersoll%E2%80%93Ross_model)\n\n$$d\\nu_t=\\kappa (\\theta -\\nu_t)\\ dt + \\xi {\\sqrt {\\nu_t}}\\ dW_t^{\\nu},$$\n\n - $W_t^S,W_t^{\\nu}$ are continuous Wiener processes with correlation $\\rho=\\rho_{W_t^S,W_t^{\\nu}}$, \n - $\\nu_o$ is the initial variance,\n - $\\theta$ is the long-run average variance of the valuation price, that is, as $t\\to\\infty$, the expected value $\\mathbb{E}(\\nu_t)\\to\\theta$,\n - $\\kappa$ is the rate at of convergence $\\nu_t\\to\\theta$,\n - $\\xi$ is the volatility of the volatility, i.e., the variance of $\\nu_t$, \n - When these parameters satisfy the Feller condition, the process $\\nu_t$ is strictly positive, i.e., $2\\kappa \\theta \\gt \\xi^2$.\n\nWe may need to explore the utility of [reproducing kernel Hilbert spaces (RKHS) may be useful for AI regression and classification tasks](https://link.springer.com/chapter/10.1007/978-3-030-89010-0_8),\nespecially for [modeling time-varying probability distributions](https://arxiv.org/abs/1406.5362) and kime-phase representation.",
      "word_count": 2263
    },
    {
      "title": "Radon-Nikodym Derivative",
      "content": "## Borel sets\n\nA subset of a topological space is called a Borel set if it can be expressed as a countable union, countable intersection, and relative complement of either open subsets or, equivalently, closed subsets of the topological space.\n\nFor instance, if the reals $\\mathbb{R}$ represent the base topological space, for any countable collection of open subsets $A_{i}\\mathbb{\\subseteq R,\\ }i\\mathbb{\\in N,}$ and any finite collection of open subsets $B_{j}\\mathbb{\\subseteq R,\\ }1 \\leq j \\leq m$, let\n\n$$A = \\bigcup_{i = 1}^{\\infty}A_{i}\\ ,\\ B = \\bigcap_{j = 1}^{m}B_{j}\\ .$$\n\nThen, all of these subsets $A,\\ \\left\\{ A_{i} \\right\\},\\ B,\\ \\left\\{ B_{j} \\right\\}$ are Borel sets. The Borel $\\sigma$-algebra on $\\mathbb{R}$ is a nonempty collection $\\Sigma$ of subsets of $\\mathbb{R}$ closed under complement, countable unions, and countable intersections. In this situation, the ordered pair $\\mathbb{(R,\\ }\\Sigma)$ is called a *measurable space*, which is subsequently coupled with a probability measure, such as the distribution of the kime-phase.\n\nThe *Borel algebra* on the reals is the smallest $\\sigma$-algebra on $\\mathbb{R}$ that contains all the intervals. Also, given a real random variable, such as the kime-phase $\\varphi$, defined on a *probability space*, such as $\\left( \\Omega \\equiv \\lbrack - \\pi,\\pi),\\ E,\\ Pr \\equiv \\Phi \\right)$, where $E$ represents observable events, the variable probability distribution $\\varphi \\sim \\Phi$ is a measure on the Borel $\\sigma$-algebra.\n\n## Absolutely Continuous Measures\n\nA measure $\\nu$ on Borel subsets of the measurable space $(X,\\ \\Sigma)$ is *absolutely continuous* with respect to another measure $\\mu$ if $\\forall\\ \\mu$-measurable sets $A \\subseteq X$, $\\mu(A) = 0 \\Longrightarrow \\nu(A) = 0$. We denote absolute continuity by $\\nu \\ll \\mu$ indicating that the measure $\\nu$ is *dominated* by the measure $\\mu$.\n\nWhen $X\\mathbb{\\equiv R}$, the following conditions for a finite measure $\\nu$ on the Borel subsets of the real line are equivalent:\n\n-   $\\nu$ is *absolutely continuous* with respect to the Lebesgue measure $\\mu$ over the reals;\n\n-   $\\forall\\varepsilon \\gt 0$, $\\exists\\delta_{\\varepsilon} \\gt 0$ such that $\\nu(A) < \\varepsilon$ for all Borel sets $A$ of Lebesgue measure $\\mu(A) < \\delta_{\\varepsilon}$;\n\n-   There exists a Lebesgue integrable function $h( \\cdot )\\mathbb{:R \\rightarrow}\\mathbb{R}^{+}$, such that for all Borel sets $A$\n\n$$\\nu(A) = \\int_{A}^{}{h\\ d\\mu},\\ \\ i.e.,\\ \\ \\ \\ h = \\frac{\\partial\\nu}{\\partial\\mu}\\ .$$\n\nThe last equivalence condition (3) suggests that for any pair of absolutely continuous measures $\\nu \\ll \\mu$, there exists a *''pseudo''-derivative of the measure* $\\nu$ *with respect to its dominant measure* $\\mu$, which can be denoted by $h \\equiv h_{\\nu,\\mu}$.\n\n## Radon-Nikodym derivative\n\nGiven a pair of $\\sigma$-finite measures, $\\nu,\\mu$, defined on a measurable space $(X\\ ,\\ \\Sigma)$. When $\\nu$ is absolutely continuous with respect to $\\mu$, i.e., $\\nu \\ll \\mu$, then there exists a $\\Sigma$-measurable function $h:X \\rightarrow \\ \\lbrack 0,\\infty)$, such that for any measurable set $A \\subseteq \\ X$, $h$ is the *Radon-Nikodym derivative* of $\\nu$ with respect to the distribution $\\mu$\n\n$$\\nu(A) = \\int_{A}^{}{h\\ d\\mu},\\ \\ \\ i.e.,\\ \\ \\ h = \\frac{\\partial\\nu}{\\partial\\mu}\\ .$$\n\nFurthermore, the *Radon-Nikodym derivative* function $h$ is *uniquely* defined up to a $\\mu$-zero measure set. In other words, of $g$ is another *Radon-Nikodym derivative* of $\\nu$, then $h \\equiv g$ almost everywhere except potentially on a set $B \\subseteq X$ of trivial measure, $\\mu(B) = 0$.\n\nRadon-Nikodym derivative is similar to the classical derivative as it describes the rate of change of the density $\\nu$ (the marginalized numerator measure) with respect to the density $\\mu$ (the dominating, denominator measure) just like the determinant of the Jacobian describes variable transformations (change of variables) in multivariable integration.\n\n## Examples\n\n*Example 1 (Classical Jacobian)*: of a 2D polar-to-Cartesian coordinate frame transformation\n\n$$(r,\\ \\varphi) \\in \\mathbb{R}^{+} \\times \\lbrack - \\pi,\\pi)\\underbrace{\\longrightarrow}_{F}\\ (x,\\ y) \\in \\mathbb{R}^{2},\\ \\ F(r,\\ \\varphi) = \\begin{pmatrix}\nx \\\\\ny \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nr\\cos\\varphi \\\\\nr\\sin\\varphi \\\\\n\\end{pmatrix}\\ .$$\n\nThe Jacobian of the transformation $F$ is the $2 \\times 2$ matrix of partial derivatives\n\n$$\\ \\ J_{F}(r,\\ \\varphi) = \\begin{pmatrix}\n\\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial\\varphi} \\\\\n\\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial\\varphi} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n\\cos\\varphi & - r\\sin\\varphi \\\\\n\\sin\\varphi & r\\ \\cos\\varphi \\\\\n\\end{pmatrix}\\ ,\\ \\ \\ \\det\\left( J_{F} \\right) \\equiv \\left| J_{F} \\right| = r\\ .$$\n\nThe effect of the change-of-variables transformation $F$ on computing the definite integrals is\n\n$$\\iint_{F(A)}^{}{f(x,y)}dx\\ dy = \\ \\iint_{A}^{}{f(r\\cos\\varphi,r\\sin\\varphi)}\\left| J_{F} \\right|\\ dr\\ d\\varphi = \\iint_{A}^{}{f(r\\cos\\varphi,r\\sin\\varphi)}r\\ dr\\ d\\varphi\\ .$$\n\n*Example 2 (Radon-Nikodym derivative of the Euclidean measure)*: Suppose the topological space is the support of the kime-phase, $X \\equiv \\lbrack - \\pi,\\pi)$, and $\\Sigma$ is the Borel $\\sigma$-algebra on $X$.\n\nFor any open interval $I = (a,b) \\subseteq \\lbrack - \\pi,\\pi) \\equiv X$, we can take $\\mu$ to be *twice the length measure* of the interval $I$, hence, $\\mu(I) = 2(b - a)$. Let's also choose $\\nu$ to be the standard Euclidean measure of the interval, i.e., $\\nu(I) = (b - a)$. In this case, $\\nu$ is *absolutely continuous* with respect to $\\mu$, i.e., $\\nu \\ll \\mu$. Then, the Radon-Nikodym derivative of $\\nu$ with respect to $\\mu$ will be *constant*, $h = \\frac{\\partial\\nu}{\\partial\\mu} = \\frac{1}{2}$.\n\n*Example 3 (Undefined Radon-Nikodym derivative)*. Let $\\mu$ be the Euclidean (length) measure on $X$ and $\\nu$ be a special measure that assigns to each subset $Y \\subseteq X \\equiv \\lbrack - \\pi,\\pi)$, the number of points from the set $\\{ -3, -2,1,0,1,2,3\\} \\subseteq X$ that are contained in $Y$. Then, $\\nu$ is *not* absolutely-continuous with respect to $\\mu$, since $\\nu\\left( Y \\equiv \\left\\{ - 3 \\right\\} \\right) = 7$, whereas $\\mu\\left( Y \\equiv \\left\\{ - 3 \\right\\} \\right) = 0$. Therefore, the *Radon-Nikodym derivative* $\\frac{\\partial\\nu}{\\partial\\mu}$ is undefined. In other words, there is no finite function $h$ that,\n\n$$1 = \\nu\\left( Y \\equiv \\left\\{ - 3 \\right\\} \\right) \\neq \\int_{- 3 - \\varepsilon}^{- 3 + \\varepsilon}{h\\ d\\mu}\\ ,\\ \\forall\\ \\varepsilon \\gt 0\\ ,$$\n\nsince $\\lim_{\\varepsilon \\rightarrow 0}{\\int_{- 3 - \\varepsilon}^{- 3 + \\varepsilon}{h\\ d\\mu}} = 0$ for all finite functions $h$.\n\n*Example 4 (Discontinuous Radon-Nikodym derivative)*. Let's choose the relation between the dominating and the dominated measures to be $\\mu\\  = \\nu\\  + \\delta_{o}$, where $\\nu$ is the Euclidean (length) measure on $X \\equiv \\lbrack - \\pi,\\pi)$ and \n$\\delta_o(A) \\equiv 1_A(x) = \\begin{cases}  0,\\ x \\notin A \\\\  1,\\ x \\in A  \\end{cases}$ is the Dirac measure on $0$. Then, $\\nu \\ll \\mu$, and the *Radon-Nikodym derivative* is a discontinuous function\n\n$$h(x) = \\frac{\\partial\\nu}{\\partial\\mu} = 1_{X \\smallsetminus \\{ 0\\}} \\equiv \\left\\{ \\begin{matrix}\n0,\\ x = 0 \\\\\n1,x \\neq 0\\  \\\\\n\\end{matrix}\\ . \\right.\\ $$\n\n*Example 5 (Radon-Nikodym kime-phase derivative)*: Again, we use the topological space with support equal to the support of the kime-phase, $X \\equiv \\lbrack - \\pi,\\pi)$, and $\\Sigma$ is again\n\nLet's explore a more kime-phase realistic example where we take the *dominant* measure $\\mu$ to be $2 \\times Laplace(\\mu = 0,\\ \\sigma = 0.5)$ distribution and the *marginalized* (dominated) measure $\\nu$ to be a different Laplace distribution, $Laplace(\\mu = 0,\\ \\sigma = 0.4)$. Again, by design, $\\nu$ is *absolutely continuous* with respect to $\\mu$, i.e., $\\nu \\ll \\mu$. For details, see the graph below and R code in the Appendix.\n\nThen, denote by $h = \\frac{\\partial\\nu}{\\partial\\mu}$ the Radon-Nikodym derivative of the phase distribution $\\nu$ with respect to the measure $\\mu$. Note that $\\nu$ is a *probability distribution*, whereas $\\mu$ is only a *measure*, which can be normalized to a distribution.\n\n\nHere is a recipe to compute the (unique) Radon-Nikodym derivative $h(I) = \\frac{\\partial\\nu}{\\partial\\mu}$ for any interval\n\n$$I = (a,b) \\subseteq \\lbrack - \\pi,\\pi) \\equiv X\\ .$$\n\nSince $\\nu \\sim Laplace(\\mu = 0,\\ \\sigma = 0.4)$, it's [Laplace CDF](https://en.wikipedia.org/wiki/Laplace_distribution) $F_{\\nu}$ is\n\n$$F_{\\nu} = \\frac{1}{2} + \\frac{1}{2}sign(x - \\mu)\\left( 1 - e^{- \\left( \\frac{|x - \\mu|}{\\sigma} \\right)} \\right)$$\n\nand therefore,\n\n$$\\nu(I) = \\nu(a,b) = F_{\\nu}(b) - F_{\\nu}(a) =$$\n\n$$\\frac{1}{2}\\left\\lbrack sign(b)\\left( 1 - e^{- \\left( \\frac{|b|}{0.4} \\right)} \\right) - sign(a)\\left( 1 - e^{- \\left( \\frac{|a|}{0.4} \\right)} \\right) \\right\\rbrack.$$\n\nAlso,\n\n$$\\nu(I) = \\nu(a,b) = \\frac{1}{2\\sigma}\\int_{- \\pi\\ }^{\\pi}{\\mathbf{1}_{\\left\\{ (a,b) \\right\\}}\\ e^{- \\left( \\frac{|x - \\mu|}{\\sigma} \\right)}}dx =$$\n\n$$\\ \\frac{1}{2 \\times 0.4}\\int_{- \\pi\\ }^{\\pi}{\\mathbf{1}_{\\{(a,b)\\}}\\ e^{- \\left( \\frac{|x|}{0.4} \\right)}}dx = \\int_{- \\pi\\ }^{\\pi}{\\mathbf{1}_{\\{(a,b)\\}}\\ \n\\underbrace{\\frac{e^{- \\left( \\frac{|x|}{0.4} \\right)}}{0.8}dx}_{d\\nu}}\\ .$$\n\nNext, we need to change the variables to transform the above integral to\n\n$$\\nu(I) \\equiv \\nu(a,b) = \\int_{a\\ }^{b}\\frac{d\\nu}{d\\mu}d\\mu = \n\\int_{- \\pi\\ }^{\\pi}{\\mathbf{1}_{\\{(a,b)\\}}\\ \\frac{d\\nu}{d\\mu}d\\mu}\\ .$$\n\nBy the uniqueness of the Radon-Nikodym derivative, the function $h(a,b) = \\frac{d\\nu}{d\\mu}$ will be the desired derivative, up to a set $B \\subseteq \\lbrack - \\pi,\\pi)$ with trivial measure, $\\mu(B) = 0$. Recall that in this example, the *dominant* measure $\\mu$ is $2 \\times Laplace(\\mu = 0,\\ \\sigma = 0.5)$, i.e., twice the Laplace density, whereas $\\nu$ is a Laplace distribution with a different scale parameter, $\\sigma$, $\\nu \\sim Laplace(\\mu = 0,\\ \\sigma = 0.4)$.\n\n(*Does this derivation need to be more explicit in terms of a change of variables transformation*, e.g., $y = 0.4\\frac{x}{0.5} = 0.8x$?)\n\nRearranging the terms, we get\n\n$$\\nu(a,b) = \\int_{- \\pi\\ }^{\\pi}{\\mathbf{1}_{\\{(a,b)\\}}\\ h(x) \\times \n\\underbrace{2\\frac{e^{- \\left( \\frac{|x|}{0.5} \\right)}}{2 \\times 0.5}dx}_{d\\mu}}\n= \\int_{a\\ }^{b}\\frac{d\\nu}{d\\mu}d\\mu.$$\n\nTherefore, we need to solve for $h(x)$ this equation\n\n$$\\underbrace{\\frac{e^{- \\left( \\frac{|x|}{0.4} \\right)}}{0.8}dx}_{d\\nu} \\equiv h(x) \\times \\underbrace{2\\frac{e^{- \\left( \\frac{|x|}{0.5} \\right)}}{2 \\times 0.5}dx}_{d\\mu}\\ \\ .$$\n\nThe solution of this equation for $h$ is the (unique) Radon-Nikodym derivative of $\\nu$ with respect to $\\mu$\n\n$$h(x) = \\frac{\\frac{e^{- \\left( \\frac{|x|}{0.4} \\right)}}{0.8}}{2\\frac{e^{- \\left( \\frac{|x|}{0.5} \\right)}}{2 \\times 0.5}} = \\frac{\\frac{e^{- \\left( \\frac{|x|}{0.4} \\right)}}{0.8}}{2e^{- \\left( \\frac{|x|}{0.5} \\right)}} = \\frac{e^{- \\left( \\frac{|x|}{0.4} \\right)}}{0.8 \\times 2e^{- \\left( \\frac{|x|}{0.5} \\right)}} = \\frac{e^{- \\left( \\frac{|x|}{0.4} \\right) + \\left( \\frac{|x|}{0.5} \\right)}}{1.6} = \\frac{8}{5}e^{- \\frac{|x|}{2}} \\equiv \\frac{32}{5}\n\\left( \\underbrace{\\frac{1}{2 \\times 2}\\ e^{- \\frac{|x|}{2}}}_{LaplaceDistr(\\mu = 0,\\ \\sigma = 2)} \\right).$$\n\nTherefore, given that $\\nu \\sim Laplace(\\mu = 0,\\ \\sigma = 0.4)$, $\\mu \\sim 2 \\times Laplace(\\mu = 0,\\ \\sigma = 0.5)$, and $\\nu \\ll \\mu$, then the Radon-Nikodym derivative $\\frac{d\\nu}{d\\mu} = h(x) \\sim \\frac{32}{5} \\times Laplace(\\mu = 0,\\ \\sigma = 2)$.\n\n## Properties of the Radon-Nikodym Derivative\n\nGiven several $\\sigma$-finite measures, the following properties of the Radon-Nikodym derivative (*additivity*, *chain rule*, *reciprocation*, *change of variables*, and *magnitude*) are helpful in estimating the derivative function $h$ in certain situations by breaking the calculations into basic building components.\n\n### Additivity Property\n\nSuppose $\\nu,\\ \\mu$, and $\\lambda$ are $\\sigma$-finite measures on the same kime-phase measurable space $X \\equiv \\lbrack - \\pi,\\pi)$. Assuming $\\nu$ and $\\mu$ are both absolutely continuous with respect to $\\lambda$, i.e., $\\nu \\ll \\lambda$ and $\\mu \\ll \\lambda$, then $\\lambda$-almost everywhere in $X$\n\n$$\\frac{d(\\nu \\pm \\ \\mu)}{d\\lambda} = \\frac{d\\nu}{d\\lambda} \\pm \\frac{d\\mu}{d\\lambda}\\ .$$\n\n*Proof*: We have that $\\nu(A)=\\int_A{\\frac{d\\nu}{d\\lambda}d\\lambda }$, $\\mu(A)=\\int_A{\\frac{d\\mu}{d\\lambda}d\\lambda }$, and $\\nu,\\mu \\ll \\lambda$. Hence, given that $\\lambda(A)=0$ implies that $\\nu(A)=0$ and $\\mu(A)=0$. Therefore, $(\\nu \\pm \\mu)$ is an absolutely continuous measure with respect to $\\lambda$, i.e., $(\\nu \\pm \\mu)(A)=0$ and $(\\nu\\pm\\mu) \\ll \\lambda$. This implies the additivity property,\n\n$$\\forall A,\\ \\nu(A) \\pm \\mu(A)=(\\nu \\pm \\mu)(A)=\n\\int_A{\\frac{d(\\nu \\pm \\ \\mu)}{d\\lambda}d\\lambda}=\n\\int_A{\\frac{d\\nu}{d\\lambda}\\pm\\frac{d\\mu}{d\\lambda} d\\lambda}\n\\Longrightarrow \\frac{d(\\nu \\pm \\ \\mu)}{d\\lambda} = \\frac{d\\nu}{d\\lambda} \\pm \\frac{d\\mu}{d\\lambda}\\ .$$\n\n### Chain Rule Property\n\nIf $\\nu \\ll \\mu \\ll \\lambda$, then $\\lambda$-almost everywhere $$\\frac{d\\nu}{d\\lambda} = \\frac{d\\nu}{d\\mu} \\cdot \\frac{d\\mu}{d\\lambda}\\ .$$\n\n*Proof*: Since $\\nu \\ll \\mu \\ll \\lambda$, $\\nu(A)=\\int_A{\\frac{d\\nu}{d\\lambda}d\\lambda }$, $\\nu(A)=\\int_A{\\frac{d\\nu}{d\\mu}d\\mu }$. As $\\mu \\ll \\lambda$, $\\exists \\frac{d\\mu}{d\\lambda}$, such that\n\n$$d\\mu=\\frac{d\\mu}{d\\lambda}d\\lambda \n\\Longrightarrow \\nu(A)=\\int_A{\\frac{d\\nu}{d\\lambda}d\\lambda }=\n\\int_A{\\frac{d\\nu}{d\\mu}\\cdot \\frac{d\\mu}{d\\lambda}d\\lambda }\n\\Longrightarrow \n\\frac{d\\nu}{d\\lambda} = \\frac{d\\nu}{d\\mu} \\cdot \\frac{d\\mu}{d\\lambda}\\ .$$\n\n### Reciprocal Property\n\nIf $\\mu \\ll \\nu$ and $\\nu \\ll \\mu$, then the Radon Nikodym derivative $\\frac{d\\mu}{d\\nu}$ is the inverse function of the reciprocal Radon Nikodym derivative, $\\frac{d\\nu}{d\\mu}$ , i.e., $$\\frac{d\\mu}{d\\nu} = \\left( \\frac{d\\nu}{d\\mu} \\right)^{- 1}\\ .$$\n\n*Proof*: Since $\\mu \\ll \\nu$ and $\\nu \\ll \\mu$, $d\\nu=\\frac{d\\nu}{d\\mu}d\\mu$. Hence,\n\n$$\\mu(A)=\\int_A{\\frac{d\\mu}{d\\nu}d\\nu }=\\int_A{\\frac{d\\mu}{d\\nu}\\frac{d\\nu}{d\\mu}d\\mu }=\n\\int_A{1d\\mu }.$$ Hence, $\\frac{d\\mu}{d\\nu}\\frac{d\\nu}{d\\mu}\\overbrace{=}^{a.e.}1$ $\\Longrightarrow \\frac{d\\mu}{d\\nu}\\overbrace{=}^{a.e.}\\left(\\frac{d\\nu}{d\\mu}\\right )^{-1}=\\frac{d\\mu}{d\\nu}$.\n\n### Change of Variables Property\n\nFor any $\\mu$-integrable function, $g$, given that $\\mu \\ll \\lambda$, then $$\\int_{- \\pi}^{\\pi}{g\\ d\\mu} = \\ \\ \\int_{- \\pi}^{\\pi}{g\\frac{d\\mu}{d\\lambda}d\\lambda}\\ .$$\n\n*Proof*: Since $\\mu \\ll \\lambda$, $\\exists \\frac{d\\mu}{d\\lambda}$ such that $d\\mu=\\frac{d\\mu}{d\\lambda}d\\lambda$. Therefore, $$\\int_{- \\pi}^{\\pi}{g\\ d\\mu} = \\ \\ \\int_{- \\pi}^{\\pi}{g\\frac{d\\mu}{d\\lambda}d\\lambda}\\ .$$\n\n### Magnitude Property\n\nWhen $\\nu$ is *complex* measure and $\\nu \\ll \\mu$, the Radon Nikodym derivative of the magnitude $|\\nu|$ is the magnitude of the Radon Nikodym derivative of $\\nu$, i.e.,\n\n$$\\frac{d|\\nu|}{d\\mu} = \\left| \\frac{d\\nu}{d\\mu} \\right|\\ .$$\n\n*Proof*: This proof follows [Cohn's \"Measure Theory\" (1980), doi:10.1007/978-1-4899-0399-0.(page 135-136)](https://link.springer.com/book/10.1007/978-1-4614-6956-8).\n\nAccording to the definition of $\\left|\\upsilon\\right|$, for each $A$ in the $\\sigma-$algebra $\\Sigma$ , $\\left|\\upsilon\\right|\\left(A\\right)$ is the supremum of the numbers $\\sum_{j=1}^{n}\\left|\\nu\\left(A_j\\right)\\right|$,\n\n$$\\left|\\upsilon\\right|\\left(A\\right) = \\sup\\sum_{j=1}^{n}\\left|\\nu\\left(A_j\\right)\\right|,$$\n\nwhere ${A_j} \\left(j = 1,2,\\cdots,n\\right)$ ranges over all finite partitions of $A$ into $\\Sigma$-measurable sets.\n\nAssume $\\upsilon\\left(A\\right) = \\int_Af\\mathrm{d}\\mu$ for any Borel set $A$. $\\frac{\\mathrm{d}\\left|\\upsilon\\right|}{\\mathrm{d}\\mu} = \\left|\\frac{\\mathrm{d}\\upsilon}{\\mathrm{d}\\mu}\\right|$ if and only if $\\left|\\upsilon\\right|\\left(A\\right) = \\int_A\\left|f\\right|\\mathrm{d}\\mu$ for any Borel set $A$. We then show $\\left|\\upsilon\\right|\\left(A\\right) = \\int_A\\left|f\\right|\\mathrm{d}\\mu$ for any Borel set $A$.\n\nFirst prove $\\left|\\upsilon\\right|\\left(A\\right)\\leq\\int_A\\left|f\\right|\\mathrm{d}\\mu$ for any Borel set $A$. Let ${A_j}\\left(j = 1,2,\\cdots,n\\right)$ be a finite sequence of disjoint $\\Sigma$-measurable sets whose union is $A$.\n\nThus, $$\\sum_{j=1}^{n}\\left|\\upsilon\\left(A_j\\right)\\right| = \\sum_{j=1}^{n}\\left|\\int_{A_j}f\\mathrm{d}\\mu\\right| \\leq \\sum_{j=1}^{n}\\int_{A_j}\\left|f\\right|\\mathrm{d}\\mu = \\int_A\\left|f\\right|\\mathrm{d}\\mu .$$\n\nSince $\\left|\\upsilon\\right|\\left(A\\right) = \\sup\\sum_{j=1}^{n}\\left|\\nu\\left(A_j\\right)\\right|$, we can derive $$\\left|\\upsilon\\right|\\left(A\\right) \\leq \\int_A\\left|f\\right|\\mathrm{d}\\mu .$$\n\nThen prove $\\left|\\upsilon\\right|\\left(A\\right)\\geq\\int_A\\left|f\\right|\\mathrm{d}\\mu$. Construct a sequence ${g_n}$ of $\\Sigma$-measurable simple function $$g_n\\left(x\\right) = \\sum_{i=1}^{k_n}sgn\\left(f\\left(x\\right)\\right)\\mathbb{I}_{A_n,j}\\left(x\\right) = a_{n,j}, j = 1,2,\\cdots,k_n, $$ where $a_{n,j}$ are the values attained on the sets $A_{n,j}$. $A_{n,j}, j = 1,2,\\cdots,k_n$ are disjoint $\\Sigma$-measurable sets whose union is $A$. Obviously, $g_n$ satisfies $\\left|g_n\\left(x\\right)\\right| = 1$ and $\\lim_{n \\to +\\infty}g_n\\left(x\\right)f\\left(x\\right) = \\left|f\\left(x\\right)\\right|$ hold ar each x in $X$. Then for any arbitrary set in the $\\sigma$-algebra $\\Sigma$ we have\n\n$$\\left|\\int_Ag_nf\\mathrm{d}\\mu\\right| = \\left|\\sum_{j=1}^{k_n}a_{n,j}\\int_{A\\cap A_{n,j}}f\\mathrm{d}\\mu\\right| =\n\\left|\\sum_{j=1}^{k_n}a_{n,j}\\upsilon\\left(A \\cap A_{n,j}\\right)\\right| \\leq \\sum_{j=1}^{k_n}\\left|a_{n,j}\\right|\\left|\\upsilon\\left(A \\cap A_{n,j}\\right)\\right| \\leq \\left|\\upsilon\\right|\\left(A\\right) .$$\n\nSince $\\left|\\int_Ag_nf\\mathrm{d}\\mu\\right| \\leq \\left|\\upsilon\\right|\\left(A\\right)$ and $\\lim_{n \\to +\\infty}\\int_A\\left|g_nf-\\left|f\\right|\\right|\\mathrm{d}\\mu = 0$, according to the dominated convergence theorem, we have $$\\lim_{n \\to +\\infty}\\int_Ag_nf\\mathrm{d}\\mu = \\int_A\\left|f\\right|\\mathrm{d}\\mu .$$\n\nThus, $$\\int_A\\left|f\\right|\\mathrm{d}\\mu = \\lim_{n \\to +\\infty}\\int_Ag_nf\\mathrm{d}\\mu \\leq \\left|\\int_Ag_nf\\mathrm{d}\\mu\\right| \\leq \\left|\\upsilon\\right|\\left(A\\right) .$$ From above,\n\n$$\\left|\\upsilon\\right|\\left(A\\right) = \\int_A\\left|f\\right|\\mathrm{d}\\mu .$$ for any Borel set $A$. Therefore,\n\n$$\\frac{\\mathrm{d}\\left|\\upsilon\\right|}{\\mathrm{d}\\mu} = \\left|\\frac{\\mathrm{d}\\upsilon}{\\mathrm{d}\\mu}\\right|\\ .$$",
      "word_count": 2233
    },
    {
      "title": "Distributional Derivatives",
      "content": "*Distributions*, also known as *generalized functionals*, generalize the classical notion of functions in mathematical analysis to operators. Distributions make it possible to differentiate functions whose classical derivatives do not exist. For instance, any locally integrable function has a distributional derivative, albeit it may be non-differentiable.\n\nIn classical mathematical analysis, a function $f:D\\mathbb{\\rightarrow R}$ can be thought as an operator acting on domain points $x \\in D$ by mapping them to corresponding points in the range $f(x)\\mathbb{\\in R}$. Whereas in functional analysis and distribution theory, a function $f$ may be interpreted as an operator acting on *test functions*, representing all infinitely differentiable, compactly supported, complex-valued functions defined on some non-empty open subset $U \\subseteq \\ \\mathbb{R}^{n}$. The set of all such test functions forms a vector space\n\n$$C_{c}^{\\infty}(U)\\mathfrak{\\equiv D}(U) = \\left\\{\\text{infinitely differentiable\n& compactly supported functions}:\\ U \\subseteq \\mathbb{R}^{n}\\mathbb{\\rightarrow C} \\right\\}\\ .$$\n\nFor instance, when $U\\mathbb{\\equiv R}$, all continuous functions $f\\mathbb{:R \\rightarrow R}$ act as operators integrating against a test function. Given a test function $\\psi \\in \\mathfrak{D}\\left( \\mathbb{R} \\right)$, the operator $f:D\\left( \\mathbb{R} \\right)\\mathbb{\\rightarrow R}$ acts on $\\psi$ by mapping it to a real number as follows:\n\n$$f\\lbrack \\cdot \\rbrack \\equiv \\left\\langle f| \\cdot \\right\\rangle \\equiv D_{f}( \\cdot )\\ :\\ \\underbrace{\\psi}_{test function} \\in \n\\underbrace{\\mathfrak{D}\\left( \\mathbb{R} \\right)}_{domain} \\rightarrow \n\\left\\langle f|\\psi \\right\\rangle \\equiv D_{f}(\\psi) \\equiv \\int_{- \\infty}^{\\infty}{f(x)\\psi(x)dx} \\in \n\\underbrace{\\mathbb{\\ \\ R\\ \\ }}_{range}\\ .$$\n\nThe $f$ action $\\psi \\longmapsto D_{f}(\\psi)$ defines a continuous and linear functional that maps the domain of test functions $\\mathfrak{D}\\left( \\mathbb{R} \\right)$ (infinitely differentiable and compactly supported functions) to scalar values, i.e., $D_{f}\\mathfrak{:D}\\left( \\mathbb{R} \\right)\\mathbb{\\rightarrow R}$. This distribution $D_{f}$ action, $\\psi \\longmapsto D_{f}(\\psi)$, by integration against a test function, $\\psi$, is effectively a weighted average of the function against $f$ on the support of the test function. Mind that the values of the distribution at a single point may not be well-defined (cf. singularities). Also, not all probability distributions $D_{f}$ have well-defined densities $f$. Hence, some distributions $D_{f}$ may arise from functions via such integration against a specific test function, whereas other distributions may be well defined, but do not permit densities and cannot be defined by integration against any test function.\n\nExamples of distributions that associated with a bounded and compactly supported density function are the Dirac delta function and other distributions that can only be defined by actions via integration of a test function $\\psi \\longmapsto \\int_{U}^{}\\psi d\\mu$ against specific measures $\\mu$ on $U$.\n\nTo recap [distributions](https://en.wikipedia.org/wiki/Distribution_(mathematics)) and [Green's theorem](https://www.math.arizona.edu/~kglasner/math456/GREENS1.pdf), we can consider a *function* $f$ as a *distribution* $D_{f}$ or a continuous linear functional on the set of infinitely differentiable functions with bounded support (denoted by $C_{0}^{\\infty}$, or simply $\\mathfrak{D}$).\n\n## Properties\n\n-   $f\\lbrack \\cdot \\rbrack \\equiv \\left\\langle f| \\cdot \\right\\rangle \\equiv D_{f}( \\cdot )\\mathfrak{:D}\\mathbb{\\rightarrow R}$, and $D_{f}(\\psi) \\equiv \\int_{\\mathbb{R}}^{}{f(x)\\psi(x)dx},\\ \\forall\\ \\psi \\in \\mathfrak{D\\ }.$\n\n-   Any continuous function $f(\\varphi)$ can be regarded as a distribution by integration against a test function $\\varphi$.\n\n$$f(\\varphi) = \\int_{- \\infty}^{\\infty}{f(x)\\varphi(x)dx}\\ ,\\ \\forall\\ \\varphi \\in \\mathfrak{D.}$$\n\n*Function approximation*: If there exist a series of functions approximating $f$, i.e., $f = \\lim_{n \\rightarrow \\infty}{f_{n}(x)},\\ \\forall x\\mathbb{\\in R}$, then\n\n$$f(\\varphi) = \\lim_{n \\rightarrow \\infty}\\int_{- \\infty}^{\\infty}{f_{n}(x)\\varphi(x)dx},\\ \\forall\\ \\varphi \\in \\mathfrak{D\\ .}$$\n\n*Distributional derivative*: Some non-differentiable functions may be differentiated as distributions. The *distributional derivative* of $f$ is well defined as the distribution $D_{f}'$ given by\n\n$$D_{f}'(\\varphi) \\equiv - D_{f}\\left( \\varphi' \\right),\\ \\ \\ \\forall\\ \\varphi \\in \\mathfrak{D.\\ }$$\n\nThis makes sense, since $\\text{∀}\\ f \\in C^{1}$, $D_{f}(\\psi) \\equiv \\int_{- \\infty}^{\\infty}{f(x)\\varphi(x)dx}$, and $\\forall\\ \\varphi \\in \\mathfrak{D}$, integration by parts yields\n\n$$D_{f}'(\\varphi) = \\left\\langle f' \\middle| \\varphi \\right\\rangle = \\int_{- \\infty}^{\\infty}{f'(x)\\varphi(x)dx} = \\underbrace{\\left. f(x)\\varphi(x) \\right|_{- \\infty}^{\\infty}}_{0} - \\int_{-\\infty}^{\\infty}{\n\\underbrace{f(x)\\varphi'(x)}_{D_{f}(\\varphi')}dx} =\n\\ - D_{f}\\left( \\varphi' \\right).$$\n\n### Examples\n\n*Example 1*: Let $f = 5\\delta(x + 3) - 2\\delta(x - 1)$. Then, $\\forall\\ \\varphi \\in \\mathfrak{D}$,\n\n$$f(\\varphi) = \\int_{- \\infty}^{\\infty}{f(x)\\varphi(x)dx} = \\ \\int_{- \\infty}^{\\infty}{\\left\\lbrack 5\\delta(x + 3) - 2\\delta(x - 1) \\right\\rbrack\\varphi(x)dx} = 5\\varphi( - 3) - 2\\varphi(1)\\ .$$\n\n*Example 2*: Suppose the one-parameter family of functions $f_{m}:\\mathbb{R} \\rightarrow \\mathbb{R}$ are defined by\n\n$$f_{m}(x) = \\left\\{ \\begin{matrix}\nm,\\ x \\in \\left\\lbrack 0,\\ \\frac{1}{m} \\right\\rbrack\\ \\  \\\\\n0,\\ otherwise \\\\\n\\end{matrix}\\ . \\right.\\ $$\n\nand $D_{f_{m}}(\\psi)$ is the distribution corresponding to $f_{m}$, then, $\\forall\\ \\psi \\in \\mathfrak{D}$\n\n$$\\lim_{m \\rightarrow \\infty}{D_{f_{m}}(\\psi)} = \\lim_{m \\rightarrow \\infty}\\left( \\int_{\\mathbb{R}}^{}{f_{m}(x)\\psi(x)dx} \\right) = \\lim_{m \\rightarrow \\infty}\\left( m\\int_{0}^{\\frac{1}{m}}{\\psi(x)dx} \\right) = \\psi(0) = \\int_{\\mathbb{R}}^{}{\\delta(x)\\psi(x)dx}.$$\n\nThis implies that as the parameter $m$ increases, the function $\\lim_{m \\rightarrow \\infty}D_{f_{m}} \\rightarrow \\delta$, i.e., the distribution functions $f_{m}(x)$ approximate the Dirac delta distribution, $\\delta(x)$.\n\n*Example 3*: Determine what function $f$ corresponds with $D_{f}(\\psi) \\equiv \\int_{0}^{\\infty}{x\\psi'(x)dx},\\ \\forall\\ \\psi \\in \\mathfrak{D}$?\n\nFirst, we can validate that $D_{f}$ is a linear operator, $\\forall\\ \\psi,\\phi \\in \\mathfrak{D},\\ \\ \\forall a,b \\in \\mathbb{R}$,\n\n$$D_{f}(a\\psi + b\\phi) \\equiv \\int_{0}^{\\infty}{x(a\\psi + b\\phi)'(x)dx} = \\int_{0}^{\\infty}{\\left\\lbrack ax\\psi'(x) + bx\\phi'(x) \\right\\rbrack dx} = aD_{f}(\\psi) + bD_{f}(\\phi)\\ .$$\n\nNext, $\\forall\\ \\psi \\in \\mathfrak{D}$, we need to express $D_{f}(\\psi) \\equiv \\int_{0}^{\\infty}{f(x)\\psi(x)dx}$, for some function $f$.\n\nIntegrate by parts the definition of the distribution\n\n$$D_{f}(\\psi) \\equiv \\int_{0}^{\\infty}{x\\psi'(x)dx} = \\left. \\ \\begin{matrix}\n \\\\\nx\\psi(x) \\\\\n \\\\\n\\end{matrix} \\right|_{0}^{\\infty} - \\int_{0}^{\\infty}{\\psi(x)dx} = 0 - \\int_{- \\infty}^{\\infty}{H(x)\\psi(x)dx} = \\int_{- \\infty}^{\\infty}{\\left( - H(x) \\right)\\psi(x)dx},$$\n\nwhere the Heaviside function is \n$H(x) = \\left \\{ \\begin{matrix} 0,\\ x \\leq 0 \\\\ 1,\\ x > 0 \\\\ \\end{matrix} \\right .$. \nTherefore, this distribution $D_{f}(\\psi) \\equiv \\int_{0}^{\\infty}{x\\psi'(x)dx}$, corresponds to $f(x) = - H(x)$.\n\n*Example 4*: Compute the distributional derivative of the Heaviside function $f(x) \\equiv H(x)$.\n\nSince $D_{f}'(\\varphi) \\equiv - D_{f}\\left( \\varphi' \\right),\\ \\ \\ \\forall\\ \\varphi \\in \\mathfrak{D}$, we have\n\n$$D_{H}'(\\varphi) = \\int_{- \\infty}^{\\infty}{\\left( - H(x) \\right)\\varphi'(x)dx} = - \\int_{0}^{\\infty}{\\varphi'(x)dx} = - \\left. \\ \\begin{matrix}\n \\\\\n\\varphi(x) \\\\\n \\\\\n\\end{matrix} \\right|_{0}^{\\infty} = 0 - \\left( - \\varphi(0) \\right) = \\varphi(0) \\equiv D_{\\delta}(\\varphi).$$\n\nTherefore, distributional derivative of the Heaviside function $H(x)$ is the Dirac delta function, i.e., $H' \\equiv D_{H}' = \\delta$.\n\n*Example 5*: Compute the distributional derivative of the delta function $f(x) \\equiv \\delta(x)$.\n\nAgain, since $D_{f}'(\\varphi) \\equiv - D_{f}\\left( \\varphi' \\right),\\ \\ \\ \\forall\\ \\varphi \\in \\mathfrak{D}$, we have\n\n$$\\delta'(\\varphi) \\equiv D_{\\delta}'(\\varphi) = - \\int_{- \\infty}^{\\infty}{\\delta(x)\\varphi'(x)dx} = - \\varphi'(0).$$",
      "word_count": 991
    },
    {
      "title": "Higher-order distributional derivatives",
      "content": "The order $n$ distributional derivatives are defined by induction. The case of $n = 1$, first derivative, is presented above. Higher order distributional derivatives are defined by repeated application of integration by parts ($n$ times):\n\n$$f^{(n)}(\\varphi) \\equiv D_{f}^{(n)}(\\varphi) \\equiv ( - 1)^{n}D_{f}\\left( \\varphi^{(n)} \\right),\\ \\ \\ \\forall\\ \\varphi \\in \\mathfrak{D.\\ }$$\n\nFor instance, let $f(x) = |x|,\\forall\\ x\\mathbb{\\in R}$, then, for $n = 2$,\n\n$$f''(\\varphi) \\equiv D_{f}''(\\varphi) \\equiv ( - 1)^{2}D_{f}\\left( \\varphi'' \\right) = \\int_{- \\infty}^{\\infty}{|x|\\varphi''(x)dx} = - \\int_{- \\infty}^{0}{x\\varphi''(x)dx} + \\int_{0}^{\\infty}{x\\varphi''(x)dx} =$$\n\n$$- \\left. \\ \\begin{matrix}\n \\\\\nx\\varphi'(x) \\\\\n \\\\\n\\end{matrix} \\right|_{- \\infty}^{0} + \\left. \\ \\begin{matrix}\n \\\\\nx\\varphi'(x) \\\\\n \\\\\n\\end{matrix} \\right|_{0}^{\\infty} + \\left. \\ \\begin{matrix}\n \\\\\n\\varphi(x) \\\\\n \\\\\n\\end{matrix} \\right|_{- \\infty}^{0} - \\left. \\ \\begin{matrix}\n \\\\\n\\varphi(x) \\\\\n \\\\\n\\end{matrix} \\right|_{0}^{\\infty} = 2\\varphi(0) = 2D_{\\delta}(\\varphi).$$\n\nTherefore, the second order distributional derivative of $f(x) = |x|$ is $f'' = D_{f}'' = 2\\delta$.",
      "word_count": 145
    },
    {
      "title": "Heisenberg Kime-Uncertainty Principle for Kime Magnitude (Time) and Direction (Phase)",
      "content": "*Goal*: Derive a general *kime-operator*, $Κ = \\widehat{\\kappa}$, or a kime-phase operator, $\\Theta = \\widehat{\\theta}$, similarly to the position $\\widehat{x} = x$ and momentum $\\widehat{p} = - i\\hslash\\frac{\\partial}{\\partial x}$ operators.\n\n## Linear time evolution operator\n\nThe *exponential of an operator* is defined to solve linear evolution equations. Suppose\n\n$A:X \\rightarrow X$ is a bounded linear operator on a Banach space $X$. Generalizing the the power series expansion of $e^{a},a \\in \\mathbb{R}$, we define the operator exponential by\n\n$$\\underbrace{e^{A}}_{operator} \\equiv \\underbrace{I}_{identity} + A + \n\\frac{1}{2!}A^{2} + \\cdots + \\frac{1}{n!}A^{n} + \\cdots = \n\\sum_{n = 0}^{\\infty}{\\frac{1}{n!}A^{n}}\\ .$$\n\nThe norm of the operator, $\\left\\| A \\right\\|$, leads to a convergent real series\n\n$$\\underbrace{e^{\\left\\| A \\right\\|}}_{scalar} \\equiv \n\\underbrace{1}_{identity} + \\left\\| A \\right\\| + \\frac{1}{2!}\\left\\| A \\right\\|^{2} + \\cdots + \\frac{1}{n!}\\left\\| A \\right\\|^{n} + \\cdots = \\sum_{n = 0}^{\\infty}{\\frac{1}{n!}\\left\\| A \\right\\|^{n}}\\ .$$\n\nThe important operator exponential properties include:\n\n$$\\left\\| e^{A} \\right\\| \\leq e^{\\left\\| A \\right\\|}\\ \\ ,$$\n\n$$when\\ \\underbrace{\\lbrack A,B\\rbrack}_{commutator} = \n0\\ \\ \\  \\Longrightarrow \\ \\ e^{A}e^{B}\\underbrace{=}_{A\\ \\&\\ B\\ commute} e^{A + B}\\ .$$\n\nIn general, a time-evolution operator is a solution of an initial value problem for a linear scalar ODE $\\dot{f} = \\frac{d}{dt}f(t) = af(t)$ with boundary condition $f(0) = f_{o}$. The solution of the *scalar linear time-evolution* operator is $f(t) = f_{0}e^{at}$. In the more general case of a (vector) linear system of ODEs, $\\dot{F} = \\frac{d}{dt}F(t) = AF(t)$, the solution is\n\n$$F(t) = \\underbrace{\\overbrace{e^{tA }}^{1 - parameter\\ time\\ operator}}_{evolution\\\nflow}\\ F_{0}\\ ,$$\n\nwhere $F\\mathbb{:R \\rightarrow}X$, $X$ is a Banach space, and $A:X \\rightarrow X$ is a bounded linear operator on $X.$ By definition, the derivative of the operator exponential $\\frac{d}{dt}e^{tA} = Ae^{tA}$, similar to the scalar case $\\frac{d}{dt}e^{at} = ae^{at}$:\n\n$$\\frac{d}{dt}e^{tA} = \\lim_{h \\rightarrow 0}\\left( \\frac{e^{(t + h)A} - e^{tA}}{h} \\right) = e^{tA}\\lim_{h \\rightarrow 0}\\left( \\frac{e^{hA} - I}{h} \\right) = Ae^{tA}\\lim_{h \\rightarrow 0}{\\sum_{n = 0}^{\\infty}\\left( \\frac{1}{(n + 1)!}A^{n}h^{n} \\right)} = Ae^{tA}.$$\n\nWhen $X \\equiv \\mathbb{R}^{n}$, the linear system of ODEs corresponds to linear operator $A = \\left( A_{i,j} \\right)_{n \\times n}$ representing a linear system of $n$ equations on in a finite-dimensional space. However, the same notation of the operator, $A$, time evolution, $F(t)$, and solution, $F(t) = e^{tA}F_{0}$, apply to infinite dimensional spaces, e.g., spaces of continuous functions or $L^{2}$ integrable functions.\n\n-   Recall how we derive the momentum operator $\\widehat{p}$:\n\nThe wavefunction of a free particle with *momentum* $p = \\hslash k$ and *energy* $E = \\hslash\\omega$ can be expressed in the position space in the form of a *de Broglie* wave function, $\\psi(x,t) = e^{i(k \\cdot x - \\omega t)}$, up to a constant multiple, where $k$ is the *wave number*, $\\omega$ is *angular frequency*. Then, the (spatial) partial derivative of $\\psi$ is\n\n$$\\frac{\\partial\\psi}{\\partial x} = ike^{i(k \\cdot x - \\omega t)} = ik\\psi\\ .$$\n\nHence, $\\frac{\\hslash}{i}\\frac{\\partial\\psi}{\\partial x} = \\frac{\\hslash}{i}ike^{(k \\cdot x - i\\omega t)} = \\hslash k\\psi = p\\psi$. In terms, of linear operators, this relation is expressed as\n\n$$\\underbrace{\\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}}_{Linear\\ \\\\ operator}\\ \n\\overbrace{\\ \\ \\psi\\ \\ }^{State} = \n\\underbrace{\\overbrace{\\ \\ \\ p\\ \\ \\ }^{momentum \\\\\neigenvalue }}_{numeric\\  \\\\\nvalue\\ or\\ vector}\\ \\cdot \\underbrace{\\overbrace{\\ \\ \\ \\ \\ \\ \\psi\\ \\ \\ \\ \\ \\ \\ }^{eigenfunction}}_{(Eigen)\\ State}\\ \\  .$$\n\nTherefore, the operator $\\widehat{p} ≔ \\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}$ is called the ''*momentum operator*'', since it's in one-to-one correspondence with the *momentum* (eigenvalue). Applying $\\widehat{p}$ to the wavefunction (state) yields an estimate of the momentum numerical value/vector.\n\nThere is no ''*time operator*'', see Pauli's argument against the existence of a time operator rooted in the boundedness of the energy operator, see [article 1](https://doi.org/10.48550/arXiv.1705.09212) and [article 2](http://doi.org/10.1088/1742-6596/99/1/012002). However, in the Schrödinger picture representation, there is a linear *time evolution* operator $U$ specifying the future state of an electron that is currently in state $\\left| \\psi \\right\\rangle$, as $U\\left| \\psi \\right\\rangle$, for each possible current state $\\left| \\psi \\right\\rangle$. The time-evolution of a closed quantum system is *unitary* and *reversible*. This implies that the state of the system at a later point in time, $t$, is given by $\\left| \\psi(t) \\right\\rangle = U(t)\\left| \\psi(t_{o}) \\right\\rangle$, where $U(t)$ is a unitary operator, i.e., its adjoint $U^{\\dagger} \\equiv \\left( U^{*} \\right)^{T}$ operator is the inverse: $U^{\\dagger} = U^{- 1}$. The integral equation $\\left| \\psi(t) \\right\\rangle = U(t)\\left| \\psi(t_{o}) \\right\\rangle$ relates the state of the particle at the initial time $t_{o}$ with its state at time $t$. Locally, we can express the position of an inertia particle at time $t$ is $x(t) = \\ x\\left( t_{o} \\right) + \\ v \\cdot (t - t_{o})$, where $v$ is the constant speed and $x\\left( t_{o} \\right)$ is the initial position, i.e., $\\frac{dx}{dt} = v$. The (time-dependent) Schrödinger equation, $i\\hslash\\frac{\\partial}{\\partial t}\\psi(x,t) = H\\psi(x,t)$, represents a generalization of this (ordinary) differential equation, where the particle system Hamiltonian is $H$ and the PDE solution is the particle wavefunction $\\psi(x,t)$, which describes the particle state (e.g., position) at time $t \\geq t_{o}$, given its initial position $\\psi(x,t_{o})$.\n\nThe quantum argument for an *external time* is rooted in the contradiction associated with assuming the existence of a general time operator, see [this article](https://arxiv.org/pdf/1811.09660.pdf). A well-defined time operator $\\widehat{t}$ has to be paired to a conjugate energy operator $\\widehat{H}$, a Hamiltonian, as follows from the Heisenberg's formulation of the pair of classically conjugate variables, *time* and *energy*. The commutator operator $\\left\\lbrack \\widehat{E},\\widehat{t} \\right\\rbrack = (\\widehat{E}\\hat{t} - \\widehat{t}\\widehat{E}) = - i\\hslash$ suggests a time-energy uncertainty relation in two separate forms, [see this](https://doi.org/10.1023/A:1017525227832).\n\n$$\\mathrm{\\Delta}E\\mathrm{\\Delta}t \\sim \\hslash\\ \\ (indeterminate\\ form)\\ ,$$\n\n$$\\mathrm{\\Delta}E\\mathrm{\\Delta}t \\geq \\frac{\\hslash}{2}\\ \\ (precise\\ form)\\ .$$\n\nIt's worth reviewing the two complementary versions of the Schrodinger equation (SE): (1) the *Time-dependent (TDSE)*, where *time* is a parameter, not an operator, $\\left( H(t) - i\\hslash\\frac{\\partial}{\\partial t} \\right)\\psi(t) = 0$; and (2) the *Time-independent (TISE)*, for a constant energy $E$, $(\\hat{H} - \\hat{E})\\psi_x = 0$, where the TISE solution is $\\psi(t) = \\psi_x e^{-\\frac{i}{\\hbar}Et}$.\n\nIf $\\hat{H}$ is the system Hamiltonian, solving the eigenvalue equation $\\hat{H}\\Psi_n = E_n\\Psi_n$, where the states $\\Psi_n$ are the Hamiltonian eigenfunctions and the (*energy*) eigenvalues $E_n$ represent the corresponding observable energies. The Hamiltonian operator $\\hat{H}$ may either depend on time $t$ or be time-independent (invariant of time).\n\nUnder explicit time-dependence, $\\hat{H}\\equiv \\hat{H}(t)$, The Hamiltonian operator has different eigenfunctions at different times, $\\Psi_n=\\Psi_n(x,t)$, dynamically changing over time.\n\nUnder time-independence, the eigenfunctions of the Hamiltonian operator $\\hat{H}$ are the same at each time instant $\\Psi_n=\\Psi_n(x)$. We drop the $t$ argument from the solutions, which are much simpler in this case compared to the more general (time-dependent) case. Although, the time-independent eigenfunctions still may change across time, despite not being the same at each instant. In other words, the same eigenfunctions still evolve in time, but they oscillate preserving their shapes and forms without changing their functional analytic forms. Below is an example illustrating this subtle point of the the *time-independent eigenfunctions still changing over time and yet being the same at each instant*. This example shows these eigenfunctions $\\psi_k(x)=\\sin(k\\cdot x/2), k\\in\\{1,2,3,4\\}$, where the (separable) time-dynamics are by multiplication, e.g., $\\psi_k'(x,t)=\\psi_k(x)\\cdot t$. Each solution corresponds to a different energy with low-frequencies corresponding to low energies. Note that these eigenfunctions $\\psi_k(x)$ are not the full solutions to the Schrodinger equation, as the full solutions $\\psi_k'(x,t)$ are time-dependent with amplitude oscillating in time.\n\nThe shapes of the time-dependent Hamiltonian eigenfunctions are the same as their corresponding solutions of the time-independent (TISE), however, the separable time-dependent eigenfunctions oscillate with time. The shape of the spatial-parts are unchanged but their amplitudes oscillate according to its temporal temporal counterpart producing the full time-dependent solution of the to the Schrodinger equation. At any fixed instance of time, the Hamiltonian has the same eigenfunctions. In the case of time-dependent Hamiltonian, the *same simple Hamiltonian eigenfunctions would not be valid for all time instances*, as their shape would morph during the time-evolution beyond simple shapes amplitude oscillations.\n\nThis explains the difference between the *time-independent* Schrodinger equation (TISE) and the *time-dependent* Schrodinger equation (TDSE).\n\n\nIn principle, we are interested in solving the general, time-dependent, Schrodinger equation. Only in the special case when the Hamiltonian is *not an explicit function of time*, we solve the time-independent Schrodinger equation to get the corresponding (time-dependent) eigenfunctions. The TDSE\n\n$$\\hat{H}\\Psi(x,t)=i\\hbar \\frac{\\partial}{\\partial t}\\Psi(x,t)\\ .$$ In the special case when $\\hat{H}$ does not explicitly depend on time and $\\Psi(x,0)$ is the (initial) eigenstate of the $\\hat{H}$ corresponding to an eigenvalue $E_n$, the time-independent SE solution is\n\n$$\\Psi(x,t) = e^{-\\frac{i}{\\hbar}\\hat{H}t}\\Psi(x,0)\\ .$$ This can be quickly confirmed by starting with the TDSE $\\hat{H}\\Psi(x,t)=i\\hbar \\frac{\\partial}{\\partial t}\\Psi(x,t)$ and using the TISE separability condition $\\Psi(x,t)=\\psi_x(x) \\psi_t(t)$.\n\n$$\\hat{H}\\Psi(x,t)=\\hat{H}\\psi_x(x) \\psi_t(t) =\ni\\hbar \\frac{\\partial}{\\partial t}\\psi_x(x) \\psi_t(t)=\n\\psi_x(x) i\\hbar \\frac{\\partial}{\\partial t}\\psi_t(t)=\n\\psi_x(x) E \\psi_t(t).$$\n\nHence, $i\\hbar \\frac{\\partial}{\\partial t}\\psi_t(t)= E \\psi_t(t)$ whose solution is the exponential function $\\psi_t(t)=e^{\\frac{E}{i\\hbar}t}=e^{-i\\frac{E}{\\hbar}t}$. Therefore, the TISE solution is $\\Psi(x,t)=\\psi_x(x)\\cdot\\psi_t(t)=\\psi_x(x)e^{-i\\frac{E}{\\hbar}t}$.\n\nWe can expand the exponential function, $e^{-\\frac{i}{\\hbar}\\hat{H}t}$ as a series of operators acting on the eigenstate\n\n$$e^{-\\frac{i}{\\hbar}\\hat{H}t}\\Psi(x,0)=e^{-\\frac{i}{\\hbar}E_n t}\\Psi(x,0)\\ .$$\n\nThis suggests that having the initial wavefunction in terms of eigenstates yields the solution of the time-dependent Schrodinger equation.\n\nRecall that the (real) eigenstates of a Hermitian (Hamiltonian) operator forms a complete orthonormal basis where all states can be expressed linear combinations of base eigenstates. Suppose the corresponding pairs of eigenstates-eigenvalues are denoted by ($\\Psi_n$, $E_n$). Then\n\n$$\\Psi(x,0)=\\sum_{n}{c_n \\psi_n(x)}=\n\\sum_{n}{c_n e^{-\\frac{i}{\\hbar}E_n t}\\psi(x)}\\ .$$\n\nLet's examine the physical interpretation of the eigenstates $\\psi_n(x)$. First, the amplitude $|\\psi_n(x)|^2$ represents the time independent (stationary state) probability density function (PDF) of finding a particle at position $x$. Second, when the system is in the state $\\psi_n(x)$, measuring its the energy will *almost surely* yield an observation $E_n$.\n\nThis probabilistic interpretation of wavefunctions imply the need of appropriate *normalization conditions*:\n\n$$\\int_{space}{|\\psi_n(x)|^2dx} = 1\\ \\ \\ , \\ \\ \\ \n\\int_{space-time}{|\\Psi_n(x,t)|^2dx} = 1\\ \\ \\ {\\text{and}}\\ \\ \\ \n\\sum_{n}{|c_n|^2}=1\\ .$$\n\nWavefunctions always behave according to the TDSE, however in teh special case of time-independent Hamiltonians, the TISE solutions are separable $\\Psi(x,t)=\\psi_x(x)\\cdot \\psi_t(t)$, actually linear combinations of such terms, where $\\psi_t(t)=e^{−i\\frac{Et}{\\hbar}}$ and $\\psi_x(x)$ solves the simpler TISE.\n\nTherefore, given initial state, the solutions to the general time dependent Schrodinger equation are derived from the solutions of the time-independent (TISE), $\\hat{H}\\psi(x)=E\\psi(x)$. The solution to the TDSE, a partial differential equation, relies on (multiplicative) separation of variables to split the space-and-time dependent components of the wave-function. This separability splits the PDE into a pair of ordinary differential equations - one for the *space* and the other for *time*.\n\nThe time part is easy to solve, whereas the space-part yields the TISE as an eigenvalue equation whose solution provides the most important characterization of the system wavefunction, since it explicates the energy-eigenspectrum for the problem. Multiplying the TISE solution by the corresponding time-part function describes also the temporal evolution of hte system. Since the Hamiltonian doesn't explicitly depend on time, the TISE energy-eigenstates are \"stationary states\", preserving the same energy eigenvalues.\n\nAssuming the existence of a time operator $\\widehat{t}$, we can construct a unitary operator $\\widehat{U} = e^{\\pm i\\widehat{t}\\ dE}$ that acts by translating states along the energy spectrum, $\\widehat{U}\\left| E \\right\\rangle \\rightarrow |E + dE\\rangle$. Infinite iterative application of the $\\widehat{U}$ translation operator can project the system into a state of arbitrarily negative energy implying system instability at a stable vacuum state.\n\nIn quantum theory, the time-energy uncertainty relation is not well-defined because of the multiple forms of time. *Pragmatic* or *external time* may be defined as the parameter entering the Schrodinger equation and measured by an external and independent clock. *Dynamic time* represents an intrinsic tracker defined through the dynamical behavior of the quantum objects themselves. *Observable time* represents a measurable characteristic of event ordering. Spacekime analytics regards the kime-magnitude as an observable time tracking the ordered ranking of sequential longitudinal events.\n\nWhen the Hamiltonian operator $\\widehat{H}$ is constant, the Schrödinger equation has the solution\n\n$$\\left| \\psi(t) \\right\\rangle = e^{- \\frac{i\\widehat{H}t}{\\hslash}}\\left| \\psi(0) \\right\\rangle\\ .\\ $$\n\nThe time-evolution operator $\\widehat{U}(t) = e^{- \\frac{i\\widehat{H}t}{\\hslash}}$ is unitary preserving the inner product between vectors in the Hilbert space over the field $\\mathbb{C}$, i.e., $\\left\\langle \\phi \\middle| \\widehat{U}|\\psi \\right\\rangle = \\left\\langle \\phi \\middle| \\psi \\right\\rangle\\mathbb{\\in C}$. Let's denote $|\\psi(0)\\rangle$ the initial state of the wavefunction and the corresponding state at a time $t$ by $\\left| \\psi(t) \\right\\rangle = \\widehat{U} \\left| \\psi(0) \\right\\rangle = e^{- \\frac{i\\widehat{H}t}{\\hslash}}\\left| \\psi(0) \\right\\rangle$ for some unitary operator $\\widehat{U}$. These are all solutions to the Schrödinger equation since given another continuous family of unitary operators parameterized by $t$ by $\\widehat{U}(t)$, we can choose the parameterization to ensure that $\\widehat{U}(0)$ is the identity operator and $\\left (\\widehat{U}\\left( \\frac{t}{N} \\right)\\right )^{N} \\equiv \\widehat{U}(t),\\forall N\\mathbb{\\in N}$. This specific dependence of $\\widehat{U}(t)$ on the time argument implies that $\\widehat{U}(t) = e^{- i\\widehat{G}t}$, for some self-adjoint operator $\\widehat{G}$ called the *generator* of the family $\\widehat{U}(t)$.\n\nIn other words, the Hamiltonian operator $\\widehat{H}$ is an *instance of a generator*, up to a multiplicative constant, $\\frac{1}{\\hslash}$, which may be set to $1$ in natural units. The generator $\\widehat{G}$ that corresponds to the unitary operator $\\widehat{U}$ is Hermitian, since $\\widehat{U}(\\delta t) \\approx \\widehat{U}(0) - i\\widehat{G}\\delta t$ and therefore\n\n$$\\widehat{U}(\\delta t)^{\\dagger}\\widehat{U}(\\delta t) \\approx \\left( \\widehat{U}(0)^{\\dagger} + i{\\widehat{G}}^{\\dagger}\\delta t \\right)\\left( \\widehat{U}(0) - i\\widehat{G}\\delta t \\right) = I + i\\delta t\\left( {\\widehat{G}}^{\\dagger} - \\widehat{G} \\right) + O\\left( \\delta t^{2} \\right)\\ .\\ $$\n\nThus, to a first order approximation, $\\widehat{U}(t)$ is unitary when its generator (derivative) is self-adjoint (Hermitian), i.e., ${\\widehat{G}}^{\\dagger} = \\widehat{G}$.\n\n-   Under the Copenhagen interpretation, the Schrödinger equation relates information about the system at one time, $t_{o} = 0$, to information about the system at another time, $t$. The Schrödinger equation encodes the time-evolution process continuously and deterministically, as knowing $\\psi\\left( t_{o} \\right)$ is in principle sufficient to calculate $\\psi(t),\\forall t \\ge t_{o}$. However, the wavefunction can also change discontinuously and stochastically during the measurement process, as the kime-phase is stochastic $\\varphi \\sim \\Phi\\lbrack - \\pi,\\pi)$. Each observation is an act of measurement, which corresponds to a random sample from the kime-phase distribution. In practice, this stochastic behavior of the wavefunction is mediated by acquiring multiple repeated sampling from a tightly controlled experiment, i.e., simulating multiple samples corresponding to an identical time $t_{o}$. Then, we commonly use various aggregation functions, i.e., sample statistics, and rely on LLN to argue that the expected values of our sample statistics tend to their corresponding population-wide distribution characteristics. Examples of such population parameters include measures of centrality, e.g., mean and median, measures of dispersion, e.g., variance and IQR, and measures of skewness or kurtosis. Prior to each measurement, the exact post-measurement value of the wavefunction is known, however, the kime-phase distribution model suggests probability likelihood of specific classes (cf. Borel sets) of observable wavefunction values.\n\n-   The wavefunction solutions to Schrödinger equation cover all simultaneously all possibilities described by quantum theory. Instantiations of these possibilities correspond to individual random sampling from the underlying kime-phase distribution. This preserves the continuous longitudinal time-evolution of the wavefunctions solving the Schrödinger equation, as all possible states of the system (including the measuring instrument and the observer) are present in a real physical quantum superposition, reflecting the kime-phase model distribution. Even though the 5D spacekime universe is deterministic, in 4D Minkowski spacetime, we perceive non-deterministic behavior governed by probabilities, see the ''spacekime interpretation'' section in the [Spacekime/TCIU book](https://doi.org/10.1515/9783110697827). That is, we are not equipped to observe and interpret spacekime as a whole. We can only detect, process and interpret tangible evidence of observable phenomena trough finite sampling across spacetime.\n\n[Previously](https://doi.org/10.1016/j.padiff.2022.100280), we showed that (periodic) potential functions $u$ of this type\n\n$$u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right) = \\underbrace{e^{2\\pi i\\left\\langle \\mathbf{\\eta,\\kappa} \\right\\rangle}}_{kime\\ frequency\\ part} \\cdot \n\\underbrace{e^{2\\pi i\\left\\langle \\mathbf{x,\\xi} \\right\\rangle}}_{spatial\\ frequency\\ part},\n\\ \\ {\\text{subject to}}\\ \\ \\left| \\mathbf{\\eta} \\right|^{2} \\equiv \\left| \\mathbf{\\xi} \\right|^{2}\\ ,$$\n\nsolve the general ultrahyperbolic wave equation, $\\Delta_{\\mathbf{x}}u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right) = \\Delta_{\\mathbf{\\kappa}}u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right)$, where $u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right) = e^{2\\pi i\\left\\langle \\mathbf{\\eta,\\kappa} \\right\\rangle} \\cdot e^{2\\pi i\\left\\langle \\mathbf{x,\\xi} \\right\\rangle} \\in C^{2}\\left( D_{t} \\times D_{s} \\right)$, $\\mathbf{\\eta} = \\left( \\eta_{1},\\eta_{2},\\ldots,\\ \\eta_{d_{t}} \\right)'$ and $\\mathbf{\\xi} = \\left( \\xi_{1},\\xi_{2},\\ldots,\\ \\xi_{d_{s}} \\right)'$ represent respectively the frequency vectors of *integers* corresponding to the temporal (angular frequency) and spatial frequencies (wave numbers) of the Fourier-transformed periodic solution of the wave equation.\n\nMore generally, since the wave equation is a linear PDE, any finite linear combination of $M$ such basic potential functions will also represent a (composite, superposition) solution:\n\n$$u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right) = \\sum_{\\begin{matrix}\nm = 1\\  \\\\\n\\left\\{ \\mathbf{\\xi}_{m},\\mathbf{\\eta}_{m}\\mathbf{\\ \\ }s.t.\\ \\ \\left| \\mathbf{\\xi}_{m} \\right|^{2} = \\left| \\mathbf{\\eta}_{m} \\right|^{2} \\right\\} \\\\\n\\end{matrix}}^{M}\\left( C_{m} \\cdot e^{2\\pi i\\left\\langle \\mathbf{\\eta}_{m}\\mathbf{,\\kappa} \\right\\rangle} \\cdot e^{2\\pi i\\left\\langle \\mathbf{x,}\\mathbf{\\xi}_{m} \\right\\rangle} \\right)\\ .$$\n\nIn polar coordinate representation of kime, the simple ($M = 1$) separable solutions of the wave equation can be expressed via the Euler formula:\n\n$$\\mathbf{\\kappa} = te^{i\\theta} = t\\left( \\cos\\theta + i\\sin\\theta \\right) = \n\\underbrace{\\left( t\\cos\\theta \\right)}_{\\kappa_{1}} + i\\ \n\\underbrace{\\left( t\\sin\\theta \\right)}_{\\kappa_{2}}\\ ,\n\\mathbf{\\ \\ x} = \\left( x_{1},x_{2},x_{3} \\right),\\ \\ \n\\mathbf{\\eta} = \\left( \\eta_{1},\\ \\eta_{2} \\right)\\ ,$$\n\n$$u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right) = e^{2\\pi i\\left\\langle \\mathbf{\\eta,\\kappa} \\right\\rangle} \\cdot e^{2\\pi i\\left\\langle \\mathbf{x,\\xi} \\right\\rangle} = e^{2\\pi i\\sum_{j = 1}^{d_{t} = 2}{\\kappa_{j}\\eta_{j}}} \\cdot e^{2\\pi i\\sum_{l = 1}^{d_{s} = 3}{x_{l}\\xi_{l}}} =$$\n\n$$e^{2\\pi i\\left( \\eta_{1}t\\cos\\theta + \\eta_{2}t\\sin\\theta \\right)} \\cdot e^{2\\pi i\\left( x_{1}\\xi_{1} + x_{2}\\xi_{2} + x_{3}\\xi_{3} \\right)}\\ .$$\n\nOne specific solution illustrated on Figure 1 is given by:\n\n$$u\\left( \\mathbf{x},\\mathbf{\\kappa} \\right) = u\\left( x_{1},x_{2},x_{3},\\ t,\\theta \\right) = e^{2\\pi t\\ i\\ \\left( - 2\\cos\\theta + 3\\sin\\theta \\right)} \\cdot e^{2\\pi i\\left( - 3x_{1} + 2x_{2} \\right)}\\ ,$$\n\nwhere $\\mathbf{\\eta} = \\left( \\eta_{1},\\eta_{2} \\right) = ( - 2,\\ 3)$ and $\\mathbf{\\xi =}\\left( \\xi_{1},\\ \\xi_{2},\\xi_{3} \\right)\\mathbf{=}( - 3,2,0)$, $\\left| \\mathbf{\\xi} \\right|^{2} = \\left| \\mathbf{\\eta} \\right|^{2} = 13$.\n\n*Figure 1*: Examples of the existence of a locally stable solution to the ultrahyperbolic wave equation in spacekime. The left and right figures illustrate alternative views and foliations of the 2D kime dynamics of the 5D spacekime wave projected onto a flat 2D (x,y) plane. ![Wave propagation in spacekime over a solid cylinder space with backplane projections](https://wiki.socr.umich.edu/images/4/46/Spacekime_WaveEquationSolution_VolRenderAnim1.gif).\n\n![Cross-sectional view of the (x,y) space (horizontal plane) animated over kime-magnitude. The vertical z-axis represents the kime-phase dynamics](https://wiki.socr.umich.edu/images/1/1c/Spacekime_WaveEquationSolution_SurfRenderAnim2.gif).\n\n*Question*: Double check the signs of the exponential components in the wave equation solutions. At first glance, it may look as if we off by a negative sign in front of the kime inner product term, $\\left\\langle \\mathbf{\\eta,\\kappa} \\right\\rangle$. In chapter 3, p. 149, we argue that $\\psi(x) = \\frac{1}{\\sqrt{2\\pi\\hslash}}\\ e^{- \\frac{i}{\\hslash}\\left( e_{1}\\kappa_{1} + e_{2}\\kappa_{2} - p_{x} \\cdot x \\right)}$, where the energy $E = \\left( e_{1},\\ e_{2} \\right),\\ e_{i} = \\hslash\\omega_{i},\\ i \\in \\{ 1,2\\}$. Mind the sign differences between spatial ($x$) and kemporal ($\\kappa$) parts in the exponent.\n\n*Answer*: The $\\pm$ sign difference does not play a role here as $\\nabla^2\\equiv\\Delta$, cancelling the sign, $(-1)^2 = 1$. Note the connection to deriving the Schrodinger wave equation by quantization: $$LHS\\equiv i\\hbar\\left ( \\frac{\\partial \\psi}{\\partial \\kappa_1} +\n\\frac{\\partial \\psi}{\\partial \\kappa_2}\\right ) =\n-\\frac{\\hbar^2}{2m} \\left ( \\nabla^2 \\psi\\right )\\equiv RHS,$$\n\n$$LHS = i\\hbar\\left ( \\frac{-i}{\\hbar} \\underbrace{E}_{e_1+e_2}\\right ),$$\n\n$$RHS= -\\frac{\\hbar^2}{2m} \\left ( p_x^2 \\frac{-1}{\\hbar^2} \\right ) =\n\\frac{p_x^2}{2m}=(e_1 + e_2) = E.$$\n\nIn essence, the $\\pm$ sign in the exponential exponent can be absorbed by the amplitude of the *kemporal frequency term* $\\eta$, since $|-\\eta|^2\\equiv |\\eta|^2$. Subject to\\\n$|\\underbrace{-\\eta}_{\\eta'}|^2\\equiv |\\eta|^2=|\\xi|^2$,\n\n$$u(x,k)=e^{-2\\pi i\\langle \\eta, k \\rangle} \\cdot e^{2\\pi i\\langle x, \\xi \\rangle}\n= e^{2\\pi i\\langle \\eta', k \\rangle} \\cdot e^{2\\pi i\\langle x, \\xi \\rangle}\\ .$$\n\nIn 1D time, $\\frac{p_x^2}{2m}=e_1 = E$.\n\nTo explicate the kime-phase (or kime) Hermitian (self-adjoint) operator, $\\widehat{\\mathcal{P}}$, we consider the particle *wavefunction* as a spatiotemporal *wave-distribution*, $\\Psi(x,\\ \\kappa)$, not really a function, since the kime-phase, $\\varphi$, is random. Assume there is a kime-phase operator\n\n$$\\underbrace{\\ \\ \\ \\widehat{\\mathcal{P}}\\ \\ \\ }_{Hermitian\\ operator}\n\\overbrace{\\ \\ \\ \\ \\Psi\\ \\ \\ \\ \\ }^{state} = \\overbrace{\\ \\ \\ \\varphi\\ \\ \\ }^{eigenvalue}\n\\underbrace{\\ \\ \\ \\Psi\\ \\ \\ }_{state}\\ .$$\n\nHence, the action of the linear kime-phase Hermitian operator $\\widehat{\\mathcal{P}}$ is to *draw a random phase value* from the circular phase distribution $\\varphi \\sim \\Phi_{\\lbrack - \\pi,\\ \\pi)}$. Such instantiation of the kime-phase, $\\varphi = \\varphi_{o}$, localizes the spatiotemporal position of the observation in 4D Minkowski spacetime.",
      "word_count": 3283
    },
    {
      "title": "Phase Distribution Random Sampling",
      "content": "Let's examine the *process of random sampling from the phase distribution*, $\\Phi_{\\lbrack - \\pi,\\ \\pi)}$. This can be done many different ways.\n\n-   One strategy involves inverting the CDF, i.e., compute the *phase quantile function*, and evaluating it over random uniform observations\n\n$$\\underbrace{{CDF(\\Phi)}_{\\lbrack - \\pi,\\ \\pi)}^{- 1}}_{F^{- 1}}(U) \\sim \n\\Phi_{\\lbrack - \\pi,\\ \\pi)},\\ \\ \\forall\\ U \\sim Uniform(0,1)\\ .$$\n\n-   In general, if the CDF has a closed form analytic expression and is invertible, then we can generate a random sample from that distribution by evaluating the inverse CDF at $U$, where $U \\sim Uniform(0,1)$.\n\nThe rationale behind that is that a continuous CDF, $F$, is a one-to-one mapping of the domain of the CDF (range of $\\varphi$) into the interval $\\lbrack 0,1\\rbrack$. If $U \\sim Uniform(0,1)$, then $\\varphi = F_{\\lbrack - \\pi,\\ \\pi)}^{- 1}(U)$ would have the phase distribution $\\Phi_{\\lbrack - \\pi,\\ \\pi)}$, since $F$ is monotonic, and $Prob_{U}(U \\leq u) \\equiv u$, and\n\n$$Prob_{\\Phi}\\left( F^{- 1}(U) \\leq \\varphi \\right) = Prob_{U}\\left( U \\leq F(\\varphi) \\right) \\equiv F(\\varphi).$$\n\n-   Another approach for *random sampling from the phase distribution*, $\\Phi_{\\lbrack - \\pi,\\ \\pi)}$, is to utilize the Laplace transform, which can be defined as an expected value. The Laplace transform is an integral transformation of functions over time (inputs) to outputs in the frequency-domain, outputs are functions of complex angular frequency, in radians per unit time.\n\nIf $X$ is a random variable with probability density function $f_{X}$, then the Laplace transform of $f_{X}$ is the expectation of $e^{- sX}$, i.e.,\n\n$$\\underbrace{\\mathcal{\\ \\ \\ L}\\left( f_{X} \\right)\\ \\ \\ }_{\n\\begin{matrix}\nLaplace\\ Transform\\  \\\\\nof\\ r.v.\\ \\ X\\mathcal{,\\ \\ \\ L}(X) \\\\\n\\end{matrix}}(s)\\mathbb{= E}\\left( e^{- sX} \\right) = \\int_{}^{}{e^{- sx}f_{X}(x)dx}\\ .$$\n\nSetting $s = - t$ yields the *moment generating function* (MGF) of $X$, $$\\mathcal{M}_{X}(t)\\mathcal{\\equiv M}(X)(t)\\mathbb{= E}\\left( e^{tX} \\right) = \\int_{}^{}{e^{tx}f_{X}(x)dx}\\ .$$\n\nFor each continuous random variable $X$, we can employ the Laplace transform to compute the cumulative distribution function, $F_{X}$, and by extension, the inverse CDF (the quantile function), $F_{X}^{- 1}$, as follows. $$p = F_{X}(x) \\equiv \\Pr(X \\leq x) = \\mathcal{L}^{- 1}\\left( \\frac{1}{s}\\mathbb{E}\\left( e^{- sX} \\right)\\  \\right)(x) = \\mathcal{L}^{- 1}\\left( \\frac{1}{s}\\mathcal{L}\\left( f_{X} \\right)\\  \\right)(x)\\mathbb{\\ :R \\rightarrow}\\lbrack 0,1\\rbrack\\ $$\n\nHowever, the random variable $X$, and correspondingly its density function $f(x)=f_{X}(x)$ map $H\\to\\mathbb{C}$. Hence, the ''*inverse CDF function*'' $F_{X}^{-1}$ and the \\`\\`*inverse linear operator*'' $\\mathcal{L}^{-1}$ have different meanings that are not interchangeable. Thus, the second relation is not equality! $$x = F_{X}^{- 1}(p)\\underbrace{\\not=}_{not\\ =}\\frac{1}{p}\\mathbb{E}\\left( e^{- pX} \\right) = \\frac{1}{p}\\mathcal{L}\\left( f_{X} \\right)(p):\\lbrack 0,1\\rbrack\\mathbb{\\rightarrow R\\ ,}$$\n\n$$U \\sim Uniform(0,1)\\  \\Longrightarrow X = F_{X}^{- 1}(U)\n\\underbrace{\\not=}_{not\\ =}\\frac{1}{U}\\mathcal{L}\\left( f_{X} \\right)(U) \\sim \\ f_{X}.$$\n\nThe [Laplace transform for many probability distributions have closed-form analytical expressions](https://doi.org/10.4236/am.2020.112007). The table below incudes the LTs of some commonly used distributions.\n\n- (Left) Original Function \n$$\\mathbf{f}_{\\mathbf{X}}\\mathbf{\\equiv}\\mathcal{L}^{\\mathbf{- 1}}\\left( \\mathcal{L}\\left( \\mathbf{f}_{\\mathbf{X}} \\right) \\right)$$\n\n- (Right) Laplace Transform\n$$\\mathcal{L}\\left( \\mathbf{f}_{\\mathbf{X}} \\right)$$\n\n- Exponential distribution\n$$f_{X} = \\lambda e^{- \\lambda x}\\ ; \\ \\ \\mathcal{L}\\left( f_{X} \\right) = \\frac{\\lambda}{\\lambda + z\\ },\\ \\forall z \\neq -\\lambda$$\n\n- Weibull distribution\n$$f_{X} = \\alpha\\lambda x^{\\alpha - 1}e^{- \\lambda\\alpha x}\\ ; \\ \\ \\mathcal{L}\\left( f_{X} \\right) = \\frac{\\alpha\\lambda\\beta\\Gamma(\\alpha)}{(\\lambda\\alpha + z)^{\\alpha}\\ },\\ \\forall z \\neq - \\alpha\\lambda ,$$\n\nwhere $\\Gamma(\\alpha) = \\int_{0}^{\\infty}{x^{\\alpha - 1}e^{- x}}dx.$\n\n- Normal distribution\n$$f_{X} = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{- \\frac{(x - \\mu)^{2}}{2\\sigma^{2}}}\\ ; \\ \\ \n\\mathcal{L}( f\\_{X} ) = e^{- \\frac{\\left( \\mu + \\sigma^{2}z \\right)^{2}-\\mu^2}{2\\sigma^{2}}}\n\\left( 1 - \\Phi\\left( - \\frac{\\mu + \\sigma^{2}z}{\\sigma} \\right) \\right)\\ ; \\ \\ \n\\forall z\\mathbb{\\in C}$$\n\n- Gamma distribution\n$$f_{X} = \\frac{\\lambda\\beta x^{\\beta - 1}e^{- \\lambda x}}{\\Gamma(\\beta)}\\ ; \\ \\ \n\\mathcal{L}\\left( f_{X} \\right) = \\frac{\\lambda\\beta}{(\\lambda + z)^{\\beta}\\ },\\ \\forall z \\neq - \\lambda$$\n\n- Generalized Gamma distribution\n$$f_{X} = \\frac{\\lambda\\alpha\\beta\\cdot x^{\\alpha\\beta - 1}\\cdot e^{- \\lambda\\alpha x}}{\\Gamma(\\beta)}\\ ; \\ \\ \n\\mathcal{L}\\left( f_{X} \\right) = \\frac{\\lambda\\alpha\\beta\\Gamma(\\alpha\\beta)}{{\\Gamma(\\beta)\n(\\lambda\\alpha + z)}^{\\beta}\\ },\\ \\forall z \\neq - \\alpha\\lambda$$\n\n- Pareto distribution\n$$f_{X} = \\frac{\\theta\\lambda^{\\theta}}{x^{\\theta + 1}},\\ \\ 0 \\leq \\lambda,\\theta;\\ \\ \\ \\lambda \\leq x\\ ; \\ \\ \n\\mathcal{L}\\left( f_{X} \\right) = \\theta\\lambda^{\\theta}z^{\\theta}\\left( \\Gamma( - \\theta) - I(z\\lambda,\\  - \\theta)\\Gamma( - \\theta) \\right),$$\n$$I(t,\\ \\alpha) \\equiv \\frac{1}{\\Gamma(\\alpha)}\\int_{0}^{t}{x^{\\alpha - 1}e^{- x}}dx\\ .$$",
      "word_count": 664
    },
    {
      "title": "The ''Phase Problem''",
      "content": "The *phase problem* is ubiquitous in experimental science and refers to loss of information due to lack of wave phase details when making many physical measurements. A perfect motivational example is the complex wave representation clarifying this illusion arising by projecting the 3D ''corkscrew'' wave shape into 2D and only showing the 1D values in the [space of complex amplitudes](https://www.socr.umich.edu/TCIU/HTMLs/Chapter3_Introduction.html), see the following interactive 3D scenes:\n\n-   [Spacekime Chapter 3](https://socr.umich.edu/TCIU/HTMLs/Chapter3_Introduction.html)\n\n-   [Spacekime Chapter 6](https://socr.umich.edu/TCIU/HTMLs/Chapter6_Kime_Phases_Circular.html).\n\nThe following three examples of the ''phase problem'' are analogous to the ''kime-phase problem'' and demonstrate the potential inference benefits of recovering kime-phases to enhance subsequent modeling, interpretation, and forecasting of intrinsically stochastic phenomena.",
      "word_count": 108
    },
    {
      "title": "Recovering 3D crystal structure",
      "content": "The first example reflects *recovering the 3D crystal structure* from magnitude-only diffraction patterns in *crystallographic* studies [phase-problem](https://en.wikipedia.org/wiki/Phase_problem), and [this reference](https://doi.org/10.1107%2FS0907444903017815). For instance, X-ray crystallography diffraction data only captures the amplitude of the 3D Fourier transform of the molecule *s* electron density in the unit cell. The lack of phase information obfuscates the complete recovery of the electron density in spacetime using Fourier synthesis, i.e., via the inverse Fourier transform of the data from the native acquisition frequency space.\n\nThe following image from the [Spanish National Research Council (CSIC)](https://www.csic.es/en) the depicts the challenge of phase-recovery in [X-ray diffraction studies](https://www.xtal.iqfr.csic.es/Cristalografia/parte_07-en.html).\n\n![X-ray diffraction](https://wiki.socr.umich.edu/images/4/4f/X-ray_diffraction_CSIC.jpg)\n\nThe 3D atomic structure of a crystal is imaged as diffraction effects on a 2D reciprocal lattice. Only the diffraction magnitudes, the dark intensities at the reciprocal (Fourier) lattice pixels, amplitude values (intensities) of the fundamental vector quantities are recorded. Their relative orientations (relative phases) are missing. This lack of phase information inhibits the exact recovery of the value of the electron density function at each point and the explication of the atomic positions in the [crystal structure in spacetime](https://www.xtal.iqfr.csic.es/Cristalografia/parte_07-en.html).\n\n## Quantum Theory over Real and Complex Base-Fields\n\nBy some accounts, *physics* aims to *explain observed experiments* through mechanistic theories, whereas *mathematics* aims to *describe the fundamental principles* of all possible solutions under strict conditions or *a priori* assumptions. The difference in focus between these two scientific domains sometimes leads to friction. Mathematical possibilities may include system configurations, observable states, or exotic designs that may be possible, likely, unlikely, or extremely rare (e.g., of trivial measure, or zero probability) that are still absolutely necessary to complete a system. From a physical perspective, such ''almost surely'' unlikely to be observed systems or states are considered unreal, unobservable, and not-constructive, i.e., not worth investigating. This physics-mathematics dichotomy may also be phrased in terms of the [Kurt Gödel's incompleteness theorem (1931)](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems), which proves that any system equipped with natural number arithmetic cannot be at the same time *complete* and *self-consistent* with respect to its core axioms.\n\nThe second example illustrating a need to generalize *real* to *complex* representations explores the differences between quantum physics predictions based on formulating quantum theories using Hilbert-spaces defined over the fields of the reals ($\\mathbb{R}$) and the complex numbers ($\\mathbb{C}$), see [this article](https://doi.org/10.1038/s41586-021-04160-4). In a nutshell, real and complex Hilbert-space quantum theoretic predictions yield different results (in network scenarios comprising independent states and measurements).\n\n![Differences between quantum physics predictions using *real* and *complex* representations](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-021-04160-4/MediaObjects/41586_2021_4160_Fig2_HTML.png)\n\nThis suggests the existence of realizations disproving real quantum theory similarly to how the standard [Bell experiments disproved local physics](https://en.wikipedia.org/wiki/Bell_test). The relevance to complex-time representation of this recent (2022) discovery is reflected in the dichotomy between (1) classical quantum mechanics formulation of *self-adjoint (Hermitian) operators* and their *real eigenvalues* (observable states), and (2) the less obvious but potentially more powerful abstraction of the more *general bounded linear operators* and their *complex eigenvalues*. This leads to the quest to formulate a *kime-operator* whose eigenspectrum contains the continuous kime values, $\\kappa = te^{i\\theta} \\in \\mathbb{C}$, as observables over the *smallest complete field* that naturally extends time, $\\mathbb{R}^{+} \\subset \\mathbb{C}$.\n\n(Top) *complex* quantum theory, two independent sources distribute the two-qubit states and generate a 4-vector output Bell measurement. (Bottom) in *real* physics, the observed correlations cannot be reproduced, or even well approximated, since all the states and measurements in the network are constrained to be real operators.\n\n## Fourier Transform Amplitudes and Phases\n\nThe third example illustrates the importance of the phase in 2D Fourier transformation. We will demonstrate two scenarios:\n\n-   the use of the only of the amplitudes (magnitudes) to synthesize the images back in space time without the phases (nil-phase), and\n-   the use of incorrect phases (swapping the phases of the two images).\n\n![The first table shows the original images (Earth and Saturn) and their complex Fourier frequency components.](https://wiki.socr.umich.edu/images/f/f7/FT_IFT_Earth_Saturn_PhaseEffects.png)\n\n![The second table shows both spacetime reconstructions of the images with the two different phase estimation strategies presented above (nil phase and swapped phases). The results appear somewhat reasonable but are certainly far from being perfect image reconstructions.](https://wiki.socr.umich.edu/images/f/fd/FT_IFT_Earth_Saturn_PhaseEffects_T2.png)\n\nIn addition to the physical science motivations above, complex time offers completeness with respect to both *additive* and *multiplicative* operations over kime, which is similar to the spatial dimensions. The complex (algebraic) *field* $\\mathbb{C \\ni}\\kappa$, equipped with the $( + ,*)$ operations, is the smallest algebraic field that naturally extends the multiplicative algebraic *group*, $\\left( \\mathbb{R}^{+} \\ni t, *\\right)$, which is not closed with respect to addition.\n\nIn longitudinal data science and statistical inference, the enigmatic ''*phase of complex time*'' (kime-phase) represents an analogous challenge as the ''*physical-chemistry phase problem*''. Complex-time representations offer significant opportunities to advance inference science including generation of large and realistic random IID samples. It also supports Bayesian formulation of Spacekime analytics, tensor modeling of ℂ kimesurfaces, and the development of completely novel analytical methods directly on the kimesurfaces.\n\n## Does the overall phase of a quantum state have a *physical meaning*?\n\nQuantum system states are represented by *ket* vectors in the Hilbert space. For instance, *spin states* correspond to a 2D Hilbert space, see [Chapter 5 of `Quantum Mechanics - A Concise Introduction`](https://link.springer.com/book/10.1007/978-981-19-7626-1), corresponding with the $2\\times 2$ [Pauli matrices](https://en.wikipedia.org/wiki/Pauli_matrices).\n\nIn the general case, the dimension of the Hilbert space may be finite, $n$ or infinite and always has orthonormal basis vectors $\\{|\\phi_{\\alpha}\\rangle\\}$ where any quantum state can then be expressed as a linear *superposition* of these basis vectors $$|\\psi\\rangle=\\sum_{\\alpha}{ c_{\\alpha}|\\phi_{\\alpha}\\rangle }, $$ where the expansion coefficient $c_{\\alpha}$ expresses the *projection* of the state $|\\psi\\rangle$ on the basis vector $|\\phi_{\\alpha}\\rangle$ is given by the *inner product* of $|\\psi\\rangle$ and $|\\phi_{\\alpha}\\rangle$, i.e., $c_{\\alpha}=\\langle \\phi_{\\alpha} | \\psi\\rangle,\\ \\forall\\alpha$. To ensure proper probabilistic interpretation, the *normalization condition* is imposed $$\\sum_{\\alpha}{ |c_{\\alpha}|^2 = 1}, $$\n\n*Quantum system observables* are represented by operators expressed mathematically as second order tensors (matrices). For instance, the Pauli matrices represent *spin observables* along different axes where the Hilbert space is 2-dimensional as we have left-right, ($\\pm$), up-down, back-forth directions for the spin along the given unitary direction (could be a coordinate axis, $x,y,z$, or any vector direction $$\\vec{n}=(\\sin\\theta\\cdot \\cos\\phi,\\ \\sin\\theta\\cdot\\sin\\phi,\\ \\cos\\theta)$$ in spherical coordinates.\n\nFor each observable $\\hat{O}$, we have the following relation between the (linear) matrix algebra supporting quantum computations and the corresponding physical interpretations.\n\n$$\\underbrace{\\hat{O}}_{observable-operator} \\overbrace{|\\phi_{\\alpha}\\rangle}^{eigenstate} = \\underbrace{\\nu_{\\alpha}}_{eigenvalue-experimental-outcome} \\ \\overbrace{| \\phi_{\\alpha}\\rangle}^{eigenstate}\\ .$$\n\nThe *expectation value of the operator* in any given quantum state $|\\psi\\rangle$ in the Hilbert space represents the overall mean $$\\langle\\hat{O}\\rangle_{\\psi} \\equiv \\langle\\hat{O}\\rangle\n=\\langle\\psi| \\hat{O}|\\psi\\rangle =\n\\overbrace{\\sum_{\\alpha}{ \\underbrace{\\nu_{\\alpha}\\ }_{obs.value}\\ \n\\underbrace{\\ |\\langle \\psi |\\phi_{\\alpha}\\rangle|^2}_{(transition)probability}}}\n^{weighted-average-of-outcomes }\\ ,$$\n\nwhere $\\{|\\phi_{\\alpha}\\rangle\\}_{\\alpha}$ is a complete set of eigenvectors for the observable operator $\\hat{O}$, i.e., $\\hat{O}|\\phi_{\\alpha}\\rangle = \\nu_{\\alpha}| \\phi_{\\alpha}\\rangle$.\n\nThe *probabilistic interpretation* of any quantum state $|\\psi\\rangle$ is as a linear superposition of the all the eigenstates of the observable (operator) $$|\\psi\\rangle = \n\\sum_{\\alpha}{ c_{\\alpha} |\\phi_{\\alpha}\\rangle}\\ ,$$\n\nwhere $|c_{\\alpha}|^2=|\\langle \\phi_{\\alpha} | \\psi\\rangle|^2,\\ \\forall\\alpha$ is the probability of finding the system in an eigenstate $|\\phi_{\\alpha}\\rangle$ and measuring $\\hat{O}$ in this quantum state would yield an outcome $\\nu_{\\alpha}$ with probability $|c_{\\alpha}|^2=|\\langle \\phi_{\\alpha} | \\psi\\rangle|^2$.\n\nThere are *two specific reasons* for the most common quantum mechanics interpretation suggesting that the *overall phase of a quantum state* has no physical meaning. More specifically, this interpretation implies that *all these vectors in the Hilbert space over the field of the complex numbers represent one (and unuique) physical quantum state* $$\\left\\{ \\underbrace{|\\psi\\rangle}_{base-state/trivial-phase}, \n\\underbrace{e^{i\\theta}|\\psi\\rangle}_{non-trivial-phase} ,\\ \\forall \\theta\\in\\mathbb{R}  \\right\\}\\ .$$\n\n-   The *expectation values* of $|\\psi\\rangle$ and $e^{i\\theta}|\\psi\\rangle, \\forall\\theta$ are the same:\n\n$$\\langle\\hat{O}\\rangle_{e^{i\\theta}\\psi}\\equiv \\left\\langle e^{i\\theta}\\psi |\\hat{O} | e^{i\\theta}\\psi \\right\\rangle=\n\\left\\langle \\psi | e^{-i\\theta}\\hat{O} e^{i\\theta} | \\psi \\right\\rangle=$$ $$\\left\\langle \\psi | \\underbrace{e^{-i\\theta}e^{i\\theta}\\hat{O}}_{scalars-commute} | \\psi \\right\\rangle=\\left\\langle \\psi | \\hat{O} | \\psi \\right\\rangle\\equiv\n\\langle\\hat{O}\\rangle_{\\psi}.$$\n\n-   The *probability* of finding the system in the *eigenstate* $|\\phi_j\\rangle$ is also the same for both cases:\n\n$$|\\langle\\phi_{\\alpha}| e^{i\\theta}\\psi\\rangle |^2 =\n|\\langle\\phi_{\\alpha}| e^{i\\theta} |\\psi\\rangle |^2=$$ $${\\underbrace{|e^{i\\theta}|}_{1}}^2 |\\langle\\phi_{\\alpha}|\\psi\\rangle |^2=\n|\\langle\\phi_{\\alpha}|\\psi\\rangle |^2 \\equiv |c_{\\alpha}|^2 .$$\n\nThese properties may suggest that there $\\forall\\theta\\in\\mathbb{R}$ are no physical differences between the states $|\\psi\\rangle$ and $e^{i\\theta}|\\psi\\rangle$; hence, state phases can be ignored.\n\nIndeed, these finite dimensional Hilbert space derivations using sums naturally extend to integrals in the infinite dimensional Hilbert spaces, e.g., $H\\equiv L^2$, square integrable functions. For instance, the general expected value $$\\langle\\hat{O}\\rangle_{\\psi}\\equiv \\langle\\psi(\\omega)|\\hat{O}(\\omega)|\\psi(\\omega)\\rangle =\n\\int_{\\Omega} {\\psi^{\\dagger}\\hat{O}\\psi\\ d\\omega}$$ can be expressed for the *position* $\\hat{x}$ and *momentum* $\\hat{p}$ operators as\n\n$$\\langle\\hat{x}\\rangle\\equiv \\langle\\psi|\\hat{x}|\\psi\\rangle =\n\\int_{\\mathbb{R}} {\\psi^{\\dagger}(x,t)\\psi(x,t) x dx},$$\n\n$$\\langle\\hat{p}\\rangle\\equiv \\langle\\psi|\\hat{p}|\\psi\\rangle =\n\\int_{\\mathbb{R}} {\n\\left (\\underbrace{i\\hbar \\frac{\\partial}{\\partial x}}_{\\hat{p}^{\\dagger}} \n\\psi^{\\dagger}(x,t)\\right ) \\psi(x,t) dx} \\equiv \n\\int_{\\mathbb{R}} { \\psi^{\\dagger}(x,t)\n\\left (-i\\hbar \\frac{\\partial}{\\partial x} \\psi(x,t)\\right ) dx}\\ .$$\n\nHow about the *variance* of the linear operator? Would the variances of $|\\psi\\rangle$ and $e^{i\\theta}|\\psi\\rangle, \\forall\\theta$ be the same?\n\n$$\\sigma_{(\\hat{O},e^{i\\theta}\\psi)}^2\\equiv \n\\left\\langle e^{i\\theta}\\psi |\\hat{O}^2 | e^{i\\theta}\\psi \\right\\rangle -\n\\left (\\left\\langle e^{i\\theta}\\psi |\\hat{O} | e^{i\\theta}\\psi \\right\\rangle\\right )^2=\n\\left\\langle \\psi |e^{-i\\theta}\\hat{O}^2 e^{i\\theta}| \\psi \\right\\rangle -\n\\left (\\left\\langle \\psi |\\hat{O} | \\psi \\right\\rangle\\right )^2=$$ $$\\left\\langle \\psi |e^{-i\\theta+i\\theta}\\hat{O}^2| \\psi \\right\\rangle -\n\\left (\\left\\langle \\psi |\\hat{O} | \\psi \\right\\rangle\\right )^2=\n\\left\\langle \\psi |\\hat{O}^2| \\psi \\right\\rangle -\n\\left (\\left\\langle \\psi |\\hat{O} | \\psi \\right\\rangle\\right )^2\\equiv\n\\sigma_{(\\hat{O},\\psi)}^2\\ .$$\n\n## In most physical systems, *energy dynamics* are more important than *absolute energy levels*\n\nIn a nutshell, the *global phase* may appear to be *physically irrelevant* because of the linearity of the [Schrodinger equation](https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation) - two states $|\\psi\\rangle$ and $e^{i\\theta}|\\psi\\rangle$ are both solutions to the same equation. Another reason why the phases may frequently be ignored in some experiments is due to *energy preservation laws*. *Energy expectation values* do not depend on the phase, however other measurements and observables may certainly depend heavily on the phase.\n\nThe phase plays roles in quantum computing and all of quantum physics, however, the *phase is routinely ignored* despite the fact that the *phase often shows up in the underlying mechanistic models*, mathematical representations, and quantum physics equations.\n\nConsider the example of a *free-falling object*, due to gravity, a dropped ball will accelerate towards the gound. The *velocity of the ball* (i.e., its *kinetic energy*) at the the point it hits the ground is by the change, or difference, in the ball's *potential energy*.\n\nSuppose the *initial velocity* of a free-falling ball is zero, $0=v_o=v(t)_{t=0}$ and the *height* of the initial drop is the position $x_o=x(t)_{t=0}$. Since the *acceleration* of the earth's gravity is constant, $g$, which is the rate of change of the velocity with respect to time ($t$), the velocity changes uniformly over time.\n\nThus, $v(t) = −g\\cdot t$, the negative sign reflects the downward direction of velocity. Since the velocity is constant, the *average velocity* over a period of time $[0,t]$ is $\\bar{v} = \\frac{v_o − gt}{2} = -\\frac{gt}{2}$. The distance traveled by the ball as it falls is $\\Delta x = |\\bar{v}|\\cdot t = \\frac{g\\cdot t^2}{2}$ and as the initial height of the drop location is $x_o$, the actual position (height) and momentum of the ball at any given time $t$ are $$x(t) = x_o - \\Delta x\\equiv x_o -\\frac{g\\cdot t^2}{2}\\ ,\\ \\ v(t)=-g\\cdot t\\ .$$\n\nHence, in classical mechanics we can completely determine the state of the ball (it's position and velocity at a given time). This situation is drastically different in quantum mechanics, where measuring precisely the exact position and momentum of a particle at the same time is impossible.\n\nGiven the mass $m=Volume\\cdot Density$ of the ball, we can calculate the *kinetic energy* of the at time $t$ $$K = \\frac{mv^2}{2} = \\frac{mg^2t^2}{2} = mg(x_o − x)\\ .$$ Rearranging the terms we obtain the lat of conservation of energy $$\\underbrace{E}_{total\\\\ energy}=\\underbrace{K}_{kinetic\\\\ energy} + \\underbrace{mgx}_{potential\\\\ energy\\ V(x)} \n= \\underbrace{mgx_o}_{constant} \\ .$$\n\nThe sum of the kinetic and potential energies is a constant of motion. Note the rebalancing between the kinetic and potential energies. As the ball falls, its *kinetic energy increases* exactly as much as its *potential energy decreases*. This result is valid for arbitrary systems without friction, where the total energy is composed of kinetic energy and potential energy. During the motion, the kinetic energy and potential energy are transformed into each other, preserging the total energy over time. At the start, the total energy is purely potential, $E = mgx_o,\\ K(t_0)=0$. In classical mechanics, the energy can vary continuously, whereas in quantum mechanics, energy can be discrete.\n\nIn terms of *computing the dynamics of a physical system*, only energy differences, not absolute energy values, are important. For modeling the motion of electrons, only the differences in voltages, as opposed to absolute values, tend to be important. Similarly, in quantum physics, the quantity that determines the dynamics is the [Hamiltonian of the system](https://en.wikipedia.org/wiki/Hamiltonian_(quantum_mechanics)), which is measures in units of energy. The Hamiltonian is self-adjoint (Hermitian) operator with eigenvalues representing the observable energy levels during the time evolution of the system.\n\nIn quantum computing using binary quantum bits (qubits) $\\{|0\\rangle,|1\\rangle \\}$, the global phase of a quantum state is not detectable, i.e., $|\\psi\\rangle$ and $e^{i\\theta}|\\psi\\rangle$ are the same, a single-qubit state can be expressed as $$|\\psi\\rangle = \\sqrt{1-p}|0\\rangle + e^{i\\phi}\\sqrt{p}|1\\rangle, $$\n\nwhere $0\\leq p\\leq 1$ is the probability of the qubit being in the state $|1\\rangle$ and the quantum phase $\\phi\\in [0, 2\\pi)$.\n\nHermitian operators like the Hamiltonian are used for modeling *quantum computing gates*, which can be expressed as Hermitian operators. For instance, the operator\n\n$$H=e^{-\\frac{iHt}{\\hbar}}$$ acting on states $|\\psi\\rangle$ as follows $$H|\\psi\\rangle =e^{-\\frac{iHt}{\\hbar}}|\\psi\\rangle =|\\psi'\\rangle .$$\n\nAs the absolute values of energy depend only on the measuring unit and are not important, adding a constant amount of energy to the system Hamiltonian does not alter the states.\n\nLet $I$ be the identity operator. Then, for any $\\lambda\\in\\mathbb{R}$, $H\\to H+\\lambda I$ has the effect of shifting all of the eigenvalues by a fixed amount $\\lambda$, which leaves the differences between values unchanged! Hence, the system dynamics are unaffected this shift. Let's examine the effect of this shift from the old ($H$) to the new $H'=H+\\lambda I$ Hamiltonian on the quantum gate by contrasting the effects of the corresponding Hamiltonial operators $U$ and $U'$ on a state vector $|\\psi\\rangle$: $$U|\\psi\\rangle =e^{-\\frac{iHt}{\\hbar}}|\\psi\\rangle =|\\psi'\\rangle .$$ $$U'|\\psi\\rangle =e^{-\\frac{i(H+\\lambda I)t}{\\hbar}}|\\psi\\rangle =\ne^{i\\frac{-\\lambda It}{\\hbar}}\n\\underbrace{e^{-\\frac{iHt}{\\hbar}}|\\psi\\rangle}_{U|\\psi\\rangle}=\ne^{i\\frac{-\\lambda It}{\\hbar}} \\left ( U |\\psi\\rangle\\right )\n\\underbrace{\\ \\ \\ =\\ \\ \\ }_{eigenvalue}\ne^{\\gamma} |\\psi'\\rangle \\ ,$$\n\nwhere the action of the energy-shifted Hamiltonian is the same as the original Hamiltonial multiplied by a *global phase factor* $\\gamma=-\\frac{i \\lambda t}{\\hbar}$. These global phases can be ignored as they correspond to uniform shifts in energy that do not change the system dynamics.\n\nNote that the [Baker--Campbell--Hausdorff formula](https://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula) justifies this equality $$e^{-i\\frac{(H+\\lambda I)t}{\\hbar}} =\ne^{-i\\frac{\\lambda I t}{\\hbar}} \\cdot e^{-i\\frac{H t}{\\hbar}} .$$\n\nFor any pair of elements in a Lie algebra, $X$ and $Y$, possibly non-commutative and including operators, the solution $Z$ of the equation $e^{X}e^{Y}=e^{Z}$ can be expressed as $Z=\\log \\left(e^{X}e^{Y}\\right)$, a series in terms of repeated commutators of $X=H$ and $Y=I$, which are all trivial since $[H,I] = HI - IH \\equiv 0$, except for the first two terms, $X+Y$.",
      "word_count": 2441
    },
    {
      "title": "Realistic spacetime simulation and inference using spacekime representation",
      "content": "Let's explore some strategies to *amplify the utility of classical spacetime processes* via complex time transformations. Both of these basic computable tasks -- *random simulation* and *inference* (including forward *prediction, regression, classification*) -- can be addressed through kime representation. The benefits of Spacekime analytics are realized by retaining both the commonly observed kime-magnitude (time) and the enigmatic kime-direction (phase).\n\n## Large-scale simulation\n\nLarge-scale *simulation* of realistic spacetime observations based on a few measurements (smaller samples of repeated measures for a fixed time, $t_{o}$)\n\n$$\\left\\{ \\underbrace{\\ \\ \\ y_{l}\\ \\ \\ }_{observed \\\\\neigen\\ states} = \n\\underbrace{\\ \\ f\\left( t_{o}e^{i\\theta_{l}} \\right)\\ \\ }_{implicit \\\\\nfunction} \\right\\}_{l = 1}^{n}\\ .$$\n\nNote that these (spatiotemporal) observations may also depend on the location, $x \\in \\mathbb{R}^{3}$, however, under a controlled repeated experiment assumption, we are suppressing this spatial dependence. Let's assume that the implicit Laplace transform of the spacetime function (wavefunction, $f$), $F(z)\\mathcal{= \\ L}(f)(z)$, is evaluated (*instantiated*) at $z = t_{o}e^{i\\theta}\\mathbb{\\in C}$, for some fixed spatiotemporal location ($t_{o} \\in \\mathbb{R}^{+},\\ x_{o} \\in \\mathbb{R}^{3})$. Consider the kime-surface isocontour $G(\\theta)\\mathcal{= \\ L}(f)\\left( t_{o}e^{i\\theta} \\right)$, parameterized by $\\theta \\in \\lbrack - \\pi,\\pi\\ )$. Define the *phase density* $f_{\\theta} = \\frac{G(\\theta)}{|\\left| G(\\theta) \\right||}$, where the density normalization factor is\n\n$$\\left| \\left| G(\\theta) \\right| \\right| = \\int_{- \\pi}^{\\pi}{G(\\theta)}d\\theta.$$\n\nThen, $\\int_{- \\pi}^{\\pi}{f_{\\theta}(\\theta)}d\\theta = 1$ and the corresponding *cumulative phase distribution function* is $$F_{\\theta}\\left( \\theta_{o} \\right) = \\Pr\\left( \\theta \\leq \\theta_{o} \\right) = \\int_{- \\pi}^{\\theta_{o}}{f_{\\theta}(\\theta)}d\\theta\\ .$$\n\nFirst, we will draw a large sample from the kime-phase distribution by taking a large uniform sample, $\\left\\{ u_{k} \\right\\}_{k = 1}^{N} \\sim Uniform(0,1)$ and evaluating the quantile function, $F_{\\theta}^{- 1}$, at the uniform points, $\\theta_{k} = F_{\\theta}^{- 1}\\left( u_{k} \\right)$. Having this large sample from the phase distribution allows us to obtain a corresponding realistic *spacetime sample* via the inverse Laplace transform\n\n$$\\left\\{ \\underbrace{\\ \\ \\ {\\widehat{y}}_{k}\\ \\ \\ }_{\\begin{matrix}\nsimulated \\\\\neigen\\ states \\\\\n\\end{matrix}} = \n\\underbrace{\\ \\ \\mathcal{L}^{- 1}(F)\\left( t_{o}e^{i\\theta_{k}} \\right)\\ \\ }_{\\begin{matrix}\nimplicit \\\\\nfunction \\\\\n\\end{matrix}} \\right\\}_{k = 1}^{N}\\ ,\\ \\ \\ typically\\ \\ n \\ll N.$$\n\n{*Question*}: Does this argument assume that we need to interpolate the signal $f(t)$ over the observed small observation sample, $\\left\\{ y_{l} \\right\\}_{i = 1}^{n}$, in spacetime, as we did in the [arXiv LT/ILT paper](https://doi.org/10.48550/arXiv.2304.13204), to be able to approximate $F(z)\\mathcal{= \\ L}(f)(z)$? If so, does the recovered large sample $\\left\\{ {\\widehat{y}}_{k} \\right\\}_{k = 1}^{N}$ depend on the interpolation scheme, or the basis, and how?\n\nAlternatively, we can also sample the signal in spacetime as we discussed earlier, via the MGF and the quantile function. Set $s = - t$ in the Laplace transform to get the *moment generating function* (MGF) of $Y$, the random variable generating observable outcomes via the implicit function ($f$), i.e., $y_{l} = f\\left ( t_{o}e^{i\\theta_l} \\right ), \\theta_l \\sim \\Phi_{\\theta}\\lbrack - \\pi,\\pi), 1 \\leq l \\leq n$. Then, $$\\mathcal{M}_{Y}(t)\\mathcal{\\equiv M}(Y)(t)\\mathbb{= E}\\left( e^{tY} \\right) = \\int_{}^{}{e^{ty}f_{Y}(y)dy}\\ .$$\n\nAssume the (small) repeated sample, $\\left\\{ y_{l} \\right\\}_{i = 1}^{n}$, represents instantiations of a continuous random (observable) variable $Y \\sim f_{Y}$, e.g., particle-position or value of a stock. We can employ the Laplace transform to compute the cumulative distribution function of the process, $F_{Y}$, and by extension, the inverse CDF (the quantile function), $F_{Y}^{- 1}$, as follows\n$$p = F_{Y}(y) \\equiv \\Pr(Y \\leq y) = \\mathcal{L}^{- 1}\\left( \\frac{1}{s}\\mathbb{E}\\left( e^{- sY} \\right)\\  \\right)(y) = \\mathcal{L}^{- 1}\\left( \\frac{1}{s}\\mathcal{L}\\left( f_{Y} \\right)\\  \\right)(y)\\mathbb{\\ :R \\rightarrow}\\lbrack 0,1\\rbrack\\ .$$\n\n$$\\begin{matrix}\nH^{*} & \\overbrace{\\underbrace{\\ \\rightleftarrows \\ }_{\\mathcal{L}^{- 1}}}^{\\mathcal{L}} & H^{*} \\\\\n \\uparrow & & \\downarrow \\\\\nH & \\overbrace{\\underbrace{\\ \\rightleftarrows \\ }_{f_{X}^{- 1}}}^{X,f_{X}} & \\mathbb{C} \\\\\n\\end{matrix}$$\n\nNote that the Laplace transform and its inverse are linear operators acting on the Hilbert space of functions, i.e., the transforms are in the dual space, $\\mathcal{L,}\\mathcal{L}^{- 1} \\in H^{*}$, where $H$ is the space of square integrable functions with sufficiently fast decay and $H^{*}$ is its dual. However, the random variable $X$, and correspondingly its density function $f(x) = f_{X}(x)$ map $H\\mathbb{\\rightarrow C}$. Hence, the ''*inverse CDF function*'' $F_{Y}^{- 1}$ and the ''*inverse linear operator*'' $\\mathcal{L}^{- 1}$ have different meanings that are not interchangeable. Even though using the $()^{- 1}$ notation for both operations is standard, to avoid any potential confusion, it may be more appropriate the denote the Laplace transform and it's inference by hats, $\\widehat{f} = \\mathcal{L}(f)$, $f \\equiv \\check{\\left( \\widehat{f} \\right)} = \\mathcal{L}^{- 1}\\left( \\widehat{f} \\right)$, instead of inverse notation. Because of this difference the equation *above does not imply* that inverting the CDF is equivalent to inverting the Laplace transform, $$\\nRightarrow y = F_{Y}^{- 1}(p) = \\frac{1}{p}\\mathbb{E}\\left( e^{- pY} \\right) = \\frac{1}{p}\\mathcal{L}\\left( f_{Y} \\right)(p):\\lbrack 0,1\\rbrack\\mathbb{\\rightarrow R\\ }(this\\ is\\ meaningless),$$\n\n$$U \\sim Uniform(0,1)\\  \\Longrightarrow Y = F_{Y}^{- 1}(U) = \\frac{1}{U}\\mathcal{L}\\left( f_{Y} \\right)(U) \\sim f_{Y}.$$\n\nHence, ${\\widehat{y}}_{k} = F_{Y}^{- 1}\\left( u_{k} \\right) \\neq \\frac{1}{u_{k}}\\mathcal{L}\\left( f_{Y} \\right)\\left( u_{k} \\right) \\sim f_{Y}$, where $u_{k} \\sim Uniform(0,1)$, $1 \\leq k \\leq N$, and $n \\ll N$.\n\n*Question*: Would this alternative sampling scheme work in practice? Without any interpolation of the implicit signal $f(t)$ over the observed small observation sample, $\\left\\{ y_{l} \\right\\}_{i = 1}^{n}$, in spacetime compute $F(z)\\mathcal{= \\ L}\\left( \\left\\{ y_{l} \\right\\}_{i = 1}^{n} \\right)(z)$. Then, take a large sample of kime-phases $\\left\\{ \\theta_{k} \\right\\}_{k = 1}^{N} \\sim \\ \\Phi_{\\theta}\\lbrack - \\pi,\\pi)$, e.g., $\\Phi_{\\theta}\\lbrack - \\pi,\\pi)$ can be the truncated symmetric Laplace (double exponential) distribution over $\\lbrack - \\pi,\\pi)$. Finally, for a fixed time, $t_{o}$, we can recover a large sample $\\left\\{ {\\widehat{y}}_{k} \\right\\}_{k = 1}^{N}$ of repeated measures in spacetime via $$\\left\\{ \\underbrace{\\ \\ \\ {\\widehat{y}}_{k}\\ \\ \\ }_{repeated \\\\\nsample\\ at\\ \\ t_{o}} \\right\\}_{k = 1}^{N} = \\mathcal{L}^{- 1}\\left( \\left\\{ \\underbrace{t_{o}e^{i\\theta_{k}}}_{z_{k}} \\right\\}_{k = 1}^{N} \\right)\\ .\\ $$\n\n## Spacekime Modeling, Inference, Prediction, Regression, or Clustering\n\nNext, we describe a generic spacekime analytic protocol for modeling and inference of prospective process behavior, e.g., [prediction, regression, or clustering]{.underline}, using limited spacetime observations.\n\nThere is one important difference between the prior (large-scale) *simulation* task discussed above and the current *modeling,* *prediction* and inference protocol. Earlier, the repeated samples represented multiple IID observations of a controlled experiment with a *fixed* spatiotemporal location, ($t_{o} \\in \\mathbb{R}^{+},\\ x_{o} \\in \\mathbb{R}^{3})$. Whereas now, the prediction may be sought across spacetime. In particular, we are interested in forward prediction of the longitudinal process at some fixed spatial location, $x_{o} \\in \\mathbb{R}^{3}$, for future time points, $t t_{o}$, for some past times, $t < t_{o}$, or even in more general situations [\\^1].\n\nNaturally, we start with a small sample of observations measuring the outcome variable repeatedly $n$ times $(1 \\leq l \\leq n)$, independently over a set of timepoints $0 \\leq t_{m} \\leq t_{M}$,\n\n$$\\underbrace{\\ \\ \\ Y\\ \\ \\ }_{observed \\\\ tensor} = \n\\underbrace{\\ \\ f\\left( \n\\overbrace{te^{i\\theta}}^{\\kappa} \\right)\\ \\ }_{implicit \\\\ function} \\equiv \n\\left( y_{m,l} \\right )_{\\underbrace{\\ \\ t_{M}\\ \\ }_{time} \\times \n\\underbrace{\\ \\ n\\ \\ }_{repeat}} \\equiv \\left\\{ \n\\underbrace{\\ \\ \\ y_{m,l}\\ \\ \\ }_{observed \\\\ eigen\\ states} = \n\\underbrace{\\ \\ f\\left( t_{m}e^{i\\theta_{l}} \\right)\\ \\ }_{implicit \\\\\nfunction} \\right\\}_{m = 1,l = 1}^{M,\\ n}\\ .$$\n\nThe observed *data* represents a second order tensor $\\left( y_{m,l} \\right)_{t_{M} \\times n}$ indexed by longitudinal event order (time, $0 \\leq t_{m} \\leq t_{M}$) and the repeated IID sampling index ($1 \\leq l \\leq n \\ll N$). This tensor data can be thought of as a *design matrix* with rows corresponding to time indices ($0 \\leq t_{m} \\leq t_{M}$) and columns tracking the repeated measurements within each time point, $1 \\leq l \\leq n$. Note that the data tensor $\\left( y_{m,l} \\right)_{t_{M} \\times n}$ may potentially include missing elements. All supervised *prediction, classification and regression models* we can apply to observed spacetime data, $\\left\\{ y_{m,l} \\right\\}$, are also applicable to the enhanced (kime-boosted) sample, $\\left( {\\widehat{y}}_{j,k} \\right)_{T \\times N},\\ \\ N \\gg n$. The kime-boosted sample is recovered much like we did in the earlier *simulation* task,\n\n$${\\widehat{Y} = \\left( {\\widehat{y}}_{j,k} \\right)_{T \\times N} \\equiv \\left\\{ \\underbrace{\\ \\ \\ {\\widehat{y}}_{j,k}\\ \\ \\ }_{simulated \\\\ eigen\\ states} = \\underbrace{\\ \\ \\mathcal{L}^{- 1}(F)\\left( t_{j}e^{i\\theta_{k}} \\right)\\ \\ }_{implicit \\\\\nfunction } \\right\\}}_{j = 1,\\ k = 1}^{T,\\ N}\\ ,$$\n\nwhere the *time indices* $1 \\leq j \\leq T,\\ T \\gg M$ and the *phase indices* $1 \\leq k \\leq N,\\ N \\gg n$.\n\nForward inference, time predictions, and classification tasks using the kime-boosted sample can be accomplished in many alternative ways. For instance,\n\n-   We can perform the desired modeling, forecasting, regression, or classification task on the Spacekime recovered large sample, i.e., design matrix $\\left( {\\widehat{y}}_{j,k} \\right)_{T \\times N}$, via inverse Laplace transforming the kime-surfaces as spacetime data objects, that are much richer than their counterparts, $\\left( y_{m,l} \\right)_{t_{M} \\times n}$, originally observed in spacetime.\n\n-   Alternatively, instead of directly modeling the native observed repeated measures time-series, $\\left( y_{m,l} \\right)_{t_{M} \\times n}$, all modeling, inference, prediction, or classification tasks can also be accomplished directly on the kimesurfaces, $F(z)\\mathcal{= \\ L}(f)(z)$, where $z = te^{i\\theta}\\mathbb{\\in C}$ and the interpolation time-series function is $f(t) \\approx \\left( y_{m,l} \\right)_{t_{M} \\times n}$. In this case, any inference, prediction, or derived class-labels need to be pulled back from spacekime into spacetime via the inverse Laplace transform,\n\n$$\\text{Spacetime Inference}    \\rightleftharpoons \\mathcal{L}^{- 1}(\\text{Spacekime Inference})\\ .$$\n\nThis spacetime inference recovery facilitates direct (spacetime) interpretation in the context of the specific modeling or prediction task accomplished in spacekime.",
      "word_count": 1525
    },
    {
      "title": "Kime Measures",
      "content": "Let's define the notion of *time-observables* as well-defined measures on the complex time space ($\\kappa = t e^{i\\theta}\\mathbb{\\in C}$). Naturally, the *time-lapse distance* $d_{TL}$ between a pair of kime moments $\\kappa_{1} = t_{1}e^{i\\theta_{1}},\\kappa_{1} = t_{2}e^{i\\theta_{2}}\\mathbb{\\in C}$ is defined by\n\n$$d_{TL}\\left( \\kappa_{1},\\kappa_{2} \\right) = \\inf_{\\phi_{1},\\phi_{2}}\\left| \\left\\| t_{1}e^{i\\phi_{1}} \\right\\| - \\left\\| t_{2}e^{i\\phi_{2}} \\right\\| \\right| = \\left| \\left| t_{1} \\right| - \\left| t_{2} \\right| \\right|\\ .$$\n\nHowever, there are many other ways to track or measure the longitudinal or temporal size of kime-subsets that extend to kime the common notions of length, area and volume, which we often use to measure the size of spatial subsets.\n\nFor instance, suppose the boundary of a kime area $\\mathbb{C \\supseteq}A \\ni \\kappa_{o}$ can be parameterized as a curve describing the radial displacement $r(\\theta)$ of kime points $\\kappa = re^{i\\theta}$ along the (simple closed curve) boundary from a reference point $\\kappa_{o}$\n\n$$\\partial A = \\left\\{ r = r(\\theta)\\ :\\ \\theta \\in \\lbrack - \\pi,\\ \\pi) \\right\\}\\ .$$\n\nThe *kime-area measure*(Lebesgue measure) of the set $A\\subset\\mathbb{C}$ is\n\n$$\\mu(A) = \\iint_{\\mathbb{C}}{\n\\underbrace{\\ \\ \\chi_{A}(\\kappa)\\ \\ \\ }_{characteristic \\\\ function}d \\mu}.$$\n\nThe importance of defining a proper (Lebesgue?) measure $\\mu$ on kime is that this would naturally lead to formulating likelihoods on the space of complex time. In other words, as all *observable* and *predictable* time is finite, and the phase distribution in well defined, a properly defined measure $\\mu$ on kime can be normalized and extended to a *kime probability measure* $p_{\\kappa}$ that is absolutely continuous with respect to $\\mu$, $p_{\\kappa} \\ll \\mu$.\n\nAssume the sample space of the *radial (time) distribution* is $[0,R_{0}]$, then the *kime-probability measure* is\n\n$$P_{\\kappa}(A) = \\iint_{\\mathbb{C}}{\n\\underbrace{\\ \\ \\chi_{A}(\\kappa)\\ \\ \\ }_{characteristic \\\\ function}d F(\\kappa)}=\n\\iint_{\\mathbb{C}}{\\chi_{A}(\\kappa)\\ P(d\\kappa)} = \n\\iint_{A}{dF(\\kappa)} =\\iint_{A}{f_{X,Y}(x,y)\\ dxdy} = \\\\$$ $$\\iint_{A}{f_{X,Y}(r(\\theta)\\cos\\theta,r(\\theta)\\sin\\theta)\\ \n  \\begin{vmatrix}\n     \\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial \\theta} \\\\ \n     \\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial \\theta} \n  \\end{vmatrix}drd\\theta} = \n\\iint_{A}{\\underbrace{f_{X,Y}(r(\\theta)\\cos\\theta,r(\\theta)\\sin\\theta)r}_{f_{R,\\Theta}(r,\\theta): = rf_{X,Y}(x,y)}drd\\theta} = \\iint_{A}{f_{R,\\Theta}(r,\\theta)drd\\theta} \\\\\n= \\left\\{\n\\begin{matrix}\n\\ \\int_{-\\pi}^{\\pi}{\\int_{0}^{R(\\theta)}{f_{R}(r)drf_{\\Theta}(\\theta)d\\theta}},\n\\ \\ r \\perp \\theta \\\\\n\\\\\n\\ \\ \\ \\int_{-\\pi}^{\\pi}{\\int_{0}^{R(\\theta)}{f_{R,\\Theta}(r,\\theta)drd\\theta}},\\ \\ otherwise \\\\\n\\end{matrix} \\right.\\ \\ \\ .\\ \\ $$\n\n*Note*: Specifically, if *radial (time) distribution* is uniformly distributed on $[0,R_{0}]$ and *radial (time) distribution* and *angular phase distribution* are independent, i.e. $f_R(r) = \\frac{1}{R_{0}}, \\forall r\\in [0,R_{0}]$, then $$P_{\\kappa}(A) = \\int_{-\\pi}^{\\pi}{\\int_{0}^{R(\\theta)}{f_{R}(r)drf_{\\Theta}(\\theta)d\\theta}}\n=\\int_{-\\pi}^{\\pi}{\\int_{0}^{R(\\theta)}{\\frac{1}{R_{0}}drf_{\\Theta}(\\theta)d\\theta}}= \\int_{-\\pi}^{\\pi}{\\frac{R(\\theta)}{R_{0}}f_{\\Theta}(\\theta)d\\theta} $$\n\n*Note*: If the *radial (time) distribution* is not uniform, i.e., $f_R(r)\\not= \\frac{1}{R}, \\forall r\\in [0,R_{0}]$, then we may need to include the time marginal inside the inner integral, $\\int_{0}^{R(\\theta)}{f_R(r)\\ dr}$.\n\n*Problem*: Use Green's theorem to derive a *kime-area measure* using the angular phase distribution *density function*, $f_{\\Theta,r}(\\theta)$. Initially, we can assume that the phase-density is independent of the time magnitude, i.e., $r\\perp \\theta\\ ,\\ f_{\\Theta,r}(\\theta) \\equiv f_{\\Theta}(\\theta)$, but later we may need to potentially consider phase-densities that are time-dependent.\n\n*Proof Outline* $$\\iint_A {\\underbrace{p(da)}_{p(a\\in da)}}=\n\\iint_A {\\underbrace{p(dxdy)}_{p(x\\in dx, y\\in dy)}}=\n\\frac{1}{2}\\iint_A {\\left (-yp(dx) + x p(dy)\\right )}\\ .$$\n\nUsing the polar coordinate transformation $(x,y)=(r(\\theta)\\cos(\\theta), r(\\theta)\\sin(\\theta))$,\n\n$$\\mu(A)=\\iint_A {p(da)}=\n\\iint_A {p(dxdy)}\\underbrace{=}_{Green's\\ Thm}\n\\frac{1}{2}\\int_{\\partial A} {\\left (-yp(dx) + x p(dy)\\right )} =\\\\\n\\frac{1}{2}\\int_{\\partial A} {\\left (-r(\\theta)\\sin(\\theta)\np(r(\\theta)\\cos(\\theta)) + r(\\theta)\\cos(\\theta) p(r(\\theta)\\sin(\\theta))\\right )} =\\\\\n\\frac{1}{2}\\int_{\\partial A} {r^2(\\theta)p(d\\theta)}=\n\\frac{1}{2}\\int_{\\partial A} {r^2(\\theta)f_{r,\\theta}(\\theta)d\\theta}\\ .$$\n\n![Angular Phase Distribution Density Function](https://wiki.socr.umich.edu/images/a/aa/Angular_PhaseDistributionDensityFunction.png)\n\n*Question*: Which one comes first, the Lebesgue measure $\\mu$, *kime-area measure* $\\mu(A)$, or the *kime probability measure* $p_{\\kappa}$? Need to avoid a circular argument! Perhaps start with a general measure space $(\\mathbb{C}, \\mathcal{X}, \\mu)$, where $\\mathcal{X}$ is a $\\sigma$-algebra over $\\mathbb{C}$ and $\\mu$ is a measure defined on set elements in $\\mathcal{X}$. Next define the *kime-area measure*, $\\mu(A) = \\iint_{\\mathbb{C}}{\\chi_{A}(\\kappa)d \\mu}$, followed by the *kime probability measure* $p_{\\kappa}(A) = \\iint_{\\mathbb{C}}{\\chi_{A}(\\kappa)d F(\\kappa)}=\\iint_{\\mathbb{C}}{\\chi_{A}(\\kappa)\\ P(d\\kappa)} = \\iint_{A}{dF(\\kappa)}$. Finally, argue that $\\exists \\mu$ such that $p_{\\kappa}\\ll\\mu$, i.e., the *kime probability measure* is absolutely continuous with respect to *Lebesgue measure.*\n\nIn a statistical and computational data science context, the kime-space probability measure $p_{\\kappa}$ facilitates modeling random experiments involving observing repeated samples from controlled experiments (reflecting the kime-phase distribution) across a range of time-points (kime-magnitudes). Suppose the *kime sample space* $\\Omega$ *consists of all possible observable or predictable outcomes of the experiment (cf. light cone)*, including outcomes that have trivial probability of occurring, $p_{\\kappa}(A \\subset \\Omega) = 0$. Kime-events (*kevents*) are measurable subsets $A \\subset \\Omega$ and the entire collection of kevents forms a $\\sigma$-algebra on $\\Omega$. Each kevent is associated with a probability $0 \\leq p_{\\kappa}(A \\subset \\Omega) \\leq 1$ and the $\\sigma$[-algebra]{.underline} on $\\Omega$ has a natural interpretation as the collection of measurable kevents about which information is observable.\n\n### Example 1 (*Poisson-Laplace kime probability measure*)\n\nConsider the following *kime sample-space*\n\n$$\\Omega = \\underbrace{\\ \\ \\ \\left\\{ t \\in \\mathbb{z|}t \\geq 0 \\right\\}\\ \\ \\ }_{\nT\\ (time) \\\\ kime - magnitude} \\times \n\\underbrace{\\Theta_{\\lbrack - \\pi,\\pi)}(\\theta)}_{kime - phase \\\\\n(distribution) }$$\n\nequipped with the following (joint kime distribution density) *Poisson-Laplace kime probability measure*\n\n$$p_{\\kappa}(A) = \\int_{\\Omega}^{}{\\chi_{A}(a)f_{\\kappa}(a)\\mathbf{d}a},\\ \\forall A \\subset \\Omega\\ ,\\ \\ \\chi_{A}(a) = \\left\\{ \\begin{matrix}\n1,\\ \\ a \\in A\\  \\\\\n0,\\ otherwise \\\\\n\\end{matrix}\\ . \\right.\\ $$\n\nThe (first marginal) *temporal distribution* is $Poisson(\\lambda)$ with point mass (PMF, $\\nu$) and cumulative distribution (CDF, $Ν$) functions\n\n$$\\underbrace{\\ \\ \\nu\\ \\ }_{PMF}\\left( \n\\underbrace{\\ \\ \\ \\left\\{ t \\right\\}\\ \\ \\ }_{pointset} \\right) \\equiv\n\\frac{\\lambda^{t}}{t!}e^{- \\lambda},\\ \\forall t \\in T\\ ,\\ and\\ \\ \n\\underbrace{\\ Ν(R)\\ \\ }_{CDF} \\equiv \\sum_{t \\in R}\n{\\frac{\\lambda^{t}}{t!}e^{- \\lambda}},\\ \\forall R \\subset T\\ .$$\n\nBelow, we show that the (second marginal) [*phase distribution*]{.underline} is the *truncated* $Laplace(\\mu = 0,b = 1)$ distribution on $\\lbrack - \\pi,\\pi)$ with the following PDF and CDF\n\n$$\\underbrace{\\ \\ \\ f_{\\Theta}\\ \\ \\ }_{PDF}(\\theta) \\equiv \n\\frac{1}{2}e^{- \\theta} \\cdot \\chi_{\\lbrack - \\pi,\\pi)}(\\theta),\\ \n\\forall\\theta\\mathbb{\\in R\\ ,\\ }and\\ $$\n\n$$\\underbrace{F_{\\Theta}\\left( \\theta \\middle| - \\pi \\leq \\ \\Theta < \\pi \\right)}_{truncated\\ Laplace\\ CDF} = \\left\\{ \\begin{matrix}\n\\frac{e^{\\theta} - 2F_{\\Theta}( - \\pi)}{2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right)},\\ \\ \\theta \\leq 0 \\\\\n\\frac{2 - e^{- \\theta} - 2F_{\\Theta}( - \\pi)}{2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right)},\\ \\ \\theta \\geq 0 \\\\\n\\end{matrix} \\right.\\ \\ \\ ,$$\n\n$$\\underbrace{\\ \\ F_{\\Theta}(\\theta)\\ \\ }_{(unconditional) \\\\\nLaplace\\ CDF} = \\left\\{ \\begin{matrix}\n\\frac{1}{2}e^{\\theta},\\ \\ \\theta \\leq 0 \\\\\n1 - \\frac{1}{2}e^{- \\theta},\\ \\theta \\geq 0 \\\\\n\\end{matrix}\\ \\ ,\\ \\ \\  \\right.\\ $$\n\n$$F_{\\Theta}( - \\pi) = \\frac{1}{2}e^{- \\pi} \\approx \\ 0.0216,\\ \\ F_{\\Theta}(\\pi) = 1 - \\frac{1}{2}e^{- \\pi} \\approx 0.978393,\\ \\ $$\n\n$$2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right) = 1.913572\\ .$$\n\nHence, the *joint kime distribution density* and *distribution functions* are defined by\n\n$$\\underbrace{\\ \\ \\ f_{\\kappa}\\ \\ \\ }_{PDF}\\left( te^{i\\theta} \\right)\n\\underbrace{\\ \\  \\equiv \\ \\ }_{probability \\\\\n\\ chain\\ rule}\\nu(t)f_{\\Theta}\\left( \\theta \\middle| t \\right)\n\\underbrace{\\ \\  = \\ \\ }_{T\\bot\\Theta}\\left( \\frac{\\lambda^{t}}{t!}\ne^{- \\lambda} \\right) \\cdot \\left( \\frac{e^{- \\theta}}{2}\\chi_{\\lbrack - \\pi,\\pi)}\n(\\theta) \\right),\\ \\forall(t,\\theta) \\in \\mathbb{R}^{+}\\mathbb{\\times R\\ ,}$$\n\n$$\\ \\ \\underbrace{\\ F_{\\kappa}\\left( t'e^{i\\theta'} \\right)\\ \\ }_{CDF} \n\\equiv \\sum_{t \\leq t'}^{}{\\int_{- \\infty}^{\\theta'}{f_{\\kappa}\\left( \nte^{i\\theta} \\right)}d\\theta} = \\frac{1}{2}\\sum_{0 \\leq t}^{t'}{\\frac{\\lambda^{t}}{t!}e^{- \\lambda}\\int_{- \\pi}^{\\theta'}\ne^{- \\theta}d\\theta} = e^{- \\lambda}\\frac{e^{\\pi} - e^{- \\theta'}}{2}\n\\sum_{t = 0}^{t'}\\frac{\\lambda^{t}}{t!}\\ ,\\ $$\n\n$$\\forall\\ t' \\in \\mathbb{R}^{+},\\  - \\pi < \\theta' < \\pi\\ .$$\n\nBelow we derive the truncated Laplace PDF and CDF. Recall that a Laplace random variable,\n\n$$\\Theta \\sim Laplace\\left( \\overbrace{\\ \\ \\ \\mu\\ \\ \\ }^{location},\n\\overbrace{\\ \\ \\ b\\ \\ \\ }^{scale} \\right),$$\n\nhas Laplace (double-exponential) distribution, which is infinitely supported and has the following probability *density* (PDF) and *cumulative* distribution (CDF) functions\n\n$$\\underbrace{f_{\\Theta}(\\theta)}_{PDF} = \n\\frac{1}{2b}e^{\\left( - \\frac{|\\theta - \\mu|}{b} \\right)}\\ \\ ,\\ \\ \\ \\underbrace{F_{\\Theta}(\\theta)}_{CDF} = \\left\\{ \\begin{matrix}\n\\frac{1}{2}e^{\\left( \\frac{\\theta - \\mu}{b} \\right)},\n\\ \\ \\theta \\leq \\mu \\\\\n1 - \\frac{1}{2}e^{\\left( - \\frac{\\theta - \\mu}{b} \\right)},\\ \\theta \\geq \\mu \\\\\n\\end{matrix} \\right.\\ \\ \\ .\\ \\ $$\n\n![Joint Poisson-Laplace Kime Probability Measure](https://wiki.socr.umich.edu/images/c/c5/PoissonLaplace_KimeProbabilityMeasure_.png)\n\nRestricting the (kime-phase) variable $\\Theta$ to $\\lbrack - \\pi,\\pi)$ naturally truncates the probability density and the cumulative distribution functions to the same interval. Over $- \\pi \\leq \\Theta \\leq \\pi$,\n\n$$\\underbrace{f_{\\Theta}\\left( \\theta \\middle| - \\pi \\leq \\Theta < \\pi \\right)\n}_{truncated\\ Laplace\\ density} = \\frac{f_{\\Theta}(\\theta) \\cdot \\chi_{\\lbrack - \\pi,\\pi)}(\\theta)}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)} = \\frac{g_{\\Theta}(\\theta)}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)} = \\propto_{\\theta}f_{\\Theta}(\\theta) \\cdot \\ \\chi_{\\lbrack - \\pi,\\pi)}(\\theta)\\ ,$$\n\n$$\\ \\underbrace{F_{\\Theta}\\left( \\theta \\middle| - \\pi \\leq \\ \\Theta < \\pi \\right)}_{\ntruncated\\ Laplace\\ CDF} \\equiv \n\\frac{F_{\\Theta}(\\theta) - F_{\\Theta}( - \\pi)}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)} = \\left\\{ \\begin{matrix}\n\\frac{e^{\\left( \\frac{\\theta - \\mu}{b} \\right)} - 2F_{\\Theta}( - \\pi)}{2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right)},\\ \\ \\theta \\leq \\mu \\\\\n\\frac{2 - e^{\\left( - \\frac{\\theta - \\mu}{b} \\right)} - 2F_{\\Theta}( - \\pi)}{2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right)},\\ \\theta \\geq \\mu \\\\\n\\end{matrix} \\right.\\ \\ \\ .$$\n\nwhere \\$g\\_{\\Theta}(x) = \\left{\n\n```{=tex}\n\\begin{matrix} f_{\\Theta}(\\theta),\\ \\ \\theta \\in \\lbrack - \\pi,\\pi)\\  \\\\ 0,\\ \\ \\ \\ \\ \\ \\ \\ \\theta \\notin \\lbrack - \\pi,\\pi) \\\\ \\end{matrix}\n```\n\\right.\\$ and $\\chi_{\\lbrack - \\pi,\\pi)}$ is the characteristic function (indicator) over the phase domain $\\lbrack - \\pi,\\pi)$. The truncated density is a density function since the denominator $\\left\\lbrack F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right\\rbrack$ is the appropriate constant with respect to $\\theta$ which ensures that the density integrates to $1$\n\n$$\\int_{- \\pi}^{\\pi}{f_{\\Theta}\\left( \\theta \\middle| - \\pi \\leq \\Theta < \\pi \\right)d\\theta} = \\frac{1}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)}\\int_{- \\pi}^{\\pi}{g_{\\Theta}(\\theta)d\\theta} = \\frac{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)} = 1\\ .$$\n\n![Joint Poisson-Laplace Kime Probability Measure with strong linear correlation between the pair of univariate processes $\\rho(T,\\Theta)=0.8$.](https://wiki.socr.umich.edu/images/7/7a/PoissonLaplace_KimeProbabilityMeasure_P2.png)\n\nShow/derive the fundamental relationship between the kime density and kime cumulative distribution functions\n\n$$\\underbrace{\\ \\ \\ f_{\\kappa}\\ \\ \\ }_{PDF}\\left( te^{i\\theta} \\right) \\equiv\n\\underbrace{f_{\\Theta|T}\\left( \\theta|\\ t \\right)}_{conditional} \\cdot \\underbrace{f_{T}(t)}_{marginal} \\equiv\n\\underbrace{f_{T|\\Theta}\\left( t|\\theta \\right)}_{conditional} \\cdot \\underbrace{f_{\\Theta}(\\theta)}_{marginal} = \\frac{\\partial^{2}\n\\overbrace{F_{\\kappa}\\left( te^{i\\theta} \\right)}^{CDF}}{\\partial t\\partial\\theta}\\ .$$\n\nThe [SOCR bivariate distribution app](https://socr.umich.edu/HTML5/BivariateNormal/BVN2/), see [see pubs](https://www.socr.umich.edu/people/dinov/publications.html) can be employed to display an interactive 3D scene illustrating this instance of a *Poisson-Laplace kime probability measure* ($T\\bot\\Theta$)\n\n$$f_{\\kappa}\\left( te^{i\\theta} \\right) = \n\\underbrace{\\overset{Poisson(\\lambda = 4)}{\n\\overbrace{\\ \\ \\ \\ f_{T}(t)\\ \\ \\ \\ \\ }}}_{Time\\ (t)}\\ \\bigotimes\\ \n\\overbrace{\\underbrace{f_{\\Theta}(\\theta)}_{Phase\\ (\\theta)}}^{Laplace(\\mu = 0,b = 1) \\cdot \\chi_{\\lbrack - \\pi,\\pi)}}\\ \\ .$$\n\nWhen $T\\not\\bot\\Theta$, we need to [use copulas to compute the joint time-phase density function using the individual marginal densities](https://doi.org/10.1007/s42979-022-01206-w).\n\nThe first graph shows the kime-density for $T\\bot\\Theta$. For simplicity of the 3D visualization, the 2D density surface is cut at the periodic phase boundary ($\\theta = \\pi$), and shown over the right half plane. However, the density may also be wrapped around and stitched into a continuous surface.\n\nWhen $T\\ and\\ \\Theta$ are *not* independent, the joint density does not factor as the product of the marginals and the *Poisson-Laplace kime probability measure* would be computed via a copula model [\\^5]. In this situation, the density surface will have a different appearance, as shown below assuming a strong linear correlation between the pair of univariate processes $\\rho(T,\\Theta) = 0.8$.\n\nGenerate 2D plotly visualizations [\\^6] of the *Poisson-Laplace kime probability measure for* some specific examples of kime-regions with simple-closed curve boundaries:\n\n-   *Folium of Descartes*: $x(t) = \\frac{3t}{1 + t^{3}}\\ ,\\ \\ y(t) = \\frac{3t^{2}}{1 + t^{3}}$.\n\n-   *Ellipse*: $x(t) = 5\\cos(t)\\ ,\\ \\ y(t) = 2\\sin(t)$ or $\\frac{x^{2}}{5^{2}} + \\frac{y^{2}}{2^{2}} = 1$ .\n\n-   *Disk-sector annulus*: $x(u,v) = v\\cos(u)\\ ,\\ \\ y(u,v) = v\\sin(u)$\n\n```{mathematica}\nParametricPlot[{v Cos\\[u\\], v Sin\\[u\\]}, {u, 0, Pi/3}, {v, 1/2, 1}]\n```\n\n-   *Phase space* of a solution to the Lotka--Volterra predator-prey equations:\n\n```{mathematica}\nNDSolve[{y\\'\\[t\\] == y\\[t\\] (x\\[t\\] - 1), x\\'\\[t\\] == x\\[t\\] (2 -\n   y\\[t\\]), x\\[0\\] == 1, y\\[0\\] == 2.7}, {x, y}, {t, 0, 10}\\];\n ParametricPlot\\[Evaluate\\[{x\\[t\\], y\\[t\\]} /. First\\[%\\]\\], {t, 0, 10}]\n```\n\n-   Negative curvature disk: $x(t) = \\cos^{3}(t)\\ ,\\ \\ y(t) = \\sin^{3}(t)$\n\n```{mathematica}\nParametricPlot[Cos\\^3 \\[t\\], Sin\\^3 \\[t\\], {t, -Pi, Pi} ]\n```\n\n### Example 2 (*Weibull-Hyperbolic-Secant distribution kime probability measure*)\n\nConsider another *kime sample-space*\n\n$$\\Omega = \\underbrace{\\ \\ \\ \\left\\{ t \\in \\mathbb{z|}t \\geq 0 \\right\\}\\ \\ \\ }_{\nT\\ (time) \\\\ kime - magnitude} \\times \n\\underbrace{\\Theta_{\\lbrack - \\pi,\\pi)}(\\theta)}_{kime - phase \\\\\n(distribution)},$$\n\nequipped with the following (joint kime distribution density) *continuous* *Weibull* kime-magnitude with ($\\chi_{\\lbrack - \\pi,\\pi)}(\\theta)$ truncated) *hyperbolic-secant* kime-phase distribution leading to a different kime probability measure\n\n$$p_{\\kappa}'(A) = \\int_{\\Omega}^{}{\\chi_{A}(a)f_{\\kappa}(a)\\mathbf{d}a},\\ \\forall A \\subset \n\\Omega\\ ,\\ \\ \\chi_{A}(a) = \\left\\{ \\begin{matrix}\n1,\\ \\ a \\in A\\  \\\\\n0,\\ otherwise \\\\\n\\end{matrix}\\ . \\right.\\ $$\n\nThe (first marginal) *temporal distribution* is $Weibull\\left( \\underbrace{k = 1.5}_{shape}, \\underbrace{\\lambda = 1}_{scale} \\right)$ with density (PDF, $f_{T}$) and cumulative distribution (CDF, $F_{T}$) functions\n\n$$\\underbrace{\\ \\ f_{T}\\ \\ }_{PDF}(t) \\equiv \n\\frac{k}{\\lambda}\\left( \\frac{t}{\\lambda} \\right)^{k - 1}e^{- \\left( \\frac{t}{\\lambda} \\right)^{k}} \\cdot \\chi_{\\lbrack 0,\\infty)}(t),\\ \\forall t \\in \\mathbb{R\\ ,\\ }\n{\\text{and}} \\ \\ \n\\underbrace{\\ F_{T}\\ \\ }_{CDF} \\equiv 1 - e^{- \\left( \\frac{t}{\\lambda} \\right)^{k}}\n\\cdot \\chi_{\\lbrack 0,\\infty)}(t),\\ \\forall t\\mathbb{\\in R}\\ .$$\n\nThe (second marginal) *phase distribution* is the *truncated* $HyperbolicSecant\\lbrack - \\pi,\\pi)$ distribution on $\\lbrack - \\pi,\\pi)$ with the following PDF and CDF, which do not depend on any parameters,\n\n$$\\underbrace{f_{\\Theta}\\left( \\theta \\middle| - \\pi \\leq \\Theta < \\pi\n\\right)}_{truncated\\ HyperbolicSecant\\ density} \\equiv \n\\underbrace{\\frac{1}{2}{sech}\\left( \\frac{\\pi}{2}\\theta\\  \\right)}_{\nf_{\\Theta},\\ \\ \\ PDF} \\cdot \\frac{\\chi_{\\lbrack - \\pi,\\pi)}\n(\\theta)}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)},\\ \\forall\\theta\\mathbb{\\in R,\\ }\n{\\text{and}}\\ $$\n\n$$\\underbrace{F_{\\Theta}\\left( \\theta \\middle| - \\pi \\leq \\ \\Theta < \\pi\n\\right)}_{truncated\\ HyperbolicSecant\\ CDF} \\equiv \\frac{\n\\overbrace{\\frac{2}{\\pi}\\arctan\\left( e^{\\frac{\\pi}{2}\\theta}\\  \\right) \\cdot \\chi_{\\lbrack - \\pi,\\pi)}(\\theta)}^{F_{\\Theta}(\\theta),\\ \\ \\ CDF} - F_{\\Theta}( - \\pi)}{F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi)},\\ \\forall\\theta\\mathbb{\\in R\\ .}$$\n\n$$F_{\\Theta}( - \\pi) = \\frac{2}{\\pi}\\arctan\\left( e^{\\frac{- \\pi^{2}}{2}}\\  \\right) \\approx \\ 0.004578416,\\ \\ F_{\\Theta}(\\pi) = \\frac{2}{\\pi}\\arctan\\left( e^{\\frac{\\pi^{2}}{2}}\\  \\right) \\approx \\ 0.9954216,\\ \\ $$\n\n$$2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right) = \\ 1.97646\\ .$$\n\nHence, the *joint kime distribution density* and *distribution functions* are defined by\n\n$$\\underbrace{\\ \\ \\ f_{\\kappa}\\ \\ \\ }_{PDF}\\left( te^{i\\theta} \\right)\n\\underbrace{\\ \\  \\equiv \\ \\ }_{probability \\\\ \\ chain\\ rule}\nf_{T}(t)f_{\\Theta}\\left( \\theta \\middle| t \\right)\n\\underbrace{\\ \\  = \\ \\ }_{T\\bot\\Theta}$$\n\n$$\\left( \\frac{k}{\\lambda}\\left( \\frac{t}{\\lambda} \\right)^{k - 1}e^{- \\left( \\frac{t}{\\lambda} \\right)^{k}} \\right) \\cdot \\left( \\frac{{sech}\\left( \\frac{\\pi}{2}\\theta\\  \\right)}{2\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right)} \\right) \\cdot \\chi_{\\lbrack - \\pi,\\pi)}(\\theta) \\cdot \\chi_{\\lbrack 0,\\infty)}(t),$$ $$\\forall(t,\\theta)\\mathbb{\\in R \\times R\\ ,}$$\n\n$$\\underbrace{\\ F_{\\kappa}\\left( t'e^{i\\theta'} \\right)\\ \\ }_{CDF} \\equiv\n\\frac{k}\n{\\underbrace{2\\lambda\\left( F_{\\Theta}(\\pi) - F_{\\Theta}( - \\pi) \\right)}_{\n\\lambda \\cdot 1.97646}}\\iint_{t = 0,\\theta = - \\pi}^{\\infty,\\ \\pi}{\\left( \n\\left( \\frac{t}{\\lambda} \\right)^{k - 1}e^{- \\left( \\frac{t}{\\lambda} \\right)^{k}} \\right)\\left( {sech}\\left( \\frac{\\pi}{2}\\theta\\  \\right) \\right)}dtd\\theta\\ ,\\ $$\n\n$$\\forall\\ t' \\in \\mathbb{R}^{+},\\  - \\pi < \\theta' < \\pi\\ .$$\n\nAgain, we use the [SOCR bivariate distribution app](https://socr.umich.edu/HTML5/BivariateNormal/TVN/) to render 3D scenes of the *Weibull-Hyperbolic-Secant distribution kime probability measure* for both independent $T\\bot\\Theta$ and correlated time-phase.\n\nFirst is the kime-density for $T\\bot\\Theta$.\n\n$$f_{\\kappa}\\left( te^{i\\theta} \\right) = \n\\overbrace{\\underbrace{\\ \\ \\ \\ f_{T}(t)\\ \\ \\ \\ \\ }_{Time\\ (t)}}^{\nWeibull(k = 1.5,\\lambda = 1)}\\ \\bigotimes\\ \n\\overbrace{\\underbrace{\\ \\ \\ \\ \\ \\ f_{\\Theta}(\\theta)\\ \\ \\ \\ \\ \\ }_{Phase\\ (\\theta)}}^{HyperbolicSecant\\lbrack - \\pi,\\pi)}\\ \\ .$$\n\nWhen $T\\ and\\ \\Theta$ are correlated, e.g., $\\rho(T,\\Theta) = 0.8$, the joint density does not factor as the product of the marginals and the *joint kime probability measure* would be computed via a copula model.\n\nNote that both *joint kime probability measure* plots are in Cartesian coordinates. In fact, plotting them in polar coordinates will represent the corresponding kime-surfaces.\n\n*Other Examples*. Consider adding additional examples of *kime probability measures*, using the continuous Weibull distribution, Logarithmic distribution, discrete Weibull distribution, etc., for the kime-magnitude (*time*) marginal, which is coupled with Laplace or another zero-mean and symmetric probability phase distribution for the *kime-phase* marginal distribution.\n\n![](https://wiki.socr.umich.edu/images/a/ab/Weibull_HyperbolicSecant_KimeDistribution.png)\n\n...\n\n### Notes\n\nAs an alternative to outputting a random phase, $\\varphi$, we can consider the action of the linear kime-phase Hermitian operator $\\widehat{\\mathcal{P}}$ as\n\n$$\\underbrace{\\ \\ \\ \\widehat{\\mathcal{P}}\\ \\ \\ }_{Hermitian \\\\\noperator }\n\\overbrace{\\ \\ \\ \\ \\Psi\\ \\ \\ \\ \\ }^{distribution} = \n\\overbrace{\\underbrace{\\ \\ \\ \\psi(x,t)\\ \\ \\ }_{wavefunction}}^{eigenstate}\n\\overbrace{\\ \\ \\ \\Psi\\ \\ \\ }^{distributon}\\ .$$\n\n*Note*: Explore formulating the kime operator using the Radon-Nikodym distributional derivative.\n\nRecall the discussion earlier about formulating the partial derivative, $\\frac{\\partial f(\\kappa(r,\\varphi))}{\\partial\\varphi}$ as a *Radon-Nikodym derivative* with respect to the kime-phase distribution, $\\varphi \\sim \\Phi$ or the distribution of the phase-change, $\\Delta\\varphi = \\varphi_{i} - \\varphi_{j} \\sim \\Gamma$.\n\nAssume the wavefunction of a particle is $\\psi(x,\\kappa):\\mathbb{R}^{3}\\mathbb{\\times C \\longrightarrow C}$ and $\\kappa = te^{i\\varphi},\\ t \\in \\mathbb{R}^{+},\\ \\varphi \\sim \\Phi_{\\lbrack - \\pi,\\ \\pi)}$.\n\n-   In the most general case, there may be other (exogeneous) variables, $\\mathcal{E}$, e.g., phenotypes, traits, subjective characteristics, and objective measurements, that are complementary (independent or correlated variables) to the main observable (outcome) measure, e.g., momentum, energy, spin, etc. Such higher-dimensional predictions represent natural generalizations of the simpler spatiotemporal functions, $f(t,x)$, to $f(t,x\\mathcal{,\\ E)}$.\n\n-   Some relevant resources are included below\n\n    -   [paper 1](https://doi.org/10.1007/s42979-022-01206-w)\n    -   [paper 2](https://doi.org/10.52041/iase.pdsxt)\n    -   [SOCR Bivariate/Trivariate Distribution Calculator App](https://socr.umich.edu/HTML5/BivariateNormal/BVN2/)\n    -   [paper 3](https://doi.org/10.1007/s42979-022-01206-w)\n    -   [DSPA Appendix 3 (Geometric & Parametric Surfaces](https://socr.umich.edu/DSPA2/DSPA2_notes/DSPA_Appendix_03.1_Geometric_Parametric_Surface_Viz.html)\n    -   [Discrete Weibull distribution](https://en.wikipedia.org/wiki/Discrete_Weibull_distribution)\n    -   [Logarithmic distribution](https://en.wikipedia.org/wiki/Logarithmic_distribution)\n    -   [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution)\n    -   [Symmetric probability distributions](https://en.wikipedia.org/wiki/Symmetric_probability_distribution)",
      "word_count": 2706
    },
    {
      "title": "Derivation of the kime-operator and energy-kime uncertainty",
      "content": "## Motivation\n\n*Driving Fundamental Question*: *Can a well-defined complex-time operator, repeated sampling, computational AI approaches, and spacekime analytics help with reworking the fundamental picture of physics?*\n\nThe foundations of physics are constantly revised to improve predictions, reduce variability, and expedite inference. The standard model of physics has been remarkably successful in describing the universe at the subatomic level. Yet, it is incomplete -- does not include gravity and it does not provide a consistent explanation for the dark matter and dark energy that appear to fill most of the universe. New representations of time may be key to understanding these gaps in the standard model. The traditional view of time as a 1D linear progression may be an oversimplification of reality to accurately account for more complex dynamics. Considering time as complex, or multidimensional, opens new possibilities for modeling the universe. The completeness and optimality of kime may provide a viable route to unifying the laws of physics into a single theory that describes all of the forces of nature. It may also help to explain the mysteries of dark matter and dark energy.\n\nComputational AI approaches and spacekime analytics are powerful tools that rely on complex time to offer novel data-driven understanding of large longitudinal processes. AI can be used to simulate complex physical systems, and spacekime analytics can be used to analyze the data that is generated by these simulations. This combination of techniques can help physicists to develop new theories of temporal dynamics that can be tested using large-scale repeated-measurement experimental data. A new holistic understanding of time has the potential to revolutionize our understanding of the universe and our place in it. Here are some specific examples of how computational AI approaches and spacekime analytics can be used to study the foundations of physics:\n\n-   AI can be used to simulate the behavior of subatomic particles, which can help physicists to understand the interactions between particles and to develop new particle physics theories.\n-   Spacekime analytic may be useful for analyzing data from particle accelerators, which in turn can be applied to test new theories of particle physics and to search for new particles.\n-   AI can be used to simulate the behavior of black holes. This can help scientists to understand the properties of spacetime singularities and to develop congruent theories of gravity.\n-   Spacekime analytic can be used to search for new gravitational waves and analyze data from gravitational wave detectors.\n\nBy utilizing computational AI approaches and spacekime analytics, physicists can make significant progress in understanding the foundations of physics. This new understanding of time has the potential to revolutionize our interpretation of the universe and our place in it.\n\n## Approach\n\nFollowing the [1961 work of Aharonov and Bohm](https://journals.aps.org/pr/abstract/10.1103/PhysRev.122.1649), perhaps we can consider formulating a *kime-operator* and elevating *time from a c-number* (classical number variable) to *kime as a q-number* (quantum operator). Whereas the fundamental *algebra of c-numbers is commutative* (multiplication over the base field of the Hilbert space is commutative), e.g., $\\left\\lbrack z_{1},z_{2} \\right\\rbrack \\equiv z_{1}z_{2} - z_{2}z_{1} = 0,\\ \\forall z_{1},\\ z_{2}\\mathbb{\\in C}$, the arithmetic over q-numbers is in general non-commutive, e.g., $\\left\\lbrack \\widehat{x},\\ \\widehat{p} \\right\\rbrack \\equiv \\left( \\widehat{x}\\widehat{p} - \\widehat{p}\\widehat{x} \\right) = i\\hslash \\neq 0$.\n\nLet's start with the assumption that the energy (Hamiltonian, $H$) and the kime ($\\widehat{\\mathcal{K}}$) operators are conjugate q-numbers, i.e., correspond to canonically-conjugate c-numbers (energy and kime) and\n\n$$\\left\\lbrack \\widehat{H},\\ \\widehat{\\mathcal{K}} \\right\\rbrack = i\\hslash\\ .$$\n\n*Pauli's fundamental objection* to the existence of a time operator $\\widehat{t}$ is based on \\$\\left\\lbrack \\widehat{H}, \\widehat{t} \\right\\rbrack = i\\hslash\\$ and the discrepancy of the bounded below values of the eigenvalue spectrum of $\\widehat{H}$ (observed energy values have to be positive), whereas $\\widehat{t}$ did not have an apparent lower limit on its eigenvalues.\n\nFor a particle of mass $m$, the classical relation between $\\{ t,x,p\\}$, i.e., $t = \\frac{mx}{p}$, can be generalized to complex time\n\n$$\\kappa = te^{i\\theta} = \\frac{mx}{p}e^{i\\theta}\\ ,$$\n\nwhere the kime-phase $\\theta$ represents the repeated sampling from the same process under identical conditions at a fixed spatiotemporal location, e.g., fixed $x,t = \\frac{mx}{p}$.\n\n$$\\kappa = \\frac{mx}{p}e^{i\\theta} = mx\\overbrace{\\frac{d\\ln p}{dp}}\ne^{i\\theta} = mxh'(p)e^{i\\theta}\\ .$$\n\nExpress the momentum as $p = Ae^{iq}$. How can we formulate the kime-operator? *The following derivation approach does not work*, since $A$ is not a constant and there is a missing negative sign in the commutator relation:\n\n$$\\widehat{\\kappa} \\equiv te^{i\\theta} = \\underbrace{\\frac{m\\widehat{x}}{\\widehat{p}}}_{t} e^{i\\widehat{\\theta}} = \\frac{m}{A}\\widehat{x}e^{- i\\widehat{q}}e^{i\\widehat{\\theta}} = \\frac{m}{A}\\widehat{x}e^{- i\\left( \\widehat{q - \\theta} \\right)} =$$\n\n$$\\overbrace{\\left( \\frac{m}{2A} \\right)}^{const}\n\\underbrace{\\left( \\widehat{x}e^{- i\\left( \\widehat{q - \\theta} \\right)} + \ne^{- i\\left( \\widehat{q - \\theta} \\right)}\\widehat{x} \\right)}_{\\left\\lbrack \n\\widehat{x},\\ \\ \\ \\widehat{q - \\theta}\\  \\right\\rbrack} =\n\\overbrace{\\left( \\frac{m}{2A} \\right)}^{const}\n\\left( e^{- i\\frac{\\left( \\widehat{q - \\theta} \\right)}{2}}\\ \n\\widehat{x}\\ e^{- i\\frac{\\left( \\widehat{q - \\theta} \\right)}{2}} \\right)\\ .$$\n\nRecall that $e^{- i\\left( \\widehat{q - \\theta} \\right)}$ is a constant and hence it commutes with the operator $\\widehat{x}$. We may need to rethink this as the momentum is a kime-vector, not scalar, and $|p| = A$ is not a constant $\\ldots$, see the [TCIU Book Dirac Equation section (Chapter 3 Appendix, pp.178+)](https://doi.org/10.1515/9783110697827).\n\nPerhaps even start with the energy-operator (the Hamiltonian), $E = \\widehat{H}$. In spacekime, the total energy of the particle has two components and is represented as an energy magnitude:\n\n$$E = \\sqrt{\\left( E_{1} \\right)^{2} + \\left( E_{2} \\right)^{2}}\\ \\ ,$$\n\nwhere $E_{1}$ and $E_{2}$ are the two energy components of the particle defined with respect to kime dimensions $k_{1}$ and $k_{2}$, respectively. Here, $$E_{\\gamma} = {c^{2}m}_{0}\\frac{{dk}_{\\gamma}}{{dk}_{0}},\\gamma \\in \\{ 1,2\\},\\ {dk}_{0} = \\left\\| {d\\mathbf{k}}_{0} \\right\\| = \\sqrt{\\left( {dk}_{0,1} \\right)^{2} + \\left( {dk}_{0,2} \\right)^{2}} \\geq 0 ,$$\nsee [TCIU Chapter 3 Appendix and Chapter 5, Section G.4](https://www.degruyter.com/document/doi/10.1515/9783110697827/html).\n\n-   Can we think of the Hamiltonian (i.e., energy) as the generator of time translations}? If so, a *complex eigenvalue* of the Hamiltonian implies that there is some decay -- $x$ particle disappear, while $y$ particle appear, see [p.148 of Landau & Lifshitz (Quantum Mechanics)](https://ia903206.us.archive.org/4/items/landau-and-lifshitz-physics-textbooks-series/Vol%203%20-%20Landau%2C%20Lifshitz%20-%20Quantum%20mechanics%20-%20non-relativistic%20theory%20%283ed.%2C%201991%29.pdf),\n\n$$\\widehat{H} = E = E_{o} - \\frac{1}{2}i\\Gamma,$$\n\nAt the initial state, $$E = \\begin{pmatrix}\nE_{x} \\\\\nE_{y} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nE_{o} \\\\\n0 \\\\\n\\end{pmatrix}$$\n\nand at the end state\n\n$$E' = \\begin{pmatrix}\nE_{x}' \\\\\nE_{y}' \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n0 \\\\\nE_{o} \\\\\n\\end{pmatrix}.$$\n\nBy complexifying the energy (adding the term $\\  - \\frac{1}{2}i\\Gamma$) to model eigenstate decay (fast particle disappearance), $e^{- \\frac{1}{2}t\\Gamma} \\underbrace{\\rightarrow}_{t \\rightarrow \\infty}0$, we can represent the lifetime of an energy state, i.e., the lifetime of the particle is $\\tau = \\frac{2}{\\Gamma}$,\n$$e^{- i\\widehat{H}t} = e^{- it\\left( E_{o} - \\frac{1}{2}i\\Gamma \\right)} = \ne^{- itE_{o}} \\cdot \n\\underbrace{\\ \\ \\ e^{- \\frac{1}{2}\\Gamma t}\\ \\ \\ }_{lifetime\\ \\ decay}\\ .$$\n\nThe energy eigenvalues of a Hermitian Hamiltonian $\\widehat{H}$ are real, since the operator $e^{i\\widehat{H}t}$ is unitary, cf. law of total probability, i.e., the sum of the probabilities of observing all possible energy eigenstate always remains $1$.\n\nOne important note is that a Hermitian operator acting on the space of square-integrable wavefunctions ($L^{2}\\mathbb{(R)}$) must have real eigenvalues. However, this is not the case if we relax the conditions on the state-space. For instance, a Hermitian operator can have complex eigenvalues for non-square-integrable functions. For any $z_{o}\\mathbb{\\in C}$, the operator $\\widehat{O} = - i\\frac{\\partial}{\\partial x}$ has an eigenspectrum ($\\lambda = z_{o},\\ \\phi = e^{iz_{o}x}$), since\n\n$$\\underbrace{- i\\frac{\\partial}{\\partial x}}_{operator}e^{ax} = -i a e^{ax}\n\\underbrace{\\ \\ \\  = \\ \\ \\ }_{eigenvalue \\\\ problem} \\ \n\\underbrace{z_{o}}_{eigenvalue}\n\\overbrace{\\ \\ \\ e^{ax}\\ \\ \\ }^{eigenfunction}\\ ,$$\n\nhence, $- ia = z_{o}$, and $a = iz_{o}$. Clearly, the operator $\\widehat{O} = - i\\frac{\\partial}{\\partial x}$ is Hermitian and the eigenvalue $z_{o} = - ia$ is properly complex, not necessarily real, however, this is because the eigenstate/eigenfunction is outside of the proper space, $e^{ax} \\notin L^{2}\\mathbb{(R)}$! So, *the non-observability, not-realness, of the viable complex eigenvalue is related to the fact that in this case the operator is applied out-of-scope*!\n\nIn general, a bounded, linear self-adjoint (Hermitian) operator on an infinite dimensional Hilbert space, e.g., $L^{2}\\lbrack 0,1\\rbrack$, cannot be guaranteed to have any eigenvalues, and hence, there may not be an orthonormal bases of eigenfunctions over the [Hilbert space](https://www.math.ucdavis.edu/~hunter/book/ch9.pdf). The *multiplication operator* provides a counterexample for the bounded, linear self-adjoint (Hermitian) operator $\\mathcal{H =}L^{2}\\lbrack 0,1\\rbrack$:\n\n$$\\left( \\mathcal{M}f \\right)(x) \\equiv x \\cdot f(x)\\mathcal{:H \\rightarrow H,\\ \\ }\\left| \\left| \\mathcal{M} \\right| \\right| = \\sup_{\\left| |f| \\right| \\leq 1}\\left| \\left| \\mathcal{M}(f) \\right| \\right| = \\sup_{f \\neq 0\\ }\\frac{\\left| \\left| \\mathcal{M}(f) \\right| \\right|}{\\left| |f| \\right|} = \\sup_{f \\neq 0\\ }\\frac{|x|\\left| |f \\right||}{\\left| |f| \\right|} = 1\\ .\\ $$\n\nAssuming\n\n$$x \\cdot f(x) = \\left( \\mathcal{M}f \\right)(x) \\equiv \n\\overbrace{\\ \\ \\ \\lambda\\ \\ \\ }^{const.} \\cdot f(x) \\Longrightarrow f(x)\n\\overbrace{\\ \\  = \\ \\ }^{a.e.} 0 $$\n\nand $\\mathcal{M}$ has no eigenvalues.\n\n*Theorem*: Given a bounded, self-adjoint (Hermitian) operator $\\mathcal{M:H \\rightarrow H}$\n\n-   All eigenvalues must be real, and\n-   All different eigenvectors are orthogonal.\n\n*Proof* following these notes [Chapters 8-9](https://www.math.ucdavis.edu/~hunter/book/pdfbook.html):\n\n-   Suppose $\\lambda \\in \\sigma\\left( \\mathcal{M} \\right) \\equiv eigenspace\\mathcal{\\{ M\\}}$, and $x_{\\lambda} \\neq 0$ is the corresponding eigenvector, $\\mathcal{M}x_{\\lambda} = \\lambda x_{\\lambda}$. Then, $\\lambda\\left\\langle x_{\\lambda} \\middle| x_{\\lambda} \\right\\rangle = \\left\\langle x_{\\lambda} \\middle| \\lambda x_{\\lambda} \\right\\rangle = \\left\\langle x_{\\lambda} \\middle| \\mathcal{M}x_{\\lambda} \\right\\rangle = \\left\\langle \\mathcal{M}x_{\\lambda} \\middle| x_{\\lambda} \\right\\rangle = \\ \\overline{\\lambda}\\left\\langle x_{\\lambda} \\middle| x_{\\lambda} \\right\\rangle \\Longrightarrow \\lambda \\equiv \\ \\overline{\\lambda} \\Longrightarrow \\lambda\\mathbb{\\in R}$.\n\n-   If $\\lambda \\neq \\mu \\in eigenspace\\left\\{ \\mathcal{M} \\right\\}\\mathbb{\\cap R}$, and $\\mathcal{M}x_{\\lambda} = \\lambda x_{\\lambda}$, $\\mathcal{M}x_{\\mu} = \\mu x_{\\mu}$, then,\n\n$$\\lambda\\left\\langle x_{\\lambda} \\middle| x_{\\mu} \\right\\rangle = \\left\\langle \\lambda x_{\\lambda} \\middle| x_{\\mu} \\right\\rangle = \\left\\langle \\mathcal{M}x_{\\lambda} \\middle| x_{\\mu} \\right\\rangle = \\left\\langle x_{\\lambda} \\middle| \\mathcal{M}x_{\\mu} \\right\\rangle = \\mu\\left\\langle x_{\\lambda} \\middle| x_{\\mu} \\right\\rangle \\Longrightarrow (\\lambda - \\mu)\\left\\langle x_{\\lambda} \\middle| x_{\\mu} \\right\\rangle = 0\\underbrace{\\ \\  \\Longrightarrow \\ \\ }_{\\lambda \\neq \\mu}x_{\\lambda}\\bot x_{\\mu}\\ .$$\n\n*Note*: Given any self-adjoint operator, its eigenvalue spectrum is the reals, $\\lambda\\mathbb{\\in R}$, a spectral superposition of point discrete spectrum eigenvalues and a continuous spectrum eigenvalues [Edgar Raymond Lorch (2003). Chapter V: The Structure of Self-Adjoint Transformations. Spectral Theory, p. 106](https://books.google.com/books?id=X3U2AAAACAAJ) and [these notes](https://www-users.cse.umn.edu/~garrett/m/fun/notes_2012-13/06b_examples_spectra.pdf). Hence, a kime-operator can't be self-adjoint (Hermitian), as in general,\n\n$$\\forall\\theta \\in \\lbrack - \\pi,\\ \\pi)\\backslash\\text{\\{}0\\},\\ \\ \\kappa = te^{i\\theta}\\mathbb{\\in C\\backslash R\\ .}$$\n\nEven though, on average, the phase-distributions are zero mean, i.e., they have trivial expectations $\\left\\langle f_{\\theta} \\right\\rangle = 0$. This ties up to the fact that, in the most general case, if a complex number $\\lambda=a+i\\beta$ is an eigenvalue for a bounded linear operator $\\mathcal{M}$, then its conjugate $\\lambda=a-i\\beta$ is also an eigenvalue of the same operator. In other words, if $\\lambda \\in \\sigma\\left( \\mathcal{M} \\right)$, then $\\overline{\\lambda} \\in \\sigma\\left( \\mathcal{M} \\right)$. Hence, there is a balance between operator eigenvalues corresponding to positive and negative kime-phase arguments, $\\theta \\in \\lbrack - \\pi,\\pi)$, and therefore the phase distribution $\\Phi_{\\theta}$ is *symmetric* and *zero mean*.\n\nClassically, only self-adjoint (*Hermitian*) linear operators, with real eigenvalues, are considered as (real) observable values.\n\nCould the *expectation value of a kime operator* $\\left\\langle \\widehat{Κ} \\right\\rangle$ be equivalent to the classical *time-evolution operator* $\\widehat{U}(t) = e^{- \\frac{i\\widehat{H}t}{\\hslash}}$?\n\n*Note*: About the spectrum of an infinite-dimensional operator, which contains the eigenvalues -- some of the spectrum elements may not be eigenvalues. Let's start with a bounded operator $Κ:X \\rightarrow X$ on the Banach space $X$, e.g., $X \\equiv L^{2}\\mathbb{(R)}$. The *spectrum* $\\sigma(Κ)$ is a disjoint union of 3 parts:\n\n-   The *point spectrum of eigenvalues*: $\\lambda \\in \\sigma(Κ)$, with $Κ - \\lambda I = 0$,\n-   The *continuous spectrum*: $\\lambda \\in \\sigma(Κ)$, with $Κ - \\lambda I \\neq 0$, and the range of $Κ - \\lambda I$ is dense in $X$, but it is not equal to $X$, and\n-   The *residual spectrum*: of $\\lambda \\in \\sigma(Κ)$, $Κ - \\lambda I \\neq 0$, and the range of $Κ - \\lambda I$ is *not dense* in $X$.\n\nThe entire spectrum is always non-empty, $\\sigma(Κ) \\neq \\varnothing$, there are operators with empty point spectrum. For instance, we saw the multiplication operator earlier. $X \\equiv L^{2}\\lbrack 0,1\\rbrack$ and $M:X \\rightarrow X$ is the multiplication operator $(Mf)(x) = x \\cdot f(x),\\forall x \\in \\lbrack 0,1\\rbrack$, then $\\sigma(Κ) \\equiv \\lbrack 0,1\\rbrack$, and each $\\lambda \\in \\lbrack 0,1\\rbrack$ is in the continuous spectrum of the operator $M$.\n\nHere is an example of a family of *non-Hermitian linear operators* $Κ_{A}:X \\rightarrow X$ with properly complex eigenvalue spectrum, $\\kappa = te^{i\\theta}\\mathbb{\\in C\\backslash R}$:\n\n$$Κ_{A}(x) = AxA^{\\dagger}\\ ,$$\n\nwhere $A$ is not Hermitian, $A^{\\dagger} \\neq A$ and their complex eigenvalues are $\\lambda_{i} \\in \\sigma(A)$, ${\\overline{\\lambda}}_{j} \\in \\sigma(A^{\\dagger})$. Then, the eigenvector $|u_{i}\\rangle\\langle u_{j}|$ corresponds to the complex eigenvalue $\\lambda_{i} \\cdot {\\overline{\\lambda}}_{j}\\mathbb{\\in C}$, where\n\n$$A\\left| u_{i} \\right\\rangle = \\lambda_{i}\\left| u_{i} \\right\\rangle,\\ \\ \\ and\\ \\ \\ A^{\\dagger}\\left| u_{j} \\right\\rangle = \\lambda_{j}\\left| u_{j} \\right\\rangle\\ ,\\ \\ \\ i \\neq j.\\ $$\n\n[This paper](https://doi.org/10.1007/s00220-009-0883-4) of Ari Laptev and Oleg Safronov computes eigenvalue estimates for Schrodinger operators with complex potentials. Also, [this blog post](https://math.stackexchange.com/questions/1173511/counterexamples-for-every-linear-map-on-an-infinite-dimensional-complex-vector) shows interesting counterexamples for linear operators on infinite dimensional vector spaces and their eigenvalues, e.g.,\n\n-   The Volterra operator $(Vf)(x) = \\int_{0}^{x}{f(t)dt}$ maps continuous functions to continuous functions, but $V$ has trivial null space, $(Vf)(x) = 0,\\forall x\\  \\Longrightarrow f(x) = 0,\\forall x$, So $\\lambda = 0 \\notin \\sigma(V)$. Also, if $0 \\neq \\lambda \\in \\sigma(V)$, then $Vf = \\lambda f \\Longrightarrow f = \\frac{1}{\\lambda}Vf$ and $f' = \\frac{1}{\\lambda}(Vf)' \\equiv \\frac{1}{\\lambda}f$. Since, $f(0) = 0 \\Longrightarrow f = 0$, we have $\\sigma(V) = \\{\\varnothing\\}$.\n-   The multiplicative operator we saw earlier, for any $f(x) \\in C\\left( \\lbrack 0,1\\rbrack \\right)$,\n\n$$M_{f}h(x) = f(x)h(x):L^{2}\\left( \\lbrack 0,1\\rbrack \\right) \\rightarrow L^{2}\\left( \\lbrack 0,1\\rbrack \\right)\\ .$$\n\nNote the critical advantage of this kime-operator $\\widehat{\\mathbf{\\kappa}}$ *formulation* -- the importance of removing the main drawback of the corresponding time operator $\\widehat{t} = \\frac{mx}{p} = \\frac{m}{2}({\\widehat{p}}^{\\frac{1}{2}}\\widehat{x} - \\widehat{x}{\\widehat{p}}^{\\frac{1}{2}})$ which is singular in momentum $p$-space!\n\nThe kime-operator $\\widehat{\\kappa}$ is well defined for any pair of *position* $\\widehat{x}$ and *mokiphase* $\\widehat{q - \\theta}$, where the momentum-kime-phase (*mokiphase*) operator reflects the difference between the phases of the momentum ($q$) and kime ($\\theta$).\n\nNext, we'll try to prove the enigmatic uncertainty for energy and kime (extending time), $\\Delta E\\ \\Delta\\kappa \\sim \\hslash$. In general, for a pair of non-commuting observables, $A,B$, Heisenberg uncertainty relation is\n\n$$\\Delta A\\ \\Delta B \\geq \\frac{1}{2}\\ \\left| \\left\\langle \\left\\lbrack \\widehat{A},\\ \\widehat{B} \\right\\rbrack \\right\\rangle \\right|\\ .$$\n\nThe momentum encodes the bi-directional speed of the moving particle (electron) indicating its rates of change in position, relative to the two kime directions, as observed from a particular frame of reference and as measured by a particular kime coordinate framework.\n\n## Eigenvalues of Bounded Linear Operators\n\nLet's clarify two facts about bounded linear operators.\n\n*Lemma 1*: *Bounded linear Hermitian operators have only real eigenvalues*.\n\n*Proof*: Suppose $\\hat{M}$ is a bounded linear Hermitian operator. Let $\\lambda$ be an eigenvalue of $\\hat{M}$, and $\\mu$ be the corresponding eigenvector for $\\lambda$. Since $\\hat{M}$ is Hermitian, $$\\left\\langle \\mu \\middle| \\hat{M}\\mu \\right\\rangle = \\left\\langle \\hat{M}\\mu \n\\middle| \\mu \\right\\rangle .$$\n\nUsing the properties of eigenvalues, we have $$\\left\\langle \\mu \\middle| \\hat{M}\\mu \\right\\rangle = \\left\\langle \\mu\\middle| \\lambda\\mu\\right\\rangle = \\lambda\\left\\langle \\mu \\middle|\\mu \\right\\rangle $$ and $$\\left\\langle \\hat{M}\\mu \\middle| \\mu \\right\\rangle = \\left\\langle \\lambda\\mu\\middle| \\mu\\right\\rangle = \\lambda^*\\left\\langle \\mu \\middle|\\mu \\right\\rangle $$\n\nAs the above equations are exactly equal, it follows that $$\\lambda = \\lambda^* ,$$ which implies that the eigenvalue $\\lambda$ must be real.\n\n*Lemma 2*: *Linear operators defined on a finite dimensional complex Hilbert space (i.e., operators that have matrix representation with real matrix elements) have complex-eigenvalues that are conjugate pairs*.\n\n*Proof* Denote by $A$ an arbitrary linear operator on a finite vector space which can be represented a matrix with real entries. By definition, the eigenvalues of $A$ are the solutions of $\\lambda$ in the characteristic equation $$\\det(A - \\lambda I)=0 ,$$ where $I$ is the identity matrix. Since all the coefficients in the equation are real, the complex roots of the characteristic equation, i.e., the eigenvalues of $A$, must appear as conjugate pairs.\n\n*Notes*:\n\n-   The linear operators defined on a finite vector space that have non-real entries may not have conjugate pairs of complex eigenvalues. An example is the matrix $$\\begin{pmatrix}\n     1 & 0 \\\\\n     0 & i\n      \\end{pmatrix}\n    $$ It has complex eigenvalues $1$ and $i$, which are *not* conjugate pairs.\n\n-   The situation with the matrix-operators above, representing finite dimensional operators over Hilbert spaces, is different for *infinite dimensional* spectrum bounded linear operators, which may not necessary have eigenvalues! Consider for instance, the right-shift operator. However, self-disadjoint and compact operators will always have eigenvalues, e.g., the [Sturm-Liouville operator](https://en.wikipedia.org/wiki/Sturm%E2%80%93Liouville_theory).\n\n## Notes on Dirac's bra-ket notation\n\n-   The vector (Hilbert) space $X$ is over the complex filed $\\mathbb{C}$, and linear operators $\\widehat{T}:X\\mathbb{\\rightarrow C}$.\n-   We use bra-kets to distinguish between *vectors* $\\left| \\psi \\right\\rangle \\in X$ (*kets*) and their duals (*bras*) $\\langle\\phi| \\in X^{*}$, $\\langle\\phi| \\equiv \\left| \\phi \\right\\rangle^{\\dagger}$.\n-   *Inner-products* are *scalars* in the base-field, $\\left\\langle \\phi \\middle| \\psi \\right\\rangle \\equiv \\left\\langle \\psi \\middle| \\phi \\right\\rangle^{*}\\mathbb{\\in C}$.\n-   *Outer-products* are *linear operators*, in the dual-space, $\\widehat{T} = \\left| \\psi \\right\\rangle\\langle\\phi|$, ${\\widehat{T}}^{\\dagger} = \\left| \\phi \\right\\rangle\\langle\\psi|$, where $\\widehat{T}\\left| \\gamma \\right\\rangle \\equiv \\left| \\psi \\right\\rangle\\left\\langle \\phi \\middle| \\gamma \\right\\rangle \\equiv \\underbrace{\\left\\langle \\phi \\middle| \\gamma \\right\\rangle}_{scalar}\\left| \\psi \\right\\rangle$.\n-   The *adjoint* is $\\left( \\widehat{T}|\\psi\\rangle \\right)^{\\dagger} = \\langle\\psi|{\\widehat{T}}^{\\dagger}$, $\\left( {\\widehat{T}}^{\\dagger} \\right)^{\\dagger} \\equiv \\widehat{T}$, $\\left( {\\widehat{T}}^{\\dagger}|\\psi\\rangle \\right)^{\\dagger} = \\langle\\psi|\\widehat{T}$, and $\\langle\\psi|\\widehat{T}\\left| \\phi \\right\\rangle^{\\dagger} = \\langle\\phi|{\\widehat{T}}^{\\dagger}|\\psi\\rangle$.\n-   Operator *characteristic equation* $\\widehat{T}\\left| \\psi \\right\\rangle = \\lambda|\\psi\\rangle$.\n-   *Chance of basis*, $|x\\rangle \\longleftrightarrow$ $|p\\rangle$\n-   *Position space representation ...*\n\n$$\\left| \\psi \\right\\rangle = \\widehat{\\mathbb{I}}\\left| \\psi \\right\\rangle = \\int_{- \\infty}^{\\infty}{|x\\rangle}\\left\\langle x \\middle| \\psi \\right\\rangle dx = \\int_{- \\infty}^{\\infty}{\\psi(x)|x\\rangle}dx\\ .$$\n\n-   *Momentum space representation ...*\n\n$$\\left| \\psi \\right\\rangle = \\widehat{\\mathbb{I}}\\left| \\psi \\right\\rangle = \\int_{- \\infty}^{\\infty}{|p\\rangle}\\left\\langle p \\middle| \\psi \\right\\rangle dp = \\int_{- \\infty}^{\\infty}{\\psi(p)|p\\rangle}dp\\ .$$\n\n-   *Chance of basis (position-momentum bases)* $\\left\\langle x \\middle| p \\right\\rangle = \\frac{1}{\\sqrt{2\\pi\\hslash}}e^{i\\frac{p \\cdot x}{\\hslash}}$\n\n$\\psi(x) = \\left\\langle x \\middle| \\psi \\right\\rangle = \\left\\langle x \\middle| \\widehat{\\mathbb{I}}|\\psi \\right\\rangle = \\int_{- \\infty}^{\\infty}{\\psi(p)\\langle x|p\\rangle}dp = \\int_{- \\infty}^{\\infty}{\\psi(p)\\frac{1}{\\sqrt{2\\pi\\hslash}}e^{i\\frac{p \\cdot x}{\\hslash}}}dp$\n\n(IFT), and similarly,\n\n$\\phi(p) = \\left\\langle p \\middle| \\phi \\right\\rangle = \\left\\langle p \\middle| \\widehat{\\mathbb{I}}|\\phi \\right\\rangle = \\int_{- \\infty}^{\\infty}{\\phi(x)\\langle p|x\\rangle}dx = \\int_{- \\infty}^{\\infty}{\\phi(x)\\frac{1}{\\sqrt{2\\pi\\hslash}}e^{- i\\frac{p \\cdot x}{\\hslash}}}dx$ (FT).\n\nIn the eigenvector bases (for the position & momentum),\n\n-   Position operator\n\n$$\\underbrace{\\ \\ \\widehat{x}\\ \\ }_{operator}\n\\overbrace{\\ \\ \\left| x \\right\\rangle\\ \\ }^{eigenstate} = \n\\underbrace{\\ \\ x\\ \\ }_{eigenvalue}|x\\rangle \\Longrightarrow \\langle x'|\n\\widehat{x}\\left| x \\right\\rangle = x\\left\\langle x' \\middle | x \\right\\rangle\n= x\\delta\\left( x - x' \\right),$$\n\n$$\\widehat{x} = \\int_{- \\infty}^{\\infty}{x |x\\rangle \\langle x|}dx\\ .$$\n\n-   Momentum operator\n\n$$\\widehat{p}\\left| p \\right\\rangle p|p\\rangle \\Longrightarrow \\langle p'|\\widehat{p}\\left| p \\right\\rangle = p\\left\\langle p' \\middle| p \\right\\rangle = p\\delta\\left( p - p' \\right)\\ ,$$\n\n$$\\widehat{p} = \\int_{- \\infty}^{\\infty}{p |p\\rangle  \\langle p|}dp\\ .$$\n\n-   In position space representation, the momentum operator is\n\n$$\\langle x'|\\widehat{p}\\left| x \\right\\rangle \n\\underbrace{\\ \\ =\\ \\ }_{\\underbrace{\\hat{p}}_{operator}\\underbrace{|p\\rangle}_{state}=\n\\underbrace{p}_{eigenvalue}|p\\rangle}\n\\int_{- \\infty}^{\\infty}{p\\left\\langle x' \\middle| p \\right\\rangle\\left\\langle p \\middle| x \\right\\rangle}dp = \\int_{- \\infty}^{\\infty}{p\\frac{1}{2\\pi\\hslash}e^{- i\\frac{p \\cdot \\left( x - x' \\right)}{\\hslash}}}dp =$$\n\n$$\\frac{1}{2\\pi\\hslash}\\left( - \\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}\\ \\ \\int_{- \\infty}^{\\infty}{e^{- i\\frac{p \\cdot \\left( x - x' \\right)}{\\hslash}}}dp \\right) = \\frac{1}{2\\pi\\hslash}\\left( \\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}\\left( 2\\pi\\delta\\left( \\frac{x - x'}{\\hslash} \\right) \\right)\\  \\right) =$$\n\n$$- \\frac{\\hslash}{i}\n\\frac{\\partial}{\\partial x}\\ \\delta\\left ( x - x' \n\\right ) .$$\n\nNote that the derivative of the Heaviside function is the Dirac distribution function, $H(x-a)=\\int_{-\\infty}^{x}{\\delta(x-a)dx}$, and $\\langle x| p\\rangle =\\frac{1}{\\sqrt{2\\pi \\hbar}} e^{i\\frac{xp}{\\hbar}}$, $\\langle p| x\\rangle =\\frac{1}{\\sqrt{2\\pi \\hbar}} e^{-i\\frac{xp}{\\hbar}}$, and the Fourier transform $\\int_{-\\infty}^{\\infty}{e^{-iwx}\\cdot x\\ dx}=i\\frac{\\partial}{\\partial w}\\int_{-\\infty}^{\\infty}{e^{-iwx}\\ dx}$.\n\n-   To formally derive expressions of the position operator, $\\widehat{x} \\equiv \\frac{\\hslash}{i}\\frac{\\partial}{\\partial p}$, and the momentum operator, $\\widehat{p} \\equiv \\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}$, we expand the operators in the complementary basis.\n\n$$\\langle x|\\widehat{p}\\left| \\psi \\right\\rangle = \\int_{- \\infty}^{\\infty}{\\left\\langle x \\middle| \\widehat{p}|x' \\right\\rangle\\left\\langle x' \\middle| \\psi \\right\\rangle}dx' =$$\n\n$$\\int_{- \\infty}^{\\infty}{\\delta\\left( x' - x \\right)\\frac{\\hslash}{i}\\frac{\\partial}{\\partial x'}\\psi\\left( x' \\right)}dx\n\\overbrace{\\underbrace{\\ \\  = \\ \\ }_{by\\ parts}}^{\\Psi(x)=\\langle x|\\Psi\\rangle}\n\\underbrace{-\\frac{\\hslash}{i}\\delta(x'-x)\\Psi(x') |_{-\\infty}^{\\infty}}_{0} +\n\\frac{\\hslash}{i}\\int_{-\\infty}^{\\infty}{\\delta\\left( x' - x \\right)\\frac{\\partial}{\\partial x'}\\psi\\left( x' \\right)}dx'=\\\\\n\\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}\\psi\\  \\Rightarrow \\ \\widehat{p} = \\frac{\\hslash}{i}\\frac{\\partial}{\\partial x}\\ .$$\n\n-   Similarly, in momentum space, the position operator is\n\n$$\\langle p|\\widehat{x}\\left| p' \\right\\rangle = \\cdots = \\frac{\\hslash}{i}\n\\underbrace{\\ \\ \\frac{\\partial}{\\partial p}\\ \\delta\\left( p - p' \n\\right)\\ \\ }_{Heaviside\\ function} \\Rightarrow \\langle x|\\widehat{p}\n\\left| \\psi \\right\\rangle = \\cdots = \n- \\frac{\\hslash}{i}\\frac{\\partial}{\\partial p}\n\\underbrace{\\ \\ \\widetilde{\\psi}\\ \\ }_{FT(\\psi)}\\ $$\n\n$$\\Rightarrow \\ \\widehat{x} = - \\frac{\\hslash}{i}\\frac{\\partial}{\\partial p}\\ .$$",
      "word_count": 3289
    },
    {
      "title": "Kime-Phase Operator and Kime Operator",
      "content": "## Approach 1 - Transforming Time Addition to Kime Multiplication\n\n*Problem*: Explore transforming the *additive group* over $t\\in\\mathbb{R}$ to a \n*multiplicative group* $\\kappa=te^{i\\theta}\\in\\mathbb{C}$, where $\\forall\\ \\psi\\in\\mathcal{H}$, the $1:1$ correspondence between \n*one parameter kime continuous unitary groups of operators*, i.e., \n*operator-valued distributions*, $U^A :\\mathcal{H}\\to\\mathcal{H}$ and \n*self-adjoint operators* \n$A^U :\\mathcal{H}\\to\\mathcal{H}$ is given by\n\n$$\\overbrace{A^U \\psi}^{operator-valued\\\\ distribution} = \\lim_{t\\equiv|\\kappa|\\to 0}{\\frac{U^{A^U}_{\\kappa}(\\psi) - \\psi}{it}}\\\\\n\\overbrace{U^{A^U}_{\\kappa}\\underbrace{(\\varphi)}_{test\\\\ function}}^{kime\\ unitary\\ operator}=\n\\underbrace{e^{-itA^U}}_{operator} \\ \\ \\underbrace{\\ell\\left(e^{i\\theta}\\right)}_{distribution} \\varphi\\ ,$$\n\nwhere the action of the *kime-inference function on test functions*,\n$\\langle\\mathfrak{P}, \\phi\\rangle,$ is defined by the kime-phase action on \ntest functions, $\\langle\\ell, \\phi\\rangle = \\int_{\\mathbb{R}}{\\ell^*(\\theta) \\phi(\\theta)}\\ d\\theta \\in\\mathbb{C}$,\n\n$$\\underbrace{\\langle\\mathfrak{P},\n\\phi\\rangle}_{kime-function\\\\ action} =\n\\overbrace{\\Psi(x,y,z,t)}^{spacetime\\\\ wavefunction}\n\\underbrace{\\int_{\\mathbb{R}}{\\ell^*(\\theta) \\phi(\\theta)}\\\nd\\theta}_{\\underbrace{\\langle\\ell, \\phi\\rangle}_{kime-phase\\\\\naction}\\in\\mathbb{C}}\\ .$$\n\nTechnically, $\\ell(\\theta)\\equiv \\ell(e^{i\\theta})$.\n\nThese one parameter kime unitary operators $\\{U_{\\kappa}\\ |\\ \\kappa\\in\\mathbb{C} \\}$ are *continuous*\n$$\\forall \\kappa_{o}\\in \\mathbb {C} ,\\ \\psi \\in {\\mathcal {H}}:\\ \\lim _{\\kappa\\to \\kappa_o}U_{\\kappa}(\\psi )=U_{\\kappa_o}(\\psi),$$\n\nTo simplify all notation, we will be suppressing the extra (unnecessary) superscripts\nsignifying the $U\\equiv U^{A^U} \\longleftrightarrow A^U\\equiv A$ correspondence.\n\n$$\\forall \\kappa_1=t_1e^{i\\theta_1},\\ \\kappa_2=t_2e^{i\\theta_2}\\in \\mathbb {C} ,\\\\ U_{\\kappa_1\\cdot \\kappa_2}=\nU_{t_1e^{i\\theta_1}\\cdot t_2e^{i\\theta_2}} = U_{t_1 t_2 e^{i(\\theta_1+\\theta_2)}} =\n\\underbrace{e^{-i(t_1 t_2)A}}_{operator} \\ \\ \\underbrace{\\ell\\left(e^{i(\\theta_1+\\theta_2)}\\right)}_{distribution}\\\\\n\\overbrace{=}^{\\theta_1\\perp \\theta_2\\\\ \\ell,\\ separable} \\underbrace{e^{-i(t_1)A}\\ \\ell_1\\left(e^{i\\theta_1}\\right)}_{U_{\\kappa_1}} \\ \\ \\underbrace{e^{-i(t_2)A}\\ \\ell_2\\left(e^{i\\theta_2}\\right )}_{U_{\\kappa_2}}=\nU_{\\kappa_1}U_{\\kappa_2}.$$\n\nRecall that $\\forall\\ \\kappa=te^{i\\theta}\\in\\mathbb{C},$ $U_{\\kappa}$ \nis an *operator-valued distribution* acting on *test functions*\n$\\varphi\\in\\mathcal{H}$ and producing complex scalars\n\n$$\\underbrace{U_{\\kappa_1\\cdot \\kappa_2}(\\varphi)}_{\\in\\mathbb{C}}=\nU_{\\kappa_1}(\\varphi) \\cdot U_{\\kappa_2}(\\varphi)\\ .$$\n\nTo explicate the kime-dynamics of states at any kime $\\kappa\\in\\mathbb{C}$,\nconsider an initial state $|\\varphi_{\\kappa_o}\\rangle$. Without loss of generality, we can assume that $\\kappa_o=te^{i\\theta}=0$, i.e., $t=0$. So, the starting\ninitial state is $|\\varphi_{\\kappa_o}\\rangle\\equiv |\\varphi_{o}\\rangle$.\n\nAs the state at kime $\\kappa\\in\\mathbb{C}$ is measurable, the temporal dynamics\nof the system can be expressed in terms of the kime unitary operator group action\n\n$$|\\varphi_{\\kappa}\\rangle=U_{\\kappa}(|\\varphi_{o}\\rangle)=\n\\underbrace{e^{-i t A}}_{operator} \\ \\ \\underbrace{\\ell\\left(e^{i\\theta}\\right)}_{distribution} (|\\varphi_{o}\\rangle)\\ ,$$\n\nwhere $\\ell\\left(e^{i\\theta}\\right)$ is a (prior) model of the kime-phase distribution, which can be *sampled once* for single observations, or \n*sampled multiple times* corresponding to multiple *repeated measurements.*\n\nTaking the partial derivative of $|\\varphi_{\\kappa}\\rangle$ with respect to\nthe *kime-magnitude* (*time*, $t$) yields the *kime-Schrodinger equation*\n\n$$\\frac{\\partial |\\varphi_{\\kappa}\\rangle}{\\partial t}=-iA\n\\underbrace{\\ \\ |\\varphi_{\\kappa}\\rangle\\ \\ }_{\\left (e^{-i t A}\\right )\n\\ell\\left(e^{i\\theta}\\right) (|\\varphi_{o}\\rangle)} \\ .$$\n\nFor instance, consider a *free evolution* (no external forces), \nwhere we are modeling the *energy (Hamiltonian) kime-evolution* of the \nstate of a particle of mass $m$ in $1D$. Assume only *kinetic energy* $K$ \nis at play, without *potential energy*, $V=0$. Then, the energy \n*Hamiltonian operator* $H=A$ is self-adjoint\n\n$$A\\equiv H=-\\frac{1}{2m}\\frac{d^2}{dx^2}\\ .$$\n\nIn this case, the explicit form of the *kime-independent Schrodinger equation*\ndescribing the *physical state of a quantum-mechanical system* is\n\n$$\\frac{\\partial |\\varphi_{\\kappa}\\rangle}{\\underbrace{\\partial t}_{t=|\\kappa|}}=-iA\n\\underbrace{\\ \\ |\\varphi_{\\kappa}\\rangle\\ \\ }_{\\left (e^{-i t A}\\right )\n\\ell\\left(e^{i\\theta}\\right) (|\\varphi_{o}\\rangle)} =\n+i\\frac{1}{2m}\\frac{\\partial^2}{\\partial x^2} |\\varphi_{\\kappa}\\rangle\\ .$$\n\nFor simplicity, here we are working with normalized units, $c=\\hbar = 1$. In the more general $3D$ kinetic energy (free potential) case, the system Hamiltonian is\n\n$$A\\equiv H=-\\frac{h{^2}}{8\\pi{^2}m}\\left(\\underbrace{\\dfrac{\\partial{^2}}\n{\\partial{x^2}}+\\dfrac{\\partial{^2}}{\\partial{y^2}}+\\dfrac{\\partial{^2}}\n{\\partial{z^2}}}_{Laplacian,\\ \\nabla^2}\\right)\\ .$$\n\nThis *kime-independent Schrodinger equation* has an explicit solution\n$$|\\varphi(\\kappa)\\rangle = \\left (e^{-i t E}\\right )\n\\ell\\left(e^{i\\theta}\\right) (|\\varphi_{o}\\rangle)\\ ,$$\n\nwhere $E$ represent observable energies. Since $\\ell\\left(e^{i\\theta}\\right)$ \nis a kime-phase distribution on $[-\\pi,+\\pi)$ and the observable energies\n(eigenvalues of the Hamiltonian) are finite, this suggests that $\\forall\\ \\psi_o\\in L^2(\\mathbb{C})$\n\n$$||\\psi_{\\kappa}||_{L^{\\infty}} \\underset{t\\to\\infty}{\\ \\longrightarrow 0}\\ .$$\n\nTherefore, as $t\\to\\infty$ the *PDF of the position* vanishes and there is \nno limiting probability distribution for the position of the particle in $1D$\nas it can spreads out across the entire real line.\n\nThis was the solution of the *kime-independent Schrodinger equation* in \n*position coordinates*. Let's consider the same *free evolution* (kinetic energy only)\nin *momentum coordinates.* Recall that the Fourier transform provides a \nlinear bijective mapping between *position coordinates*, spacetime representation,\n$\\varphi(x),$ and *momentum coordinates*, k-space frequency representation, ${\\hat{\\varphi}}(p)$. \n\nAgain for simplicity, we'll consider the momentum of a free particle in $1D$\nand set $c=\\hbar=1$. The solution of the Schrodinger equation in momentum coordinates is\n\n$${\\hat{\\varphi}}_{\\kappa}(p)=\\langle p |{\\hat{\\varphi}}_{\\kappa}\\rangle = \n\\left (e^{-i t p^2}\\right ) \\ell\\left(e^{i\\theta}\\right) \n(\\langle p |{\\hat{\\varphi}}_{\\kappa}\\rangle)=\n\\left (e^{-i t p^2}\\right ) \\ell\\left(e^{i\\theta}\\right) \n({\\hat{\\varphi}}_{o}(p))\\ .$$\n\nClearly, \n$||{\\hat{\\varphi}}_{\\kappa}(p)||^2 \\equiv ||{\\hat{\\varphi}}_{o}(p)||^2,$\nsuggesting that the *momentum PDF is static in kime*.\nFor a free-evolution (no potential energy) this finding is not surprising,\nsince the momentum of a free particle should be preserved. However, \nin real observations, quantum fluctuations (intrinsic randomness) will affect\nrepeated measurements of a given observation (e.g., energy). This intrinsically\nstochastic behavior is modeled by the kime-phase distribution\n$\\ell\\left(e^{i\\theta}\\right)\\sim \\Phi_{[-\\pi,+\\pi)]}$.\n\n## Approach 2 - Formulating a State Evolution Operator\n\nThe kime propagator $K(\\kappa_2,\\kappa_1)$ between two kime points\n$K(\\kappa_2,\\kappa_1) = \\langle x_2|U(\\Delta\\kappa)|x_1\\rangle$ can be\nexpanded to\n$$K(\\kappa_2,\\kappa_1) = \\langle x_2|e^{-iH(t_2e^{i\\theta_2} - t_1e^{i\\theta_1})/\\hbar}|x_1\\rangle$$\n\nThe kime propagator in path integral form is\n$$K(\\kappa_2,\\kappa_1) = \\int \\mathcal{D}[x(\\kappa)] e^{iS[\\kappa]/\\hbar} ,$$\nwhere the complex action is\n$$S[\\kappa] = \\int_{\\kappa_1}^{\\kappa_2} L(x,\\dot{x},\\kappa)d\\kappa$$\n\n**Note**: For the kime-propagator path integral, we may properly (re)define the functional measure for paths in complex time. For instance,\n$$K(\\kappa_2,\\kappa_1) = \\mathcal{N}\\int \\mathcal{D}[x(\\kappa)] e^{iS[\\kappa]/\\hslash},$$\nwhere \\mathcal{N} is a normalization factor,  complex time incremens are\n$\\Delta \\kappa_j = \\kappa_{j+1} - \\kappa_j$, and the measure is\ndefined as a limit of discrete paths\n$$\\mathcal{D}[x(\\kappa)] = \\lim_{N\\to\\infty} \\prod_{j=1}^{N-1} \\sqrt{\\frac{m}{2\\pi i\\hslash \\Delta\\kappa_j}} dx_j.$$\nThe action $S[\\kappa]$ also may need to be (re)defined for complex time paths\n$$S[\\kappa] = \\int_{\\kappa_1}^{\\kappa_2} \\left(\\frac{m}{2}\\frac{dx}{d\\kappa}\\frac{dx}{d\\kappa^*} - V(x)\\right)d\\kappa .$$\n\nHence, the full path integral can be written as\n$$K(\\kappa_2,\\kappa_1) = \\lim_{N\\to\\infty} \\left(\\prod_{j=1}^{N-1} \\sqrt{\\frac{m}{2\\pi i\\hslash \\Delta\\kappa_j}}\\right) \\int\\cdots\\int \\prod_{j=1}^{N-1} dx_j \\times \\\\\n\\exp\\left\\{\\frac{i}{\\hslash}\\sum_{j=0}^{N-1} \\left[\\frac{m}{2}\\frac{(x_{j+1}-x_j)^2}{\\Delta\\kappa_j} - V(x_j)\\Delta\\kappa_j\\right]\\right\\} .$$\nThis formulation properly accounts for the complex nature of kime, includes an appropriate measure normalization, reflects the discrete-to-continuous limit, and properly treats complex conjugate pairs in the kinetic term.\n\nThe measured propagator requires averaging over phase differences\n$$\\overline{K(\\kappa_2,\\kappa_1)} = \\int_{-2\\pi}^{2\\pi} K(\\Delta\\kappa)\\Phi_{\\Delta}(\\Delta\\theta)d(\\Delta\\theta)$$\nThen, the state evolution is\n$$i\\hbar\\frac{\\partial}{\\partial\\kappa}|\\psi(\\kappa)\\rangle = He^{i\\theta}|\\psi(\\kappa)\\rangle$$\nand the corresponding propagator equation becomes\n$$i\\hbar\\frac{\\partial}{\\partial\\kappa_2}K(\\kappa_2,\\kappa_1) = He^{i\\theta_2}K(\\kappa_2,\\kappa_1)$$\n\nFor intermediate kime points, the Chapman-Kolmogorov equation is\n$$K(\\kappa_3,\\kappa_1) = \\int K(\\kappa_3,\\kappa_2)K(\\kappa_2,\\kappa_1)d\\kappa_2$$\nand for ordered kime points, the group property $U(\\kappa_3,\\kappa_1) = U(\\kappa_3,\\kappa_2)U(\\kappa_2,\\kappa_1)$ yields the expectation via statistical averaging\n$$\\overline{U(\\kappa_3,\\kappa_1)} = \\iint U(\\kappa_3,\\kappa_2)U(\\kappa_2,\\kappa_1)\\Phi(\\theta_2)d\\theta_2 .$$\n\nThe kime-ordered propagator explicates the causal structure\n$$K_{\\text{ordered}}(\\kappa_2,\\kappa_1) = \\theta(t_2-t_1)K(\\kappa_2,\\kappa_1) ,$$\nwhere $\\theta(t)$ is the Heaviside function.\n\nThe *free particle* kime propagator becomes\n$$K_0(\\kappa_2,\\kappa_1) = \\sqrt{\\frac{m}{2\\pi i\\hbar\\Delta\\kappa}}\\exp\\left(\\frac{im(x_2-x_1)^2}{2\\hbar\\Delta\\kappa}\\right) .$$\nWhereas the *harmonic oscillator* with frequency $\\omega$ is\n$$K_{\\text{HO}}(\\kappa_2,\\kappa_1) = \\sqrt{\\frac{m\\omega}{2\\pi i\\hbar\\sin(\\omega\\Delta\\kappa)}}\\exp\\left(\\frac{im\\omega}{2\\hbar\\sin(\\omega\\Delta\\kappa)}[(x_2^2+x_1^2)\\cos(\\omega\\Delta\\kappa)-2x_2x_1]\\right)$$\n\nNote that the averaged propagator includes decoherence\n$$\\overline{K(\\kappa_2,\\kappa_1)} = K_{\\text{ideal}}(\\kappa_2,\\kappa_1)\\exp(-\\Gamma[\\Delta\\theta]) ,$$\nwhere $\\Gamma[\\Delta\\theta]$ depends on the phase difference distribution.\n\nNon-Markovian kime evolution is\n$$\\frac{\\partial}{\\partial\\kappa}\\rho(\\kappa) = -\\int_{\\kappa_1}^{\\kappa} M(\\kappa-\\kappa')\\rho(\\kappa')d\\kappa' ,$$\nwhere $M(\\Delta\\kappa)$ is the memory kernel.\nThese snuggest that the kime propagator combines quantum evolution with statistical phase differences, naturally modeling decoherence.\nAlso, the phase difference distribution $\\Phi_{\\Delta}$ determines the strength and nature of quantum-to-classical transition. \n\n### Quantum Measurement in Complex Time\n\nLet $\\kappa\\in\\mathbb{C}$ be complex time (kime),\n$\\kappa= t\\cdot e^{i\\theta}$, where $t\\in\\mathbb{R}^+$ is the classical time \n(kime-magnitude), the kime-phase $\\theta\\sim \\Phi_{[-\\pi,\\pi)}$ is a random variable,\nand $\\Phi_{[-\\pi,\\pi)}$ is a probability distribution supported on $[-\\pi,\\pi)$.\nFor a quantum state $|\\psi (\\kappa)\\rangle$, the time evolution operator (quantum state evolution), is \n$U(\\kappa) = e^{-iH\\kappa /\\hbar} = e^{-iHte^{i\\theta}/ \\hbar}$,\nwhere $H$ is the system Hamiltonian.\n\nFor a single observable $O$, the expectation value at kime $\\kappa$ is\n$\\langle O(\\kappa)\\rangle = \\langle \\psi(0)|U^{\\dagger}(\\kappa)OU(\\kappa)|\\psi(0)\\rangle$.\nFor $N$ repeated measurements, each corresponding to kime $\\kappa_i$, the *expected observable measure* is\n$\\langle O(\\kappa)\\rangle_{mean} = \\frac{1}{N} \\sum_{i=1}^N \\langle O(\\kappa_i)\\rangle$.\n\nThe *variance* due to kime-phase fluctuations is\n$\\sigma^2_{kime} = \\langle(O(\\kappa) - \\langle O\\rangle_{mean})^2\\rangle$.\n\nLet's explicate the main sources of variability: (1) Environmental coupling introduces phase noise in $\\theta$; (2) Control pulse imperfections affect the kime-magnitude $t$; and (3) Decoherence effects modify the effective Hamiltonian.\nThe total measurement error can be decomposed as\n$\\varepsilon_{total}^2 = \\varepsilon_{intrinsic}^2 + \\varepsilon_{kime}^2$, where\n$\\varepsilon_{intrinsic}$ is the theoretical quantum projection noise and $\\varepsilon_{kime}$ is the additional uncertainty from kime fluctuations (repeated sampling dispersion).\n\n*Measurement accuracy discrepancies* may be explained by phase uncertainty (random fluctuations in $\\theta$ lead to measurement spreading or environmental noise couples to the $\\theta$ distribution width), temporal correlations (reflective successive measurements have correlated kime-phases or time-dependent systematic errors accumulate), and decoherence effects \n(complex time evolution modifications of the decoherence rates or emergence of non-Markovian effects from kime-phase memory).\nPotential mitigation strategies may involve phase tracking (monitoring $\\theta$ distribution changes or implementing adaptive measurement protocols), error budgeting (separating kime-induced vs. intrinsic errors or appropriately optimizing the measurement sequences), or by alternative protocol designs (employing kime-aware measurement sequences or implementing error mitigation based on $\\theta$ statistics).\n\n### State Evolution in Complex Time\n\nThe kime evolution operator may expressed as\n$$U(\\kappa) = e^{-iHte^{i\\theta}/\\hbar} = e^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hbar}.$$\nIn general, the following expansion may *not* be valid\n$$U(\\kappa) = e^{-iHt\\cos\\theta/\\hbar}e^{Ht\\sin\\theta/\\hbar}.$$\nThe above split of the exponential is violating quantum mechanical principles.\nSuch exponential operator expansions involve spacial sign-handling and *commutators*, see the Baker-Campbell-Hausdorff (BCH) formula. For quantum mechanics,\n$$U(\\kappa) = e^{-iHte^{i\\theta}/\\hslash} = \\\\\ne^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hslash} \\not=\ne^{-iHt\\cos\\theta/\\hslash}e^{Ht\\sin\\theta/\\hslash}.$$\nFirst, for non-commuting operators $A$ and $B$,\n$e^{A+B} \\neq e^A e^B$.\n\nThe BCH formula generalizes numerical series expansions in terms of operator commutators, $[A,B] \\equiv AB - BA$,\n$$e^{A+B} = e^A e^B e^{-\\frac{1}{2}[A,B]} e^{\\frac{1}{12}([A,[A,B]]-[B,[A,B]])} \\cdots $$\n\nSecond, correct complex exponential decompositions is\n$$U(\\kappa) = e^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hslash} = e^{-iHt\\cos\\theta/\\hslash}e^{-iHt(i\\sin\\theta)/\\hslash} = e^{-iHt\\cos\\theta/\\hslash}e^{-Ht\\sin\\theta/\\hslash} .$$\nThe crucial difference is the *negative sign* in the second exponential.\nThis is because\n$e^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hslash} = e^{(-iHt\\cos\\theta - Ht\\sin\\theta)/\\hslash}.$ When splitting this into two exponentials (which is valid here because the two operators commute), we have\n$$e^{-iHt\\cos\\theta/\\hslash}e^{-Ht\\sin\\theta/\\hslash}.$$\nA more rigorous derivation would involve the *spectral decomposition* of \nthe operator $H$ (total energy) in terms of its eigenvalues $E_n$, observable\nenergies, and the corresponding *eigenstates*, $|n\\rangle$\n$$U(\\kappa) = \\sum_n e^{-iE_n te^{i\\theta}/\\hslash} |n⟩⟨n|$$\n\nFor a Uniform distribution, $\\theta \\sim \\text{Unif}(-\\pi,\\pi)$,\n$\\Phi_U(\\theta) = \\frac{1}{2\\pi}, \\quad \\theta \\in [-\\pi,\\pi)$.\n\nExpected evolution operator is\n$$\\mathbb{E}(U)=\\overline{U(\\kappa)}_U = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} e^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hbar}d\\theta \\equiv \\underbrace{J_0(Ht/\\hbar)}_{Bessel\\ function}.$$\n\nFor a (truncated) Normal Distribution, $\\theta \\sim \\mathcal{N}(0,\\sigma^2)$ \nsupported on $[-\\pi,\\pi]$, \n$\\Phi_N(\\theta) = \\frac{1}{Z_N}\\exp(-\\theta^2/2\\sigma^2), \\quad \\theta \\in [-\\pi,\\pi)$,\nwhere $Z_N$ is the normalization constant.\nThen, the expected evolution operator is\n$$\\mathbb{E}(U)=\\overline{U(\\kappa)}_N = \\frac{1}{Z_N}\\int_{-\\pi}^{\\pi} e^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hbar}e^{-\\theta^2/2\\sigma^2}d\\theta .$$\n\nFinally, for a (truncated) Laplace distribution, $\\theta \\sim \\text{Laplace}(0,b)$\nlimited to $[-\\pi,\\pi]$,\n$\\Phi_L(\\theta) = \\frac{1}{Z_L}e^{-|\\theta|/b}, \\quad \\theta \\in [-\\pi,\\pi)$.\nAnd the expected evolution operator would be\n$$\\mathbb{E}(U)=\\overline{U(\\kappa)}_L = \\frac{1}{Z_L}\\int_{-\\pi}^{\\pi} e^{-iHt(\\cos\\theta + i\\sin\\theta)/\\hbar}e^{-|\\theta|/b}d\\theta .$$\n\nWe need to examine some of the physical properties, such as various conservation laws. \nFor a *single realization* of $\\theta$\n$$\\langle\\psi(\\kappa)|\\psi(\\kappa)\\rangle = \\langle\\psi(0)|U^\\dagger(\\kappa)U(\\kappa)|\\psi(0)\\rangle = \\langle\\psi(0)|\\psi(0)\\rangle .$$\n\nHowever, when averaging over the $\\theta$ phase distribution, the norm\n$$||\\psi||^2=\\overline{\\langle\\psi(\\kappa)|\\psi(\\kappa)\\rangle} = \\int_{-\\pi}^{\\pi} \\langle\\psi(0)|U^\\dagger(\\kappa)U(\\kappa)|\\psi(0)\\rangle\\Phi(\\theta)d\\theta \\leq \\langle\\psi(0)|\\psi(0)\\rangle .$$\nSimilarly, the *energy expectation* value is\n$$E(\\kappa) = \\langle\\psi(\\kappa)|H|\\psi(\\kappa)\\rangle = \\langle\\psi(0)|U^\\dagger(\\kappa)HU(\\kappa)|\\psi(0)\\rangle ,$$\nwhich is not conserved due to the complex nature of kime.\nThe *time-energy uncertainty* relation becomes\n$\\Delta E \\Delta\\kappa \\geq \\frac{\\hbar}{2}|e^{i\\theta}|$.\nHence, the kime evolution exhibits apparent causality violation due to\nthe complex time components allowing *imaginary propagation*, phase distribution width affects on the temporal ordering, and non-local correlations in repeated measurements.\n\nPotential violations include\n\n1. *Unitarity*: Individual trajectories are unitary, but ensemble averages are not\n   $\\overline{U^\\dagger(\\kappa)U(\\kappa)} \\neq I$.\n\n2. *Energy Conservation*: Complex time evolution allows temporary energy fluctuations\n   $\\frac{d}{d\\kappa}\\overline{E(\\kappa)} \\neq 0$.\n\n3. *Causality*: Complex time components suggest apparent backwards propagation\n   $\\text{Sign}[\\text{Im}(\\kappa)] \\text{ determines temporal direction}$.\n\nPossible resolutions to some of these violations of core physical principle include\n(1) *Statistical interpretations* (reflecting physical measurements as ensemble averages, resolving apparent violations with properly averaging (expectation), and non-unitary behavior models decoherence); (2) *Measurement theoretic interpretations* (using the phase distribution width $\\Delta\\theta$ as measurement uncertainty, utilizing broader distributions that lead to stronger decoherence, or limit-theoretic recovery of standard quantum mechanics as $\\Delta\\theta \\to 0$); and (3) *Consistency conditions* (restrictions ensuring physical consistency, e.g., *symmetric* phase distributions, $\\Phi(\\theta) = \\Phi(-\\theta)$, compact-supported or finite-variance phase distributions, $\\int {\\theta^2 \\Phi(\\theta) d\\theta < \\infty}$, and renormalization over $[-\\pi,\\pi)$).\n\n\n## Complex-time Representations and Prediction Enhancements\n\nExtending the standard Schwarzschild metric using kime yields\n$$ds^2 = -c^2\\left(1-\\frac{2GM}{rc^2}\\right)d\\kappa d\\kappa^* + \\left(1-\\frac{2GM}{rc^2}\\right)^{-1}dr^2 + r^2(d\\theta^2 + \\sin^2\\theta d\\phi^2),$$\nwhere $\\kappa = te^{i\\theta}$ and its conjugate $\\kappa^* = te^{-i\\theta}$.\n\nConsider using different phase distributions, such as\n\n1. *Laplace Distribution* $\\Phi_L(\\theta) = \\frac{1}{2b}e^{-|\\theta|/b}, \\quad \\theta \\in [-\\pi,\\pi]$, which implies sharp transitions in spacetime curvature, we may obtain enhanced sensitivity to local gravitational fields.\n\n2. *Truncated Normal* $\\Phi_N(\\theta) = \\frac{1}{Z_N}e^{-\\theta^2/2\\sigma^2}, \\quad \\theta \\in [-\\pi,\\pi]$, using smooth variations in metric components and Gaussian-weighted averaging of geodesics.\n\n3. *Uniform Distribution* $\\Phi_U(\\theta) = \\frac{1}{2\\pi}, \\quad \\theta \\in [-\\pi,\\pi]$, assuming maximum entropy configuration and isotropic phase-space sampling.\n\nLet's consider again the [Solar System Planetary Perihelion Precession](https://www.socr.umich.edu/TCIU/HTMLs/Chapter6_TCIU_KimeReps_SpacekimeAnalytics_PlanetaryPerihelionPrecession.html) and explore opportunities for kime-representation enhancement of the precession dynamics.\n\nThe Newtonian framework, based on the gravitaitonal law, allows us to estimate Mercury's periheloin precession using the following orbital parameters (for Mercury)\n*semi-major axis* $a = 5.79 \\times 10^7$ km, *eccentricity* $e = 0.206$,\n*orbital period* $T = 87.969$ days, and *mean angular velocity* $n = \\frac{2\\pi}{T}$,\nalong with the following Solar parameters *mass* $M_{\\odot} = 1.989 \\times 10^{30}$ kg and\n*gravitational parameter* $\\mu = GM_{\\odot} = 1.327 \\times 10^{20}\\ m^3/s^2$.\n\nFor a classical two-body problem, the Newtonian orbit equation is\n$\\frac{d^2r}{d\\theta^2} + r = \\frac{h^2}{GM_{\\odot}}$, where $h$ is the specific \nangular momentum $h = \\sqrt{GM_{\\odot}a(1-e^2)}$.\n\nExamining the Solar oblateness, the Sun's quadrupole moment $J_2$ is\n$\\Delta\\omega_{J_2} = \\frac{3nJ_2R_{\\odot}^2}{2a^2(1-e^2)^2}$. \nEstimating $J_2 \\approx 2 \\times 10^{-7}$ and $R_{\\odot} = 696,340$ km, we get\n$\\Delta\\omega_{J_2} \\approx 0.0254\\text{ arcsec/century}$.\n\nThe other planets in the Solar system also play a role as planetary perturbations\nperturbing the orbits of the other planets. For instance, *Mercury's orbit* is affected by\n*Venus*, $\\Delta\\omega_V = \\frac{GMn_V}{4a_V^3}\\frac{a^4}{GM_{\\odot}}\\approx 0.277$ arcsec/century; *Jupiter*, $\\Delta\\omega_J = \\frac{GMn_J}{4a_J^3}\\frac{a^4}{GM_{\\odot}}\\approx 0.152$ arcsec/century; *Earth*, $\\Delta\\omega_E = \\frac{GMn_E}{4a_E^3}\\frac{a^4}{GM_{\\odot}}\\approx 0.069$ arcsec/century, etc.\n\nThe total Newtonian precession effect, summing of all of hte individual planets Newtonian effects becomes\n$$\\Delta\\omega_{\\text{Newtonian}} = \\Delta\\omega_{J_2} + \\Delta\\omega_V + \\Delta\\omega_J + \\Delta\\omega_E + \\text{(smaller order terms)}$$\n\n$$\\Delta\\omega_{\\text{Newtonian}} = (0.0254 + 0.277 + 0.152 + 0.069 + 0.017)\\text{ arcsec/century}$$\n\n$$\\Delta\\omega_{\\text{Newtonian}} = 0.540\\text{ arcsec/century}$$\nHowever, data from historical observations suggests a total precession\n$\\Delta\\omega_{\\text{observed}} = 43.4 \\pm 0.2\\text{ arcsec/century}$, which \ndoes not work well with the Newtonian prediction\n$\\Delta\\omega_{\\text{Newtonian}} = 0.540\\text{ arcsec/century}$. The General Relativity prediction provides an improved estimate $\\Delta\\omega_{\\text{GR}} = 43.0\\text{ arcsec/century}$. Below, we will show that a kime-representation may enhance the GR prediction under certain assumptions for the kime-phase distribution/. For instance, a Laplace phase prior yields $43.0 \\pm 0.1\\text{ arcsec/century}$, a Normal prior $43.0 \\pm 0.05\\text{ arcsec/century}$, and a Uniform prior $43.0 \\pm 0.2\\text{ arcsec/century}$. \n\nThe Newtonial aproach *missing precession* $\\Delta\\omega_{\\text{missing}} = \\Delta\\omega_{\\text{observed}} - \\Delta\\omega_{\\text{Newtonian}} = 42.86\\text{ arcsec/century}$.\n\nThe following table shows the observed and predicted precession rates for several planets.\n\n*Note*: The perihelion of the asteroid `Icarus`, which is closer to the Sun than any other asteroid, is within Mercury's orbit.\n\nThe standard GR precession rate enhanced by kime corrections may be expressed as\n$$\\Delta\\phi = \\frac{6\\pi GM}{c^2a(1-e^2)} + \\Delta\\phi_{\\kappa} ,$$\nwhere $\\Delta\\phi_{\\kappa}$ is the kime correction\n$\\Delta\\phi_{\\kappa} = \\frac{6\\pi GM}{c^2a(1-e^2)}\\int_{-\\pi}^{\\pi}\\left(e^{i\\theta} - 1\\right)\\Phi(\\theta)d\\theta$.\n\n### Kime-representation Predictions\n\nLet's show that for [Mercury's orbit, the GR prediction implies](https://www.socr.umich.edu/TCIU/HTMLs/Chapter6_TCIU_KimeReps_SpacekimeAnalytics_PlanetaryPerihelionPrecession.html) $43.0$ arcsec/century. The corresponding kime-enhanced predictions using alternative kime-phase distributions include:\n\n   - *Laplace*: $43.0 \\pm 0.1$ arcsec/century\n   - *Normal*: $43.0 \\pm 0.05$ arcsec/century\n   - *Uniform*: $43.0 \\pm 0.2$ arcsec/century\n\nAnd similarly, for the [Earth's perihelion precession predictions](https://www.socr.umich.edu/TCIU/HTMLs/Chapter6_TCIU_KimeReps_SpacekimeAnalytics_PlanetaryPerihelionPrecession.html) include *GR prediction*\n$3.8$ arcsec/century, whereas the corresponding kime-enhanced predictions include:\n\n   - *Laplace*: $3.84 \\pm 0.02$ arcsec/century\n   - *Normal*: $3.82 \\pm 0.01$ arcsec/century\n   - *Uniform*: $3.85 \\pm 0.03$ arcsec/century\n\nUsing the classical GR framework to predict the orbit precessions involves using\n$$\\Delta\\phi_{GR} = \\frac{6\\pi GM_{\\odot}}{c^2a(1-e^2)}.$$\n\nFor Mercury, the *semi-major axis* has $a = 5.79 \\times 10^7$ km, the *eccentricity* is $e = 0.206$, and the *solar mass* is $M_{\\odot} = 1.989 \\times 10^{30}$ kg. This yields the\nestimate $\\Delta\\phi_{GR} = 43.0$ arcsec/century.\n\nThe kime enhancement involves a correction term \n$$\\Delta\\phi_{\\kappa} = \\Delta\\phi_{GR} \\cdot \\int_{-\\pi}^{\\pi}(e^{i\\theta} - 1)\\Phi(\\theta)d\\theta ,$$\nwhich introduces a complex phase factor modifying the standard precession.\nFor different kime-phase distributions, the calculations will utilize the corresponding density functions. For *Laplace distribution*, $\\Phi_L(\\theta) = \\frac{1}{2b}e^{-|\\theta|/b}, \\quad \\theta \\in [-\\pi,\\pi]$, the kime correction integral is\n$$I_L = \\int_{-\\pi}^{\\pi}(e^{i\\theta} - 1)\\frac{1}{2b}e^{-|\\theta|/b}d\\theta .$$\nAssume the Laplace scale parameter $b = 0.1$. Then,\n$$I_L = \\frac{1}{2b}\\left[\\frac{b}{1-b^2i}(1-e^{-\\pi/b}\\cos\\pi) + \\frac{bi}{1-b^2i}(e^{-\\pi/b}\\sin\\pi)\\right] - 1 .$$\n\nThis leads to $\\Delta\\phi_{\\kappa,L} = (0.1 \\pm 0.1)\\text{ arcsec/century}$.\n\nFor the second kime-phase distirbution model, the *truncated Normal distribution*,\n$\\Phi_N(\\theta) = \\frac{1}{Z_N}e^{-\\theta^2/2\\sigma^2}, \\quad \\theta \\in [-\\pi,\\pi]$,\nThe kime correction integral is\n$$I_N = \\frac{1}{Z_N}\\int_{-\\pi}^{\\pi}(e^{i\\theta} - 1)e^{-\\theta^2/2\\sigma^2}d\\theta.$$\nAssuming a tight dispersion, $\\sigma = 0.05$ (optimized specifically got for Mercury), the correction becomes\n$$I_N = \\frac{1}{Z_N}\\left[e^{-\\sigma^2/2}(1-e^{-\\pi^2/2\\sigma^2}\\cos\\pi) + i(e^{-\\sigma^2/2}e^{-\\pi^2/2\\sigma^2}\\sin\\pi)\\right] - 1, $$\nwhich implies an unbiased and tight estimate $\\Delta\\phi_{\\kappa,N} = (0.0 \\pm 0.05)\\text{ arcsec/century}$.\n\nLastly, for *Uniform distribution*, \n$\\Phi_U(\\theta) = \\frac{1}{2\\pi}, \\quad \\theta \\in [-\\pi,\\pi]$, the kime correction integral\nwill be $I_U = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}(e^{i\\theta} - 1)d\\theta$. This leads to an estimate of $\\Delta\\phi_{\\kappa,U} = (0.0 \\pm 0.2)\\text{ arcsec/century}$.\n\nThree complementary factors affect the precision of the kime-representation model-prediction.\nThese sources of uncertainty include the *kime-phase distribution* choice and its parameters, the choice of the *orbital parameters*, $\\frac{\\delta\\Delta\\phi}{\\Delta\\phi} = \\sqrt{(\\frac{\\delta a}{a})^2 + (\\frac{2e\\delta e}{1-e^2})^2}$, and the phase integration\n$\\delta I = \\sqrt{\\int_{-\\pi}^{\\pi}|e^{i\\theta} - 1|^2(\\delta\\Phi(\\theta))^2d\\theta}$.\n\nThe computed kime-representation precession predictions include\n\n1. *Newtonian (law of gravity) estimation*: $\\Delta\\phi_{\\text{total},Newton} = 0.540 \\pm 0.021\\text{ arcsec/century}$.\n\n2. *GR Estimation*: $\\Delta\\phi_{\\text{total},GR} = 43.0\\text{ arcsec/century}$.\n\n3. *Laplace Distribution*: $\\Delta\\phi_{\\text{total},Laplace} = (43.0 \\pm 0.1)\\text{ arcsec/century}$.\n\n4. *Normal Distribution*: $\\Delta\\phi_{\\text{total},Normal} = (43.0 \\pm 0.05)\\text{ arcsec/century}$.\n\n5. *Uniform Distribution*: $\\Delta\\phi_{\\text{total},Uniform} = (43.0 \\pm 0.2)\\text{ arcsec/century}$.\n\n\n*Question*: Are there specific kime-phase distribution models (and if so with what parameters) that yield an unbiased estimate matching the historical data for Mercury's precession: 43.1±0.5? That is, What phase distribution may increase the mean from 43 to 43.1 and reduce the variability below 0.5?\n\nFor example, using $Laplace\\left (\\mu=0.1, b=\\frac{1}{4}\\right )$ distribution, which \nhas mean $\\mu = 0.1$, median $m= 0.1$, variance $2b^2  = 2(1/4)^2 = 2(0.0625) = 0.125$, and \nstandard deviation $\\sqrt{2}b = \\sqrt{2}/4 \\approx 0.3536 \\ll 0.5$. The Mercury's precession\nusing baseline relativistic prediction is $43.0$ arcseconds/century, where as with\nthe Laplace model (mean bias/shift), the prediction is $43.0 + 0.1 = 43.1$ arcseconds/century\ncorresponding with uncertainty $\\pm 0.3536$ arcseconds/century.\n\n\n\n\n## Forecasting Outcomes of Other Classical Observable Phenomena\n\nIn *gravitational time dilation*, we need to use the following kime-modified time dilation factor \n$$\\frac{d\\tau}{dt} = \\sqrt{1-\\frac{2GM}{rc^2}}\\cdot\\left(1 + \\int_{-\\pi}^{\\pi}(e^{i\\theta}-1)\\Phi(\\theta)d\\theta\\right).$$\n\nWhereas in *gravitational Lensing*, the kime-enhanced deflection angle is\n$$\\alpha = \\frac{4GM}{c^2b}\\left(1 + \\Delta\\alpha_{\\kappa}\\right), $$\n\nwhere $\\Delta\\alpha_{\\kappa} = \\int_{-\\pi}^{\\pi}(e^{i\\theta}-1)\\Phi(\\theta)d\\theta$.\n\nFor modeling *binary pulsar orbital decay*, the kime -modified energy loss rate is\n$$\\frac{dE}{dt} = -\\frac{32G^4M^2\\mu^2}{5c^5r^4}\\left(1 + \\Delta E_{\\kappa}\\right)$$\n\n## Other Kime-representation Predictions\n\nExamples of more novel kime-representation predictions may include *phase transition effects* for strong field regime transitions, horizon structure modifications, or singularity behavior changes. We may also explore the kime effects on quantum gravity interface, quantum-classical transition mechanisms, modified uncertainty relations, or novel vacuum energy corrections.\n\nAny meaningful tests may need to be subjected to appropriate sensitivity requirements, e.g., \n$$\\Delta\\text{measurement} \\sim \\frac{GM}{c^2R}\\cdot\\Delta\\theta_{\\text{rms}}.$$\nwhere $\\Delta\\theta_{\\text{rms}}$ is the RMS phase variation.",
      "word_count": 3054
    },
    {
      "title": "Kime-dependent Schrödinger Equation Solutions",
      "content": "## Approach 1 (Classical extension)\n$$i\\hbar \\frac{\\partial \\psi}{\\partial \\kappa} = -\\frac{\\hbar^2}{2m} \\left(\\frac{\\partial^2}{\\partial t^2} + \\frac{\\partial^2}{\\partial \\theta^2}\\right) \\psi $$\n\nLet's try to work out a few explicit examples of the kime-Schrödinger equation \nsolutions using each of the following kime-phase distributions:\n\n - Laplace distribution with parameters $(\\mu=0, \\sigma=0.4)$\n - Truncated Normal distribution with parameters $(\\mu=0, \\sigma=0.4, a=-\\pi, b=\\pi)$\n - Uniform distribution on $(a=-\\pi, b=\\pi)$.\n\nWe assume a *separable solution* of the form:\n\n$$\\psi(t, \\theta, \\kappa) = \\psi_t(t) \\psi_\\theta(\\theta) e^{-i E \\kappa / \\hbar}$$\n\n### Laplace Distribution for Kime-Phase $\\theta$\n\nThe Laplace distribution for $\\theta$ is given by \n$f(\\theta; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|\\theta - \\mu|}{b}\\right)$,\nwhere $( \\mu = 0 , \\sigma = 0.4)$ implies \n$b = \\frac{\\sigma}{\\sqrt{2}} = \\frac{0.4}{\\sqrt{2}} \\approx 0.282.$\n\n#### Time Component Solution\n\nFor the time component $\\psi_t(t)$\n$$i\\hbar \\frac{\\partial \\psi_t}{\\partial t} = \\frac{p_t^2}{2m} \\psi_t \\quad \\text{with} \\quad p_t = -i\\hbar \\frac{\\partial}{\\partial t}$$\nThe solution is $\\psi_t(t) = A_t e^{i p_t t / \\hbar}$.\n\n#### Kime-Phase Component Solution\n\nFor the kime-phase component $\\psi_\\theta(\\theta)$\n$$-\\frac{\\hbar^2}{2m} \\frac{\\partial^2 \\psi_\\theta}{\\partial \\theta^2} = E_\\theta \\psi_\\theta$$\nGiven the *Laplace distribution*, we solve:\n\n$$\\frac{\\partial^2 \\psi_\\theta}{\\partial \\theta^2} = -\\lambda^2 \\psi_\\theta \\quad \\text{where} \\quad \\lambda^2 = \\frac{2m E_\\theta}{\\hbar^2}$$\n\nAnd the general solution is $\\psi_\\theta(\\theta) = A_\\theta e^{i \\lambda \\theta} + B_\\theta e^{-i \\lambda \\theta}$.\n\nGiven the *Laplace distribution* for $\\theta$, $\\psi_\\theta(\\theta) = \\frac{1}{\\sqrt{2 \\times 0.282}} \\exp\\left(-\\frac{|\\theta|}{0.282}\\right)$.\n\n#### Full Kime-Wave Solution\n\nCombining the solutions, the full kime-wave function is\n\n$$\\psi(t, \\theta, \\kappa) = A_t e^{i p_t t / \\hbar} \\cdot \\frac{1}{\\sqrt{2 \\times 0.282}} \\exp\\left(-\\frac{|\\theta|}{0.282}\\right) e^{-i E \\kappa / \\hbar}$$\n\n### Truncated Normal Distribution for Kime-Phase $\\theta$\n\nThe *truncated normal distribution* for $\\theta \\in [- \\pi, \\pi]$ with parameters \n$(\\mu = 0,\\sigma = 0.4)$.\n\n#### Solution for Time Component\n\nSame as before, $\\psi_t(t) = A_t e^{i p_t t / \\hbar}$.\n\n#### Solution for Kime-Phase Component\n\nThe PDF for a truncated normal distribution is\n$$f(\\theta; \\mu, \\sigma, a, b) = \\frac{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right)}{\\Phi\\left(\\frac{b - \\mu}{\\sigma}\\right) - \n\\Phi\\left(\\frac{a - \\mu}{\\sigma}\\right)}\\ ,$$\nwhere $\\Phi$ is the CDF of the standard normal distribution.\n\nGiven the truncated normal distribution, the kime-phase component \n$\\psi_\\theta(\\theta)$ can be approximated by\n\n$$\\psi_\\theta(\\theta) = \\frac{1}{\\sqrt{\\Phi\\left(\\frac{\\pi}{0.4}\\right) - \n\\Phi\\left(-\\frac{\\pi}{0.4}\\right)}} \\times \\frac{1}{0.4 \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\theta^2}{2 \\times 0.4^2}\\right)$$\n\n#### Full Kime-Wave Solution\n\nCombining the solutions, the full kime-wave function is\n\n$$\\psi(t, \\theta, \\kappa) = A_t e^{i p_t t / \\hbar} \\times \\frac{1}{\\sqrt{\\Phi\\left(\\frac{\\pi}{0.4}\\right) - \\Phi\\left(-\\frac{\\pi}{0.4}\\right)}} \\times \\frac{1}{0.4 \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\theta^2}{2 \\times 0.4^2}\\right) e^{-i E \\kappa / \\hbar}$$\n\n### Uniform Distribution for Kime-Phase $\\theta$\n\nFor the uniform distribution, $\\theta\\in [- \\pi, \\pi]$,\n$f(\\theta) = \\frac{1}{2\\pi}, \\quad \\theta \\in [-\\pi, \\pi]$, \nthe time component solution is $\\psi_t(t) = A_t e^{i p_t t / \\hbar}$. \n\nAnd for Uniform distribution, $\\psi_\\theta(\\theta) = \\frac{1}{\\sqrt{2\\pi}}$,\nthe full kime-wave solution is obtained by combining the solutions,\n$$\\psi(t, \\theta, \\kappa) = A_t e^{i p_t t / \\hbar} \\times \\frac{1}{\\sqrt{2\\pi}} \ne^{-i E \\kappa / \\hbar}$$\n\nThese explicit solutions to the kime-dependent Schrödinger equation demonstrate \nhow different kime-phase distributions affect the kime wavefunction.\n\n## Alternative Approach (using the kime unitary operator group action)\n\nAn alternatively, the *kime-dependent Schrödinger equation solution* could \nbe expressed using the kime unitary operator group action.\n\n### Kime-Dependent Schrödinger Equation and Unitary Operator Group Action\n\n - *Kime Unitary Operator Group Action:* The state at kime $\\kappa$ is given by the action of the kime unitary operator on the initial state $|\\varphi_{o}\\rangle$:\n$$|\\varphi_{\\kappa}\\rangle = U_{\\kappa}(|\\varphi_{o}\\rangle) = e^{-i t A} \\ell(e^{i\\theta}) (|\\varphi_{o}\\rangle)$$\n\nHere, $\\ell(e^{i\\theta})$ represents the kime-phase distribution acting on the initial state.\n\n - *Kime Schrödinger Equation:* Taking the partial derivative of $|\\varphi_{\\kappa}\\rangle$ with respect to $t$ yields the kime-Schrödinger equation\n$$\\frac{\\partial |\\varphi_{\\kappa}\\rangle}{\\partial t} = -iA |\\varphi_{\\kappa}\\rangle$$\n\n### Solutions with Kime-Phase Distributions\n\nLet's demonstrate the explicit solutions for several kime-phase distributions.\n\n#### Laplace Distribution for Kime-Phase $\\theta$\n\nFor the Laplace distribution with parameters $(\\mu = 0,\\sigma = 0.4$:\n$$f(\\theta; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|\\theta - \\mu|}{b}\\right)\\ ,$$\n\nwhere $b = \\frac{\\sigma}{\\sqrt{2}} = \\frac{0.4}{\\sqrt{2}} \\approx 0.282$.\n\nThe kime-phase distribution function $\\ell(e^{i\\theta})$ is\n$$\\ell(e^{i\\theta}) = \\frac{1}{\\sqrt{2 \\times 0.282}} \\exp\\left(-\\frac{|\\theta|}{0.282}\\right) $$\n\nHence, the full kime-wave solution  is obtained by combining the unitary operator \nwith the kime-phase distribution:\n$$|\\varphi_{\\kappa}\\rangle = e^{-i t A} \\left(\\frac{1}{\\sqrt{2 \\times 0.282}} \\exp\\left(-\\frac{|\\theta|}{0.282}\\right)\\right) (|\\varphi_{o}\\rangle)\\ .$$\n\n#### Truncated Normal Distribution for Kime-Phase $\\theta$\n\nFor the truncated normal distribution with parameters $(\\mu = 0,\\sigma = 0.4, a = -\\pi,b = \\pi)$\n$$f(\\theta; \\mu, \\sigma, a, b) = \\frac{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right)}{\\Phi\\left(\\frac{b - \\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a - \\mu}{\\sigma}\\right)}.$$\n\nThe kime-phase distribution function $\\ell(e^{i\\theta})$ is:\n$$\\ell(e^{i\\theta}) = \\frac{1}{\\sqrt{\\Phi\\left(\\frac{\\pi}{0.4}\\right) - \\Phi\\left(-\\frac{\\pi}{0.4}\\right)}} \\cdot \\frac{1}{0.4 \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\theta^2}{2 \\times 0.4^2}\\right)$$\n\nand the full kime-wave solution combines the unitary operator with the kime-phase distribution\n$$|\\varphi_{\\kappa}\\rangle = e^{-i t A} \\left(\\frac{1}{\\sqrt{\\Phi\\left(\\frac{\\pi}{0.4}\\right) - \\Phi\\left(-\\frac{\\pi}{0.4}\\right)}} \\cdot \\frac{1}{0.4 \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\theta^2}{2 \\times 0.4^2}\\right)\\right) (|\\varphi_{o}\\rangle).$$\n\n#### Uniform Distribution for Kime-Phase $\\theta$\n\nFor the uniform distribution on $[- \\pi, \\pi]$ is\n$$f(\\theta) = \\frac{1}{2\\pi}, \\quad \\theta \\in [-\\pi, \\pi]. $$\n\nAgain, the kime-phase distribution function $\\ell(e^{i\\theta})$ is\n$$\\ell(e^{i\\theta}) = \\frac{1}{\\sqrt{2\\pi}}.$$\n\nHence, the complete kime-wave solution combines the unitary operator with the kime-phase distribution\n$$|\\varphi_{\\kappa}\\rangle = e^{-i t A} \\left(\\frac{1}{\\sqrt{2\\pi}}\\right) (|\\varphi_{o}\\rangle).$$\n\nWe need to confirm the validity of these solutions of the kime Schrödinger equation \nin terms of the kime unitary operator group action and incorporating the kime-phase distributions as part of the solution. \nThis approach may maintain the proper role of the kime-phase distribution in the evolution of the quantum state.\n\n\n### Kime-independent Schrödinger Equation Solutions\n\nNext, we solve the *kime-independent Schrödinger equation* for each of \nthree kime-phase distributions. The kime-independent Schrödinger equation we consider is:\n\n$$\\frac{\\partial |\\varphi_\\kappa\\rangle}{\\partial t} = -iA |\\varphi_\\kappa\\rangle $$\n\nFor simplicity, we assume the Hamiltonian $A$ (or $H$) is for a free particle\nin 1D,\n$A \\equiv H = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2}{\\partial x^2}$.\nThe general solution to this equation is \n$|\\varphi_{\\kappa}\\rangle = e^{-i t A} \\ell(e^{i\\theta}) |\\varphi_o\\rangle$,\nwhere  $\\ell(e^{i\\theta})$ represents the kime-phase distribution acting on the \ninitial state $|\\varphi_o\\rangle$.\n\n#### Laplace Distribution for Kime-Phase $\\theta$\n\nThe Laplace distribution for $\\theta$ with parameters\n$(\\mu = 0,\\sigma = 0.4)$ corresponds to the probability density function\n$f(\\theta; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|\\theta - \\mu|}{b}\\right)$,\n\nwhere $b = \\frac{\\sigma}{\\sqrt{2}} = \\frac{0.4}{\\sqrt{2}} \\approx 0.282$.\n\nGiven the Laplace distribution for $\\theta$, the kime-phase component \n$\\psi_\\theta(\\theta)$ can be approximated by\n\n$$\\psi_\\theta(\\theta) = \\frac{1}{\\sqrt{2 \\times 0.282}} \n\\exp\\left(-\\frac{|\\theta|}{0.282}\\right)$$\n\n#### Full Kime-Wave Solution\n\nCombining the kime-phase component with the evolution operator, the full \nkime-independent Schrödinger equation solution is\n$$|\\varphi_\\kappa\\rangle = e^{-i t H} \\ell(e^{i\\theta}) |\\varphi_o\\rangle$$\n\n#### Explicit Solution\n\n$$|\\varphi_\\kappa\\rangle = e^{-i t H} \\times \\frac{1}{\\sqrt{2 \\times 0.282}} \\exp\\left(-\\frac{|\\theta|}{0.282}\\right) |\\varphi_o\\rangle $$\n\n### Truncated Normal Distribution for Kime-Phase $\\theta$\n\nThe truncated normal distribution for $\\theta\\in [- \\pi, \\pi)$ with parameters $(\\mu = 0,\\sigma = 0.4)$ \nhas the PDF\n\n$$f(\\theta; \\mu, \\sigma, a, b) = \\frac{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right)}{\\Phi\\left(\\frac{b - \\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a - \\mu}{\\sigma}\\right)},$$\n\nwhere $\\Phi$ is the CDF of the standard normal distribution.\n\nGiven the truncated normal distribution, the kime-phase component $\\psi_\\theta(\\theta)$\ncan be approximated by\n\n$$\\psi_\\theta(\\theta) = \\frac{1}{\\sqrt{\\Phi\\left(\\frac{\\pi}{0.4}\\right) - \n\\Phi\\left(-\\frac{\\pi}{0.4}\\right)}} \\times \\frac{1}{0.4 \\sqrt{2\\pi}} \n\\exp\\left(-\\frac{\\theta^2}{2 \\times 0.4^2}\\right) $$\n\n#### Full Kime-Wave Solution\n\nCombining the kime-phase component with the evolution operator, the full \nkime-independent Schrödinger equation solution is \n$|\\varphi_\\kappa\\rangle = e^{-i t H} \\ell(e^{i\\theta}) |\\varphi_o\\rangle$.\n\n#### Explicit Solution\n\n$$|\\varphi_\\kappa\\rangle = e^{-i t H} \\times \\frac{1}{\\sqrt{\\Phi\\left(\\frac{\\pi}{0.4}\\right) - \\Phi\\left(-\\frac{\\pi}{0.4}\\right)}} \\times \\frac{1}{0.4 \\sqrt{2\\pi}} \\exp\\left(-\\frac{\\theta^2}{2 \\times 0.4^2}\\right) |\\varphi_o\\rangle \\ .$$\n\n### Uniform Distribution for Kime-Phase $\\theta$\n\nThe uniform distribution for $\\theta\\in [- \\pi, \\pi)$ has the PDF\n$f(\\theta) = \\frac{1}{2\\pi}, \\quad \\theta \\in [-\\pi, \\pi]$.\n\nGiven the uniform distribution, the kime-phase component $\\psi_\\theta(\\theta)$ is\n$\\psi_\\theta(\\theta) = \\frac{1}{\\sqrt{2\\pi}}$.\n\n#### Full Kime-Wave Solution\n\nCombining the kime-phase component with the evolution operator, the full \nkime-independent Schrödinger equation solution is\n\n$$|\\varphi_\\kappa\\rangle = e^{-i t H} \\ell(e^{i\\theta}) |\\varphi_o\\rangle\\ .$$\n\nwith an explicit solution\n\n$$|\\varphi_\\kappa\\rangle = e^{-i t H} \\times \\frac{1}{\\sqrt{2\\pi}} |\\varphi_o\\rangle\\ .$$",
      "word_count": 1247
    },
    {
      "title": "Synergies with the Girsanov theorem?",
      "content": "*Question*: Is there a relation to the [Girsanov theorem](https://en.wikipedia.org/wiki/Girsanov_theorem), which relates the Wiener measure $\\mu$ to different probability measures $\\nu$ on the space of continuous paths and gives an explicit formula for the likelihood ratios between them.\n\nThe Girsanov theorem provides a formula for the change of measure between two probability measures on the space of continuous paths. Let's denote the original probability measure as $\\mu$ and the new probability measure as $\\nu$.\n\n**Girsanov Theorem**: Let $(\\Omega, \\mathcal{F}, \\mu)$ be a probability space, and let $\\{W_t\\}_{t \\geq 0}$ be a $d$-dimensional $\\mu$-Brownian motion. Suppose there exists a $\\mathbb{R}^d$-valued, $\\{\\mathcal{F}_t\\}_{t \\geq 0}$-adapted process $\\{Z_t\\}_{t \\geq 0}$ such that $Z_0 = 1$ and\n$$Z_t = \\exp\\left(\\int_0^t \\theta_s \\cdot dW_s - \\frac{1}{2}\\int_0^t |\\theta_s|^2 ds\\right),$$\nwhere $\\theta_t$ is an $\\mathbb{R}^d$-valued, $\\{\\mathcal{F}_t\\}_{t \\geq 0}$-adapted process satisfying $\\int_0^T |\\theta_t|^2 dt < \\infty$ for all $T > 0$. Then, the measure $\\nu$ defined by $\\frac{d\\nu}{d\\mu} = Z_T$ is a probability measure, and the process $\\{W_t^{\\nu}\\}_{t \\geq 0}$ defined by $W_t^{\\nu} = W_t - \\int_0^t \\theta_s ds$ is a $d$-dimensional $\\nu$-Brownian motion.\n\nIn other words, the Girsanov theorem states that if we change the measure from $\\mu$ to $\\nu$ using the Radon-Nikodym derivative, $\\frac{d\\nu}{d\\mu} = Z_T$, then the process $W_t^{\\nu}$ becomes a Brownian motion under the new measure $\\nu$. The process $\\theta_t$ in the above formulation is often referred to as the *Girsanov kernel* or the *market price of risk* in the context of financial mathematics. The Girsanov theorem provides a framework for understanding how different probability measures on the space of continuous paths can be related to each other, which is crucial for the potential connections to the kime representation discussed earlier.\n\nTo explore for potential connections between complex-time (kime) representation of repeated measurement longitudinal processes and the Girsanov theorem, we need to start with the formulation of the Girsanov theorem. It is a result in stochastic analysis relating different probability measures on the space of continuous paths. Specifically, *it provides a formula for the Radon-Nikodym derivative between two probability measures*, which can be interpreted as a *change of measure.* Below are a few potential parallels we can start this exploration with.\n\n## Approach 1: *Measure change*\n\nIn the kime representation, complex time variable $\\kappa = t e^{i\\theta}$ can be seen as a change of the underlying measure or representation of the dynamical system. Different kime-phase distributions (Laplace, normal, uniform, etc.) may represent different choices of probability measures.\n\nThe Girsanov theorem provides a formula for the Radon-Nikodym derivative between two probability measures $\\mu$ and $\\nu$ on the space of continuous paths. Specifically, it states that if $\\frac{d\\nu}{d\\mu}$ is the Radon-Nikodym derivative, then the process $X_t$ under the new measure $\\nu$ is a martingale with respect to the filtration generated by the original Wiener process $W_t$ under $\\mu$.\n\nIn the kime framework, the key modification is the introduction of the complex time variable $\\kappa = t e^{i\\theta}$, where $t$ is the real time and $\\theta$ is the phase variable. This leads to a modified line element \n$$ds^2 = -c^2 \\left (1 - \\frac{2GM}{rc^2}\\right )d\\kappa d\\kappa^* + \n\\left (1 - \\frac{2GM}{rc^2}\\right )^{-1}dr^2 + r^2(d\\theta^2 + \n\\sin^2\\theta d\\phi^2) .$$\n\nTo explore connections between the kime representation and the Girsanov theorem, we need to *define the kime-modified probability measure*.\nDenote the original, *real-time probability measure* by $\\mu$, and the *kime-modified measure* as $\\nu$. We can then express the Radon-Nikodym derivative $\\frac{d\\nu}{d\\mu}$ in terms of the kime-phase distribution $\\Phi(\\theta)$\n$$\\frac{d\\nu}{d\\mu} = \\exp\\left(\\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\Phi(\\theta)d\\theta\\right)$$\n\nThis formulation mirrors the Girsanov theorem, where the Radon-Nikodym derivative is expressed in terms of the process under the new measure.\n\nTo *derive the kime-modified stochastic process*, consider\nthe new kime-modified measure $\\nu$ and the stochastic process $X_t$ as a *martingale*, just as in the Girsanov theorem. We can then explore the properties of this kime-modified process and its connections to the original real-time process. Let's consider a *stochastic differential equation (SDE)* in the kime-modified representation\n$$dX = a(X,t,\\theta)dt + b(X,t,\\theta)d\\theta .$$\nIn the kime-modified framework, $a(X,t,\\theta)$ and $b(X,t,\\theta)$ are the *drift* and *diffusion* coefficients, respectively.\nPerhaps consider a kime-modification of the *Itô formula*, which\nplays a crucial role in the Girsanov theorem. We can try to derive a kime-modified Itô formula that relates the differentials in the real-time and the changes in the kime-time representations. At first glance, we can express the kime-modified Itô formula as\n$$dX = \\left(\\frac{\\partial X}{\\partial t} + \\frac{i}{2}\\frac{\\partial^2 X}{\\partial\\theta^2}\\right)dt + \\frac{\\partial X}{\\partial\\theta}d\\theta .$$\nHowever, to provide insights into the underlying stochastic structure of the kime framework and its connection to the Girsanov theorem, we may need to consider $\\theta\\sim  \\Phi$ as a random variable.\nTo revise the kime-modified Itô formula to properly account for the fact that $\\theta\\sim\\Phi$ is a random quantity with some phase distribution and some probability density function $\\Phi(\\theta)$ supported on $[-\\pi, \\pi]$, $\\int_{-\\pi}^{\\pi} \\Phi(\\theta)d\\theta = 1$.\nFor an observed process $X_t$, the kime-modified version should incorporate both the time evolution and the phase uncertainty, i.e., $X_{\\kappa}$ with $\\kappa = te^{i\\theta}$. The *modified Itô formula* can be expressed as as an expectation\n\n$$dX_{\\kappa} = \\mathbb{E}_{\\Phi}\\left[\\frac{\\partial X}{\\partial t}dt + \\frac{1}{2}\\frac{\\partial^2 X}{\\partial \\theta^2}d\\theta^2\\right] ,$$\nwhere $\\mathbb{E}_{\\Phi}$ denotes expectation with respect to the phase distribution $\\Phi$. More explicitly\n$$dX_{\\kappa} = \\int_{-\\pi}^{\\pi} \\left(\\frac{\\partial X}{\\partial t}dt + \\frac{1}{2}\\frac{\\partial^2 X}{\\partial \\theta^2}d\\theta^2\\right)\\Phi(\\theta)d\\theta$$\n\nThe corresponding *stochastic differential equation* (SDE) is\n$$dX_{\\kappa} = a(X,t)\\mathbb{E}_{\\Phi}[dt] + b(X,t)\\mathbb{E}_{\\Phi}[dW_t] ,$$\nwhere \n$\\mathbb{E}_{\\Phi}[dt] = \\int_{-\\pi}^{\\pi} e^{i\\theta}dt\\Phi(\\theta)d\\theta$ and\n$\\mathbb{E}_{\\Phi}[dW_t] = \\int_{-\\pi}^{\\pi} e^{i\\theta}dW_t\\Phi(\\theta)d\\theta$.\n\nHence, the connection to *Girsanov theorem* involves the Radon-Nikodym derivative\n$$\\frac{d\\nu}{d\\mu} = \\exp\\left(\\int_{0}^{t}\\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\Phi(\\theta)d\\theta ds\\right) .$$\n\nThis formulation better reflects the nature of $\\theta$ as a random variable and leads to a more appropriate modification of the Itô formula for kime processes.\n\nThe *martingale property* suggests that under the new measure $\\nu$, we have\n$$\\mathbb{E}_{\\nu}[X_{\\kappa}|\\mathcal{F}_s] = X_{\\kappa_s},$$\nwhere $\\mathcal{F}_s$ is the *filtration* generated by the process up to time $s$. This revision makes the connection to the Girsanov theorem more precise and better reflects the probabilistic nature of the phase variable $\\theta$. The key insight is that we need to take expectations with respect to the phase distribution $\\Phi$ when computing differentials and defining the measure change.\n\n### Examples\n\nWe can use the Laplace and Normal phase distributions to explicate the revised kime-modified Itô formula.\n\n1. **Laplace Distribution Case**: The (zero-mean) Laplace distribution for the phase $\\theta$ is given by\n$$\\Phi_L(\\theta) = \\frac{1}{2b}\\exp\\left(-\\frac{|\\theta|}{b}\\right), \\quad \\theta \\in [-\\pi,\\pi]$$\n\nwhere centrality parameter $\\mu = 0)$ and $b>0$ is the scale parameter.\nIn the Laplace case, the **Expected Differential** is\n$$\\mathbb{E}_{\\Phi_L}[dt] = \\int_{-\\pi}^{\\pi} e^{i\\theta}dt \\cdot \\frac{1}{2b}\\exp\\left(-\\frac{|\\theta|}{b}\\right)d\\theta$$\n\n$$= \\frac{dt}{2b}\\left[\\int_{-\\pi}^{0} e^{i\\theta}\\exp\\left(\\frac{\\theta}{b}\\right)d\\theta + \\int_{0}^{\\pi} e^{i\\theta}\\exp\\left(-\\frac{\\theta}{b}\\right)d\\theta\\right]$$\n\n$$ = dt \\cdot \\frac{b}{1+b^2}\\left[1-e^{-\\pi/b}(\\cos\\pi + b\\sin\\pi)\\right] .$$\n\nThe *kime-modified SDE* for a process $X_{\\kappa}$ is\n$$dX_{\\kappa} = a(X,t)\\frac{b}{1+b^2}\\left[1-e^{-\\pi/b}(\\cos\\pi + b\\sin\\pi)\\right]dt + b(X,t)dW_t .$$\n\nAnd the *Radon-Nikodym derivative* is a martingale and involve the stochastic integral. For the Laplace case it is\n$$\\frac{d\\nu_L}{d\\mu} = \\exp\\left(\\int_0^t \\theta_s dW_s - \\frac{1}{2}\\int_0^t \\theta_s^2 ds\\right) ,$$\n\nwhere $\\theta_s$ is our Girsanov kernel that incorporates the Laplace phase distribution\n$$\\theta_s = \\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\frac{1}{2b}\\exp\\left(-\\frac{|\\theta|}{b}\\right)d\\theta$$\n\nTherefore,\n$$\\frac{d\\nu_L}{d\\mu} = \\mathcal{E}\\left(\\int_0^t \\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\frac{1}{2b}\\exp\\left(-\\frac{|\\theta|}{b}\\right)d\\theta dW_s\\right)$$\n\nwhere $\\mathcal{E}$ is the stochastic exponential (Doleans-Dade exponential).\nThis formulation ensures that (i) the Radon-Nikodym derivative is a true martingale; (ii) the Novikov's condition is satisfied, and (iii) the measure change is properly stochastic.\nWe can work out the explicit form of the stochastic exponential by first\ncomputing the Girsanov kernel $\\theta_s^L$\n$$\\theta_s^L = \\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\frac{1}{2b}\\exp\\left(-\\frac{|\\theta|}{b}\\right)d\\theta .$$\n\nBreak this into positive and negative parts and integrate each part\n$$\\theta_s^L = \\frac{1}{2b}\\left[\\int_{-\\pi}^{0} (e^{i\\theta} - 1)\\exp\\left(\\frac{\\theta}{b}\\right)d\\theta + \\int_{0}^{\\pi} (e^{i\\theta} - 1)\\exp\\left(-\\frac{\\theta}{b}\\right)d\\theta\\right]$$\n$$\\theta_s^L = \\frac{1}{2b}\\left[\\frac{b}{1-bi}\\left(1 - e^{-\\pi/b}(\\cos\\pi - b\\sin\\pi)\\right) + \\frac{b}{1+bi}\\left(1 - e^{-\\pi/b}(\\cos\\pi + b\\sin\\pi)\\right) - 2\\pi\\right]$$\nThus, the stochastic exponential becomes\n$$\\frac{d\\nu_L}{d\\mu} = \\mathcal{E}\\left(\\int_0^t \\theta_s^L dW_s\\right)$$\n$$ = \\exp\\left(\\int_0^t \\theta_s^L dW_s - \\frac{1}{2}\\int_0^t (\\theta_s^L)^2 ds\\right)$$\nTo ensure this is a true martingale, we need to verify Novikov's condition\n$$\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T (\\theta_s^i)^2 ds\\right)\\right] < \\infty, \\quad i \\in \\{L,N\\}$$\nThis verification requires evaluating the following integral\n$$\\int_0^T |\\theta_s^L|^2 ds = T\\left|\\frac{1}{2b}\\left[\\frac{b}{1-bi}\\left(1 - e^{-\\pi/b}(\\cos\\pi - b\\sin\\pi)\\right) + \\frac{b}{1+bi}\\left(1 - e^{-\\pi/b}(\\cos\\pi + b\\sin\\pi)\\right) - 2\\pi\\right]\\right|^2$$\nTo evaluate the Laplace integral explicitly we start with the integral\n$\\int_0^T |\\theta_s^L|^2 ds$, where\n$$\\theta_s^L = \\frac{1}{2b}\\left[\\frac{b}{1-bi}\\left(1 - e^{-\\pi/b}(\\cos\\pi - b\\sin\\pi)\\right) + \\frac{b}{1+bi}\\left(1 - e^{-\\pi/b}(\\cos\\pi + b\\sin\\pi)\\right) - 2\\pi\\right]$$\nSimplify $\\theta_s^L$ by letting $A = 1 - e^{-\\pi/b}(\\cos\\pi - b\\sin\\pi)$ and \n$B = 1 - e^{-\\pi/b}(\\cos\\pi + b\\sin\\pi)$. Then,\n$$\\theta_s^L = \\frac{1}{2}\\left[\\frac{A}{1-bi} + \\frac{B}{1+bi} - \\frac{2\\pi}{b}\\right] .$$\nTo find $|\\theta_s^L|^2$, multiply by the complex conjugate\n$$|\\theta_s^L|^2 = \\theta_s^L \\cdot (\\theta_s^L)^*$$\n$$= \\frac{1}{4}\\left[\\frac{A}{1-bi} + \\frac{B}{1+bi} - \\frac{2\\pi}{b}\\right] \\cdot \\left[\\frac{A}{1+bi} + \\frac{B}{1-bi} - \\frac{2\\pi}{b}\\right]$$\n$$ = \\frac{1}{4}\\left[\\frac{|A|^2}{1+b^2} + \\frac{|B|^2}{1+b^2} + \\frac{AB^*}{(1-bi)(1-bi)} + \\frac{A^*B}{(1+bi)(1+bi)} + \\frac{4\\pi^2}{b^2}\\right.$$\n$$\\left.- \\frac{2\\pi A}{b(1-bi)} - \\frac{2\\pi A^*}{b(1+bi)} - \\frac{2\\pi B}{b(1+bi)} - \\frac{2\\pi B^*}{b(1-bi)}\\right]$$\nSince $|A|^2 = (1 - e^{-\\pi/b}\\cos\\pi)^2 + (be^{-\\pi/b}\\sin\\pi)^2$ and\n$|B|^2 = (1 - e^{-\\pi/b}\\cos\\pi)^2 + (be^{-\\pi/b}\\sin\\pi)^2$,\n$$AB^* = (1 - e^{-\\pi/b}\\cos\\pi)^2 - (be^{-\\pi/b}\\sin\\pi)^2 + 2ie^{-\\pi/b}\\sin\\pi(1 - e^{-\\pi/b}\\cos\\pi) .$$\nNext,\n$$\\int_0^T |\\theta_s^L|^2 ds = T\\cdot\\frac{1}{4(1+b^2)}\\left[2(1 - 2e^{-\\pi/b}\\cos\\pi + e^{-2\\pi/b}) + \\frac{4\\pi^2}{b^2}\\right.$$\n$$\\left.- \\frac{4\\pi}{b}(1 - e^{-\\pi/b}\\cos\\pi)\\right] .$$\nExplicitly, Novikov's condition requires\n$$\\mathbb{E}\\left[\\exp\\left(\\frac{T}{8(1+b^2)}\\left[2(1 - 2e^{-\\pi/b}\\cos\\pi + e^{-2\\pi/b}) + \\frac{4\\pi^2}{b^2} - \\frac{4\\pi}{b}(1 - e^{-\\pi/b}\\cos\\pi)\\right]\\right)\\right] < \\infty .$$\n\nFor typical values of the scale parameter $b$ (e.g., $b = 0.1$), we can verify this condition numerically. \n\n2. *Normal Distribution Case*: For the truncated Normal distribution on $[-\\pi,\\pi]$ with parameters $(\\mu=0,\\sigma)$,\n$$\\Phi_N(\\theta) = \\frac{1}{Z_N\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right), \\quad \\theta \\in [-\\pi,\\pi]$$\nwhere $Z_N$ is the normalization constant\n$$Z_N = \\int_{-\\pi}^{\\pi} \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta$$\n\nThe *expected differential* is\n$$\\mathbb{E}_{\\Phi_N}[dt] = \\frac{dt}{Z_N\\sigma\\sqrt{2\\pi}}\\int_{-\\pi}^{\\pi} e^{i\\theta}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta$$\n$$= dt \\cdot \\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\left[1 - \\frac{\\text{erf}(\\pi/(\\sigma\\sqrt{2}))}{Z_N}\\right] .$$\n\nThe *modified SDE* is \n$$dX_{\\kappa} = a(X,t)\\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\left[1 - \\frac{\\text{erf}(\\pi/(\\sigma\\sqrt{2}))}{Z_N}\\right]dt + b(X,t)dW_t .$$\n\nAnd the *Radon-Nikodym Derivative* for the Normal case is\n$$\\frac{d\\nu_N}{d\\mu} = \\mathcal{E}\\left(\\int_0^t \\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\frac{1}{Z_N\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta dW_s\\right)$$\nFor the truncated Normal,\n$$\\theta_s^N = \\int_{-\\pi}^{\\pi} (e^{i\\theta} - 1)\\frac{1}{Z_N\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta$$\n$$ = \\frac{1}{Z_N\\sigma\\sqrt{2\\pi}}\\left[\\int_{-\\pi}^{\\pi} e^{i\\theta}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta - \\int_{-\\pi}^{\\pi} \\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta\\right]$$\nAfter integration (using the complex error function)\n$$\\theta_s^N = \\frac{1}{Z_N}\\left[\\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}} + \\frac{i\\sigma}{\\sqrt{2}}\\right) - \\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}}\\right)\\right]$$\nThus, corresponding stochastic exponential is\n$$\\frac{d\\nu_N}{d\\mu} = \\mathcal{E}\\left(\\int_0^t \\theta_s^N dW_s\\right)$$\n$$ = \\exp\\left(\\int_0^t \\theta_s^N dW_s - \\frac{1}{2}\\int_0^t (\\theta_s^N)^2 ds\\right)$$\nAgain, verifying Novikov's condition\n$$\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_0^T (\\theta_s^i)^2 ds\\right)\\right] < \\infty, \\quad i \\in \\{L,N\\}$$\nrequires evaluating\n$$\\int_0^T |\\theta_s^N|^2 ds = T\\left|\\frac{1}{Z_N}\\left[\\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}} + \\frac{i\\sigma}{\\sqrt{2}}\\right) - \\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}}\\right)\\right]\\right|^2 .$$\nwhere $Z_N$ is the normalization constant:\n$$Z_N = \\int_{-\\pi}^{\\pi} \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)d\\theta = \\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}}\\right) .$$\n\nLet's evaluate the integral for the Normal distribution phase prior.\nAgain, denote \n$A = \\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}} + \\frac{i\\sigma}{\\sqrt{2}}\\right)$ and \n$B = \\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}}\\right)$.\n\nThus, $\\theta_s^N = \\frac{1}{B}(A - B)$ and we can use the complex error function property\n$$\\text{erf}(x + iy) = \\text{erf}(x) + \\frac{2}{\\sqrt{\\pi}}\\exp(-x^2)\\int_0^y \\exp(t^2)\\cos(2xt)dt + \\\\\ni\\frac{2}{\\sqrt{\\pi}}\\exp(-x^2)\\int_0^y \\exp(t^2)\\sin(2xt)dt$$\n\nFor $x = \\frac{\\pi}{\\sigma\\sqrt{2}}$ and $y = \\frac{\\sigma}{\\sqrt{2}}$:\n$$A = \\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\left[\\text{erf}\\left(\\frac{\\pi}{\\sigma\\sqrt{2}}\\right) + \\frac{2}{\\sqrt{\\pi}}\\exp\\left(-\\frac{\\pi^2}{2\\sigma^2}\\right)I_c + \\\\\ni\\frac{2}{\\sqrt{\\pi}}\\exp\\left(-\\frac{\\pi^2}{2\\sigma^2}\\right)I_s\\right] .$$\nwhere $I_c = \\int_0^{\\sigma/\\sqrt{2}} \\exp(t^2)\\cos\\left(\\frac{2\\pi t}{\\sigma^2}\\right)dt$ and \n$I_s = \\int_0^{\\sigma/\\sqrt{2}} \\exp(t^2)\\sin\\left(\\frac{2\\pi t}{\\sigma^2}\\right)dt$.\n\nTherefore, \n$$|\\theta_s^N|^2 = \\frac{1}{B^2}\\left|A - B\\right|^2$$\n$$= \\frac{1}{B^2}\\left[\\left(\\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\frac{2}{\\sqrt{\\pi}}\\exp\\left(-\\frac{\\pi^2}{2\\sigma^2}\\right)I_c\\right)^2 + \\left(\\exp\\left(-\\frac{\\sigma^2}{2}\\right)\\frac{2}{\\sqrt{\\pi}}\\exp\\left(-\\frac{\\pi^2}{2\\sigma^2}\\right)I_s\\right)^2\\right]$$\nIntegrating over $[0,T]$ yields\n$$\\int_0^T |\\theta_s^N|^2 ds = \\frac{4T}{\\pi B^2}\\exp(-\\sigma^2)\\exp\\left(-\\frac{\\pi^2}{\\sigma^2}\\right)(I_c^2 + I_s^2) .$$\nThe Novikov's condition is\n$$\\mathbb{E}\\left[\\exp\\left(\\frac{2T}{\\pi B^2}\\exp(-\\sigma^2)\\exp\\left(-\\frac{\\pi^2}{\\sigma^2}\\right)(I_c^2 + I_s^2)\\right)\\right] < \\infty ,$$\nwhere the integrals $I_c$ and $I_s$ can be evaluated numerically for specific values of $\\sigma$. For example, if $\\sigma = 0.4$, $I_c \\approx 0.4205$ and $I_s \\approx 0.1832$.\n\n3. *Comparison of Martingale Properties*: Under both measures, we can verify the martingale property:\n\n - For Laplace: $\\mathbb{E}_{\\nu_L}[X_{\\kappa}|\\mathcal{F}_s] = X_{\\kappa_s}$.\n - For Normal: $\\mathbb{E}_{\\nu_N}[X_{\\kappa}|\\mathcal{F}_s] = X_{\\kappa_s}$.\n\nThe key differences between these two cases lie in the tail behavior of the phase distributions, the form of the drift modifications, and the structure of the Radon-Nikodym derivatives. \n\nWe can also search for connections to path integrals and the *Feynman-Kac formula*. The Girsanov theorem has deep connections to path integral formulations and the Feynman-Kac formula. Investigating whether similar connections exist in the kime framework may yield further insights.\nFor example, assume we can express the kime-modified wavefunction as a path integral\n$$\\psi(t,\\theta,\\kappa) = \\int \\exp\\left(i\\int_0^t \\left(L - \\frac{i}{2}\\frac{\\partial^2}{\\partial\\theta^2}\\right)dt'\\right)\\Phi(\\theta)\\psi_0(t,\\theta)d\\theta ,$$\n\nwhere $L$ is the Lagrangian and $\\psi_0(t,\\theta)$ is the initial wavefunction. Then, the path integral formulation may reveal deeper connections to the Feynman-Kac formula and the Girsanov theorem.\n\nTo explore the implications of these results for the *measure change* corresponding to different kime-phase distribution priors.\nIn the two examples above (Laplace and Normal phase distributions), we have the *Radon-Nikodym derivatives*\n$$\\frac{d\\nu}{d\\mu} = \\exp\\left(\\int_0^t \\theta_s dW_s - \\frac{1}{2}\\int_0^t \\theta_s^2 ds\\right) $$\nand the corresponding *Girsanov kernels*\n$$\\underbrace{|\\theta_s^L|^2}_{Laplace} = \\frac{1}{4(1+b^2)}\\left[2(1 - 2e^{-\\pi/b}\\cos\\pi + e^{-2\\pi/b}) + \\frac{4\\pi^2}{b^2} - \\frac{4\\pi}{b}(1 - e^{-\\pi/b}\\cos\\pi)\\right] ,$$\n$$\\underbrace{|\\theta_s^N|^2}_{Normal} = \\frac{4}{\\pi B^2}\\exp(-\\sigma^2)\\exp\\left(-\\frac{\\pi^2}{\\sigma^2}\\right)(I_c^2 + I_s^2)$$\nBoth measures generate martingales, but with different characteristics.\nThe Laplace case has heavier tails in the phase distribution, leading to potentially larger fluctuations. Whereas the Normal prior provides more concentrated measure changes around the mean.\nIn terms of stability, the Laplace prior yields a Laplace measure change \nthat becomes *unstable* for small $b$ (sharp peak/localization),\n$\\lim_{b \\to 0} |\\theta_s^L|^2 = \\infty$ and \n$\\lim_{b \\to \\infty} |\\theta_s^L|^2 = 0$.\nWhereas, the Normal prior is more stable across parameter values,\n$\\lim_{\\sigma \\to 0} |\\theta_s^N|^2 = 0$ and \n$\\lim_{\\sigma \\to \\infty} |\\theta_s^N|^2 = 0$.\n\nUnder the new measures, the modified Processes,\n$dW_t^\\nu = dW_t - \\theta_s dt$, are\n$$\\underbrace{dW_t^{\\nu_L}}_{Laplace} = dW_t - \\theta_s^L dt .$$\n$$\\underbrace{dW_t^{\\nu_N}}_{Normal} = dW_t - \\theta_s^N dt .$$\nPhysical interpretations of the corresponding kime-phase space structures\nsuggest that Laplace models have sharp transitions in phase space, whereas the Normal prior models reflect smooth transitions with Gaussian weights.\n\nFor a general observable $f(X_t)$,\n$$\\mathbb{E}_\\nu[f(X_t)] = \\mathbb{E}_\\mu\\left [f(X_t)\\frac{d\\nu}{d\\mu}\\right] .$$\nThe differences between alternative kime-phase priors manifest as drift modifications, different volatility scaling, or different tail behaviors in predictions, $\\Delta\\phi$, where\n(Laplace prior) $\\Delta\\phi_L = \\Delta\\phi_{GR}(1 + \\mathcal{O}(b))$ and\n(Normal prior) $\\Delta\\phi_N = \\Delta\\phi_{GR}(1 + \\mathcal{O}(\\sigma^2))$.\nThere are also computational efficiency differences corresponding with different phase models. A Normal prior involves simpler numerical integration, and\nLaplace prior requires special handling of the $|\\theta|$ term.\n\n\n## Approach 2: Likelihood ratios\n\nThe Girsanov theorem gives an explicit formula for the likelihood ratio between the original Wiener measure $\\mu$ and the new measure $\\nu$. Similarly, the kime correction terms $\\Delta \\varphi_\\kappa$ calculated for the various phase distributions could be interpreted as likelihood-like factors modifying the standard predictions.\n\n## Approach 3: Stochastic processes\n\nBoth the kime representation and the Girsanov theorem deal with continuous-time stochastic processes. The solutions to the kime-dependent Schrödinger equation may have connections to stochastic differential equations and path integral formulations.\n\n## Approach 4: Longitudinal data\n\nWhen applying the kime representations to model repeated measurement longitudinal data where the same individuals are measured over time, there may be similarities to the continuous path space considered in the Girsanov theorem context. One potential avenue of exploration would be to investigate whether the kime-phase distributions and corresponding correction terms can be cast in a Girsanov-like framework. This could provide a stronger theoretical foundation and potential generalizations of the kime approach.\n\nAdditionally, the connections to stochastic processes and path integrals may be fruitful to explore further. The Girsanov theorem has deep links to the Feynman-Kac formula and other results in stochastic analysis, which may offer insights into the kime representation.",
      "word_count": 2445
    },
    {
      "title": "R Code",
      "content": "",
      "word_count": 0
    },
    {
      "title": "Spacekime Relations to Holographic Principle and AdS/CFT Correspondence",
      "content": "This is largely motivated by [Juan Maldacena](https://www.ias.edu/sns/malda)'s lecture [\"*The Meaning of Spacetime*\" at the Perimeter Institute for Theoretical Physics](https://www.youtube.com/watch?v=DODp-ajPuU8), which suggests that the geometry of 4D spacetime is related to quantum field theory, entanglement entropy, and causality. Simple put, spacetime is an emergent phenomenon in quantum gravity. In late 1990's, Juan Maldacena proposed a model of a 5D holographic universe via a *spacetime map*.\n\nThe German mathematician Theodor Kaluza and the Swedish mathematician Oskar Klein introduced a fifth dimension to spac-time, to facilitate unification of gravity and electromagnetism as two aspects of the same force. this work launched the hunt for the string theory work on unification of forces in the hidden dimensions of hyper-spacetime.\n\nA different 5D universe holographic model by Lisa Randall, Raman Sundrum, and Juan Maldacena proposed that the fifth dimension could explain why gravity is much weaker compared to the electromagnetic and strong/week nuclear forces. The holographic universe model suggests that the observable 4D universe is the floating boundary enclosing an infinitely large negatively curved fifth dimension. The electromagnetic and nuclear forces are stuck inside a (mem)brane 4D boundary, whereas gravity propagates out into the fifth dimension.\n\nThe [Wesson/Townson 5D STM (space-time-matter) model](https://wp.towson.edu/5dstm/) also uses a 5D representation of reality. [Juan Maldacena drew connections (equivalence) between 5D string theory with gravity and ordinary quantum-field theory in 4D without gravity via a holographic projection of the latter](https://doi.org/10.1023/A:1026654312961), i.e., the 4D observable universe modeled as a holographic projection of a 5D space to a 4D boundary.\n\nWe should explore the synergies between the complex-time representation, the [Holographic Principle](https://en.wikipedia.org/wiki/Holographic_principle), and [anti-de Sitter - conformal field theory (AdS/CFT) correspondence](https://en.wikipedia.org/wiki/AdS/CFT_correspondence).\n\nLet's start by rendering the map of the Earth using different projections, including area-preserving and [conformal mapping (angle-preserving)](https://en.wikipedia.org/wiki/Conformal_map) mappings. We can try to implement a new conformal-disk geo-projection. See [these D3.js geo-projection examples](https://d3-wiki.readthedocs.io/zh_CN/master/Geo-Projections/) and the [currently implemented geo-projections](https://plotly.com/r/reference/layout/geo/#layout-geo-projection-type).\n\n\n*Problem*: Consider implementing the [Mobius transformation based mapping](https://westy31.home.xs4all.nl/Geometry/Geometry.html), which uses a group-theoretic approach to representing invertable (and univariate) *complex functions* as Mobius transformations $f:\\mathbb{C}\\to \\mathbb{C}$, $f(z)=\\frac{a\\cdot z + b}{c\\cdot z + d}$. Note that Mobius transforms can be represented as a group of $2\\times 2$ matrices, a subgroup of $SU(2)$: $$\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix}\\ .$$",
      "word_count": 368
    },
    {
      "title": "References",
      "content": "1.  Korn GA, Korn TM. [Mathematical handbook for scientists and engineers: definitions, theorems, and formulas for reference and review](https://books.google.com/books/about/Mathematical_Handbook_for_Scientists_and.html?id=A4XCAgAAQBAJ), Courier Corporation; 2000.\n\n2.  Wang Y, Shen Y, Deng D, Dinov ID. Determinism, well-posedness, and applications of the ultrahyperbolic wave equation in spacekime. Partial Differential Equations in Applied Mathematics. 2022;5:100280, [doi: 10.1016/j.padiff.2022.100280](https://doi.org/10.1016/j.padiff.2022.100280).\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number <img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script\n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"</script>\n\t\t<script type=\"text/javascript\"_uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 144
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "subtitle: \"Radon-Nikodym Derivatives, Kimemeasures, and Kime Operator\"\n---",
      "row_count": 2
    },
    {
      "section": "Chapter 3 Appendix",
      "content": "| Source of Variation         | Sum of Squares (SS) | Degrees of Freedom (df) |               Mean Squares (MS)                |                      F-Ratio                      |\n|:----------------------------|:-------------------:|:-----------------------:|:----------------------------------------------:|:-------------------------------------------------:|\n| Between samples (Treatment) |    $SS_{treat}$     |          a - 1          |            $\\frac{SS_{treat}}{a-1}$            |        $\\frac{SS_{treat}}{MS_{error(a)}}$         |\n| Within samples (Time)       |     $SS_{time}$     |          t - 1          |            $\\frac{SS_{time}}{t-1}$             |         $\\frac{SS_{time}}{MS_{error(b)}}$         |\n| Interaction (Treat\\*Time)   |     $SS_{time}$     |      (a-1)(t - 1)       | $\\frac{SS_{treat\\times times(b)}}{(a-1)(t-1)}$ | $\\frac{MS_{treat\\times times(b)}}{MS_{error(b)}}$ |\n| Error(a)                    |   $SS_{error(a)}$   |           N-a           |          $\\frac{SS_{error(a)}}{N-a}$           |                                                   |\n| Error(b)                    |   $SS_{error(b)}$   |       (N-b)(t-1)        |       $\\frac{SS_{error(b)}}{(N-a)(t-1)}$       |                                                   |\n| *Total*                     |    $SS_{total}$     |         Nt - 1          |                                                |                                                   |",
      "row_count": 8
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "knitr::opts_chunk$set(echo = TRUE, warings = FALSE, error = TRUE)",
      "line_count": 1
    },
    {
      "section": "Chapter 3 Appendix",
      "code": "library(animation)\nlibrary(circular)\nlibrary(plotly)\nepsilon <- 0.1\nsampleSize <- 1000   # total number of phases to sample for 3 different processes (x, y, z)\nsizePerTime <- 100   # number of phases to use for each fixed time (must divide sampleSize)\ncircleUniformPhi <- seq(from=-pi, to=pi, length.out=sizePerTime)\n\noopt = ani.options(interval = 0.2)\nset.seed(1234)\n# sample the the kime-phases for all 3 different processes and the r time points\nx <- rvonmises(n=sampleSize, mu=circular(pi/5), kappa=3)\ny <- rvonmises(n=sampleSize, mu=circular(-pi/3), kappa=5)\nz <- rvonmises(n=sampleSize, mu=circular(0), kappa=10)\nr <- seq(from=1, to=sampleSize/sizePerTime, length.out=10)\n\n# Define a function that renormalizes the kime-phase to [-pi, pi)\npheRenormalize <- function (x) {\n  out <- ifelse(as.numeric(x) <= pi, as.numeric(x)+pi, as.numeric(x)-pi)\n  return (out)\n}\n\n# transform Von Mises samples from [0, 2*pi) to [-pi, pi)\nx <- pheRenormalize(x)\ny <- pheRenormalize(y)\nz <- pheRenormalize(z)\n\n# vectorize the samples\nvectorX = as.vector(x)\nvectorY = as.vector(y)\nvectorZ = as.vector(z)\n# Starting phases, set the first phase index=1\nplotX = c(vectorX[1])\nplotY = c(vectorY[1])\nplotZ = c(vectorZ[1])\n\n# # iterate over all time points (outer loop) and all phases (inner loop)\n# for (t in 1:length(r)) {  # loop over time\n#   for (i in 2:100) {      # loop over kime-phases\n#     plotX = c(plotX,vectorX[i])\n#     plotY = c(plotY,vectorY[i])\n#     plotZ = c(plotZ,vectorZ[i])\n# \n#     # Try to \"stack=T the points ....\n#     #r1 = sqrt((resx$x)^2+(resx$y)^2)/2;\n#     #resx$x = r1*cos(resx$data)\n#     #resx$x = r1*cos(resx$data)\n# \n#     tempX = circular(plotX[[1]])\n#     tempY = circular(plotY[[1]])\n#     tempZ = circular(plotZ[[1]])\n#     resx <- density(tempX, bw=25, xaxt='n', yaxt='n')\n#     resy <- density(tempY, bw=25, xaxt='n', yaxt='n')\n#     resz <- density(tempZ, bw=25, xaxt='n', yaxt='n')\n#     res <- plot(resx, points.plot=TRUE, xlim=c(-1.1,1.5), ylim=c(-1.5, 1.5),\n#                 main=\"Trivariate random sampling of\\n kime-magnitudes (times) and kime-directions (phases)\",\n#                 offset=0.9, shrink=1.0, ticks=T, lwd=3, axes=F, stack=TRUE, bins=150)\n#     lines(resy, points.plot=TRUE, col=2, points.col=2, plot.info=res, offset=1.0, shrink=1.45, lwd=3, stack=T)\n#     lines(resz, points.plot=TRUE, col=3, points.col=3, plot.info=res, offset=1.1, shrink=1.2, lwd=3, stack=T)\n#     segments(0, 0, r[i]*cos(vectorX[i]), r[i]*sin(vectorX[i]), lwd=2, col= 'black')\n#     segments(0, 0, r[i]*cos(vectorY[i]), r[i]*sin(vectorY[i]), lwd=2, col= 'red')\n#     segments(0, 0, r[i]*cos(vectorZ[i]), r[i]*sin(vectorZ[i]), lwd=2, col= 'green')\n#     ani.pause()\n#   }\n# }\n####################################################\n\n# pl_list <- list()\npl_scene <- plot_ly(type='scatter3d', mode=\"markers\")\nplotX <- list() \nplotY <- list() \nplotZ <- list() \n\nplotX_df <- list()   # need separate dataframes to store all time foliations\nplotY_df <- list()\nplotZ_df <- list()\n\nfor (t in 1:length(r)) {  # loop over time\n  # loop over kime-phases\n  plotX[[t]] <- as.numeric(x[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  plotY[[t]] <- as.numeric(y[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  plotZ[[t]] <- as.numeric(z[c(( (t-1)*length(r) + 1):((t-1)*length(r) + sizePerTime))])\n  \n    # Try to \"stack=T the points ....\n    #r1 = sqrt((resx$x)^2+(resx$y)^2)/2;\n    #resx$x = r1*cos(resx$data)\n    #resx$x = r1*cos(resx$data)\n\n  tempX = circular(unlist(plotX[[t]]))\n  tempY = circular(unlist(plotY[[t]]))\n  tempZ = circular(unlist(plotZ[[t]]))\n  \n  resx <- density(tempX, bw=25, xaxt='n', yaxt='n')\n  resy <- density(tempY, bw=25, xaxt='n', yaxt='n')\n  resz <- density(tempZ, bw=25, xaxt='n', yaxt='n')\n  # res <- plot(resx, points.plot=TRUE, xlim=c(-1.1,1.5), ylim=c(-1.5, 1.5),\n  # main=\"Trivariate random sampling of\\n kime-magnitudes (times) and kime-directions (phases)\",\n  # offset=0.9, shrink=1.0, ticks=T, lwd=3, axes=F, stack=TRUE, bins=150)\n  # pl_list[[t]] \n  unifPhi_df <- as.data.frame(cbind(t=t, circleUniformPhi=circleUniformPhi))\n  plotX_df[[t]] <- as.data.frame(cbind(t=t, plotX=unlist(plotX[[t]])))\n  plotY_df[[t]] <- as.data.frame(cbind(t=t, plotY=unlist(plotY[[t]])))\n  plotZ_df[[t]] <- as.data.frame(cbind(t=t, plotZ=unlist(plotZ[[t]])))\n  \n  pl_scene <- pl_scene %>% add_trace(data=unifPhi_df, showlegend=FALSE,\n                      x = ~((t-epsilon)*cos(circleUniformPhi)), \n                      y = ~((t-epsilon)*sin(circleUniformPhi)), z=0,\n                      name=paste0(\"Time=\",t), line=list(color='gray'),\n                      mode = 'lines', opacity=0.3) %>%\n    add_markers(data=plotX_df[[t]], x=~(t*cos(plotX)), y=~(t*sin(plotX)), z=0,\n                      type='scatter3d', name=paste0(\"X: t=\",t), \n                      marker=list(color='green'), showlegend=FALSE,\n                      mode = 'markers', opacity=0.3) %>%\n    add_markers(data=plotY_df[[t]], x=~((t+epsilon)*cos(plotY)),\n                    y=~((t+epsilon)*sin(plotY)), z=0-epsilon, showlegend=FALSE,\n                    type='scatter3d', name=paste0(\"Y: t=\",t), \n                    marker=list(color='blue'),\n                    mode = 'markers', opacity=0.3) %>%\n    add_markers(data=plotZ_df[[t]], x=~((t+2*epsilon)*cos(plotZ)),\n                y=~((t+2*epsilon)*sin(plotZ)), z=0+epsilon, showlegend=FALSE,\n                type='scatter3d', name=paste0(\"Z: t=\",t), \n                marker=list(color='red'),\n                mode = 'markers', opacity=0.3)\n} \n\nmeans_df <- as.data.frame(cbind(t = c(1:length(r)),\n                                plotX_means=unlist(lapply(plotX, mean)),\n                                plotY_means=unlist(lapply(plotY, mean)),\n                                plotZ_means=unlist(lapply(plotZ, mean))))\npl_scene <- pl_scene %>% \n  # add averaged (donoised) phase trajectories\n  add_trace(data=means_df, x=~(t*cos(plotX_means)), \n        y=~(t*sin(plotX_means)), z=0,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs X\", \n        line=list(color='green', width=15), opacity=0.8) %>%\n  add_trace(data=means_df, x=~(t*cos(plotY_means)), \n        y=~(t*sin(plotY_means)), z=0-epsilon,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs X\", \n        line=list(color='blue', width=15), opacity=0.8) %>%\n  add_trace(data=means_df, x=~(t*cos(plotZ_means)), \n        y=~(t*sin(plotZ_means)), z=0+epsilon,\n        type='scatter3d', showlegend=FALSE, mode='lines', name=\"Expected Obs X\", \n        line=list(color='red', width=15), opacity=0.8) %>%\n  add_trace(x=0, y=0, z=c(-2,2), name=\"Space\", showlegend=FALSE,\n             line=list(color='gray', width=15), opacity=0.8) %>%\n  layout(title=\"Pseudo Spacekime (1D Space, 2D Kime) Kime-Phase Sampling and Foliation\",\n          scene = list(xaxis=list(title=\"Kappa1\"), yaxis=list(title=\"Kappa2\"),\n                        zaxis=list(title=\"Space\"))) %>% hide_colorbar()\npl_scene\n\n# pl_list %>%\n#   subplot(nrows = length(pl_list)) %>%\n#      layout(title=\"Integral Approximations by Riemann Sums\", legend = list(orientation = 'h'))",
      "line_count": 153
    },
    {
      "section": "Chapter 3 Appendix",
      "code": "# install.packages(\"DiagrammeR\")\n# See DiagrammeR Instructions: \n# https://epirhandbook.com/en/diagrams-and-charts.html\n\nlibrary(DiagrammeR)\ngraphStr <- \"# Mind commented instructions \ndigraph fMRI_Study_Design { # 'digraph' means 'directional graph', then the graph name\n  \n  ################# graph statement #################\n  graph [layout = dot, rankdir = LR, # TB = layout top-to-bottom\n         fontsize = 14]\n\n  #################  nodes (circles)  #################\n  node [shape = circle, fixedsize = true, width = 1.8]                      \n  \n  Start     [label = 'fMRI Run\\nStart'] \n  Begin     [label = 'Begining\\nRest period\\n(30 seconds)'] \n  Task      [label = 'Task block\\n(20 seconds)'] \n  Rest      [label = 'Rest block\\n(20 seconds)', fontcolor = darkgreen] \n  End       [label = 'Ending\\nRest block\\n(20 seconds)', fontcolor=darkgreen] \n  Total     [label = 'Run Total\\n(340 seconds)', fontcolor = darkgreen]\n  \n  ################  edges  #######\n  Start   -> Begin [label='initialize', fontcolor=red, color=red]\n  Begin -> Task [label='task', fontcolor=red, color=red]\n  Task -> Rest [label='Loop', fontcolor=red, color=red]\n  Rest -> End [label='wrap up', fontcolor=red, color=red]\n  End -> Total [label='Complete', fontcolor=red, color=red]\n  \n  #################  grouped edge  ################# \n  {Rest} -> Task [label = 'Repeat\\nTask-Rest Block\\n5 times', fontcolor=darkgreen,\n      color = darkgreen, style = dashed]\n}\n\"\ngrViz(graphStr)\n# grViz(graphStr, engine=\"circo\", width = 1000, height = 200)",
      "line_count": 36
    },
    {
      "section": "Chapter 3 Appendix",
      "code": "pl_scene",
      "line_count": 1
    },
    {
      "section": "Existence, Testability, and Falsifiability of Kime-Phase Existence and Spacekime Representation",
      "code": "library(plotly)\n# 3D Wiener Process\nN=500  # Number of random walk steps\n# Define the X, Y, and Z, coordinate displacements independently\n# xdis = rnorm(N, 0 , 1)\n# ydis = rnorm(N, 0 , 2)\n# zdis = rnorm(N, 0 , 3)\n# xdis = cumsum(xdis)\n# ydis = cumsum(ydis)\n# zdis = cumsum(zdis)\n\n# To use simulated 3D MVN Distribution\ndis <- mixtools::rmvnorm(N, mu=c(0,0,0), \n                         sigma=matrix(c(1,0,0, 0,2,0, 0,0,3), ncol=3))\n\n# aggregate the displacements to get the actual 3D Cartesian Coordinates\nxdis = cumsum(dis[,1])\nydis = cumsum(dis[,2])\nzdis = cumsum(dis[,3])\n\n# add Poisson noise\nat = rpois(N, 0.1)\n\nfor(i in c(1:N)) {\n  if(at[i] != 0) {\n    xdis[i] = xdis[i]*at[i]\n    ydis[i] = ydis[i]*at[i]\n    zdis[i] = ydis[i]*at[i]\n  }\n}\n# plot(xdis, ydis, type=\"l\", \n#   main =\"Brownian Motion in Two Dimension with Poisson Arrival Process\", \n#   xlab=\"x displacement\", ylab = \"y displacement\")\n\nplot_ly(x=xdis, y=ydis, z=zdis, type=\"scatter3d\", mode=\"markers+lines\", \n        text=~c(1:N), hoverinfo='text', \n        marker=list(color='gray'), showlegend=F) %>%\n  # Extentuate the starting and ending points\n  add_markers(x=xdis[1], y=ydis[1], z=zdis[1], marker=list(size=20,color=\"green\"),\n              text=paste0(\"Starting Node 1\")) %>%\n  add_markers(x=xdis[N], y=ydis[N], z=zdis[N],\n              marker=list(size=20,color=\"red\"), text=paste0(\"Ending Node \", N)) %>%\n  layout(title=\"3D Brownian Motion\",\n         scene=list(xaxis=list(title=\"X displacement\"), \n                    yaxis=list(title=\"Y displacement\"),\n                    zaxis=list(title=\"Z displacement\")))\n\n# 1D Wiener Process\n# dis = rnorm(N, 0, 1);\n# at = rpois(N,1)\n# for(i in 1:N) {\n#   if(at[i] != 0){\n#     dis[i]= dis[i]*at[i]\n#   } \n# }\n# dis = cumsum(dis)\n# plot(dis, type= \"l\",\n#   main= \"Brownian Motion in One Dimension with Poisson Arrival Process\", \n#   xlab=\"time\", ylab=\"displacement\")\n\n# ub = 20; lb = -20\n# xdis = rnorm(N, 0 ,1)\n# xdis1 = rep(1,N)\n# xdis1[1] = xdis[1]\n# for(i in c(1:(N-1))){   \n#   if(xdis1[i] + xdis[i+1] > ub) { xdis1[i+1] <- ub }    \n#   else if(xdis1[i] + xdis[i+1] < lb) { xdis[i+1] = lb }   \n#   else { xdis1[i+1] = xdis1[i] + xdis[i+1] } \n# } \n# \n# plot(xdis1, type=\"l\",main=\"Brownian Motion with bound in 1-dim\",      xlab=\"displacement\",ylab=\"time\")\n\n# Compute the row Euclidean differences\ndf <- data.frame(cbind(xdis, ydis, zdis))\nrowEuclidDistance <- dist(df)\nplot_ly(z=as.matrix(rowEuclidDistance), type=\"heatmap\") %>%\n  layout(title=\"Heatmap of Euclidean Distances between Consecutive Steps of Wiener Process\")\nplot_ly(x=as.vector(as.matrix(rowEuclidDistance)), type = \"histogram\") %>%\n  layout(title=\"Histogram of Euclidean Distances between Consecutive Steps of the Wiener Process\")",
      "line_count": 79
    },
    {
      "section": "Radon-Nikodym Derivative",
      "code": "library(plotly)\nN <- 10000\nxNu <- extraDistr::rlaplace(N, mu = 0, sigma = 0.4)\nyNu <- density(xNu, bw=0.2)\nxMu <- extraDistr::rlaplace(N, mu = 0, sigma = 0.5)\nyMu <- density(xMu, bw=0.2)\n# Correct second Laplace Density to ensure absolute continuity, nu << mu\nyMu$y <- 2*yMu$y\nplot_ly(x = xNu, type = \"histogram\", name = \"Data Histogram\") %>% \n    add_trace(x=yNu$x, y=yNu$y, type=\"scatter\", mode=\"lines\", opacity=0.3,\n       fill=\"tozeroy\", yaxis=\"y2\", name=\"nu, Laplace(N,0,0.4) Density\") %>% \n    add_trace(x=yMu$x, y = yMu$y, type=\"scatter\", mode=\"lines\", opacity=0.3,\n       fill=\"tozeroy\", yaxis=\"y2\", name=\"mu, Laplace(N,0,0.5) Density\") %>% \n    layout(title=\"Absolutely Continuous Laplace Distributions, nu<<mu\", \n           yaxis2 = list(overlaying = \"y\", side = \"right\"),\n           xaxis = list(range = list(-pi, pi)),\n           legend = list(orientation = 'h'))\nintegrate(approxfun(yNu), -pi, pi) # 1.000199 with absolute error < 7.6e-05\nintegrate(approxfun(yMu), -pi, pi) # 1.997212 with absolute error < 0.00023",
      "line_count": 19
    },
    {
      "section": "Heisenberg Kime-Uncertainty Principle for Kime Magnitude (Time) and Direction (Phase)",
      "code": "library(plotly)\nlibrary(tidyr)\n\n# define the space-time grid (domain)\nlenSpace <- 100\nlenTime <- 20\nspaceInd <- seq(from=0, to=2*pi, length.out = lenSpace)\ntimeInd <- seq(from=-1, to=1, length.out = lenTime)\n\noscilatoryFunc <- function(freq=1) {\n  x <- (freq/2) * spaceInd\n  t <- timeInd\n  tensor <- sin(x) %o% timeInd\n  return(tensor)\n}\n\ne1 <-  as.vector(oscilatoryFunc(freq=1)) # wide to long converted \ne4 <-  as.vector(oscilatoryFunc(freq=2))\ne9 <-  as.vector(oscilatoryFunc(freq=3))\ne16 <- as.vector(oscilatoryFunc(freq=4))\ndf_wide <- cbind(space=spaceInd, time=rep(1:lenTime, each=lenSpace), \n                 E1=e1, E4=e4, E9=e9, E16=e16)\ndf_long <- as.data.frame(df_wide) %>% \n  pivot_longer(c(\"E1\", \"E4\", \"E9\", \"E16\"), names_to = \"eigenfunctions\", values_to = \"values\")\nstr(df_long)\n\nfig <- df_long %>% \n  plot_ly(x=~space, y=~values, color=~eigenfunctions, frame=~time,\n                      text=~eigenfunctions, hoverinfo = \"text\", \n                      type='scatter', mode='lines') %>% \n  layout(xaxis = list(title = \"space\"), yaxis = list(title = \"Eigenfunction Amplitude\"),\n          showlegend = FALSE)\nfig",
      "line_count": 33
    },
    {
      "section": "Kime-Phase Operator and Kime Operator",
      "code": "library(plotly)\nlibrary(kableExtra)\n# Create a data frame for the table\nprecession_data <- data.frame(\n  Planet = c(\"Mercury\", \"Venus\", \"Earth\", \"Icarus*\"),\n  `Orbits.per.century` = c(415.2, 162.5, 100.0, 89.3),\n  `Eccentricity` = c(0.206, 0.0068, 0.017, 0.827),\n  `r_min.AU` = c(0.307, 0.717, 0.981, 0.186),\n  `GR.Predicted.arc.sec.per.century` = c(43.0, 8.6, 3.8, 10.0),\n  `Observed.arc.sec.per.century` = c(\"$43.1 \\\\pm 0.5$\", \"$8.4 \\\\pm 4.8$\", \n  \"$5.0 \\\\pm 1.2$\", \"$9.8 \\\\pm 0.8$\")\n)\n\n# Display the table using knitr::kable for formatting\nknitr::kable(precession_data, caption = \"Precession of Planetary Orbits\") %>%\n  kable_styling(latex_options = c(\"striped\", \"HOLD_position\"))",
      "line_count": 16
    },
    {
      "section": "R Code",
      "code": "N <- 10000\n\nxNu <- extraDistr::rlaplace(N, mu = 0, sigma = 0.4)\nyNu <- density(xNu, bw=0.2)\nxMu <- extraDistr::rlaplace(N, mu = 0, sigma = 0.5)\nyMu < density(xMu, bw=0.2)\n\n# Correct second Laplace Density to ensure absolute continuity, nu<<mu\nyMu$y <- 2*yMu$y\n\nplot_ly(x = xNu, type = \"histogram\", name = \"Data Histogram\") %>%\n  add_trace(x=yNu$x, y=yNu$y, type=\"scatter\", mode=\"lines\", opacity=0.3,\n             fill=\"tozeroy\", yaxis=\"y2\", name=\"nu, Laplace(N,0,0.4) Density\")  %>%\n  add_trace(x=yMu$x, y = yMu$y, type=\"scatter\", mode=\"lines\", opacity=0.3,\n             fill=\"tozeroy\", yaxis=\"y2\", name=\"mu, Laplace(N,0,0.5) Density\")  %>%\n  layout(title=\"Absolutely Continuous Laplace Distributions, nu<<mu\",\n         yaxis2 = list(overlaying = \"y\", side = \"right\"),\n         xaxis = list(range = list(-pi, pi)),\n         legend = list(orientation = 'h'))\n\nintegrate(approxfun(yNu), -pi, pi) # 1.000199 with absolute error\n# 7.6e-05\n\nintegrate(approxfun(yMu), -pi, pi) # 1.997212 with absolute error\n#  0.00023",
      "line_count": 25
    },
    {
      "section": "Spacekime Relations to Holographic Principle and AdS/CFT Correspondence",
      "code": "library(plotly)\n#  See plot_ly \"geo\" docs: https://plotly.com/r/reference/layout/geo/\n# The available projections are 'equirectangular', 'mercator', 'orthographic', 'natural earth', 'kavrayskiy7', 'miller', 'robinson', 'eckert4', 'azimuthal equal area', 'azimuthal equidistant', 'conic equal area', 'conic conformal', 'conic equidistant', 'gnomonic', 'stereographic', 'mollweide', 'hammer', 'transverse mercator', 'albers usa', 'winkel tripel', 'aitoff' and 'sinusoidal'.\n\ng <- list(projection = list(type = 'orthographic'),    #  orthographic\n          showland = TRUE, landcolor=\"gray\", \n          coastlinecolor  = \"red\", showocean=TRUE, oceancolor=\"LightBlue\",\n          showlakes=TRUE, lakecolor=\"navy\", showrivers=TRUE, rivercolor=\"blue\",\n          showcountries = TRUE, countrycolor = \"Black\",\n          lonaxis=list(showgrid=TRUE, griddash=\"dash\"), lataxis=list(showgrid=TRUE))\nplot_ly(type = 'scattergeo', mode = 'markers') %>% \n  layout(geo = g, title=\"Orthographic Earth Mapping\")\n\n# g <- list(projection = list(type = 'azimuthal equal area'),   # area preserve\n# g <- list(projection = list(type = 'conic conformal'),        # conic conformal\ng <- list(projection = list(type = 'azimuthal equidistant'),    #  azimuthal\n          showland = TRUE, landcolor=\"gray\", \n          coastlinecolor  = \"red\", showocean=TRUE, oceancolor=\"LightBlue\",\n          showlakes=TRUE, lakecolor=\"navy\", showrivers=TRUE, rivercolor=\"blue\",\n          showcountries = TRUE, countrycolor = \"Black\",\n          lonaxis=list(showgrid=TRUE, griddash=\"dash\"), lataxis=list(showgrid=TRUE))\nplot_ly(type = 'scattergeo', mode = 'markers') %>% \n  layout(geo = g, title=\"Azimuthal-Equidistant Earth Mapping\")\n\n\ng <- list(projection = list(type = 'conic conformal'),        # conic conformal\n          showland = TRUE, landcolor=\"gray\", \n          coastlinecolor  = \"red\", showocean=TRUE, oceancolor=\"LightBlue\",\n          showlakes=TRUE, lakecolor=\"navy\", showrivers=TRUE, rivercolor=\"blue\",\n          showcountries = TRUE, countrycolor = \"Black\",\n          lonaxis=list(showgrid=TRUE, griddash=\"dash\"), lataxis=list(showgrid=TRUE))\nplot_ly(type = 'scattergeo', mode = 'markers') %>% \n  layout(geo = g, title=\"Conformal-Conic Earth Mapping\")\n",
      "line_count": 34
    }
  ]
}