{
  "metadata": {
    "created_at": "2024-11-30T13:46:17.445580",
    "total_sections": 8,
    "total_code_chunks": 25,
    "total_tables": 1,
    "r_libraries": [
      "DBI",
      "RMySQL",
      "RODBC",
      "RSQLite",
      "bigrquery",
      "devtools",
      "ggplot2",
      "ggpubr",
      "kableExtra",
      "knitr",
      "magrittr"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Appendix 5: Database SQL Queries</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Jared Tianyi Chai, I Hun Chan, Brandon Cummings, Ziyuan Sun, Alan Tran, Emily Wall, Jessica Wild, Vanessa Yuzhe You, Audrey Zhang, and Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: ../SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\nIn this [DSPA Appendix](https://dspa2.predictive.space/), we will present some common strategies to interact with relational databases.\n\nThe [Structured Query Language (SQL)](https://en.wikipedia.org/wiki/SQL) is used to communicate with complex databases (DBs)that include large, structured, and relational information. SQL is the *de facto* standard language for managing, storing, and retrieval of information from relational database management systems. *SQL statements* may be used to update data elements in a database, retrieve data from a database, or query the meta-data in the system. Many relational database management systems like Oracle, Sybase, Microsoft SQL Server, Access, Ingres, and so on provide SQL [application programming interfaces (API's)](https://en.wikipedia.org/wiki/Application_programming_interface). There are also [no-SQL database systems](https://en.wikipedia.org/wiki/NoSQL), e.g., [MongoDB](https://www.mongodb.com/nosql-explained), that provide more flexibility by relaxing the *relational* requirement of SQL systems. In many situations, relational SQL DBs are impractical for large amounts of heterogeneous, incongruent, and rapidly-changing data that blends structured, semi-structured, unstructured, and polymorphic data elements. We will predominantly focus on the [standard SQL commands](https://www.w3schools.com/sql/sql_syntax.asp), such as `Select`, `Insert`, `Update`, `Delete`, `Create`, and `Drop` to interact with external databases. The [MongoDB noSQL tutorial](https://docs.mongodb.com/manual/reference/command/) provides information on interacting with noSQL DBs. In this module, we mostly focus on *data import* from SQL Databases.\n\nLet's start by exploring `R`-based data-import from SQL databases. First, we need to install and load the [RODBC(R Open Database Connectivity) package](https://cran.r-project.org/web/packages/RODBC/index.html).  \n\n\nThen, we will open a connection to the SQL server database with Data Source Name (DSN), using Microsoft Access. More details are provided in the [Microsoft documentaiton](https://technet.microsoft.com/en-us/library/cc879308%28v=sql.105%29.aspx) and in the [RODBC documentaiton](https://cran.r-project.org/web/packages/RODBC/vignettes/RODBC.pdf).",
      "word_count": 318
    },
    {
      "title": "Connections to SQL Database",
      "content": "## Set a connection\n\nAs a demonstration, we will be connecting with username \"genome\" to a MySQL database **hg19** hosted by [genome.ucsc.edu server](https://genome.ucsc.edu). \n\n\nThe RODBC library also allows connecting to other Relational Databases such as [SQL Server](https://en.wikipedia.org/wiki/Microsoft_SQL_Server) or Database-like platforms such as [Dremio](https://www.dremio.com/tutorials/). In order to connect to each of these data sources, the specific driver for that data source needs to be downloaded and installed in R. \n\nIf connecting to multiple SQL data sources simultaneously within one R project is desired, establish multiple connections in fashion similar to above.\n\n## Managing and closing existing connections\n\nAfter all needed jobs are done on the database, it is important to disconnect from the database, because the connection isn't automatically closed and there can be a limit to how many connections can be set up at the same time in R.\n\nHere we will demonstrate how to see all active connections and how to close connections.",
      "word_count": 154
    },
    {
      "title": "Basic Functions in RODBC",
      "content": "There are several other important functions in RODBC besides *dbConnect*, *dbDisconnect* and *dbListConnections* that can be useful for database users. These functions are demonstrated below.\n\n\nThe function `dbReadTable()` is equivalent to the SQL command `SELECT * FROM affyU133Plus2`.",
      "word_count": 38
    },
    {
      "title": "Querying with SQL",
      "content": "After establishing the connection to a SQL data-source, you can then pass SQL commands in string formats to the database. Here we will demonstrate such interactions with several basic SQL commands.\n\n\nThe difference between dbSendQuery and dbGetQuery is the following:\n\n - *dbSendQuery* sends a query to the database and doesn't retrieve any data until being fetched, while dbGetQuery retrieves data back from the server and can be written directly into a data structure.\n - *dbSendQuery* needs to be used with `dbClearResult()` to close the query activity before any other interaction with the server, while dbGetQuery closes the query activity on the database automatically.\n\nAs a result, dbGetQuery is more suitable for running quick queries on the server and retrieving data immediately, while *dbSendQuery* is more useful when running queries on larger datasets and when immediate results are not required.\n\nBoth of these two functions are intended for data users only, so only `SELECT` statements are intended to be sent through them. If the user has higher access to the database and intend to manipulate data, *dbExecute* is needed. This is demonstrated (but not executed) below.",
      "word_count": 185
    },
    {
      "title": "Fetching Results",
      "content": "When using *dbGetQuery*, data is automatically retrieved, or \"fetched\" from the server. However, this is not always intended. If the demand is to query first and fetch at a later time, a combination of *dbSendQuery* and fetch. Here is an example.\n\n\nRegardless which function is used to query on the database, the results from the query must be fetched into R if analysis with R is intended.",
      "word_count": 67
    },
    {
      "title": "Important SQL clauses",
      "content": "Here are a few important SQL commands that can be used when working with databases.\n\n## Basic SELECT \n\n\"SELECT (...) FROM (...)\" selects all rows (unless further specified) of specific columns of data from a table in the database. It can be used in combination with these following clauses:\n - \"DISTINCT\" selects data where replicated rows with the same values for all specified columns are removed.\n - \"WHERE\" selects data where values at specified columns meet conditions defined after this clause\n - \"AND/OR/NOT\" logic operators that allow setting complex requirement for the WHERE clause\n - \"GROUP BY\" separates the table into groups by values in a specified column. The number of groups created is equal to the number of distinct values in the column specified.\n - \"AS\" renames a column or the results yielded from operations on it to another specified name\n - \"HAVING\" works similar to WHERE class in terms of serving the purpose of qualification of values, but HAVING is different in that it allows conditioning on aggregate functions like COUNT() and works best with GROUP BY\n - \"ORDER BY\" rearrange the rows by specified orders on values of specified column(s)\n - \"LIMIT\" sets the limit to how many rows of data are selected, useful when working with large datasets\nThese clauses are shown below.\n\n\n## SELECT from multiple tables\n\nLet's now show several clauses that will be useful to work across multiple tables\n\nWorking with *Multiple Tables*:\n - \"JOIN/LEFT JOIN/RIGHT JOIN/INNER JOIN/\" JOIN functions combine two tables together where they have one or more columns sharing the same type of data.\n - \"USING\" can be an alternative to the ON clause in JOINs. It allows joining with columns of the same name while with ON you can specify and join columns with different names. \n - \"UNION\" appends one table to another where the columns of the two tables have the same data types, replicates are removed.\n - \"UNION ALL\"  appends one table to another where the columns of the two tables have the same data types without removing replicates.\n\n\nAs demonstrated, the `USING` clause eliminates the repeated name column, while the `ON` clause version of `JOIN` keeps both columns from both tables.\n\nThe difference between `JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `INNER JOIN`, and `FULL` join will be explained later.\n\nHere are a few other multi-table operators that can be useful:\n\n - \"EXCEPT\" selects rows in one table that are not present in another table. \n    - SELECT columnA FROM Table1 EXCEPT SELECT columnA FROM Table2\n\n - \"INTERSECT\" selects rows that are present in both tables. \n    - SELECT columnA FROM Table1 INTERSECT SELECT columnA FROM Table2\n\nWe will be demonstrating these two operators later with the JOINS.",
      "word_count": 448
    },
    {
      "title": "Connecting to Google `BigQuery`",
      "content": "RODBC can also connect to PaaS providers such as [Google Cloud Platform](https://cloud.google.com) and use it as a data-source. Utilizing it will allow faster query executions and minimize usage of local memory.\n\n## Navigating Google `BigQuery`\n\nSimilar to querying from a Database, running queries on Google `BigQuery` requires data sources on the platform first. These sources could be from open libraries (as we will demonstrate in Case Study I), credentialed libraries (as in Case Study II), or others that the user defines. These data sources, like any other relational database, have multiple layers of structures.\n\nThe bottom layer of `BigQuery` are `tables`, where data are stored, viewed, and queried from. When executing SQL statements, these tables are the sources of data in each statement. Tables have columns and rows, similar to other relational databases.\n\nMultiple tables can make up a `dataset`. Datasets are an intermediate layer for users to categorize and group tables of different purposes. This layer is required to be defined before tables, as tables can only be defined as a member of a specific dataset.\n\nMultiple datasets then make up a `project`. A project can be started by anyone registered on Google `BigQuery`, and registered users can also start multiple projects. However, the power of Google `BigQuery` allows users to share projects with others and gain access to projects defined by other users. These different access types include viewing data, running queries,  modifying data, and other operations on data in specific projects. These accesses and permissions are monitored by the project owners in their `IAM & Admin` page of their GCP console.\n\nAs a result of the structure described above, whenever we try to access a specific table, we need to specify its path like [ProjectName.DatasetName.TableName].\n\n## Billing in Google `BigQuery`\n\nWith smaller scale querying operations, `BigQuery` is free for public use, but it charges a fee for heavy usage. That's why it's important to specify billing information when querying.\n\nFees for `BigQuery` are billed on a per-project basis, and when supplying billing information, just provide the project that should be billed for. When you are not an eligible member of the project with the permission to bill the project, billing that project will be declined.\n\nFor many public libraries and projects, they make their data publicly available, but they don't allow the public to bill these projects. That means, users who want to query data from a public library will need to additionally provide another project that Google can bill.\n\n## Case Study I: Open Library in `BigQuery`\n\nHere we will connect to a [sample dataset](https://cloud.google.com/bigquery/public-data/) in [Google Cloud Platform](https://cloud.google.com)'s list of open libraries and query from it.\n\n### Set up `BigQuery` on GCP\n\nIn order to use BigQuery functionalities, a Google Cloud Platform account is needed\n\n 1. Go to [Google Cloud Platform](https://cloud.google.com) and register a GCP account with your email. New members to GCP are given 12-months of free trials and $300 credits.\n 2. [HS 650 DSPA Studests](https://predictive.space/) will receive $50 GCP credit (from the Instructor)\n 3. In Google Cloud Platform, start a new project.\n 4. After starting a new project, on GCP, go to [IAM & Admin](https://console.cloud.google.com/iam-admin) tab.\n 5. On this tab, you will see a list of members of the project. You can add new members and assign each member specific role(s).\n 6. Edit and make sure all members (including yourself) who need to access BigQuery functionalities from R are assigned the role of `BigQuery Admin` or similar ones that have the permission to create jobs remotely on BigQuery.\n 7. After assigning roles and permissions, go to GCP [BigQuery User Interface](https://console.cloud.google.com/bigquery)\n\nThis is the primary interface for setting up data sources. We will come to this tab only when needed to add new data sources and datasets and the remaining of the time we only need R to access data and query from GCP.\n\n\n### Connecting to Dataset\n\nWe will now demonstrate connecting to your project and the `bigquery-public-data.samples.shakespeare`.\n\n\n\n\n\n## Case Study II: MIMIC-III Intro\n\nHere, we will demonstrate how to connect to a [MIMIC-III dataset](https://mimic.physionet.org) through [Google Cloud's BigQuery platform](https://cloud.google.com/bigquery/) and query some data.\n\n### Setting Up MIMIC III on `BigQuery`\n\nSimilar to in Case Study I, MIMIC III is a project available on `BigQuery` so we will need to pin this project to the `BigQuery` interface. However, since this project is not in the public library, we will need to gain access to this project first then pin it.\n\n#### Getting Access to MIMIC III\n\nThe access to MIMIC III is granted on a per-email basis. Follow these steps to gain access to MIMIC III for the email address registered for the project owner:\n\n 1. MIMIC II data is initially stored on [PhysioNet](https://physionet.org/content/mimiciii/1.4/#files), and this is also the site for gaining access to MIMIC III.\n 2. Go to [PhysioNet Registration](https://physionet.org/register/), where you will then register an account with the email address that you desire to gain access for.\n 3. Confirm the registration after receiving an email, then log back into PhysioNet.\n 4. Go the page for [MIMIC II Data](https://physionet.org/content/mimiciii/1.4/#files), scroll to the bottom and click on `credentialed user`.\n 5. In the `Credentialing` tab of the page, click on \"apply for access\".\n 6. You will then be directed to the application page for accessing MIMIC III.\n 7. Complete the application, which involves completing an online course: CITI Program in \"Data or Specimens Only Research\", and listing your faculty supervisor as a reference.\n\n#### Adding MIMICIII to BigQuery\n\nAfter gaining access to MIMIC II data (it may take up to 2 weeks to process application), follow these steps to add the MIMIC III project to `BigQuery`:\n\n 1. Go to GCP [BigQuery User Interface](https://console.cloud.google.com/bigquery)\n 2. On the `Resources` Tab, click on `ADD DATA` then `Pin a project`.\n 3. Type in \"physionet-data\", which will pin the MIMIC III dataset to your `BigQuery` Resources tab.\n\nAfter these steps are done, you can now query directly from the MIMICIII dataset from R. However, you first need to link the physionet and the BigQuery accounts by:\n\n 1. Going to [Physionet MIMIC II Dataset](https://physionet.org/content/mimiciii/1.4/)\n 2. Scroll down to Files, Access Files, where you will see lots of CSV.gz files\n 3. Click the `Request access using Google BigQuery`.\n\n### Simple Demonstration\n\n\n\n## Case Study III: MIMIC III & Acute HF\n\n### Introduction\n\nThe following section of R-codes are developed by the University of Michigan SOCR Data Analytics - MIMICIII team and posted on [GitHub](https://github.com/SOCR/MIMIC-Analytics/blob/master/AcuteHeartFailure.Rmd). The following are excerpts from the project:\n\nHeart failure (HF) is a \"multifactorial, systemic disease\" in which a network of autoprotective mechanisms activated after cardiac injury cause significant problems with heart function ([Tanai & Frantz, 2015](https://www.ncbi.nlm.nih.gov/pubmed/26673558)). HF is one of the largest contributing factors to mortality in the United States, playing a role in 1 in 9 deaths and accounting for more than $30 billion in annual healthcare expenditures ([CDC.gov](https://www.cdc.gov/dhdsp/data_statistics/fact_sheets/fs_heart_failure.htm), [Mozaffarian et. al](https://www.ncbi.nlm.nih.gov/pubmed/26673558)). In addition to the estimated 5.7 million Americans living with HF, an additional 915,000 are newly diagnosed each year (Mozaffarian et. al). Despite recent advancements in the treatment of underlying risk factors, long-term mortality remains high with less than half of those newly diagnosed patients surviving for five years (Mozaffarian et. al).\n\nRisk stratification systems have become an important part of patient management, and have the potential to \"improve clinical outcome and resource allocation\" by \"avoiding the overtreatment of low-risk subjects or the early, inappropriate discharge of high-risk patients\" ([Passantino et. al, 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691817/)). In their comprehensive 2015 publication, Andrea Passantino and colleagues review a collection of the \"most relevant\" risk stratification systems targeted at acute heart failure (AHF) including EFFECT, OPTIMIZE-HF, APACHE-HF, and ESCAPE, among others. Passantino and her team describe the wide array of data sources and techniques used by the original authors to validate these scoring systems, including \"public registries, clinical trials, and retrospective data\" ([Passantino et. al, 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691817/)). The heterogeneity of these data sources makes direct performance comparisons difficult.\n\nIn this document, we aim to extend the work contributed by Passantino and colleagues and provide a direct comparison of the performance of these AHF risk stratification metrics on a single, unified dataset. To this end, we leverage the Medical Information Mart for Intensive Care ([MIMIC-III](https://mimic.physionet.org/)), a dataset developed by the Massachusetts Institute of Technology Lab for Computational Physiology (MIT-LCP) which contains de-identified health data for more than 40,000 intensive care unit (ICU) patients over the years 2001-2012 ([Johnson et. al, 2012](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4878278/)). It includes much of the same data commonly found in electronic health records (EHR), including demographic information, diagnosis codes, procedures, laboratory values, vital signs, free-text clinical notes, and admission, discharge, and mortality information. Moreover, it is large and comprehensive enough to serve as a proving ground for each of the individual risk stratification systems, allowing a level playing field from which to make direct comparisons of the efficacy and performance of each proposed metric.\n\nIn the spirit of [SOCR's](http://www.socr.umich.edu) [open science](https://en.wikipedia.org/wiki/Open_science) initiative, this document and it's parent repository contain the complete end-to-end computational protocol, results, and validation procedures. It is subdivided into the following partitions:\n\n 1. *Cohort selection*: criteria for inclusion into the study are codified and explained.\n 2. *Data extraction*: specific data points are extracted from the larger dataset and transformed for further analysis.\n\n### Cohort Selection\n\n\nAs the MIMIC-III dataset contains a wide variety of patients, those suitable for our particular analysis must first be isolated. Based on the manuscripts cited in [Passantino et. al's review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4691817/), we developed a set of inclusion criteria to select such a cohort. These criteria include:\n\n1. Patients diagnosed with acute heart failure, excluding those with existing chronic heart failure.\n2. Patients who survived to ICU discharge, preferring to instead focus on out-of-hospital mortality.\n3. Patients who stayed in the ICU between 24 hours and 45 days, yielding enough time to facilitate a number of test results while excluding the few ultra-long term ICU stays present in the dataset.\n4. Patients who are between 18 and 89 years of age (inclusive).\n\nThese criteria were inspired by the inclusion criteria schema described in the following references:\n\n 1. [Auble et. al, A Prediction Rule to Identify Low-risk Patients with Heart Failure. *Academic Emergency Medicine*, 2005.](https://onlinelibrary.wiley.com/doi/pdf/10.1197/j.aem.2004.11.026)  \n 2. [Abraham et. al, Predictors of In-Hospital Mortality in Patients Hospitalized for Heart Failure: Insights From the Organized Program to Initiate Lifesaving Treatment in Hospitalized Patients With Heart Failure (OPTIMIZE-HF). *Journal of the American College of Cardiology*, 2008.](https://www.sciencedirect.com/science/article/pii/S0735109708016720)  \n 3. [Peterson et. al, A Validated Risk Score for In-Hospital Mortality in Patients With Heart Failure From the American Heart Association Get With the Guidelines Program. *Circulation*, 2009.](https://www.ahajournals.org/doi/pdf/10.1161/CIRCOUTCOMES.109.854877)  \n 4. [Lee et. al, Prediction of Heart Failure Mortality in Emergent Care: A Cohort Study. *Annals of Internal Medicine*, 2012.](https://annals.org/aim/article-abstract/1170879/prediction-heart-failure-mortality-emergent-care-cohort-study)   \n 5. [Okazaki et. al, New scoring system (APACHE-HF) for predicting adverse outcomes in patients with acute heart failure: Evaluation of the APACHE II and Modified APACHE II scoring systems. *Journal of Cardiology*, 2014](https://www.sciencedirect.com/science/article/pii/S0914508714000951?via%3Dihub)  \n 6. [Salah et. al, A novel discharge risk model for patients hospitalised for acute decompensated heart failure incorporating N-terminal pro-B-type natriuretic peptide levels: a European coLlaboration on Acute decompeNsated Heart Failure: Ã‰LAN-HF Score. *Heart*, 2013](https://heart.bmj.com/content/100/2/115.long)  \n 7. [Lee et. al, Predicting Mortality Among Patients Hospitalized for Heart Failure. *JAMA*, 2003](https://jamanetwork.com/journals/jama/fullarticle/197670)  \n 8. [O'Connor, et. al, Triage After Hospitalization With Advanced Heart Failure: The ESCAPE (Evaluation Study of Congestive Heart Failure and Pulmonary Artery Catheterization Effectiveness) Risk Model and Discharge Score, *Journal of the American College of Cardiology*, 2010](https://www.sciencedirect.com/science/article/pii/S0735109709040595?via%3Dihub)\n\n#### Criteria 1: Patients diagnosed with acute heart failure\n\nTo select patients with acute heart failure, we first constructed a cohort of all heart failure patients, denoted by the ICD9 code `4280`, then subtracted those that had an additional chronic heart failure diagnosis (codes `42822, 42823, 42832, 42833, 42842 & 42843`).\n\n*Note*: To successfully run the [MIMIC-III data](https://mimic.physionet.org/) analytics example below, you need to have a [credentialed PhysioNet account](https://mimic.physionet.org/gettingstarted/access/). If you don't have [CITI certification](https://www.citiprogram.org) and/or [Physionet access](https://physionet.org), you can just check the [MIMIC-III BigQuery description/meta-data](https://mimic.physionet.org/tutorials/intro-to-mimic-iii-bq/) and skill over this example.\n\n\n#### Criteria 2: Subject survived to ICU discharge\n\nAs our target is predicting out-of-hospital mortality due to acute heart failure, we exclude patients who expired in the hospital.\n\n\n#### Criteria 3: Length of stay between 24-hours and 45 days\n\n\n#### Criteria 4: Patients 18 years or older \n\nTo select patients 18 or older, first we join the patients table and the admissions table to get the admissions ID and the date of birth. DOB is the date of birth of the given patient. All dates in the database have been shifted to protect patient confidentiality. Dates are internally consistent for the same patient, but randomly distributed in the future. Dates of birth which occur in the present time are not true dates of birth. Furthermore, dates of birth which occur before the year 1900 occur if the patient is older than 89. In these cases, the patient's age at their first admission has been fixed to 300 to obscure their age and comply with HIPAA. The shift process was as follows: the patient's age at their first admission was determined. The date of birth was then set to exactly 300 years before their first admission. Therefore, we also extract the first admission time so we can calculate the age of the patient at the time of first admission. Finally, we exclude those younger than 18 at the time of first admission.\n\n\n### Data extraction\n\n\n#### Systolic Blood Pressure",
      "word_count": 2214
    },
    {
      "title": "Additional Applications",
      "content": "There are several additional applications of RODBC that may be useful.\n\n## Cross-Data-Source Projects\n\nYou can connect to multiple databases simultaneously and query from them at the same time in one project. You can thereby transfer data across platforms utilizing R as an intermediary.These databases include local databases, which can also be created through R.\n\nHere we demonstrate how we can create a local SQL Database in your RAM and connect to it at the same time as the UCSC Genome database. We will attempt to copy some data from the Genome database into the local database.\n\n\n## JOINs, EXCEPT, INTERSECT\n\nStructured Query Language [`join`s are SQL instructions](https://en.wikipedia.org/wiki/Join_(SQL)) combining data from two or more datasets or relational tables. Let's examine the main differences between the different JOINs.\n\n - Inner Join/Join\n    + Select all rows from both tables as long as the conditions are met.\n    + Similar to finding the intersection of two sets\n - Left Join\n    + Select all rows in the first table and the matching rows in the second table.\n    + The size of the joined table has the same number of rows as the first table.\n - Right Join\n    + Similar to Left Join, but all rows of the second tables are selected instead\n - Full Join\n    + Select all rows from both tables\n    + Similar to finding the union of two sets.\n    \nWe will also be demonstrating the EXCEPT and INTERSECT operators from the previous section:\n\n - \"EXCEPT\" selects rows in one table that are not present in another table. \n    + SELECT columnA FROM Table1 EXCEPT SELECT columnA FROM Table2\n\n - \"INTERSECT\" selects rows that are present in both tables. \n    + SELECT columnA FROM Table1 INTERSECT SELECT columnA FROM Table2\n    \nThese will be demonstrated with the [pseudoYale60](https://hpcwebapps.cit.nih.gov/eyebrowse/cgi-bin/hgTables?db=mm9&hgta_group=genes&hgta_track=pseudoYale60&hgta_table=pseudoYale60) dataset here:\n\n\n\n## Database Management with SQL  \n\nThe access of the user on the database limits the types of queries one can send. However, when having administrator access, there are a few more important SQL clauses that are important to remember:\n\n - \"CREATE TABLE (...)\" generates a new table and requires definition of the table's schema\n    + CREATE TABLE (...) AS SELECT * FROM (...) LIMIT 0 copies the schema of an existing table\n - \"INSERT INTO (...) SELECT (...) FROM (...)\" takes data from one table and puts it into an existing table with known schema and structure. This requires the data to be inserted to have the same structure.\n    + \"INSERT INTO (table_name) (column1, column2, column3, ...) VALUES (value1, value2, value3, ...) appends rows to table directly without selection from another table.\n - \"UPDATE(...)SET(...)\" changes values in a table\n - \"ALTER TABLE (...) ADD/DROP/ALTER COLUMN (...)\" changes the schema of a table\n - \"DROP TABLE (...)\" removes a table and data within from a database\n\nThese statements will be demonstrated below utilizing ephemeral local SQLite database, where initial data is copied from UCSC Genome database (similar to what we did earlier). Keep in mind that dbExecute() is needed for these administrator level commands.\n\n\n## Querying Automation\n\nAs SQL doesn't have the power to automate its execution, R can be used as an automation tool as well as an user interface. User can write SQL in either R script as string or separate *.sql* files. R can then take the string or input file, parse them appropriately, then pass the SQL commands into databases one after another. This allows users to change SQL lines at one place while avoiding the risk of being able to fetch data only after the entire SQL command has been executed.\n\nLet's demonstrated this with an example.\n\n\nIn the codes above, we used a for-loop to execute the SQL commands. We used the try-catch function so that when an error occurs in execution, throw an error message and keep the remaining SQL codes running. This is especially useful if the dataset is large and the SQL commands don't depend on each other, then you can run a large dataset and multiple lines of SQL over a long period of time(e.g. overnight) without the concern of stopping in the middle.\n\nIn the example above, we intentionally made the fourth line of SQL command wrong in syntax, and the try-catch function successfully detected this error and continued executing SQL commands after throwing the error message.\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 805
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "      smooth_scroll: true\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "# install.packages(\"RODBC\", repos = \"http://cran.us.r-project.org\")\nlibrary(RODBC)",
      "line_count": 2
    },
    {
      "section": "Connections to SQL Database",
      "code": "# install.packages(\"RMySQL\"); \nlibrary(RMySQL)\n# install.packages(\"RODBC\"); \nlibrary(RODBC)\n# install.packages(\"DBI\"); \nlibrary(DBI)\n\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')",
      "line_count": 11
    },
    {
      "section": "Connections to SQL Database",
      "code": "# install.packages(\"DBI\"); \nlibrary(DBI)\n# install.packages(\"RMySQL\"); \nlibrary(RMySQL)\n# install.packages(\"RODBC\"); \nlibrary(RODBC)\n\n\n#display all current connections to MySQL databases\ndbListConnections(MySQL())\n\n#close connection [1] to MySQL database\n#dbDisconnect(dbListConnections(dbDriver(drv=\"MySQL\"))[[1]])\n\n#list and close all connections to MySQL databases\nlapply(dbListConnections(MySQL()), dbDisconnect)\n\n#setup connection\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n\n#disconnect current session from the database\ndbDisconnect(ucscGenomeConn)",
      "line_count": 25
    },
    {
      "section": "Basic Functions in RODBC",
      "code": "# install.packages(\"RMySQL\"); \nlibrary(RMySQL)\n# install.packages(\"RODBC\"); \nlibrary(RODBC)\n#setup connection\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n#Store the names of all the tables in the database into 'allTables' and display the total number of tables in the database\nallTables <- dbListTables(ucscGenomeConn); length(allTables)\n\n#List the fields in the table \"affyU133Plus2\"\ndbListFields(ucscGenomeConn, \"affyU133Plus2\")\n\n#Read the table \"affyU133Plus2\" into \"affyData\" and display the first few lines\naffyData <- dbReadTable(ucscGenomeConn, \"affyU133Plus2\");head(affyData)",
      "line_count": 17
    },
    {
      "section": "Querying with SQL",
      "code": "# set up connection\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n\n# select the top 10 rows of the genoName column from table rmsk with dbSendQuery and dbClearResult.\nrs <- dbSendQuery(ucscGenomeConn,'select genoName from rmsk limit 10')\ndbClearResult(rs)\n\n\n# select the top 10 rows of genoName column from table rmsk with dbGetQuery\ndf <- dbGetQuery(ucscGenomeConn,'select genoName from rmsk limit 10')\n\n#disconnect from database\ndbDisconnect(ucscGenomeConn)",
      "line_count": 16
    },
    {
      "section": "Querying with SQL",
      "code": "# create a new empty table named newTable using dbExecute\n#dbExecute(ucscGenomeConn,'create table newTable')",
      "line_count": 2
    },
    {
      "section": "Fetching Results",
      "code": "# set up connection\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n\n# select the top 10 rows of the genoName column from table rmsk with deSendQuery and fetch.\nrs <- dbSendQuery(ucscGenomeConn,'select genoName from rmsk limit 10')\nfetch(rs)\ndbClearResult(rs)\n\n#disconnect from database\ndbDisconnect(ucscGenomeConn)",
      "line_count": 13
    },
    {
      "section": "Important SQL clauses",
      "code": "ucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n#The tables we will be working with: pseudoYale60 and pseudoYale60Class from the hg19 database:\ndbListFields(ucscGenomeConn, \"pseudoYale60\")\ndbListFields(ucscGenomeConn, \"pseudoYale60Class\")\n\n\n# select the top 10 rows of all columns from table pseudoYale60Class.\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class limit 10')\n\n# select the top 10 rows of the class column from table pseudoYale60Class.\ndbGetQuery(ucscGenomeConn,'select class from pseudoYale60Class limit 10')\n\n# select the top 10 distinct rows of the class column from table pseudoYale60Class.\ndbGetQuery(ucscGenomeConn,'select distinct class from pseudoYale60Class limit 10')\n\n# select the top 10 rows of name and class columns from table pseudoYale60Class where class is 'Ambiguous'.\ndbGetQuery(ucscGenomeConn,'select name, class from pseudoYale60Class WHERE class = \\'Ambiguous\\' limit 10')\n\n# select the top 10 rows of all columns from table pseudoYale60Class where class is 'Ambiguous' and owner is 'Yale'.\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE class = \\'Ambiguous\\' AND owner = \\'Yale\\' limit 10')\n\n# select the top 10 rows of all columns from table pseudoYale60Class where class is 'Ambiguous' or owner is 'Yale'.\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE class = \\'Ambiguous\\' OR owner = \\'Yale\\' limit 10')\n\n# select the top 10 rows of all columns from table pseudoYale60Class where class is \\'Ambiguous\\' or \\'Processed\\'.\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE class = \\'Ambiguous\\' OR class = \\'Processed\\' limit 10')\n\n# select the top 10 rows of all columns from table pseudoYale60Class where class is not \\'Ambiguous\\'.\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE NOT class = \\'Ambiguous\\' limit 10')\n\n# select class and how many names in the class from pseudoYale60Class\ndbGetQuery(ucscGenomeConn,'select count(name) as number_of_names, class from pseudoYale60Class GROUP BY class')\n\n# select class and how many names in the class from pseudoYale60Class if the number of names in the class is greater than 4000\ndbGetQuery(ucscGenomeConn,'select count(name) as number_of_names, class from pseudoYale60Class group by class having count(name) > 4000')\n\n#disconnect from database\ndbDisconnect(ucscGenomeConn)",
      "line_count": 41
    },
    {
      "section": "Important SQL clauses",
      "code": "ucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n#The tables we will be working with: pseudoYale60 and pseudoYale60Class from the hg19 database\n\n#joins the pseudoYale60 table with pseudoYale60Class table, where each row is matched on the name column\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60 join pseudoYale60Class on pseudoYale60.name = pseudoYale60Class.name limit 10')\n\n#alternatively using USING Clause\ndbGetQuery(ucscGenomeConn,'select * from pseudoYale60 join pseudoYale60Class USING (name) limit 10')\n\n#Append the first 10 names from pseudoYale60Class to the first 10 names of pseudoYale60, and the replicated rows are removed\ndbGetQuery(ucscGenomeConn,'(select name from pseudoYale60 limit 10) union (select name from pseudoYale60Class limit 10)')\n\n#Append the first 10 names from pseudoYale60Class to the first 10 names of pseudoYale60\ndbGetQuery(ucscGenomeConn,'(select name from pseudoYale60 limit 10) union all (select name from pseudoYale60Class limit 10)')\n\n\n#disconnect from database\ndbDisconnect(ucscGenomeConn)",
      "line_count": 21
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "#install.packages(\"bigrquery\")\nlibrary(DBI)\nlibrary(RODBC)\nlibrary(bigrquery)",
      "line_count": 4
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "## Before you build these notes: RMD -> HTML, you have to browser authenticate\n## Run this RMD command in console\n#  > bq_auth()\n## enter your credentials\n## change the email below, and finally run this command.\n\n#Specify the project that you want to connect to\nbilling <- 'golden-agency-258816' #'noble-sun-253320','golden-agency-258816'\n#Connect using a pre-authorized email account for the project\nbq_auth(email = 'dinov@umich.edu', path = NULL,\n  scopes = c(\"https://www.googleapis.com/auth/bigquery\",\n  \"https://www.googleapis.com/auth/cloud-platform\"),\n  cache = gargle::gargle_oauth_cache(),\n  use_oob = gargle::gargle_oob_default(), token = NULL)#change email to pre-authorized email",
      "line_count": 14
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "#set 'billing' to the project that will be billed\n\n#Connect to the project\ncon <- dbConnect(bigquery(),\n                 project = \"bigquery-public-data\",\n                 billing = billing\n)\n\n#list first few lines of shakespeare table\nsql <- \"SELECT * FROM [bigquery-public-data.samples.shakespeare] LIMIT 5\"\nresult <- query_exec(sql, project=billing)\n\n############## https://github.com/r-dbi/bigrquery/issues/421\n\n# tb <- bq_project_query(billing, sql)\n# df <- bq_table_download(tb, page_size = 100000)\n\nresult\n#disconnect from Google BigQuery\ndbDisconnect(con)\n\n###############################################\ntb <- bq_project_query(\n  bq_test_project(),\n  \"SELECT count(*) FROM p[bigquery-public-data.samples.shakespeare] LIMIT 5\"\n)",
      "line_count": 26
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "#install.packages(\"bigrquery\")\nlibrary(DBI)\nlibrary(RODBC)\nlibrary(bigrquery)",
      "line_count": 4
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "#Connect to the MIMIC III database, eciu_crd_demo dataset\ncon <- dbConnect(bigquery(),\n                 project = \"physionet-data\",\n                 dataset = \"eicu_crd_demo\",\n                 billing = billing\n)\n\n#list tables in eciu_crd_demo\ndbListTables(con)\n\n#list fields in table infusiondrug\ndbListFields(con,\"infusiondrug\")\n#list fields in table medication\ndbListFields(con,\"medication\")\n\n#query from table infusiondrug\nsql <- \"SELECT drugname FROM [physionet-data.eicu_crd_demo.infusiondrug] group by drugname limit 10\"\nresult <- query_exec(sql, project=billing)\nresult\n\n#disconnect from Google BigQuery\ndbDisconnect(con)",
      "line_count": 22
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "#install.packages(\"bigrquery\")\nlibrary(DBI)\nlibrary(RODBC)\nlibrary(bigrquery)\n#install.packages(\"knitr\")\n#install.packages(\"kableExtra\")\n#install.packages(\"ggplot2\")\n#install.packages(\"ggpubr\")\n#install.packages(\"magrittr\")\nlibrary(\"magrittr\")\nlibrary('knitr')         # Make knitted tables (\"kables\")\nlibrary('kableExtra')    # Extra kable formatting options\nlibrary('ggplot2')       # Plotting library\nlibrary('ggpubr')",
      "line_count": 14
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "#Connect to the MIMIC III database, eciu_crd_demo dataset\ncon <- dbConnect(bigquery(),\n                 project = \"physionet-data\",\n                 dataset = \"eicu_crd_demo\",\n                 billing = billing #set billing to Project id\n)\n\nsql <- \"SELECT *\n        FROM [physionet-data.mimiciii_clinical.diagnoses_icd]\n        WHERE icd9_code IN ('4280')\n        AND hadm_id NOT IN (\n            SELECT hadm_id\n            FROM [physionet-data.mimiciii_clinical.diagnoses_icd]\n            WHERE icd9_code IN ('42822', '42823', '42832', '42833', '42842', '42843') \n            GROUP BY hadm_id\n        )\"\ncohort <- query_exec(sql, project=billing)",
      "line_count": 17
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "adm.tbl <- query_exec(\n  sprintf( \"SELECT HADM_ID, DISCHTIME, DEATHTIME\n        FROM [physionet-data.mimiciii_clinical.admissions] \n        WHERE hadm_id IN (%s)\",\n      paste(cohort$HADM_ID, collapse=\", \")),\n  project=billing, max_pages = Inf)\n# Merge with cohort object\ncohort <- merge(cohort, adm.tbl, by=\"HADM_ID\")\n# Remove subjects with in hospital date of death (aka they have a deathtime in admissions table)\ncohort <- cohort[is.na(cohort$DEATHTIME), ]",
      "line_count": 10
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "admissions.tbl <- query_exec(\n  sprintf( \"SELECT *\n            FROM [physionet-data.mimiciii_clinical.admissions] \n            WHERE hadm_id IN (%s)\", \n          paste(cohort$HADM_ID, collapse=\", \")),\n  project=billing, max_pages = Inf)\n# Calculate length of stay\nadmissions.tbl$LOS <- difftime(admissions.tbl$DISCHTIME, admissions.tbl$ADMITTIME, unit='days')\n# Merge with cohort object\ncohort <- merge(cohort, admissions.tbl[, c('SUBJECT_ID', 'HADM_ID', 'DISCHTIME', 'ADMITTIME', 'LOS')], by=c('SUBJECT_ID', 'HADM_ID'))\n# Plot length of stay before removal\nfig <- ggplot(cohort, aes(x=LOS)) +\n  geom_histogram() + \n  ggtitle('Length of Stay') + xlab('Days')\nprint(fig)\n# Remove encounters where LOS falls outside bounds\ncohort <- cohort[cohort$LOS > 1 & cohort$LOS <= 45, ]\ncohorttemp <- cohort",
      "line_count": 18
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "sql <- sprintf(\n       \"SELECT admissions.HADM_ID as admissions_HADM_ID, admissions.ADMITTIME as admissions_ADMITTIME, patients.DOD as DOD, patients.DOD_HOSP as DOD_HOSP, patients.DOD_SSN as DOD_SSN, patients.DOB as patients_DOB\n        FROM [physionet-data.mimiciii_clinical.admissions] AS admissions\n        JOIN [physionet-data.mimiciii_clinical.patients] AS patients\n        ON admissions.subject_id = patients.subject_id\n        WHERE admissions.hadm_id IN (%s)\",\n        paste(cohort$HADM_ID, collapse = \", \"))\nage.tbl <- query_exec(sql, project=billing)\nage.tbl$age <- difftime(age.tbl$admissions_ADMITTIME, age.tbl$patients_DOB, units='days')\nage.tbl <- data.frame(age.tbl[, c('admissions_HADM_ID', 'age', 'DOD', 'DOD_HOSP', 'DOD_SSN')])\ncolnames(age.tbl) <- c('HADM_ID', 'AGE', 'DOD', 'DOD_HOSP', 'DOD_SSN')\ncohort <- merge(cohort, age.tbl, by=\"HADM_ID\")\ncohort$AGE <- as.numeric(cohort$AGE) / 365\ncohort <- cohort[cohort$AGE < 90 & cohort$AGE >= 18, ]\n# Plot length of stay before removal\nfig <- ggplot(cohort, aes(x=AGE)) +\n  geom_histogram() + \n  ggtitle('Subject Age') + xlab('Years')\nprint(fig)",
      "line_count": 19
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "pull.last.event <- function(hadm_id, itemcodes, return.fields, table, project, max_pages=Inf) {\n  sql <- sprintf(\"\n                 SELECT *\n                 FROM [physionet-data.mimiciii_clinical.%s] AS table1\n                 INNER JOIN (\n                   SELECT hadm_id, MAX(charttime), MAX(row_id)\n                   FROM [physionet-data.mimiciii_clinical.%s]\n                   WHERE itemid IN (%s)\n                   AND hadm_id IN (%s)\n                   GROUP BY hadm_id\n                 ) AS table2\n                 ON table1.row_id = table2.f1_\",\n                 table, table, \n                 paste(itemcodes, collapse=\", \"),\n                 paste(hadm_id, collapse=\", \")\n  )\n  \n  data <- query_exec(sql, project=project, max_pages=max_pages)\n  colnames(data) <- gsub('table[0-9]_', '', colnames(data))\n  return(data[ , return.fields])\n}",
      "line_count": 21
    },
    {
      "section": "Connecting to Google `BigQuery`",
      "code": "sbp.itemcodes <- c( 6, 51, 442, 3313, 3315, 3317, 3321, 3323, 3325, 6701, 228152, 224167, 227243, 220050, 220179, 225309 )\nreturn.fields <- c('SUBJECT_ID', 'HADM_ID', 'ITEMID', 'CHARTTIME', 'VALUENUM', 'VALUEUOM')\ndata.sbp <- pull.last.event(cohort$HADM_ID, sbp.itemcodes, return.fields, 'chartevents', billing, max_pages=1)\n\n#disconnect from Google BigQuery\ndbDisconnect(con)",
      "line_count": 6
    },
    {
      "section": "Additional Applications",
      "code": "#Create an ephemeral in-memory RSQLite database connect to it through RODBC\n# install.packages(\"RSQLite\")\n#remove.packages(\"devtools\")\n#install.packages(\"devtools\", repos=\"http://cran.rstudio.com/\", dependencies=TRUE)\n#remove.packages(\"RSQLite\")\n#install.packages(\"RSQLite\", repos=\"http://cran.rstudio.com/\", dependencies=TRUE)\n#install.packages(\"rlang\", repos=\"http://cran.rstudio.com/\", dependencies=TRUE)\n#install.packages(\"usethis\", repos=\"http://cran.rstudio.com/\", dependencies=TRUE)\nlibrary(devtools)\nlibrary(\"RSQLite\")\nlibrary(RODBC)\nLocalConnection <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\n#Connect to UCSC Genome database\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n#Copy data from database into dataframe\ndf_selected <- dbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE class = \\'Ambiguous\\' AND owner = \\'Yale\\' limit 10')\ndf_selected\n\n#Write dataframe into local database\ndbWriteTable(LocalConnection,\"UCSC\",df_selected)\ndbListTables(LocalConnection)\ndbGetQuery(LocalConnection,'select * from UCSC')\n\n#Disconnect and cleanup\ndbDisconnect(LocalConnection)\ndbDisconnect(ucscGenomeConn)",
      "line_count": 30
    },
    {
      "section": "Additional Applications",
      "code": "library(\"RSQLite\")\nlibrary(RODBC)\nLocalConnection <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\n#Connect to UCSC Genome database\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n#Copy data from database into local database. Since the two tables have been pre-matched, we delete some rows so that we can artificially create differences between the two tables' keys\ndf_selected <- dbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class limit 12345')\ndbWriteTable(LocalConnection,\"Class\",df_selected)\ndf_selected <- dbGetQuery(ucscGenomeConn,'select * from pseudoYale60 limit 10000')\ndbWriteTable(LocalConnection,\"Yale\",df_selected)\n\n#Display columns of table 'Class'\ndbListFields(LocalConnection,'Class')\n#Display number of rows of table 'Class'\nnrow(dbGetQuery(LocalConnection,'select * from Class'))\n#Display columns of table 'Yale'\ndbListFields(LocalConnection,'Yale')\n#Display number of rows of table 'Yale'\nnrow(dbGetQuery(LocalConnection,'select * from Yale'))\n\n#Inner join the two tables on 'name' column\nInnerJoin <- dbGetQuery(LocalConnection, 'select Class.*, Yale.chrom, Yale.strand from Class INNER JOIN Yale on Class.name = Yale.name')\n#Display number of rows of the resulting table. It's smaller than both original tables\nnrow(InnerJoin)\n\n#Left join the two tables whereas the 'Class' tables is the left table on 'name' column\nLeftJoin <- dbGetQuery(LocalConnection, 'select Class.*, Yale.chrom, Yale.strand from Class LEFT JOIN Yale on Class.name = Yale.name')\n#Display number of rows of the resulting table. It's the same size as the left table 'Class'\nnrow(LeftJoin)\n\n#Left join the two tables whereas the 'Yale' tables is the left table on 'name' column\nLeftJoin <- dbGetQuery(LocalConnection, 'select Class.*, Yale.chrom, Yale.strand from Yale LEFT JOIN Class on Class.name = Yale.name')\n#Display number of rows of the resulting table. It's the same size as the left table 'Yale'\nnrow(LeftJoin)\n\n#Use except to select names from Yale that are not in Class\nEXCEPT1 <- dbGetQuery(LocalConnection, \"select name from Yale except select name from Class\")\nnrow(EXCEPT1)\n\n#Use except to select names from Class that are not in Yale\nEXCEPT2 <- dbGetQuery(LocalConnection, \"select name from Class except select name from Yale\")\nnrow(EXCEPT2)\n\n#Use intersect to select names that are in both Class and Yale\nINTERSECT <- dbGetQuery(LocalConnection, \"select name from Yale intersect select name from Class\")\nnrow(INTERSECT)\n\n#The number of rows in Yale that is not in Class and the number of rows in both Yale and Class add up to the total number of rows in Yale\nnrow(EXCEPT1)+nrow(INTERSECT)\n\n#The number of rows in Class that is not in Yale and the number of rows in both Yale and Class add up to the total number of rows in Class\nnrow(EXCEPT2)+nrow(INTERSECT)\n\n#As right join and Full Join are not supported on SQLite, we will display the syntax here and demonstrate later with BigQuery.\n#RightJoin <- dbGetQuery(LocalConnection, 'select Class.*, Yale.chrom, Yale.strand from Class RIGHT JOIN Yale on Class.name = Yale.name')\n#nrow(RightJoin)\n#FullJoin <- dbGetQuery(LocalConnection, 'select Class.*, Yale.chrom, Yale.strand from Class FULL JOIN Yale on Class.name = Yale.name')\n#nrow(FullJoin)\n\n#Disconnect and cleanup\ndbDisconnect(LocalConnection)\ndbDisconnect(ucscGenomeConn)",
      "line_count": 66
    },
    {
      "section": "Additional Applications",
      "code": "#Create an ephemeral in-memory RSQLite database connect to it through RODBC\n# install.packages(\"RSQLite\")\nlibrary(\"RSQLite\")\nlibrary(RODBC)\nLocalConnection <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\n#Connect to UCSC Genome database\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n\n#Copy data from database into dataframe\ndf_selected <- dbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE class = \\'Ambiguous\\' AND owner = \\'Yale\\' limit 10')\n\n#Write dataframe into local database\ndbWriteTable(LocalConnection,\"UCSC\",df_selected)\ndbListTables(LocalConnection)\n\n#Alternatively, creates a table UCSC_Insert that has the same schema as UCSC\nfb <- dbExecute(LocalConnection,'CREATE TABLE UCSC_Insert AS SELECT * FROM UCSC LIMIT 0')\ndbListTables(LocalConnection)\ndbListFields(LocalConnection,'UCSC_Insert')\n\n#Copy the table from table 'UCSC' into the new table UCSC_Insert.\nfb <- dbExecute(LocalConnection,'INSERT INTO UCSC_Insert SELECT * FROM UCSC LIMIT 4')\ndbGetQuery(LocalConnection,'SELECT * FROM UCSC_Insert')\n\n#Append a new row to table UCSC_Insert.\nfb <- dbExecute(LocalConnection,\"INSERT INTO UCSC_Insert (name,class,owner) VALUES ('123','456','789')\")\ndbGetQuery(LocalConnection,'SELECT * FROM UCSC_Insert')\n\n#Update one row of table UCSC_Insert's 'owner' column to be 'ELAY\nfb <- dbExecute(LocalConnection,\"UPDATE UCSC_Insert SET owner = 'ELAY' WHERE name = 'PGOHUM00000232568'\")\ndbGetQuery(LocalConnection,'SELECT * FROM UCSC_Insert')\n\n#change name of table UCSC_Insert to 'UCSC_Insert2'\nfb <- dbExecute(LocalConnection,\"ALTER TABLE UCSC_Insert RENAME TO UCSC_Insert2\")\ndbListTables(LocalConnection)\n\n#add new column to table UCSC_Insert2 named 'newOwner', with string as its datatype\nfb <- dbExecute(LocalConnection,\"ALTER TABLE UCSC_Insert2 ADD COLUMN newOwner string\")\ndbGetQuery(LocalConnection,'SELECT * FROM UCSC_Insert2')\n\n#remove table UCSC_Insert2\nfb <- dbExecute(LocalConnection,\"DROP TABLE UCSC_Insert2\")\ndbListTables(LocalConnection)\n\n#Disconnect and cleanup\nfb <- dbDisconnect(LocalConnection)\nfb <- dbDisconnect(ucscGenomeConn)",
      "line_count": 51
    },
    {
      "section": "Additional Applications",
      "code": "#Setup connections \n#install.packages(\"RSQLite\")\nlibrary(devtools)\nlibrary(RODBC)\nlibrary(\"RSQLite\")\nLocalConnection <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\nucscGenomeConn <- dbConnect(MySQL(),\n                user='genome',\n                dbname='hg19',\n                host='genome-mysql.soe.ucsc.edu')\n\n#and move relevant tables into an ephemeral in-memory SQLite database\ndf_selected <- dbGetQuery(ucscGenomeConn,'select * from pseudoYale60Class WHERE class = \\'Ambiguous\\' AND owner = \\'Yale\\' limit 10')\ndbWriteTable(LocalConnection,\"UCSC\",df_selected)\ndbListTables(LocalConnection)\n\n#Keep all SQL Statements at one place\n#Here they are stored in string format in a vector, but you can Write them outside of the R Script in csv, txt, sql, or other formats, and take them in as input\nSQLString <- c(\"CREATE TABLE UCSC_Insert AS SELECT * FROM UCSC LIMIT 0\",\n               \"INSERT INTO UCSC_Insert SELECT * FROM UCSC LIMIT 4\",\n               \"INSERT INTO UCSC_Insert (name,class,owner) VALUES ('123','456','789')\",\n               \"UPDATE UCSC_Insert    owner = 'ELAY' WHERE name = 'PGOHUM00000232568'\",\n               \"ALTER TABLE UCSC_Insert RENAME TO UCSC_Insert2\",\n               \"ALTER TABLE UCSC_Insert2 ADD COLUMN newOwner string\")\n\n#Use for-loop to automate the execution of these sql statements\nfor (i in 1:length(SQLString)) {\n   possibleError <- tryCatch({fb <- dbExecute(LocalConnection,SQLString[i])},\n                             error = function(e) {print(c('error at SQL Line # ', i))}\n                             )\n if(inherits(possibleError, \"error\")){\n    next\n  }\n}\n\n#Check the result of these statements\ndbListTables(LocalConnection)\ndbListFields(LocalConnection, 'UCSC_Insert2')\n\n#Store the data into a dataframe in R\n#Data can also be exported out of R in formats that you define\nresult <- dbGetQuery(LocalConnection,'SELECT * FROM UCSC_Insert2')\nresult\n\n#Disconnect and clean up\nfb <- dbDisconnect(LocalConnection)\nfb <- dbDisconnect(ucscGenomeConn)",
      "line_count": 48
    }
  ]
}