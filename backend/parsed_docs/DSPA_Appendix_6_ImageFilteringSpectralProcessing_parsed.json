{
  "metadata": {
    "created_at": "2024-11-30T13:46:17.475634",
    "total_sections": 16,
    "total_code_chunks": 86,
    "total_tables": 1,
    "r_libraries": [
      "EBImage",
      "abind",
      "brainR",
      "jpeg",
      "knitr"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Appendix 6: Image Convolution, Filtering, Fourier Transform using *EBImage*</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: ../SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\nIn this [DSPA Appendix](https://dspa2.predictive.space/), we present examples of image processing, spectral manipulation, and filtering.",
      "word_count": 69
    },
    {
      "title": "Getting started",
      "content": "*r Biocpkg(\"EBImage\")* is an `R` package distributed as part of the [Bioconductor](https://bioconductor.org) project. To install the package, start `R` and enter:\n\n\nOnce the `R` package(\"EBImage\") is installed, it can be loaded by the following command.\n\n\n - **Note**: If you can't install the [EBImage package](https://www.bioconductor.org/packages/release/bioc/html/EBImage.html) for any reason (e.g., operating system or version incompatibility, this is not a problem as you should be able to use the [*jpeg* package](https://cran.r-project.org/web/packages/jpeg) instead.",
      "word_count": 69
    },
    {
      "title": "Reading, displaying and writing images",
      "content": "Basic `R` package(\"EBImage\") functionality includes reading, writing, and displaying of images. Images are read using the function `readImage`, which takes as input a filename or an URL. To start off, let us load a sample picture distributed with the package.\n\n\nThe `R` package (\"EBImage\") currently supports three image file formats: `jpeg`, `png` and `tiff`. Additional image formats can be imported using the `R` GitHubPkg (\"aoles/RBioFormats\"), which adds support for a wide range of file formats including proprietary microscopy image data and metadata.\n\nImported images can be visualized by the function `display()`.\n\n\nIn interactive `R` sessions, `display` opens the image in a JavaScript viewer in a web browser tab. Mouse or keyboard shortcuts allow zooming in and out of the image, panning, and cycling through multiple image frames. Images can also be displayed using core `R` plotting methods, which allows for combining images with other plotting functionality, e.g., adding text labels on top of the image.\n\n\nThe graphics displayed in an `R` device can be saved using `R` package(\"base\") `R` functions `dev.print` or `dev.copy`. For example, let's save our annotated image as a JPEG file and verify its size on disk.\n\n\n\n\n\nThe default behavior of `display` can be globally changed by setting the `\"options(\"EBImage.display\") to either `\"browser\"` or `\"raster\"`. This is useful, for example, to preview images inside RStudio.\n\nIt is also possible to read and view color images,\n\n\n\n\nImages may contain multiple frames, in which case they can be displayed all at once in a grid\narrangement by specifying the function argument `all = TRUE`.\n\n\n\n\nAlternatively, single frames can be displayed.\n\n\nIn addition to importing images, images can be exported (saved) to files using the `EBImage::writeImage()`.  The image that we loaded was a `r toupper(strsplit(basename(f), split=\".\", fixed=TRUE)[[1L]][2L])` file. To save this image as a JPEG file, the JPEG format allows setting a quality value between 1 and 100 to reflect the desired level of image compression.  The default value of the `quality` argument of `writeImage` is 100. Smaller values yield images of smaller size, but worse resolution (less detail).\n\n\nSimilarly, we could have saved the image as a TIFF file and set which compression algorithm we want to use. For a complete list of available parameters see `?writeImage`.",
      "word_count": 368
    },
    {
      "title": "Image data representation",
      "content": "*EBImage* uses a package-specific class `Image` to store and process images. It extends the `R` base class `array`, and all `R` package(\"EBImage\") functions can also be called directly on matrices and arrays. You can find out more about this class by typing `?Image`.  Let us peek into the internal structure of an `Image` object.\n\n\nThe `.Data` slot contains a numeric array of pixel intensities. We see that in this case the array is two-dimensional, with `r dim(img)[1L]` times `r dim(img)[2L]` elements, and corresponds to the pixel width and height of the image. These dimensions can be accessed using the `dim` function, just like for regular arrays.\n\n\nImage data can be accessed as a plain `R` `array` using the `imageData` accessor.\n\n\nThe `as.array()` method can be used to coerce an `Image` to an `array`.\n\n\nThe distribution of pixel intensities can be plotted in a histogram, and their range inspected using the `range` function.\n\n\nA useful summary of `Image` objects is also provided by the `show` method, which is invoked if we simply type the object's name.\n\n\nFor a more compact representation without the preview of the intensities array use the `print` method with the argument `short` set to `TRUE`.\n\n\nColor images are based on 3 channels (RBG).\n\n\nThey differ from gray-scale images by the property `colorMode` and the number of dimensions, 3 (B&W) vs. 4 (color). \nThe `colorMode` slot turns out to be convenient when dealing with stacks of images.  If it is set to `gray-scale`, then the third and all higher dimensions of the array are considered as separate image frames corresponding, for instance, to different z-positions, time points, replicates, etc.  On the other hand, if `colorMode` is `Color`, then the third dimension is assumed to hold different color channels, and only the fourth and higher dimensions are\nused for multiple image frames. `imgcol` contains three color channels, which correspond to the red, green and blue intensities of the photograph.  However, this does not necessarily need to be the case, and the number of color channels is arbitrary.\n\nThe \"frames.total\" and \"frames.render\" fields shown by the object summary correspond to the total number of frames contained in the image, and to the number of rendered frames. These numbers can be accessed using the function `numberOfFrames` by specifying the `type` argument.\n\n\nImage frames can be extracted using `getFrame` and `getFrames`. `getFrame` returns the i-th frame contained in the image y. If `type` is `\"total\"`, the function is unaware of the color mode and returns an xy-plane. For `type=\"render\"` the function returns the i-th image as shown by the display function. While `getFrame` returns just a single frame, `getFrames` retrieves a list of frames which can serve as input to `lapply`-family functions. See the \"Global thresholding\" section for an illustration of this approach.\n\nLet's look at nuclear/cellular imaging data, which contains 4 total frames that correspond to the 4 separate gray-scale images, as indicated by \"frames.render\".",
      "word_count": 484
    },
    {
      "title": "Color management",
      "content": "As described in the previous section, the class `Image` extends the base class `array` and uses `colorMode` to store how the color information of the multi-dimensional data should be handled. \n\nThe function `colorMode` can be used to access and change this property, modifying the rendering mode of an image. For example, if we take a `Color` image and change its mode to `gray-scale`, then the image won't display as a single color image anymore but rather as three separate gray-scale frames corresponding to the red, green and blue channels. The function `colorMode` does not change the actual content of the image but only changes the way the image is rendered by `R` package(\"EBImage\").\n\n\nColor space conversions between `gray-scale` and `Color` images are performed using the function `channel`.\nIt has a flexible interface which allows to convert either way between the modes, and can be used \nto extract color channels. Unlike `colorMode`, `channel` changes the pixel intensity values of the image.\n\n`Color` to `gray-scale` conversion modes include taking a uniform average across the RGB channels,\nand a weighted luminescence preserving conversion mode better suited for display purposes.\n\nThe `asred`, `asgreen` and `asblue` modes convert a gray-scale image or array into a color image of the specified hue. \n\nThe convenience function `toRGB` promotes a gray-scale image to RGB color space by replicating it across the red, green and blue channels, which is equivalent to calling `channel` with mode set to `rgb`. When displayed, this image doesn't look different from its gray-scale origin, which is expected because the information between the color channels is the same. To combine three gray-scale images into a single RGB image use the function `rgbImage`.\n\nThe function `Image` can be used to construct a color image from a character vector or array of named `R` colors (as listed by `colors()`) and/or hexadecimal strings of the form \"\\#rrggbb\" or \"\\#rrggbbaa\".",
      "word_count": 311
    },
    {
      "title": "Manipulating images",
      "content": "Being numeric arrays, images can be conveniently manipulated by any of R's arithmetic operators. For example, we can produce a negative image by simply subtracting the image from its maximum value.\n\n\nWe can also increase the brightness of an image through addition, adjust the contrast through multiplication, and apply gamma correction through exponentiation.\n\n\nIn the example above we have used `combine` to merge individual images into a single multi-frame image object.\n\nFurthermore, we can crop and threshold images with standard matrix operations.\n\n\n\nThe thresholding operation returns an `Image` object with binarized pixel values. \nThe `R` data type used to store such an image is `logical`.\n\n\nFor image transposition, use `transpose` rather than R's `R` package(\"base\") function `t`. This is because the former one works also on color and multiframe images by swapping its spatial dimensions.",
      "word_count": 135
    },
    {
      "title": "Spatial transformations",
      "content": "We just saw one type of spatial transformation, transposition, but there are many more, for example translation, rotation, reflection and scaling. The `translate()` method moves the image plane by the specified two-dimensional vector in such a way that pixels that end up outside the image region are cropped, and pixels that enter into the image region are set to background.\n\n\nThe background color can be set using the argument `bg.col` common to all relevant spatial transformation functions. The default sets the value of  background pixels to zero which corresponds to black. Let us demonstrate the use of this argument with `rotate` which rotates the image clockwise by the given angle.\n\n\n\n\nTo scale an image to desired dimensions use `resize`. If you provide only one of either width or height, the other dimension is automatically computed keeping the original aspect ratio. \n\n\n\n\nThe functions `flip` and  `flop` reflect the image around the image horizontal and vertical axis, respectively.\n\n\nSpatial linear transformations are implemented using the general `affine` transformation. It maps image pixel coordinates `px` using a 3x2 transformation matrix `m` in the following way: `cbind(px, 1) %*% m`. For example, horizontal sheer mapping can be applied by",
      "word_count": 195
    },
    {
      "title": "Filtering",
      "content": "## Linear filters\n\nA common preprocessing step involves cleaning up the images by removing\nlocal artifacts or noise through smoothing.  An intuitive approach is to\ndefine a window of a selected size around each pixel and average the values within that\nneighborhood. After applying this procedure to all pixels, the new, smoothed image is obtained.\nMathematically, this can be expressed as\n$$\nf'(x,y) = \\frac{1}{N} \\sum_{s=-a}^{a}\\sum_{t=-a}^{a} f(x+s, y+t),\n$$\nwhere $f(x,y)$ is the value of the pixel at position $(x, y)$, and $a$ determines the\nwindow size, which is $2a+1$ in each direction.  $N=(2a+1)^2$ is the number of pixels\naveraged over, and $f'$ is the new, smoothed image.\n\nMore generally, we can replace the moving average by a weighted average, using a weight\nfunction $w$, which typically has the highest value at the window midpoint ($s=t=0$) and then\ndecreases towards the edges. \n$$\n(w * f)(x,y) = \\sum_{s=-\\infty}^{+\\infty} \\sum_{t=-\\infty}^{+\\infty} w(s,t)\\, f(x+s, y+s).\n$$\nFor notational convenience, we let the summations range from $-\\infty$ to $+\\infty$, even if in practice the sums are finite and $w$ has only a finite number of non-zero values. In fact, we can think of the weight function $w$ as another image, and this operation is also called the *convolution* of the images $f$ and $w$, indicated by the the symbol $*$.\n\nConvolution is a linear operation in the sense that $w*(c_1f_1+c_2f_2)=c_1w*f_1 + c_2w*f_2$\nfor any two images $f_1$, $f_2$ and numbers $c_1$, $c_2$.\n\nIn `R` Biocpkg(\"EBImage\"), the 2-dimensional convolution is implemented by the function `filter2`, and the auxiliary \nfunction `makeBrush` can be used to generate the weight function. \nIn fact, `filter2` does not directly perform the summation indicated in the equation above.\nInstead, it uses the Fast Fourier Transformation in a way that is mathematically equivalent \nbut computationally more efficient.\n\n\nHere we have used a Gaussian filter of width 5 given by `sigma`. \nOther available filter shapes include `\"box\"` (default), `\"disc\"`, `\"diamond\"` and `\"line\"`, for some of which the kernel can be binary; see `?makeBrush` for details.\n\nIf the filtered image contains multiple frames, the filter is applied to each frame separately. For convenience, images can be also smoothed using the wrapper function `gblur` which performs Gaussian smoothing with the filter size automatically adjusted to `sigma`. \n\n\nIn signal processing the operation of smoothing an image is referred to as low-pass filtering. High-pass filtering is the opposite operation which allows to detect edges and sharpen images. This can be done, for instance, using a Laplacian filter.\n\n\n## Median filter\n\nAnother approach to perform noise reduction is to apply a median filter, which is a non-linear technique as opposed to the low pass convolution filter described in the previous section. Median filtering is particularly effective in the case of speckle noise, and has the advantage of removing noise while preserving edges. \n\nThe local median filter works by scanning the image pixel by pixel, replacing each pixel by the median of its neighbors inside a window of specified size. This filtering technique is provided in the `R` package(\"EBImage\") by the function `medianFilter`. We demonstrate its use by first corrupting the image with uniform noise, and reconstructing the original image by median filtering.\n\n\n## Morphological operations\n\nBinary images are images which contain only two sets of pixels, with values, say 0 and 1, representing the background and foreground pixels. Such images are subject to several non-linear morphological operations: erosion, dilation, opening, and closing. These operations work by overlaying a mask, called the structuring element, over the binary image in the following way:\n\n* erosion: For every foreground pixel, put the mask around it, and if any pixel covered by the mask is from the background, set the pixel to background.\n\n* dilation: For every background pixel, put the mask around it, and if any pixel covered by the mask is from the foreground, set the pixel to foreground.\n\n\n\n\n\nOpening and closing are combinations of the two operations above: opening performs erosion followed by dilation, while closing does the opposite, i.e, performs dilation followed by erosion. Opening is useful for morphological noise removal, as it removes small objects from the background, and closing can be used to fill small holes in the foreground. These operations are implemented by `opening` and `closing`.",
      "word_count": 701
    },
    {
      "title": "Thresholding",
      "content": "## Global thresholding\n\nIn the \"Manipulating images\" section we have already demonstrated how to set a global threshold on an image.\nThere we used an arbitrary cutoff value.\nFor images whose distribution of pixel intensities follows a bi-modal histogram a more systematic approach involves using the *Otsu's method*. Otsu's method is a technique to automatically perform clustering-based image thresholding. Assuming a bi-modal intensity distribution, the algorithm separates image pixels into foreground and background. The optimal threshold value is determined by minimizing the combined intra-class variance.\n\nOtsu's threshold can be calculated using the function `otsu`. When called on a multi-frame image, the threshold is calculated for each frame separately resulting in an output vector of length equal to the total number of frames in the image.\n\n\nNote the use of `getFrames` to split the image into a list of individual frames, and `combine` to merge the results back together.\n\n## Adaptive thresholding\n\nThe idea of adaptive thresholding is that, compared to straightforward thresholding from \nthe previous section, the threshold is allowed to be different in different\nregions of the image. In this way, one can anticipate spatial dependencies of the\nunderlying background signal caused, for instance, by uneven illumination or by stray\nsignal from nearby bright objects.\n\nAdaptive thresholding works by comparing each pixel's intensity to the background determined from a \nlocal neighborhood. This can be achieved by comparing the image to its smoothed version, where the filtering\nwindow is bigger than the typical size of objects we want to capture.\n\n\nThis technique assumes that the objects are relatively sparsely distributed in the image, so that the signal distribution in the neighborhood is dominated by background. While for the nuclei in our images this assumption makes sense, for other situations you may need to make different assumptions. The adaptive thresholding \nusing a linear filter with a rectangular box is provided by `thresh`, which uses a faster implementation compared to directly using `filter2`.",
      "word_count": 321
    },
    {
      "title": "Fourier Transform",
      "content": "## FFT Shift Function\n\n\n\n## FT\n\n\n## Magnitude and Phase\n\n\n## IFT Reconstruction\n\n\n## Low-pass frequency filtering\n\nRemove Fourier coefficients indexed above $k_{x,max/2}$ and $k_{y,max/2}$.\n\n\n## High-pass frequency filtering\n\nRemove the Fourier coefficients indexed below $k_{x,max/4}$ and $k_{y,max/4}$.",
      "word_count": 38
    },
    {
      "title": "Image segmentation",
      "content": "Image segmentation performs partitioning of an image, and is typically used to identify objects in an image. Non-touching connected objects can be segmented using the function `bwlabel`, while `watershed` and `propagate` use more sophisticated algorithms able to separate objects which touch each other.\n\n`bwlabel` finds every connected set of pixels other than the background, and relabels these sets with a unique increasing integer. It can be called on a thresholded binary image in order to extract objects.\n\n\nThe pixel values of the `logo_label` image range from 0 corresponding to background to the number of objects it contains, which is given by\n\n\nTo display the image we normalize it to the (0,1) range expected by the display function. This results in different objects being rendered with a different shade of gray.\n\n\nThe horizontal gray-scale gradient which can be observed reflects the way `bwlabel()` scans the image and labels the connected sets: from left to right and from top to bottom. Another way of visualizing the segmentation is to use the `colorLabels` function, which color codes the objects by a random permutation of unique colors.\n\n\n## Watershed \n\nSome of the nuclei in `nuc` are quite close to each other and get merged into one big object when thresholded, as seen in `nuc_th`.\n`bwlabel` would incorrectly identify them as a single object. The watershed transformation allows us to overcome this issue.\nThe `watershed` algorithm treats a gray-scale image as a topographic relief, or height-map. Objects that stand out of the background\nare identified and separated by flooding an inverted source image. In case of a binary image its distance map  can serve as the input height-map. The distance map, which contains for each pixel the distance to the nearest background pixel, can be obtained by `distmap`.\n\n\nLet's try the watershed algorithm on structural MRI data.\n\n\n\n## Voronoi tessellation\n\nVoronoi tessellation is useful when we have a set of seed points (or regions) and want to partition the space that lies between these seeds in such a way that each point in the space is assigned to its closest seed. This function is implemented in the `R` package(\"EBImage'') by the function `propagate`. Let us illustrate the concept of Voronoi tessellation with a basic example. We use the nuclei mask `nmask` as seeds and partition the space between them.\n\nThe [2D Interactive Voronoi Tessellation webapp](https://socr.umich.edu/HTML5/others/Voronoi_App/) provides a hands-on example of dynamic 2D Voronoi tessellation.\n\n\nThe basic definition of Voronoi tessellation, which we have given above, allows\nfor two generalizations:\n\n* By default, the space that we partition is the full, rectangular image area, but indeed we could restrict ourselves to any arbitrary subspace. This is akin to finding the shortest distance from each point to the next seed not in a simple flat landscape, but in a landscape that is interspersed by lakes and rivers (which you cannot cross), so that all paths need to remain on the land. `propagate` allows for this generalization through its `mask` argument. \n\n* By default, we think of the space as flat -- but in fact it could have hills and canyons, so that the distance between two points in the landscape not only depends on their x- and y-positions but also on the ascents and descents, up and down in z-direction, that lie in between. You can specify such a landscape to `propagate` through its `x` argument.  \n      \nMathematically, we can say that instead of the simple default case (a flat rectangle image with an Euclidean metric), we perform the Voronoi segmentation on a Riemann manifold, which\ncan have an arbitrary shape and an arbitrary metric.\nLet us use the notation $x$ and $y$ for the column and row coordinates\nof the image, and $z$ for the elevation of the landscape. For two neighboring points, defined\nby coordinates $(x, y, z)$ and $(x+dx, y+dy, z+dz)$, the distance between them is given by\n\n$$\nds = \\sqrt{ \\frac{2}{\\lambda+1} \\left[ \\lambda \\left( dx^2 + dy^2 \\right) + dz^2 \\right] }.\n$$\nFor $\\lambda=1$, this reduces to $ds = ( dx^2 + dy^2 + dz^2)^{1/2}$. Distances between points further apart are obtained by summing $ds$ along the shortest path between them.  The parameter $\\lambda\\ge0$ has been introduced as a convenient control of the relative weighting between sideways movement (along the $x$ and $y$ axes) and vertical movement.  Intuitively, if you imagine yourself as a hiker in such a landscape, by choosing $\\lambda$ you can specify how much you are prepared to climb up and down to overcome a mountain, versus sideways walking around it.\n\nWhen $\\lambda$ is large, the expression becomes equivalent to $ds = \\sqrt{dx^2 + dy^2}$, i. e., \nthe importance of $dz$ becomes negligible. This is what we did when we used `lambda = 100` in our `propagate` example.\n\nA more advanced application of `propagate` to the segmentation of cell bodies is presented in the \"Cell segmentation example\" section.\n\nLet's demonstrate Voronoi tessellation using the 2D sMRI image.",
      "word_count": 818
    },
    {
      "title": "Object manipulation",
      "content": "## Object removal\n\nThe `R` package(\"EBImage\") defines an object mask as a set of pixels with the same unique integer value. Typically, images containing object masks are the result of segmentation functions such as `bwlabel`, `watershed`, or `propagate`. Objects can be removed from such images by `rmObject`, which deletes objects from the mask simply by setting their pixel values to 0. By default, after object removal all the remaining objects are relabeled so that the highest object ID corresponds to the number of objects in the mask. The `reenumerate` argument can be used to change this behavior and to preserve original object IDs. \n\n\nIn the example above we demonstrate how the object removal function can be applied to a multi-frame image by providing a list of object indices to be removed from each frame. Additionally we have set `reenumerate` to `FALSE` keeping the original object IDs.\n\n\nRecall that 0 stands for the background. If at some stage we decide to relabel the objects, we can use for this the standalone function `reenumarate`.\n\n\n## Filling holes and regions\n\nHoles in object masks can be filled using the function `fillHull`.\n\n\n`floodFill` fills a region of an image with a specified color. The filling starts at the given point, and the filling region is expanded to a connected area in which the absolute difference in pixel intensities remains below `tolerance`. The color specification uses `R` color names for `Color` images, and numeric values for `gray-scale` images.\n\n\n\n## Highlighting objects\n\nGiven an image containing object masks, the function `paintObjects()` can be used to highlight the objects from the mask in the target image provided in the `tgt` argument.  Objects can be outlined and filled with colors of given opacities specified in the `col` and `opac` arguments, respectively. If the color specification is missing or equals `NA` it is not painted.\n\n\n\nIn the example above we have created a new mask `overlay` matching the size of our target image `img`, and copied the mask containing the \"EBImage\" logo into that overlay mask. The output of `paintObjects` retains the color mode of its target image, therefore in order to have the logo highlighted in color it was necessary to convert `img` to an RGB image first, otherwise the result would be a gray-scale image. The `thick` argument controls the object contour drawing: if set to `FALSE`, only the inner one-pixel wide object boundary is marked; if set to `TRUE`, also the outer boundary gets highlighted resulting in an increased two-pixel contour width.",
      "word_count": 416
    },
    {
      "title": "Cell segmentation example",
      "content": "We conclude our vignette by applying the functions described before to the task of segmenting cells. Our goal is to computationally identify and qualitatively characterize the cells in the sample fluorescent microscopy images. Even though this by itself may seem a modest goal, this approach can be applied to collections containing thousands of images, and that need is no longer a modest aim!\n\nWe start by loading the images of nuclei and cell bodies. To visualize the cells we overlay these images as the green and the blue channel of a false-color image.\n\n\nFirst, we segment the nuclei using `thresh`, `fillHull`, `bwlabel`\nand `opening`.\n\n\nNext, we use the segmented nuclei as seeds in the Voronoi segmentation of the cytoplasm.\n\n\nTo visualize our segmentation we use `paintObject`.",
      "word_count": 126
    },
    {
      "title": "Learning Activity",
      "content": "Let's use the protocol below to gain some hands-on experience processing a 2D brain image using basic functions, without relying on the EBImage package. Of course, you can also apply all of the methods presented above on this [2D (JPEG) brain image](https://umich.instructure.com/courses/38100/files/1627149/download?download_frd=1).",
      "word_count": 42
    },
    {
      "title": "Art, Image Processing and Spectral Representation of Signals",
      "content": "Let's try to interrogate some artworks from [UMMA](https://umma.umich.edu)'s [2020 Curriculum-Collection](https://umma.umich.edu/exhibitions/2020/curriculum-collection). This museum collection exhibits a variety of University of Michigan courses that use artwork to enhance the curricula of graduate and undergraduate courses. Each course examines a set of artworks and objects to address the nature of reality, imagination, and vision relative to the current environments and scientific advances. The art exhibit demonstrates diverse and creative examples of learning across the art, science, technology, and biosocial disciplines. \n\nFor instance, the painting by [Khaled al-Saa'i, \"*Winter in Ann Arbor*\",](https://exchange.umma.umich.edu/resources/28008) constructed with natural ink, tempera and gouache on paper, may reveal interesting patterns, shapes, topological structure, and calligraphy. We can interrogate the artwork using the following image-processing protocol:\n\n - Load the image from UMMA CC archive\n - Display the native image\n - Examine the image intensity histogram (for all 3 RGB channels)\n - Generate negative image\n - Optionally crop and threshold the image\n - Apply low-pass filtering (smoothing)\n - Apply high-pass filtering (Laplace filter) for edge detection\n - Denoise the image\n - Apply watershed segmentation\n - Segment and Voronoi tessellate the image\n\nFurther studies may include text recognition, identification of obfuscated structures, or graphical representation of 2D scenes. The readers are encouraged to expand this protocol and apply it to some of the other artworks in these UMMA [collection 1](https://exchange.umma.umich.edu/resources/28008) and [collection 2](https://exchange.umma.umich.edu/resources/28421).",
      "word_count": 221
    },
    {
      "title": "Reference",
      "content": "- This module is based on [Bioconductor/EBImage documentation](https://www.bioconductor.org/packages/devel/bioc/vignettes/EBImage/inst/doc/EBImage-introduction.html) and [Spacekime Analytics/TCIU materials](https://www.socr.umich.edu/TCIU/).\n \n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 107
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "      smooth_scroll: true\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "library(knitr)\nlibrary(EBImage)\n.dpi = 100\nset.seed(0)\nopts_chunk$set(comment=NA, fig.align=\"center\", dpi=.dpi)\nknit_hooks$set(crop=NULL)\noptions(EBImage.display = \"raster\")",
      "line_count": 7
    },
    {
      "section": "Getting started",
      "code": "install.packages(\"BiocManager\")\nBiocManager::install(\"EBImage\")",
      "line_count": 2
    },
    {
      "section": "Getting started",
      "code": "library(\"EBImage\")",
      "line_count": 1
    },
    {
      "section": "Getting started",
      "code": "# install.packages(\"jpeg\")\nlibrary(jpeg)",
      "line_count": 2
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "f = system.file(\"images\", \"sample.png\", package=\"EBImage\")\nimg = readImage(f)",
      "line_count": 2
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "display(img, method=\"browser\")",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "display(img, method=\"raster\")\ntext(x = 20, y = 20, label = \"Colorful Parrots\", adj = c(0,1), col = \"orange\", cex = 2)",
      "line_count": 2
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "filename = \"parrots.jpg\"\ndev.print(jpeg, filename = filename , width = dim(img)[1], height = dim(img)[2])",
      "line_count": 2
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "display(img, method=\"raster\")\ntext(x = 20, y = 20, label = \"Colorful Parrots\", adj = c(0,1), col = \"orange\", cex = 2)\nfilename = \"parrots.jpg\"\ndev.print(jpeg, filename = filename , width = dim(img)[1], height = dim(img)[2])",
      "line_count": 4
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "file.info(filename)$size",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "invisible(file.remove(filename))",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "imgcol = readImage(system.file(\"images\", \"sample-color.png\", package=\"EBImage\"))",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "imgcol = readImage(system.file(\"images\", \"sample-color.png\", package=\"EBImage\"))\ndisplay(imgcol)",
      "line_count": 2
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "display(imgcol)",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "nuc = readImage(system.file(\"images\", \"nuclei.tif\", package=\"EBImage\"))",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "nuc = readImage(system.file(\"images\", \"nuclei.tif\", package=\"EBImage\"))\ndisplay(nuc, method = \"raster\", all = TRUE)",
      "line_count": 2
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "display(nuc, method = \"raster\", all = TRUE)",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "display(nuc, method = \"raster\", frame = 2)",
      "line_count": 1
    },
    {
      "section": "Reading, displaying and writing images",
      "code": "writeImage(imgcol, \"sample.jpeg\", quality = 85)",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "str(img)",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "dim(img)",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "imageData(img)[1:3, 1:6]",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "is.Image( as.array(img) )",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "hist(img)\nrange(img)",
      "line_count": 2
    },
    {
      "section": "Image data representation",
      "code": "img",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "print(img, short=TRUE)",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "print(imgcol, short=TRUE)",
      "line_count": 1
    },
    {
      "section": "Image data representation",
      "code": "numberOfFrames(imgcol, type = \"render\")\nnumberOfFrames(imgcol, type = \"total\")",
      "line_count": 2
    },
    {
      "section": "Image data representation",
      "code": "nuc",
      "line_count": 1
    },
    {
      "section": "Color management",
      "code": "colorMode(imgcol) = Grayscale\ndisplay(imgcol, all=TRUE)",
      "line_count": 2
    },
    {
      "section": "Color management",
      "code": "colorMat = matrix(rep(c(\"red\",\"green\", \"#0000ff\"), 25), 5, 5)\ncolorImg = Image(colorMat)\ncolorImg\ndisplay(colorImg, interpolate=FALSE)",
      "line_count": 4
    },
    {
      "section": "Manipulating images",
      "code": "img_neg = max(img) - img\ndisplay( img_neg )",
      "line_count": 2
    },
    {
      "section": "Manipulating images",
      "code": "img_comb = combine(\n  img,\n  img + 0.3,\n  img * 2,\n  img ^ 0.5\n)\n\ndisplay(img_comb, all=TRUE)",
      "line_count": 8
    },
    {
      "section": "Manipulating images",
      "code": "img_crop = img[366:749, 58:441]\nimg_thresh = img_crop > .5",
      "line_count": 2
    },
    {
      "section": "Manipulating images",
      "code": "img_crop = img[366:749, 58:441]\nimg_thresh = img_crop > .5\ndisplay(img_thresh)",
      "line_count": 3
    },
    {
      "section": "Manipulating images",
      "code": "display(img_thresh)",
      "line_count": 1
    },
    {
      "section": "Manipulating images",
      "code": "img_thresh",
      "line_count": 1
    },
    {
      "section": "Manipulating images",
      "code": "img_t = transpose(img)\ndisplay( img_t )",
      "line_count": 2
    },
    {
      "section": "Spatial transformations",
      "code": "img_translate = translate(img, c(100,-50))\ndisplay(img_translate)",
      "line_count": 2
    },
    {
      "section": "Spatial transformations",
      "code": "img_rotate = rotate(img, 30, bg.col = \"white\")",
      "line_count": 1
    },
    {
      "section": "Spatial transformations",
      "code": "img_rotate = rotate(img, 30, bg.col = \"white\")\ndisplay(img_rotate)",
      "line_count": 2
    },
    {
      "section": "Spatial transformations",
      "code": "display(img_rotate)",
      "line_count": 1
    },
    {
      "section": "Spatial transformations",
      "code": "img_resize = resize(img, w=256, h=256)",
      "line_count": 1
    },
    {
      "section": "Spatial transformations",
      "code": "img_resize = resize(img, w=256, h=256)\ndisplay(img_resize )",
      "line_count": 2
    },
    {
      "section": "Spatial transformations",
      "code": "display(img_resize)",
      "line_count": 1
    },
    {
      "section": "Spatial transformations",
      "code": "img_flip = flip(img)\nimg_flop = flop(img)\n\ndisplay(combine(img_flip, img_flop), all=TRUE)",
      "line_count": 4
    },
    {
      "section": "Spatial transformations",
      "code": "m =  matrix(c(1, -.5, 128, 0, 1, 0), nrow=3, ncol=2)\nimg_affine = affine(img, m)\ndisplay( img_affine )",
      "line_count": 3
    },
    {
      "section": "Filtering",
      "code": "w = makeBrush(size = 31, shape = 'gaussian', sigma = 5)\nplot(w[(nrow(w)+1)/2, ], ylab = \"w\", xlab = \"\", cex = 0.7)\n\nimg_flo = filter2(img, w)\ndisplay(img_flo)",
      "line_count": 5
    },
    {
      "section": "Filtering",
      "code": "nuc_gblur = gblur(nuc, sigma = 5)\ndisplay(nuc_gblur, all=TRUE )",
      "line_count": 2
    },
    {
      "section": "Filtering",
      "code": "fhi = matrix(1, nrow = 3, ncol = 3)\nfhi[2, 2] = -8\nimg_fhi = filter2(img, fhi)\ndisplay(img_fhi)",
      "line_count": 4
    },
    {
      "section": "Filtering",
      "code": "l = length(img)\nn = l/10\npixels = sample(l, n)\nimg_noisy = img\nimg_noisy[pixels] = runif(n, min=0, max=1)\ndisplay(img_noisy)\nimg_median = medianFilter(img_noisy, 1)\ndisplay(img_median)",
      "line_count": 8
    },
    {
      "section": "Filtering",
      "code": "shapes = readImage(system.file('images', 'shapes.png', package='EBImage'))\nlogo = shapes[110:512,1:130]",
      "line_count": 2
    },
    {
      "section": "Filtering",
      "code": "shapes = readImage(system.file('images', 'shapes.png', package='EBImage'))\nlogo = shapes[110:512,1:130]\ndisplay(logo)\ndisplay(logo)",
      "line_count": 4
    },
    {
      "section": "Filtering",
      "code": "kern = makeBrush(5, shape='diamond')\ndisplay(kern, interpolate=FALSE)",
      "line_count": 2
    },
    {
      "section": "Filtering",
      "code": "logo_erode= erode(logo, kern)\nlogo_dilate = dilate(logo, kern)\n\ndisplay(combine(logo_erode, logo_dilate), all=TRUE)",
      "line_count": 4
    },
    {
      "section": "Thresholding",
      "code": "threshold = otsu(nuc)\nthreshold\nnuc_th = combine( mapply(function(frame, th) frame > th, getFrames(nuc), threshold, SIMPLIFY=FALSE) )\ndisplay(nuc_th, all=TRUE)",
      "line_count": 4
    },
    {
      "section": "Thresholding",
      "code": "disc = makeBrush(31, \"disc\")\ndisc = disc / sum(disc)\noffset = 0.05\nnuc_bg = filter2( nuc, disc )\nnuc_th = nuc > nuc_bg + offset\ndisplay(nuc_th, all=TRUE)",
      "line_count": 6
    },
    {
      "section": "Thresholding",
      "code": "display( thresh(nuc, w=15, h=15, offset=0.05), all=TRUE )",
      "line_count": 1
    },
    {
      "section": "Fourier Transform",
      "code": "# FFT SHIFT\n#' This function is useful for visualizing the Fourier transform with the zero-frequency \n#' component in the middle of the spectrum.\n#' \n#' @param img_ff A Fourier transform of a 1D signal, 2D image, or 3D volume.\n#' @param dim Number of dimensions (-1, 1, 2, 3).\n#' @return A properly shifted FT of the array.\n#' \nfftshift <- function(img_ff, dim = -1) {\n\n  rows <- dim(img_ff)[1]    \n  cols <- dim(img_ff)[2]\n  # planes <- dim(img_ff)[3]\n\n  swap_up_down <- function(img_ff) {\n    rows_half <- ceiling(rows/2)\n    return(rbind(img_ff[((rows_half+1):rows), (1:cols)], img_ff[(1:rows_half), (1:cols)]))\n  }\n\n  swap_left_right <- function(img_ff) {\n    cols_half <- ceiling(cols/2)\n    return(cbind(img_ff[1:rows, ((cols_half+1):cols)], img_ff[1:rows, 1:cols_half]))\n  }\n  \n  #swap_side2side <- function(img_ff) {\n  #  planes_half <- ceiling(planes/2)\n  #  return(cbind(img_ff[1:rows, 1:cols, ((planes_half+1):planes)], img_ff[1:rows, 1:cols, 1:planes_half]))\n  #}\n\n  if (dim == -1) {\n    img_ff <- swap_up_down(img_ff)\n    return(swap_left_right(img_ff))\n  }\n  else if (dim == 1) {\n    return(swap_up_down(img_ff))\n  }\n  else if (dim == 2) {\n    return(swap_left_right(img_ff))\n  }\n  else if (dim == 3) {\n    # Use the `abind` package to bind along any dimension a pair of multi-dimensional arrays\n    # install.packages(\"abind\")\n    library(abind)\n    \n    planes <- dim(img_ff)[3]\n    rows_half <- ceiling(rows/2)\n    cols_half <- ceiling(cols/2)\n    planes_half <- ceiling(planes/2)\n    \n    img_ff <- abind(img_ff[((rows_half+1):rows), (1:cols), (1:planes)], \n                    img_ff[(1:rows_half), (1:cols), (1:planes)], along=1)\n    img_ff <- abind(img_ff[1:rows, ((cols_half+1):cols), (1:planes)], \n                    img_ff[1:rows, 1:cols_half, (1:planes)], along=2)\n    img_ff <- abind(img_ff[1:rows, 1:cols, ((planes_half+1):planes)], \n                    img_ff[1:rows, 1:cols, 1:planes_half], along=3)\n    return(img_ff)\n  }\n  else {\n    stop(\"Invalid dimension parameter\")\n  }\n}",
      "line_count": 61
    },
    {
      "section": "Fourier Transform",
      "code": "str(img)\nimgMat <- matrix(img, nrow = dim(img)[1], ncol = dim(img)[2])\ndim(imgMat)",
      "line_count": 3
    },
    {
      "section": "Fourier Transform",
      "code": "ft_imgMat <- fft(imgMat)  # fftw2d # Display Re(FT):\ndisplay(fftshift(ft_imgMat))",
      "line_count": 2
    },
    {
      "section": "Fourier Transform",
      "code": "mag_ft_imgMat <- sqrt(Re(ft_imgMat)^2+Im(ft_imgMat)^2)\ndisplay(mag_ft_imgMat)\n\n# Phase  <- atan(Im(img_ff)/Re(img_ff))\nphase_ft_imgMat  <- atan2(Im(ft_imgMat), Re(ft_imgMat))\n# display(phase_ft_imgMat)\n\n# Display FT\nEBImage::display(log(fftshift(ft_imgMat)), title=\"FT Magnitude (Cyrillic Alphabet)\")",
      "line_count": 9
    },
    {
      "section": "Fourier Transform",
      "code": "# IFT Reconstruction: Magnitude=mag_ft_imgMat   AND  Phase=phase_ft_imgMat\nReal = mag_ft_imgMat * cos(phase_ft_imgMat)\nImaginary = mag_ft_imgMat * sin(phase_ft_imgMat)\nift_ft_imgMat = Re(fft(Real+1i*Imaginary, inverse = T)/length(mag_ft_imgMat))\nEBImage::display(ift_ft_imgMat, method = \"raster\", title=\"(IFT o FT) Magnitude=Img | Phase=Img\")",
      "line_count": 5
    },
    {
      "section": "Fourier Transform",
      "code": "mag_ft_imgMat_1 <- mag_ft_imgMat\nfor (i in 1:dim(mag_ft_imgMat)[1]) {\n  for (j in 1:dim(mag_ft_imgMat)[2]) {\n    if (abs(i-(dim(mag_ft_imgMat)[1])/2) > (dim(mag_ft_imgMat)[1])/4 | \n        abs(j-(dim(mag_ft_imgMat)[2])/2) > (dim(mag_ft_imgMat)[2])/4) {\n      mag_ft_imgMat_1[i,j] <- 0\n    }\n  }\n}\n\nEBImage::display(mag_ft_imgMat_1, method = \"raster\", title=\"(IFT o LowPassFilter o FT) Magnitude=Img | Phase=Img\")\n\nReal = mag_ft_imgMat_1 * cos(phase_ft_imgMat)\nImaginary = mag_ft_imgMat_1 * sin(phase_ft_imgMat)\nift_ft_imgMat_1 = Re(fft(Real+1i*Imaginary, inverse = T)/length(mag_ft_imgMat_1))\nEBImage::display(100*ift_ft_imgMat_1, method = \"raster\", title=\"(IFT o LowPassFilter o FT) Magnitude=Img | Phase=Img\")",
      "line_count": 16
    },
    {
      "section": "Fourier Transform",
      "code": "# High-pass frequency filtering, remove Fourier coeff's indexed below k_{x,max/4} and k_{y,max/4}\nmag_ft_imgMat_2 <- mag_ft_imgMat\nfor (i in 1:dim(mag_ft_imgMat)[1]) {\n  for (j in 1:dim(mag_ft_imgMat)[2]) {\n    if (abs(i-(dim(mag_ft_imgMat)[1])/2) < (dim(mag_ft_imgMat)[1])/4 & \n        abs(j-(dim(mag_ft_imgMat)[2])/2) < (dim(mag_ft_imgMat)[2])/4) {\n      mag_ft_imgMat_2[i,j] <- 0\n    }\n  }\n}\n\nEBImage::display(mag_ft_imgMat_2, method = \"raster\", title=\"(IFT o HighPassFilter o FT) Magnitude=Img | Phase=Img\")\n\nReal = mag_ft_imgMat_2 * cos(phase_ft_imgMat)\nImaginary = mag_ft_imgMat_2 * sin(phase_ft_imgMat)\nift_ft_imgMat_2 = Re(fft(Real+1i*Imaginary, inverse = T)/length(mag_ft_imgMat_2))\nEBImage::display(ift_ft_imgMat_2, method = \"raster\", title=\"(IFT o HighPassFilter o FT) Magnitude=Img | Phase=Img\")",
      "line_count": 17
    },
    {
      "section": "Image segmentation",
      "code": "logo_label = bwlabel(logo)\ntable(logo_label)",
      "line_count": 2
    },
    {
      "section": "Image segmentation",
      "code": "max(logo_label)",
      "line_count": 1
    },
    {
      "section": "Image segmentation",
      "code": "display( normalize(logo_label) )",
      "line_count": 1
    },
    {
      "section": "Image segmentation",
      "code": "display( colorLabels(logo_label) )",
      "line_count": 1
    },
    {
      "section": "Image segmentation",
      "code": "nmask = watershed( distmap(nuc_th), 2 )\ndisplay(colorLabels(nmask), all=TRUE)",
      "line_count": 2
    },
    {
      "section": "Image segmentation",
      "code": "library(brainR)\n#load the sMRI data (same as in Problem 1)\n# 3D sMRI data: http://socr.umich.edu/HTML5/BrainViewer/data/ABIDE_MRI_MPRAGE_peds_defaced.nii.gz\nbrainURL <- \"http://socr.umich.edu/HTML5/BrainViewer/data/ABIDE_MRI_MPRAGE_peds_defaced.nii.gz\"\nbrainFile <- file.path(tempdir(), \"ABIDE_MRI_MPRAGE_peds_defaced.nii.gz\")\ndownload.file(brainURL, dest=brainFile, quiet=TRUE)\nbrainVolume <- readNIfTI(brainFile, reorient=FALSE)\n\nbrainVolDims <- dim(brainVolume); brainVolDims\n\n# get the mid-axial slice index\nmidZ = brainVolDims[3] / 2; midZ\n\n# plot 2D sMRI slice\nimage(brainVolume[,, midZ], asp=1, col = hcl.colors(12, \"YlOrRd\", rev = TRUE)) # Hot-metal\n\n# Apply watershed image segmentation.\nsMRI_Watershed_seg = watershed( brainVolume[,, midZ], ext=2 )\ndisplay(normalize(sMRI_Watershed_seg), method=\"raster\")",
      "line_count": 19
    },
    {
      "section": "Image segmentation",
      "code": "voronoiExamp = propagate(seeds = nmask, x = nmask, lambda = 100)\nvoronoiPaint = colorLabels (voronoiExamp)\ndisplay(voronoiPaint)",
      "line_count": 3
    },
    {
      "section": "Image segmentation",
      "code": "# Try Voronoi tessellation\n\nnmask = thresh(brainVolume[,, midZ], w=100, h=50, offset=0.05)\nnmask = opening(nmask, makeBrush(5, shape='disc'))\nnmask = fillHull(nmask)\nnmask = bwlabel(nmask)\ndisplay(nmask, all=TRUE)\n\nsMRI_Watershed_seg = watershed( brainVolume[,, midZ], ext=2 )\nbrain <- sMRI_Watershed_seg * nmask\ndisplay(normalize(brain), method=\"raster\")\n\nvoronoi_sMRI = propagate(seeds = brain, x = brain, lambda = 100)\nvoronoiPaint = colorLabels (voronoi_sMRI)\ndisplay(voronoiPaint, method=\"raster\")",
      "line_count": 15
    },
    {
      "section": "Object manipulation",
      "code": "objects = list(\n    seq.int(from = 2, to = max(logo_label), by = 2),\n    seq.int(from = 1, to = max(logo_label), by = 2)\n    )\nlogos = combine(logo_label, logo_label)\nz = rmObjects(logos, objects, reenumerate=FALSE)\ndisplay(z, all=TRUE)",
      "line_count": 7
    },
    {
      "section": "Object manipulation",
      "code": "showIds = function(image) lapply(getFrames(image), function(frame) unique(as.vector(frame)))\n\nshowIds(z)",
      "line_count": 3
    },
    {
      "section": "Object manipulation",
      "code": "showIds( reenumerate(z) )",
      "line_count": 1
    },
    {
      "section": "Object manipulation",
      "code": "filled_logo = fillHull(logo)\ndisplay(filled_logo)",
      "line_count": 2
    },
    {
      "section": "Object manipulation",
      "code": "rgblogo = toRGB(logo)\npoints = rbind(c(50, 50), c(100, 50), c(150, 50))\ncolors = c(\"red\", \"green\", \"blue\")\nrgblogo = floodFill(rgblogo, points, colors)\ndisplay( rgblogo )",
      "line_count": 5
    },
    {
      "section": "Object manipulation",
      "code": "display(floodFill(img, rbind(c(200, 300), c(444, 222)), col=0.2, tolerance=0.2))",
      "line_count": 1
    },
    {
      "section": "Object manipulation",
      "code": "d1 = dim(img)[1:2]\noverlay = Image(dim=d1)\nd2 = dim(logo_label)-1\n\noffset = (d1-d2) %/% 2\n\noverlay[offset[1]:(offset[1]+d2[1]), offset[2]:(offset[2]+d2[2])] = logo_label\n\nimg_logo = paintObjects(overlay, toRGB(img), col=c(\"red\", \"yellow\"), opac=c(1, 0.3), thick=TRUE)\n\ndisplay( img_logo )",
      "line_count": 11
    },
    {
      "section": "Cell segmentation example",
      "code": "nuc = readImage(system.file('images', 'nuclei.tif', package='EBImage'))\ncel = readImage(system.file('images', 'cells.tif', package='EBImage'))\n\ncells = rgbImage(green=1.5*cel, blue=nuc)\ndisplay(cells, all = TRUE)",
      "line_count": 5
    },
    {
      "section": "Cell segmentation example",
      "code": "nmask = thresh(nuc, w=10, h=10, offset=0.05)\nnmask = opening(nmask, makeBrush(5, shape='disc'))\nnmask = fillHull(nmask)\nnmask = bwlabel(nmask)\n\ndisplay(nmask, all=TRUE)",
      "line_count": 6
    },
    {
      "section": "Cell segmentation example",
      "code": "ctmask = opening(cel>0.1, makeBrush(5, shape='disc'))\ncmask = propagate(cel, seeds=nmask, mask=ctmask)\n\ndisplay(ctmask, all=TRUE)",
      "line_count": 4
    },
    {
      "section": "Cell segmentation example",
      "code": "segmented = paintObjects(cmask, cells, col='#ff00ff')\nsegmented = paintObjects(nmask, segmented, col='#ffff00')\n\ndisplay(segmented, all=TRUE)",
      "line_count": 4
    },
    {
      "section": "Learning Activity",
      "code": "# Install and load the JPEG package\n# install.packages(\"jpeg\")\nlibrary(jpeg)\n\n# Read Image in and display it\n# Either download the JPG image locally from https://umich.instructure.com/courses/38100/files/1627149/download?download_frd=1 and then read it in\n# mri <- readJPEG(\"C:/Users/Dinov/Desktop/MRI_ImageHematoma.jpg\")\n\n# Or directly load it from the web\npathToMRI <- tempfile(fileext = \".jpg\")\ndownload.file(\"https://umich.instructure.com/courses/38100/files/1627149/download?download_frd=1\", mode = \"wb\", pathToMRI)\nmri <- readJPEG(pathToMRI)\n# display(mri)\nplot.new(); rasterImage(mri,0,0,1,1)\n\n# Threshold the image\n# display( thresh(mri, offset=0.05), all=TRUE )\n# plot.new(); rasterImage(EBImage::thresh(mri, offset=0.05),0,0,1,1)\nmri_thresh = mri[,,1] > 0.35\nmri_thresh = mri[,,1] * mri_thresh\nplot.new(); rasterImage(mri_thresh,0,0,1,1)\n\n# Histogram\nhist(mri)\n\n# Affine transformation\nm = matrix(c(1, -.5, 128, 0, 1, 0), nrow=3, ncol=2)\nimg_affine = EBImage::affine(mri, m)\n# display( img_affine )\nplot.new(); rasterImage(img_affine,0,0,1,1)\n\n# Sheer Affine Transformation and translation\nm = matrix(c(1, -.5, 128, 0, 1, 0), nrow=3, ncol=2)\nimg_affine = EBImage::affine(EBImage::translate(mri, c(-50, 0)), m)\n# display( img_affine )\nplot.new(); rasterImage(img_affine,0,0,1,1)\n\n# Gaussian Kernel smoothing (Convolution)\nkernel = EBImage::makeBrush(size = 31, shape = 'gaussian', sigma = 5)\nmri_smooth = EBImage::filter2(mri, kernel)\n# display(mri_smooth)\nplot.new(); rasterImage(mri_smooth,0,0,1,1)\n\n# FFT\nft_mriMat <- fft(mri[,,1])  # fftw2d # Display Re(FT):\nEBImage::display(fftshift(ft_mriMat))\n\n# FFT Magnitude\nmag_ft_mriMat <- sqrt(Re(ft_mriMat)^2+Im(ft_mriMat)^2)\ndisplay(mag_ft_mriMat)\n#plot.new(); rasterImage(normalize(mag_ft_mriMat),0,0,1,1)\n\n# IFT\nphase_ft_mriMat <- atan2(Im(ft_mriMat), Re(ft_mriMat)); Real = mag_ft_mriMat * cos(phase_ft_mriMat)\nImaginary = mag_ft_mriMat * sin(phase_ft_mriMat)\nift_ft_mag_ft_mriMat = Re(fft(Real+1i*Imaginary, inverse = T)/length(mag_ft_mriMat)); \ndisplay(t(ift_ft_mag_ft_mriMat))\n\n# Low- and High-pass Fourier domain filtering\n\n## Low-pass frequency filtering\n## Remove Fourier coefficients indexed above $k_{x,max/2}$ and $k_{y,max/2}$.\nmag_ft_mriMat_1 <- mag_ft_mriMat\nfor (i in 1:dim(mag_ft_mriMat)[1]) {\n  for (j in 1:dim(mag_ft_mriMat)[2]) {\n    if (abs(i-(dim(mag_ft_mriMat)[1])/2) > (dim(mag_ft_mriMat)[1])/4 | \n        abs(j-(dim(mag_ft_mriMat)[2])/2) > (dim(mag_ft_mriMat)[2])/4) {\n      mag_ft_mriMat_1[i,j] <- 0\n    }\n  }\n}\n\n# EBImage::display(mag_ft_mriMat_1, method = \"raster\", title=\"(IFT o LowPassFilter o FT) Magnitude=Img | Phase=Img\")\nplot.new(); rasterImage(normalize(mag_ft_mriMat_1),0,0,1,1)\n\nReal = mag_ft_mriMat_1 * cos(phase_ft_mriMat)\nImaginary = mag_ft_mriMat_1 * sin(phase_ft_mriMat)\nift_ft_mriMat_1 = Re(fft(Real+1i*Imaginary, inverse = T)/length(mag_ft_mriMat_1))\nEBImage::display(t(400*ift_ft_mriMat_1), method = \"raster\", title=\"(IFT o LowPassFilter o FT) Magnitude=MRI | Phase=MRI\")\n# plot.new(); rasterImage(normalize(ift_ft_mriMat_1),0,0,1,1)\n\n## High-pass frequency filtering\n## Remove the Fourier coefficients indexed below $k_{x,max/4}$ and $k_{y,max/4}$.\n# High-pass frequency filtering, remove Fourier coeff's indexed below k_{x,max/4} and k_{y,max/4}\nmag_ft_mriMat_2 <- mag_ft_mriMat\nfor (i in 1:dim(mag_ft_mriMat)[1]) {\n  for (j in 1:dim(mag_ft_mriMat)[2]) {\n    if (abs(i-(dim(mag_ft_mriMat)[1])/2) < (dim(mag_ft_mriMat)[1])/4 & \n        abs(j-(dim(mag_ft_mriMat)[2])/2) < (dim(mag_ft_mriMat)[2])/4) {\n      mag_ft_mriMat_2[i,j] <- 0\n    }\n  }\n}\n\nEBImage::display(mag_ft_mriMat_2, method = \"raster\", title=\"(IFT o HighPassFilter o FT) Magnitude=Img | Phase=Img\")\n# plot.new(); rasterImage(normalize(mag_ft_mriMat_2),0,0,1,1)\n\nReal = mag_ft_mriMat_2 * cos(phase_ft_mriMat)\nImaginary = mag_ft_mriMat_2 * sin(phase_ft_mriMat)\nift_ft_mriMat_2 = Re(fft(Real+1i*Imaginary, inverse = T)/length(mag_ft_mriMat_2))\n# EBImage::display(t(ift_ft_mriMat_2), method = \"raster\", title=\"(IFT o HighPassFilter o FT) Magnitude=Img | Phase=Img\")\nplot.new(); rasterImage(normalize(ift_ft_mriMat_2),0,0,1,1)",
      "line_count": 102
    },
    {
      "section": "Art, Image Processing and Spectral Representation of Signals",
      "code": "# UMMA HS650 CC: https://umma.umich.edu/art/exhibitions/curriculum-collection/data-science-and-predictive-analytics/ \nlibrary(EBImage)\n\n# Load the image from CC\n# CC_WhitePoemFigure_Dill <- readImage(\"https://exchange.umma.umich.edu/media/W1siZiIsIjIwMjEvMDIvMTkvZGJ3dWk2cTVjX2RlZmF1bHQuanBnIl0sWyJwIiwidGh1bWIiLCIxMDAweDEwMDAiXV0?sha=bf87973a898145f8\", type=\"jpeg\")\n\nCC_WhitePoemFigure_Dill <- readImage(\"https://umma.umich.edu/wp-content/uploads/2024/01/57314_ca_object_representations_media_7703_original.jpg\", type=\"jpeg\")\n\n# Display raw image\ndisplay(CC_WhitePoemFigure_Dill, method = \"raster\", all = TRUE)\n\n# Intensity Histogram\nhist(CC_WhitePoemFigure_Dill)\n\n# Generate negative image\nimg_neg <- max(CC_WhitePoemFigure_Dill) - CC_WhitePoemFigure_Dill\ndisplay( img_neg )\n# dim: 633 1000    3\n\n# crop and threshold\nimg_crop <- CC_WhitePoemFigure_Dill  # [200:578, 250:750, ]\nimg_thresh <- img_crop > 0.55\ndisplay(img_thresh)\n\n# Low-pass Filter/smooth\nw <- makeBrush(size = 31, shape = 'gaussian', sigma = 5)\nimg_flo <- filter2(CC_WhitePoemFigure_Dill, w)\ndisplay(img_flo)\n\n# High-pass filtering Laplace Filter/Edge detection\nfhi <- matrix(1, nrow = 3, ncol = 3)\nfhi[2, 2] <- -7\nimg_fhi <- filter2(CC_WhitePoemFigure_Dill, fhi)\ndisplay(img_fhi)\n\n# Denoising\nimg_median <- medianFilter(CC_WhitePoemFigure_Dill, size=5, cacheSize=20000)\ndisplay(img_median)\n\n# Segmentation\ngrayimage <- channel(CC_WhitePoemFigure_Dill,\"gray\")\nthreshold <- otsu(grayimage)\nCC_th <- combine( mapply(function(frame, th) frame > th, \n                         getFrames(CC_WhitePoemFigure_Dill), \n                         threshold, SIMPLIFY=FALSE) )\ndisplay(CC_th, all=TRUE)\n\ndisc <- makeBrush(51, \"disc\")\ndisc <- disc/sum(disc)\noffset <- 0.0001\nCC_bg <- filter2(CC_WhitePoemFigure_Dill, disc)\nCC_th <- CC_WhitePoemFigure_Dill < CC_bg + offset\ndisplay(CC_th, all=TRUE)\n\n# Watershed segmentation\nnmask <- watershed(distmap(CC_th), tolerance=5, ext=20)\ndisplay(colorLabels(nmask), all=TRUE)\n\n# Voronoi image tessellation\nvoronoiExamp <- propagate(x = nmask, seeds = nmask, lambda = 0.001)\nvoronoiPaint <- colorLabels (voronoiExamp)\ndisplay(voronoiPaint, all=TRUE)\n\nCC <- rgbImage(green=2.0*CC_bg, blue=img_median, red=voronoiPaint[ , , , 1])\ndisplay(CC, all = TRUE)\n\nctmask = opening(img_fhi<0.6, makeBrush(5, shape='disc'))\ncmask = propagate(img_fhi, seeds=nmask, mask=ctmask)\ndisplay(ctmask, all=TRUE)\n        \nctmask = opening(img_fhi>0.1, makeBrush(5, shape='disc'))\ncmask = propagate(img_fhi, seeds=nmask, mask=ctmask)\ndisplay(ctmask, all=TRUE)\n\n# segmented <- paintObjects(channel(CC_th,\"gray\"), CC_WhitePoemFigure_Dill, col='#ff00ff')\n# segmented <- paintObjects(channel(nmask,\"gray\"), segmented, col='#ffff00')\n# display(segmented, all=TRUE)\nsegmented = paintObjects(channel(cmask,\"gray\"), CC_WhitePoemFigure_Dill, col='#ff00ff')\nsegmented = paintObjects(channel(ctmask,\"gray\"), segmented, col='#ffff00')\ndisplay(segmented, all=TRUE)",
      "line_count": 80
    }
  ]
}