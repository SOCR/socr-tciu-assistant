{
  "metadata": {
    "created_at": "2024-11-30T13:46:16.405778",
    "total_sections": 2,
    "total_code_chunks": 91,
    "total_tables": 5,
    "r_libraries": [
      "C50",
      "MASS",
      "ROCR",
      "ada",
      "adabag",
      "caret",
      "class",
      "cluster",
      "crossval",
      "doParallel",
      "dplyr",
      "e1071",
      "fastAdaboost",
      "ggplot2",
      "gmodels",
      "kernlab",
      "knitr",
      "lattice",
      "neuralnet",
      "plotly",
      "randomForest",
      "sparsediscrim",
      "tidyr",
      "tm",
      "unbalanced",
      "vcd"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Model Performance Assessment, Validation, and Improvement</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    code_folding: show",
      "word_count": 56
    },
    {
      "title": "Model Performance Assessment, Validation, and Improvement",
      "content": "In [previous chapters](https://dspa2.predictive.space/), we used several measures, such as prediction accuracy, to evaluate classification and regression models. In general, accurate predictions for one dataset does not necessarily imply that our model is perfect or that it will reproduce when tested on external or prospective data. We need additional metrics to evaluate the model performance and ensure it is robust, reproducible, reliable, and unbiased.\n\nIn this chapter, we will (1) discuss various evaluation strategies for prediction, clustering, classification, regression, and decision-making, (2) demonstrate performance visualization, e.g., visualization of ROC curves, (3) discuss performance tradeoffs, and (4) present internal statistical cross-validation and bootstrap sampling.\n\n## Measuring the performance of classification methods\n\n*Prediction accuracy* represents just one evaluation aspect of classification model performance and reliability assessment of clustering methods. Different classification models and alternative clustering techniques may be appropriate for different situations. For example, when screening newborns for rare genetic defects, we may want the model to have as few true-negatives as possible. We don't want to classify anyone as \"non-carriers\" when they actually may have a defective gene, since early treatment might impact near- and long-term life experiences. \n\nWe can use the following three types of data to evaluate the performance of a classifier model.  \n\n - Actual class values (for supervised classification).\n - Predicted class values.\n - Estimated probabilities of the prediction.\n\nWe have already seen examples of these cases. For instance, the last type of validation relies on the `predict(model, test_data)` function that we used previously in the classification and regression chapters ([Chapter 3-6](https://dspa2.predictive.space)). Let's revisit some of the models and test data we discussed in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html) - [Inpatient Head and Neck Cancer Medication data](https://umich.instructure.com/files/1614351/download?download_frd=1). We will demonstrate prediction probability estimation using this case-study  [CaseStudy14_HeadNeck_Cancer_Medication.csv](https://umich.instructure.com/files/1614350/download?download_frd=1). \n\n\n\nThe above output includes the prediction probabilities for the first 6 rows of the data. This example is based on [Naive Bayes classifier (naiveBayes)](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), however the same approach works for any other machine learning classification or prediction technique.  The `type=\"raw\"` indicates that the prediction call will return the *conditional a-posterior probabilities* for each class. When `type=\"class\"`, `predict()` returns the class label corresponding to the *maximal probability*.\n\nIn addition, we can report the predicted probability with the outputs of the Naive Bayesian decision-support system (`hn_classifier <- naiveBayes(hn_train, hn_med_train$stage)`):\n\n\nThe general `predict()` method automatically subclasses to the specific `predict.naiveBayes(object, newdata, type = c(\"class\", \"raw\"), threshold = 0.001, ...)` call where `type=\"raw\"` and `type = \"class\"` specify the output as the conditional a-posterior probabilities for each class or the class with maximal probability, respectively. Back in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), we discussed the `C5.0` and the `randomForest` classifiers to predict the chronic disease score in a another case-study [Quality of Life (QoL)](https://umich.instructure.com/files/481332/download?download_frd=1).\n\n\nBelow are the (probability) results of the `C5.0` classification tree model prediction.\n\n\nThese can be contrasted against the `C5.0` tree classification label results.\n\n\nThe same complementary types of outputs can be reported for most machine learning classification and prediction approaches.\n\n## Evaluation strategies\n\nIn [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), we saw an attempt to categorize the supervised classification and unsupervised clustering methods. Similarly, the *table* below summarizes the basic types of evaluation and validation strategies for different forecasting, prediction, ensembling, and clustering techniques. (Internal) Statistical Cross Validation or external validation should always be applied to ensure reliability and reproducibility of the results. The SciKit [clustering performance evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation) and [Classification metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) sections provide details about many alternative techniques and metrics for performance evaluation of clustering and classification methods.\n\n### Binary outcomes\nMore details about binary test assessment is available on the [Scientific Methods for Health Sciences (SMHS) EBook site](https://wiki.socr.umich.edu/index.php/SMHS_IntroEpi#Screening). The table below summarizes the key measures commonly used to evaluate the performance of binary tests, classifiers, or predictions.\n\n<table>\n<tr>\n<td colspan=\"2\" rowspan=\"2\"> </td>\n<td colspan=\"2\" align=\"middle\"> <b>Actual Condition</b></td>\n<td rowspan=\"2\"><b>Test Interpretation</b> </td>\n</tr>\n<tr>\n<td align=\"middle\"><b>Absent ($H_0$ is true)</b></td>\n<td align=\"middle\"><b>Present ($H_1$ is true)</b></td>\n</tr>\n<tr>\n<td rowspan=\"2\"> <b>Test Result</b></td>\n<td>  <b>Negative<br/>(fail to reject $H_0$)</b> </td>\n<td align=\"middle\"> <b>TN</b><br/> Condition absent + Negative result = True (accurate) Negative </td>\n<td align=\"middle\"> <b>FN</b><br/> <i>Condition present + Negative result = False (invalid) Negative <b>Type II error</b> (proportional to $\\beta$)</i></td>\n<td align=\"middle\"> $NPV$<br/>\n    $=\\frac{TN}{TN+FN}$<td>\n</tr>\n<tr>\n<td> <b>Positive <br/>(reject $H_0$)</b> </td>\n<td align=\"middle\"> <b>FP</b><br/> Condition absent + Positive result = False Positive <b>Type I error</b> ($\\alpha$) </td>\n<td align=\"middle\"> <b>TP</b><br/> Condition Present + Positive result = True Positive </td>\n<td align=\"middle\"> $PPV=Precision$<br/>\n    $=\\frac{TP}{TP+FP}$<td>\n</tr>\n<tr>\n<td><b>Test Interpretation</b> </td>\n<td align=\"middle\"> $Power =1-\\beta$ <br/>\n      $= 1-\\frac{FN}{FN+TP}$ </td>\n<td align=\"middle\">$Specificity=\\frac{TN}{TN+FP}$ </td>\n<td align=\"middle\">$Power=Recall=Sensitivity$\n<br/>$=\\frac{TP}{TP+FN}$</td>\n<td>$LOR=\\ln\\left (\\frac{TN\\times TP}{FP\\times FN}\\right )$\n</td>\n</tr>\n</table>\n\nSee also [SMHS EBook, Power, Sensitivity and Specificity section](https://wiki.socr.umich.edu/index.php/SMHS_PowerSensitivitySpecificity).\n\n### Cross-tables, contingency tables, and confusion-matrices\n\nIn ML and AI, the concepts of *cross table*, *contingency table*, and *confusion matrix* are often used to quantify the performance of a classifier. A contingency table represents a statistical cross tabulation that summarizes the multivariate frequency distribution of two (or more) categorical variables. In the special case of cross tabulating the performance of an AI classifier relative to ground truth, this comparison is referred to as a *confusion matrix*, for supervised learning, or a *matching matrix*, for unsupervised learning. Typically, the rows and columns in these cross tables represent observed instances in predicted and actual class labels, respectively.\n\nWe already saw some confusion matrices in [Chapter 8](https://socr.umich.edu/DSPA2/DSPA2_notes/08_Unsupervised_Clustering.html). For binary classes, these will be just $2\\times 2$ matrices where each of the cells represents the agreement, match, or discrepancy between the real and predicted class labels, indexed by the row and column indices.\n\nGraph $2\\times 2$ table:\n\n\n - **True Positive**(TP): Number of observations that are correctly classified as \"yes\" or \"success\".\n - **True Negative**(TN): Number of observations that are correctly classified as \"no\" or \"failure\".\n - **False Positive**(FP): Number of observations that are incorrectly classified as \"yes\" or \"success\".\n - **False Negative**(FN): Number of observations that are incorrectly classified as \"no\" or \"failure\".\n\n*Using confusion matrices to measure performance*: The way we calculate accuracy using these four cells is summarized by the following formula\n\n$$accuracy=\\frac{TP+TN}{TP+TN+FP+FN}=\\frac{TP+TN}{\\text{Total number of observations}}\\ .$$\n\nOn the other hand, the error rate, or proportion of incorrectly classified observations is calculated using:\n\n$$error rate=\\frac{FP+FN}{TP+TN+FP+FN}==\\frac{FP+FN}{\\text{Total number of observations}}=1-accuracy\\ .$$\n\nIf we look at the numerator and denominator carefully, we can see that the error rate and accuracy add up to 1. Therefore, a 95% accuracy means 5% error rate.\n\nIn `R`, we have multiple ways to obtain a confusion table. The simplest way would be `table()`. For example, in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), to get a plain $2\\times 2$ table reporting the agreement between the real clinical cancer labels and their machine learning predicted counterparts, we used:\n\n\nThe reason we sometimes use the `gmodels::CrossTable()` function, e.g., see [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), is because it reports additional information about the model performance.\n\n\nThe second entry in each cell of the *crossTable* table reports the Chi-square contribution. This uses the standard [Chi-Square formula](https://wiki.socr.umich.edu/index.php/AP_Statistics_Curriculum_2007_Contingency_Indep) for computing relative discrepancy between *observed* and *expected* counts. For instance, the Chi-square contribution of *cell(1,1)*, (`hn_med_test$stage=early_stage` and `hn_test_pred=early_stage`), can be computed as follows from the $\\frac{(Observed-Expected)^2}{Expected}$ formula. Assuming independence between the rows and columns (i.e., random classification), the *expected cell(1,1) value* is computed as the product of the corresponding row (96) and column (77) marginal counts, $\\frac{96\\times 77}{100}$. Thus the Chi-square value for cell(1,1) is:\n\n$$\\text{Chi-square cell(1,1)} = \\frac{(Observed-Expected)^2}{Expected}=$$\n$$=\\frac{\\left (73-\\frac{96\\times 77}{100}\\right ) ^2}{\\frac{96\\times 77}{100}}=0.01145022\\ .$$\n\nNote, that each cell Chi-square value represents one of the four (in this case) components of the Chi-square test-statistics, which tries to answer the question if there is no association between observed and predicted class labels. That is, under the null-hypothesis there is no association between actual and observed counts for each level of the factor variable, which allows us to quantify whether the derived classification jibes with the real class annotations (labels). The aggregate sum of all Chi-square values represents the $\\chi_o^2 = \\sum_{all-categories}{(O-E)^2 \\over E} \\sim \\chi_{(df)}^2$ statistics, where $df = (\\# rows - 1)\\times (\\# columns - 1)$.\n\nUsing either table (CrossTable, confusionMatrix), we can calculate accuracy and error rate by hand.\n\n\nFor matrices that are larger than $2\\times 2$, all diagonal elements count the observations that are correctly classified and the off-diagonal elements represent incorrectly labeled cases.\n\n### Other measures of performance beyond accuracy\n\nSo far we discussed two performance methods - `table()` and `CrossTable()`. A third function is `caret::confusionMatrix()` which provides the easiest way to report model performance. Notice that the first argument is an *actual vector of the labels*, i.e., $Test\\_Y$ and the second argument, of the same length, represents the *vector of predicted labels*. \n\nThis example was presented as the first case-study in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html).\n\n\n#### Silhouette coefficient\n\nIn [Chapter 8](https://socr.umich.edu/DSPA2/DSPA2_notes/08_Unsupervised_Clustering.html) we already saw the *Silhouette coefficient*, which captures the shape of the clustering boundaries. It is a function of the *intracluster distance* of a sample in the dataset ($i$). Recall that\n\n - $d_i$ is the average dissimilarity of point $i$ with all other data points within its cluster. Then, $d_i$ captures the quality of the assignment of $i$ to its current class label. Smaller or larger $d_i$ values suggest better or worse overall assignment for $i$ to its cluster, respectively. The average dissimilarity of $i$ to a cluster $C$ is the average distance between $i$ and all points in the cluster of points labeled $C$.\n - $l_i$ is the lowest average dissimilarity of point $i$ to any other cluster that $i$ is not a member of. The cluster corresponding to $l_i$, the lowest average dissimilarity, is called the $i$ neighboring cluster, as it is the next best fit cluster for $i$. \n\nThen, the *Silhouette coefficient* for a sample point $i$ is:\n\n$$-1\\leq Silhouette(i)=\\frac{l_i-d_i}{\\max(l_i,d_i)} \\leq 1.$$\nFor interpreting the Silhouette of point $i$, we use:\n\n$$Silhouette(i) \\approx \n\\begin{cases} \n      -1 & \\text{sample } i \\text{ is closer to a neighboring cluster} \\\\\n      0 & \\text {the sample } i \\text{ is near the border of its cluster, i.e., } i \\text{ represents the closest point in its cluster to the rest of the dataset clusters} \\\\\n      1 & \\text {the sample } i \\text{ is near the center of its cluster} \n   \\end{cases}.$$\n\nThe *mean Silhouette value* represents the arithmetic average of all Silhouette coefficients (either within a cluster, or overall) and represents the quality of the cluster (clustering). High mean Silhouette corresponds to compact clustering (dense and separated clusters), whereas low values represent more diffused clusters. The Silhouette value is useful when the number of predicted clusters is smaller than the number of samples.\n\n#### The kappa ( $\\kappa$ ) statistic\n\nThe Kappa statistic was [originally developed to measure the reliability between two human raters](https://wiki.socr.umich.edu/index.php/SMHS_ReliabilityValidity). It can be harnessed in machine learning applications to compare the accuracy of a classifier, where `one rater` represents the ground truth (for labeled data, these are the actual values of each instance) and the `second rater` represents the results of the automated machine learning classifier. The order of listing the **raters** is irrelevant.\n\nKappa statistic measures the **possibility of a correct prediction by chance alone** and answers the question of *How much better is the agreement (between the ground truth and the machine learning prediction) than would be expected by chance alone?* Its value is between $0$ and $1$. When $\\kappa=1$, we have a perfect agreement between a **computed** prediction (typically the result of a model-based or model-free technique forecasting an outcome of interest) and an **expected** prediction (typically random, by-chance, prediction). A common interpretation of the Kappa statistics includes:\n\n - *Poor* agreement: less than 0.20\n - *Fair* agreement: 0.20-0.40\n - *Moderate* agreement: 0.40-0.60\n - *Good* agreement: 0.60-0.80\n - *Very good* agreement: 0.80-1\n\nIn the above `confusionMatrix` output, we have a fair agreement. For different problems, we may have different interpretations of Kappa statistics.\n\nTo understand Kappa statistic better, let's look at its definition.\n\n\nIn this table, $A=143, B=71, C=72, D=157$ denote the frequencies (counts) of cases within each of the cells in the $2\\times 2$ design. Then \n\n$$ObservedAgreement = (A+D)=300.$$\n\n$$ExpectedAgreement = \\frac{(A+B)\\times (A+C)+(C+D)\\times (B+D)}{A+B+C+D}=221.72.$$\n\n$$(Kappa)\\ \\kappa = \\frac{(ObservedAgreement) – (ExpectedAgreement)}\n  {(A+B+C+D) – (ExpectedAgreement)}=0.35.$$\n\nIn this manual calculation of `kappa` statistics ($\\kappa$) we used the corresponding values we saw earlier in the Quality of Life (QoL) case-study, where chronic-disease binary outcome `qol$cd<-qol$CHRONICDISEASESCORE>1.497`, and we used the `cd` prediction (qol_pred).\n\n\nAccording to the above table, high agreement between actual and predicted clinical diagnosis (CD) corresponds to the high algorithmic performance (accuracy).\n\n\nThe manually and automatically computed accuracies coincide ($\\sim 0.35$). \n\nLet's now look at computing `Kappa`. It may be trickier to obtain the expected agreement. [Probability rules](https://wiki.socr.umich.edu/index.php/EBook#Chapter_III:_Probability) tell us that the probability of the union of two *disjoint events* equals the sum of the individual (marginal) probabilities for these two events. Thus, we have:\n\n\nWe get a similar value in the `confusionTable()` output. A more straightforward way of getting the Kappa statistics is by using the `Kappa()` function in the `vcd` package.\n\n\nThe combination of `Kappa()` and `table` function yields a $2\\times 4$ matrix. The *Kappa statistic* is under the unweighted value. \n\nGenerally speaking, predicting a severe disease outcome is a more critical problem than predicting a mild disease state. Thus, weighted Kappa is also useful. We give the severe disease a higher weight. The Kappa test result is not acceptable since the classifier may make too many mistakes for the severe disease cases. The Kappa value is $0.26374$. Notice that the range of weighted Kappa may exceed [0,1].\n\n\nWhen the predicted value is the first argument, the row and column names represent the **true labels** and the **predicted labels**, respectively.\n\n\n#### Summary of the Kappa score for calculating prediction accuracy\n\nKappa compares an **Observed classification accuracy** (output of our ML classifier) with an **Expected classification accuracy** (corresponding to random chance classification). It may be used to evaluate single classifiers and/or to compare among a set of different classifiers. It takes into account random chance (agreement with a random classifier). That makes **Kappa** more meaningful than simply using the **accuracy** as a single quality metric. For instance, the interpretation of an `Observed Accuracy of 80%` is **relative** to the `Expected Accuracy`. `Observed Accuracy of 80%` is more impactful for an `Expected Accuracy of 50%` compared to `Expected Accuracy of 75%`. \n\n#### Sensitivity and specificity\n\nTake a closer look at the `confusionMatrix()` output where we can find two important statistics - \"sensitivity\" and \"specificity\".\n\nSensitivity or true positive rate measures the proportion of \"success\" observations that are correctly classified. \n$$sensitivity=\\frac{TP}{TP+FN}.$$\nNotice $TP+FN$ are the total number of true \"success\" observations.\n\nOn the other hand, specificity or true negative rate measures the proportion of \"failure\" observations that are correctly classified. \n$$specificity=\\frac{TN}{TN+FP}.$$\nAccordingly, $TN+FP$ are the total number of true \"failure\" observations.\n\nIn the QoL data, considering \"severe_disease\" as \"success\" and using the `table()` output we can manually compute the *sensitivity* and *specificity*, as well as *precision* and *recall* (below):\n\n\nAnother R package `caret` also provides functions to directly calculate the sensitivity and specificity.  \n\n\nSensitivity and specificity both range from 0 to 1. For either measure, a value of 1 implies that the positive and negative predictions are very accurate. However, simultaneously high sensitivity and specificity may not be attainable in real world situations. There is a tradeoff between sensitivity and specificity. To compromise, some studies loosen the demands on one and focus on achieving high values on the other.\n\n#### Precision and recall\n\nVery similar to sensitivity, *precision* measures the proportion of true \"success\" observations among predicted \"success\" observations.\n\n$$precision=\\frac{TP}{TP+FP}.$$\n\n*Recall* is the proportion of true \"failures\" among all \"failures\". A model with high recall captures most \"interesting\" cases.\n\n$$recall=\\frac{TP}{TP+FN}.$$\n\nAgain, let's calculate these by hand for the QoL data, and report the Area under the ROC Curve (AUC).\n\n\nAnother way to obtain *precision* would be `posPredValue()` in the `caret` package. Remember to specify which one is the \"success\" class.\n\n\nFrom the definitions of **precision** and **recall**, we can derive the type 1 error and type 2 errors as follow:\n\n$$error_1 = 1- Precision = \\frac{FP}{TP+FP}.$$\n\n$$error_2 = 1- Recall = \\frac{FN}{TN+FN}.$$\n\nThus, we can compute the type 1 error ($0.31$) and type 2 error ($0.31$).\n\n\n#### The F-measure\n\nThe F-measure, or *F1-score*, combines precision and recall using the [harmonic mean](https://wiki.socr.umich.edu/index.php/SMHS_CenterSpreadShape#Harmonic_Mean) assuming equal weights. High F1-score means high precision and high recall. This is a convenient way of measuring model performances and comparing models.\n\n$$F1=\\frac{2\\times precision\\times recall}{recall+precision}=\\frac{2\\times TP}{2\\times TP+FP+FN}$$\n\nLet's calculate the F1-score by hand using the Quality of Life prediction results.\n\n\nWe can contrast these results against direct calculations of the F1-statistics obtained using `caret`.\n\n\n### Visualizing performance tradeoffs (ROC Curve)\n\nAnother choice for evaluating classifiers' performance is by graphs rather than scalars, numerical vectors, or statistics. Graphs are usually more comprehensive than single statistics.\n\nThe `R` package `ROCR` provides user-friendly functions for visualizing model performance. Details can be found on the [ROCR website](http://rocr.bioinf.mpi-sb.mpg.de).\n\nHere, we evaluate the model performance for the Quality of Life case study in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html). \n\n\nThe `prediction()` method argument `pred_prob[, 2]` stores the probability of classifying each observation as \"severe_disease\", and we saved all model prediction information into the object `pred`.\n\n[Receiver Operating Characteristic (ROC)](https://wiki.socr.umich.edu/index.php/SMHS_ROC) curves are often used for examining the trade-off between detecting true positives and avoiding the false positives.\n\n\nThe <span style=\"color:blue\">blue line</span> in the above graph represents the perfect classifier where we have 0% false positive and 100% true positive. The middle <span style=\"color:green\">green line</span> represents a test classifier. Most of our classifiers trained by real data will look like this. The <span style=\"color:black\">black diagonal line</span> illustrates a classifier with no predictive value. We can see that it has the same true positive rate and false positive rate. Thus, it cannot distinguish between the two states.\n\nClassifiers with high true positive values have ROC curves near the (blue) *perfect classifier* curve. Thus, we measure the area under the ROC curve (abbreviated as AUC) as a proxy of the classifier performance. To do this, we have to change the scale of the graph above. Mapping 100% to 1, we have a $1\\times 1$ square. The area under perfect classifier would be 1 and area under classifier with no predictive value being 0.5. Then, 1 and 0.5 will be the upper and lower limits for our model ROC curve. For model ROC curves, the typical interpretation of the area under curve (AUC) includes:\n\n - Outstanding:       0.9-1.0\n - Excellent/good:    0.8-0.9\n - Acceptable/fair:   0.7-0.8\n - Poor:              0.6-0.7\n - No discrimination: 0.5-0.6\n\nNote that this rating system is somewhat subjective. We can use the `ROCR` package to draw ROC curves.\n\n\nWe can specify a \"performance\" object by providing `\"tpr\"` (true positive rate) and `\"fpr\"` (false positive rate) parameters.\n\n\nThe *segments* command draws the dash line representing a classifier with no predictive value.\n\nTo measure the model performance quantitatively we need to create a new performance object with `measure=\"auc\"`, for area under the curve.\n\n\nNow the `roc_auc` is stored as a [S4 object](http://adv-r.had.co.nz/OO-essentials.html). This is quite different from data frames and matrices. First, we can use the `str()` function to examine its structure.\n\n\nIt has 6 members, or \"slots\", and the AUC value is stored in the member `y.values`. To extract object members, we use the `@` symbol according to `str()` output. \n\n\nThis reports $AUC=0.65$, which suggests a fair classifier, according to the above scoring schema. \n\n##  Estimating future performance (internal statistical cross-validation)\n\nThe evaluation methods we have talked about are all measuring re-substitution error. That involves building the model on *training data* and measuring the model performance (error/accuracy) on *testing data*. This evaluation process provides one mechanism of dealing with unseen data. Let's look at some alternative strategies.\n\n### The holdout method\n\nThe `holdout` method idea is to partition a dataset into two separate sets. Using the first set to create (train) the model and the other to test (validate) the model performance. In practice, we usually use a fraction (e.g., $60\\%$, or $\\frac{3}{5}$) of our data for training the model, and reserve the rest (e.g., $40\\%$, or $\\frac{2}{5}$) for testing. Note that the testing data may also be further split into proportions for internal repeated (e.g., cross-validation) testing and final external (independent) testing. \n\nThe partition has to be randomized. In R, the best way of doing this is to create a parameter that randomly draws numbers and use this parameter to extract random rows from the original dataset. In [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html), we used this method to partition the *Google Trends* data.\n\n\nAnother way of partitioning is using the `caret::createDatePartition()` method. We can subset the original dataset or any independent variable column of the original dataset, e.g., `google_norm$RealEstate`: \n\n\nTo make sure that the model can be applied to future datasets, we can partition the original dataset into three separate subsets. In this way, we have two subsets of data for testing and validation. The additional validation dataset can alleviate the probability that we have a good model due to chance (non-representative subsets). A common split among training, testing, and validation subsets may be 50%, 25%, and 25% respectively.\n\n\n\nHowever, when we only have a very small dataset, it's difficult to split off too much data as this may excessively reduce the training sample size. There are the following two options for evaluation of model performance with unseen data. Both of these are implemented in the `caret` package.\n\n### Cross-validation\n\nThe complete details about cross validation will be presented below. Now, we describe the fundamentals of cross-validation as an internal statistical validation technique. \n\nThis technique is known as *k-fold cross-validation*, or *k-fold CV*, which is a standard for estimating model performance. K-fold CV randomly partitions the original data into *k* separate random subsets called folds.\n\nA common practice is to use `k=10` or 10-fold CV. That is to split the data into 10 different subsets. Each time, one of the subsets is reserved for testing, and the rest are employed for learning/building the model. This can be accomplished using the `caret::createFolds()` method. Using `set.seed()` ensures the reproducibility of the created folds, in case you run the code multiple times. Here, we use `1234`, a random number, to seed the fold separation. You can use any number for `set.seed()`. We demonstrate the process using the normalized Google Trend dataset.\n\n\nAnother way to cross-validate is to use methods such as `sparsediscrim::cv_partition()` or `caret::createFolds()`.\n\n\nAnd the structure of folds may be reported by:\n\n\nNow, we have 10 different subsets in the `folds` object. We can use `lapply()` to fit the model. 90% of data will be used for training so we use `[-x, ]` to represent all observations not in a specific fold. In [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html), we built a neural network model for the *Google Trends* data. We can do the same for each fold manually. Then we can train, test, and aggregate the results. Finally, we can report the agreement (e.g., correlations between the predicted and observed `RealEstate` values).\n\n\nFrom the output, we know that in most of the folds the model predicts very well. In a typical run, one fold may yield bad results. We can use the *mean* of these 10 correlations to represent the *overall* model performance. But first, we need to use the `unlist()` function to transform `fold_cv` into a vector.\n\n\nThis correlation is high suggesting strong association between predicted and true values. Thus, the model is very good in terms of its prediction.\n\n### Bootstrap sampling\n\nThe second method is called *bootstrap sampling*. In k-fold CV, each observation can only be used once for testing. Bootstrap sampling relies on a sampling *with replacement* process. Before selecting a new sample, it recycles every observation so that each observation could appear in multiple folds. \n\nAt each iteration, bootstrap sampling uses $63.2\\%$ of the original data as our training dataset and the remaining $36.8\\%$ as the test dataset. Thus, compared to k-fold CV, bootstrap sampling is less representative of the full dataset. A special case of bootstrapping is the *0.632 bootstrap* technique, which addresses this issue with changing the final performance error assessment formula to\n\n$$error=0.632\\times error_{test}+0.368\\times error_{train}.$$\n\nThis synthesizes the *optimistic model performance* on training data ($error_{train}$) with the *pessimistic model performance* on testing data ($error_{test}$) by weighting the corresponding errors. This method is extremely reliable for small samples (it may be computationally intensive for large samples). \n\nTo see the (asymptotics) rationale behind *0.632 bootstrap* technique, consider a standard training set $T$ of cardinality $n$ where our bootstrapping sampling generates $m$ new training sets $T_i$, each of size $n'$. As sampling from $T$ is uniform *with replacement*, some observations may be repeated in each sample $T_i$. Suppose the size of the sub-samples are of the same order as $T$, i.e., $n'=n$, then for large $n$ the sample $T_{i}$ is *expected* to have $\\left (1 - \\frac{1}{e}\\right ) \\sim 0.632$ unique cases from the complete original collection $T$, the remaining proportion $0.368$ are expected to be repeated duplicates. Hence the name *0.632 bootstrap* sampling. A particular training data element has a probability of $1-\\frac{1}{n}$ of not being picked for training, and hence, its probability of being in the testing set is $\\left (1- \\frac{1}{n}\\right )^n=e^{-1} \\sim 0.368$.\n\nThe simulation below illustrates the *0.632* bootstrap experimentally.\n\n\nIn general, for large $n\\gg n'$, the sample $T_{i}$ is *expected* to have $n\\left ( 1-e^{-n'/n}\\right )$ unique cases, see [On Estimating the Size and Confidence of a Statistical Audit](http://people.csail.mit.edu/rivest/pubs/APR07.pdf)).\n\nHaving the bootstrap samples, the $m$ models can be fitted (estimated) and aggregated, e.g., by averaging the outputs (for regression) or using voting methods (for classification). We will discuss this more in later [Chapters](https://dspa2.predictive.space). Let's look at the implementation of the `0.632 Bootstrap` technique for the *QoL* case-study. \n\n\n\n## Improving model performance by parameter tuning\n\nWe already explored several alternative machine learning (ML) methods for prediction, classification, clustering and outcome forecasting. In many situations, we derive models by estimating model coefficients or parameters. The main question now is *How can we adopt crowd-sourcing advantages of social networks to aggregate different predictive analytics strategies?* \n\nAre there reasons to believe that such **ensembles** of forecasting methods actually improve the performance or boost the prediction accuracy of the resulting consensus meta-algorithm? In this chapter, we are going to introduce ways that we can search for optimal parameters for a single ML method as well as aggregate different methods into **ensembles** to augment their collective performance, relative to any of the individual methods part of the meta-algorithm.\n\nRecall that earlier in [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html), we presented strategies for improving model performance based on *meta-learning*, *bagging*, and *boosting*.\n\nOne of the methods for improving model performance relies on *tuning*. For a given ML technique, tuning is the process of searching through the parameter space for the optimal parameter(s). The following table summarizes some of the parameters used in ML techniques we covered in previous chapters.\n\n$$\\textbf{Table 1: Core parameters of several machine learning techniques.}$$\n\n### Using `caret` for automated parameter tuning\n\nIn [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html) we used kNN and plugged in random *k* parameters for the number of clusters. This time we will test simultaneously multiple *k* values and select the parameter(s) yielding the highest prediction accuracy. Using `caret` allows us to specify an outcome class variable, covariate predictor features, and a specific ML method. In [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), we showed the [Boys Town Study of Youth Development dataset](https://umich.instructure.com/files/399119/download?download_frd=1), where we normalized all the features, stored them in a `boystown_n` computable object, and defined an outcome class variable (`boystown$grade`). \n\n\n\nNow that the dataset includes an explicit class variable and predictor features, we can use the KNN method to predict the outcome `grade`. Let's plug this information into the `caret::train()` function. Note that `caret` can use the complete dataset as it will automatically do the random sampling for the internal statistical cross-validation. To make results reproducible, we may utilize the `set.seed()` function that we presented earlier.\n\n\nIn this case, using `str(m)` to summarize the object `m` may report out too much information. Instead, we can simply type the object name `m`  to get a more concise information about it.\n\n 1. Description about the dataset: number of samples, features, and classes.\n 2. Re-sampling process: here it is using 25 bootstrap samples with 200 observations (same size as the observed dataset) each to train the model.\n 3. Candidate models with different parameters that have been evaluated: by default, `caret` uses 3 different choices for each parameter, but for binary parameters, it only takes 2 choices `TRUE` and `FALSE`). As KNN has only one parameter *k*, we have 3 candidate models reported in the output above. \n 4. Optimal model: the model with largest accuracy is the one corresponding to `k=9`.\n\nLet's see how accurate this \"optimal model\" is in terms of the re-substitution error. Again, we will use the `predict()` function specifying the object `m` and the dataset `boystown_n`. Then, we can report the contingency table showing the agreement between the predictions and real class labels.\n\n\nThis model has $(17+2)/200=0.09$ re-substitution error (9%). This means that in the 200 observations that we used to train this model, 91% of them were correctly classified.  Note that re-substitution error is different from accuracy. The accuracy of this model is $0.81$, which is reported by a model summary call. As mentioned earlier, we can obtain prediction probabilities for each observation in the original `boystown_n` dataset.\n\n\n## Customizing the tuning process\n\nThe default setting of `train()` might not meet the specific needs for every study. In our case, the optimal $k$ might be smaller than $9$. The `caret` package allows us to customize the settings for `train()`. Specifically, `caret::trainControl()` can help us to customize re-sampling methods. There are 6 popular re-sampling methods that we might want to use, which are summarized in the following table.\n\n$$\\textbf{Table 2: Core re-sampling methods.}$$\n\nEach of these methods rely on alternative representative sampling strategies to train the model. Let's use *0.632 bootstrap* for example. Just specify `method=\"boot632\"` in the `trainControl()` function. The number of different samples to include can be customized by the `number=` option. Another option in `trainControl()` allows specification of the model performance evaluation. We can select a preferred method of evaluation for choosing the optimal model. For instance, the `oneSE` method chooses the simplest model within one standard error of the best performance to be the optimal model. Other strategies are also available in the `caret` package. For detailed information, type `?best` in the R console. \n\nWe can also specify a list of $k$ values we want to test by creating a matrix or a grid. \n\n\nUsually, to avoid ties, we prefer to choose an odd number of clusters $k$. Now the constraints are all set. We can start to select models again using `train()`.\n\n\nHere we added `metric=\"Kappa\"` to include the *Kappa statistics* as one of the criteria to select the optimal model. We can see the output accuracy for all the candidate models are better than the default bootstrap sampling. The optimal model has *k=1*, a high accuracy $0.861$, and a high Kappa statistics, which is much better than the model we had in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html). Note that the output based on the SE rule may not necessarily choose the model with the highest accuracy or the highest Kappa statistic as the \"optimal model\". The tuning process is more comprehensive than only looking at one statistic.\n\n\n## Comparing the performance of several alternative models\n\nEarlier in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html) we saw examples of how to choose appropriate evaluation metrics and how to contrast the performance of various AI/ML methods. Below, we illustrate model comparison based on classification of [Case-Study 6, Quality of Life (QoL) dataset](https://umich.instructure.com/courses/38100/files/folder/Case_Studies) using bagging, boosting, random forest, SVN, k nearest neighbors, and decision trees. All available [`caret` package ML/AI training methods are listed here](https://topepo.github.io/caret/train-models-by-tag.html).\n\n\n\n## Forecasting types and assessment approaches\n\nCross-validation is a strategy for validating predictive methods, classification models and clustering techniques by assessing the reliability and stability of the results of the corresponding statistical analyses (e.g., predictions, classifications, forecasts) based on independent datasets. For prediction of trend, association, clustering, and classification, a model is usually trained on one dataset (*training data*) and subsequently tested on new data (*testing or validation data*). Statistical internal cross-validation defines a test dataset to evaluate the model predictive performance as well as assess its power to avoid overfitting. *Overfitting* is the process of computing a predictive or classification model that describes random error, i.e., fits to the noise components of the observations, instead of identifying actual relationships and salient features in the data.\n\nIn this section, we will use Google Flu Trends, Autism, and Parkinson's disease case-studies to illustrate (1) alternative forecasting types using linear and non-linear predictions, (2) exhaustive and non-exhaustive internal statistical cross-validation, and (3) explore complementary predictor functions.\n\nIn [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html) we discussed the types of classification and prediction methods, including `supervised` and `unsupervised` learning. The former are direct and predictive (there are known outcome variables that can be predicted and the corresponding forecasts can be evaluated) and the latter are indirect and descriptive (there are no *a priori* labels or specific outcomes).\n\n### Overfitting  \n\nBefore we go into the cross-validation of predictive analytics, we will present several examples of *overfitting* that illustrates why a certain amount of skepticism and mistrust may be appropriate when dealing with forecasting models based on large and complex data.\n\n#### Example (US Presidential Elections)\t  \n\nBy 2022, there were only **58 US presidential elections** and **46 presidents**. That is a small dataset, and learning from it may be challenging. For instance:\n\n - If the predictor space expands to include things like *having false teeth*, it's pretty easy for the model to go from fitting the generalizable features of the data (the signal, e.g., presidential actions) to matching noise patterns (e.g., irrelevant characteristics like gender of the children of presidents, or types of dentures they may wear).\n - When overfitting noise patterns takes place, the quality of the model fit assessed on the historical data may improve (e.g., better $R^2$, more about the [Coefficient of Determination is available here](https://en.wikipedia.org/wiki/Coefficient_of_determination)). At the same time, however, the model performance may be suboptimal when used to make inferences about prospective data, e.g., future presidential elections.  \n\nThis cartoon illustrates some of the (unique) noisy presidential characteristics that are thought to be unimportant to presidential elections or presidential performance. ![Cartoon of noisy presidential characteristics](https://imgs.xkcd.com/comics/electoral_precedent.png)\n\n#### Example (Google Flu Trends)\t  \n\nA March 14, 2014 article in Science ([DOI: 10.1126/science.1248506](https://doi.org/10.1126/science.1248506)), identified problems in [Google Flu Trends (GFT)](https://www.google.org/flutrends/about/#US), [DOI  10.1371/journal.pone.0023610](https://doi.org/10.1371/journal.pone.0023610), which may be attributed in part to overfitting. The GFT was built to predict the future Centers for Disease Control and Prevention (CDC) reports of doctor office visits for influenza-like illness (ILI). In February 2013, Nature reported that GFT was predicting more than double the proportion of doctor visits compared to the CDC forecast for the same period.   \n  \nThe GFT model found the best matches among 50 million web search terms to fit 1,152 data points. It predicted quite high odds of finding search terms that match the propensity of the flu but are structurally unrelated and hence are not prospectively predictive. In fact, the GFT investigators reported weeding out unrelated to the flu seasonal search terms may have been strongly correlated to the CDC data, e.g., high school basketball season. The big GFT data may have overfitted the relatively small number of cases. This false-alarm result was also paired with a false-negative finding. The GFT model also missed the non-seasonal 2009 H1N1 influenza pandemic, which provides a cautionary tale about prediction, overfitting, and prospective validation.\n\n#### Example (Autism)\n\nAutistic brains constantly overfit visual and cognitive stimuli. To an autistic person, a general conversation of several adults may seem like a cacophony due to super-sensitive detail-oriented hearing and perception tuned to literally pick up all elements of the conversation and clues of the surrounding environment. At the same time, autistic brains downplay body language, sarcasm and non-literal cues. We can `miss the forest for the trees` when we start \"overfitting\", over-interpreting the noise on top of the actual salient information. Ambient noise, trivial observations and unrelated perceptions may hide the true communication details.  \n  \nHuman conversations and communications involve exchanges of both critical information and random noise. Fitting a perfect model requires focus only on the \"relevant\" information. Overfitting occurs when attention is (excessively) consumed with peripheral noise, or worse, overwhelmed by inconsequential noise drowning the salient aspects of the communication exchange.  \n  \nAny dataset is a mix of signal and noise. The main task of our brains is to sort these components and interpret the information (i.e., ignore the noise). \n\n**\"One person's noise is another person's treasure map!\"**\n\nOur predictions are most accurate if we can model as much of the signal and as little of the noise as possible. Note that in these terms, $R^2$ is a poor metric to identify predictive power - it measures how much of the signal **and** the noise is explained by our model. In practice, it's hard to always identify what's signal and what's noise. This is why practical applications tend to favor simpler models, since the more complicated a model is, the easier it is to overfit the noise component of the observed information.  \n\n## Internal Statistical Cross-validation\n\nInternal statistical cross-validation assesses the expected performance of a prediction method in cases (subject, units, regions, etc.) drawn from a similar population as the original training data sample. `Internal` validation is distinct from `external` validation, as the latter potentially allows for the existence of differences between the populations (training data, used to develop or train the technique, and testing data, used to independently quantify the performance of the technique).\n\nEach step in the internal statistical cross-validation protocol involves:\n  \n - Randomly partitioning a sample of data into 2 complementary subsets (training + testing).\n - Performing the analysis, fitting or estimating the model using the training set.\n - Validating the analysis or evaluating the performance of the model using a separate testing set.\n - Increasing the iteration index and repeating the process. Various termination criteria can involve a fixed number, a desired mean variability, or an upper bound on the error-rate.    \n  \nHere is one example of internal statistical cross-validation [Predictive diagnostic modeling in Parkinson's disease](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0157077).\n\nTo reduce the noise and variability at each iteration, the final validation results may include the averaged performance results each iteration.  \n  \nIn cases when new observations are hard to obtain (due to costs, reliability, time, or other constraints), cross-validation guards against testing hypotheses suggested by the data themselves (also known as `Type III error` or False-Suggestion).   \n  \nCross-validation is different from *conventional-validation* (e.g. 80%-20% partitioning the data set into training and testing subsets) where the prediction error (e.g., [Root Mean Square Error, RMSE](https://wiki.socr.umich.edu/index.php/SMHS_BiasPrecision#Precision)) evaluated on the training data is not a useful estimator of model performance, as it does not generalize across multiple samples. \n\nIn general, the errors of the conventional-valuation are based on the results of a specific test dataset and may not accurately represent the model performance. A more appropriate strategy to properly estimate model prediction performance is to use cross-validation (CV), which combines (averages) prediction errors to measure the model performance. CV corrects for the expected stochastic nature of partitioning the training and testing sets and generates a more accurate and robust estimate of the expected model performance.  \n\nRelative to a simpler model, a more complex model may *overfit-the-data* if it has a short foresight, i.e., it generates accurate fitting results for known data but less accurate results when predicting based on new data. Knowledge from past experiences may include either *relevant* or *irrelevant* (noise) information. In challenging data-driven prediction models when uncertainty (entropy) is high, more noise is present in past information that needs to be accounted for in prospective forecasting. However, it is generally hard to discriminate patterns from noise in complex systems (i.e., deciding which part to model and which to ignore). Models that reduce the chance of fitting noise are called **robust**.\n  \n### Example (Linear Regression)\t\n\nLet's demonstrate a simple model assessment using linear regression. Suppose we observe the response values {${y_1, \\cdots, y_n}$}, and the corresponding $k$ predictors represented as a $kD$ vector of covariates {${x_1, \\cdots, x_n }$}, where subjects/cases are indexed by $1\\leq i\\leq n$, and the data-elements (variables) are indexed by $1\\leq j\\leq k$.  \n  \n$$ \\begin{pmatrix} x_{1, 1} &\\cdots & x_{1, k}\\\\ \\vdots &\\ddots & \\vdots \\\\x_{n, 1} &\\cdots & x_{n, k} \\\\ \\end{pmatrix} .$$\n  \nUsing least squares to estimate the linear function parameters (effect-sizes), ${\\beta _1, \\cdots , \\beta _k }$, allows us to compute a hyperplane $y = a + x\\beta$ that best fits the observed data ${(x_i, y_i )}_{1\\leq i \\leq n}$. This is expressed as a matrix by:  \n\n$$\\begin{pmatrix} y_1\\\\ \\vdots \\\\ y_n\\\\ \\end{pmatrix} = \\begin{pmatrix} a_1\\\\ \\vdots \\\\ a_n\\\\ \\end{pmatrix}+\\begin{pmatrix} x_{1, 1} &\\cdots & x_{1, k}\\\\ \\vdots &\\ddots & \\vdots \\\\x_{n, 1} &\\cdots & x_{n, k} \\\\ \\end{pmatrix} \\begin{pmatrix} \\beta_1\\\\ \\vdots \\\\ \\beta_k\\\\ \\end{pmatrix}.$$  \nCorresponding to the system of linear hyperplanes:\n\n$$\\left\\{\n\\begin{array}{c}\ny_1=a_1+ x_{1, 1} \\beta_1 +x_{1, 2} \\beta_2 + \\cdots +x_{1, k} \\beta_k \\\\\ny_2=a_2+ x_{2, 1} \\beta_1 +x_{2, 2} \\beta_2 + \\cdots +x_{2, k} \\beta_k \\\\\n\\vdots\\\\\ny_n=a_n+ x_{n, 1} \\beta_1 +x_{n, 2} \\beta_2 + \\cdots +x_{n, k} \\beta_k\n\\end{array}\n\\right.\\ .$$   \n  \nOne measure to evaluate the model fit may be the mean squared error (MSE). The MSE for a given value of the parameters $\\alpha$ and $\\beta$ on the observed training data ${(x_i, y_i) }_{1\\leq i\\leq n}$ is expressed as\n\n$$MSE=\\frac{1}{n}\\sum_{i=1}^{n} (y_i- \\underbrace{(a_1+ x_{i, 1} \\beta_1 +x_{i, 2} \\beta_2 + \\cdots +x_{i, k} \\beta_k) }_{\\text{predicted value } \\hat{y}_i, \\text{at } {x_{i, 1}, \\cdots, x_{i, k}}}   )^2\\ .$$\n\nAnd the corresponding root mean square error (RMSE) is:  \n\n$$RMSE=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_1- \\underbrace{(a_1+ x_{1, 1} \\beta_1 +x_{1, 2} \\beta_2 + \\cdots +x_{1, k} \\beta_k) }_{\\text{predicted value } \\hat{y}_1, \\text{at } {x_{i, 1}, \\cdots, x_{i, k}}}   )^2}\\ .$$  \n  \nIn the linear model case, the expected value of the MSE (over the distribution of training sets) for the **training set** is  $\\frac{n - k - 1}{n + k + 1} E$, where $E$ is the expected value of the MSE for the **testing/validation data**. Therefore, fitting a model and computing the MSE on the training set, we may produce an over optimistic evaluation assessment (smaller RMSE) of how well the model may fit another dataset. This bias represents *in-sample* estimate of the fit, whereas we are interested in the cross-validation estimate as an *out-of-sample* estimate.  \n  \nIn the linear regression model, cross validation may not be as useful, since we can compute the **exact** correction factor $\\frac{n - k - 1}{n + k + 1}$ to obtain as estimate of the exact expected *out-of-sample* fit using the *in-sample* MSE (under)estimate. However, even in this situation, cross-validation remains useful as it can be used to select an optimal regularized cost function.   \n  \nIn most other modeling procedures (e.g. logistic regression), there are no simple general closed-form expressions (formulas) to adjust the cross-validation error estimate from the in-sample fit estimate. Cross-validation is a generally applicable way to predict the performance of a model on a validation set using stochastic computation instead of obtaining experimental, theoretical, mathematical, or analytic error estimates.\n\n### Cross-validation methods\t\nThere are 2 classes of cross-validation approaches, *exhaustive* and *non-exhaustive*.  \n\n#### Exhaustive cross-validation\t \nExhaustive cross-validation methods are based on determining all possible ways to divide the original sample into training and testing data. For instance, the *Leave-m-out cross-validation* involves using $m$ observations for testing and the remaining ($n-m$) observations as training. The case when $m=1$, i.e., leave-1-out method, is only applicable when $n$ is small, due to its huge computational cost. This process is repeated on all partitions of the original sample. This method requires model fitting and validating $C_m^n$ times ($n$ is the total number of observations in the original sample and $m$ is the number left out for validation). This requires a very large number of *steps*.  \n  \n#### Non-exhaustive cross-validation\t\n\nNon-exhaustive cross validation methods avoid computing estimates/errors using all possible partitionings of the original sample, but rather approximate these. For example, in the **k-fold cross-validation**, the original sample is randomly partitioned into $k$ equal sized subsamples, or *folds*. Of the $k$ subsamples, a single subsample is kept as final testing data for validation of the model. The other $k - 1$ subsamples are used as training data. The cross-validation process is then repeated $k$ times, corresponding to the $k$ folds.  Each of the $k$ subsamples is used once as the validation data. There are corresponding $k$ results that are averaged (or otherwise aggregated) to generate a final pooled model-quality estimation. In k-fold validation, all observations are used for both training and validation, and each observation is used for validation exactly once. In general, $k$ is a parameter that needs to be selected by an investigator (common values may be $5$ or $10$).  \n  \nA general case of the `k-fold validation` is $k=n$ (the total number of observations), when it coincides with the **leave-one-out cross-validation**.  \n    \nA variation of the k-fold validation is **stratified k-fold cross-validation**, where each fold has the same (approximately) mean response value. For instance, if the model represents a binary classification of cases (e.g., controls vs. patients), this implies that each fold contains roughly the same proportion of the two class labels.  \n  \n**Repeated random sub-sampling validation** splits randomly the entire dataset into a training set, where the model is fit, and a testing set, where the predictive accuracy is assessed. Again, the results are averaged over all iterative splits. This method has an advantage over k-fold cross validation as that the proportion of the training/testing split is not dependent on the number of iterations (folds). However, its drawback is that some observations may never be selected whereas others may be selected multiple times in the testing/validation subsample. As validation subsets may overlap, the results may vary each time we repeat the validation protocol, unless we set a seed point in the algorithm.   \n  \nAsymptotically, as the number of random splits increases, the *repeated random sub-sampling* validation approaches the *leave-k-out cross-validation*.  \n\n### Case-Studies\n\nIn the examples below, we have intentionally suppressed some of the R output to save space. This is accomplished using this Rmarkdown command, `{r eval=TRUE, results='hide'}`, however, the reader is encouraged to try hands-on all the protocols, make modifications, inspect and interpret the outputs.\n\n#### Example 1: Prediction of Parkinson's Disease using `Adaptive Boosting` (AdaBoost)\n\nThis [Parkinson's Diseases Study](https://dx.doi.org/10.1371/journal.pone.0157077)\tinvolves heterogeneous neuroimaging, genetics, clinical, and phenotypic data of over 600 volunteers. The multivariate data includes three cohorts (HC=Healthy Controls, PD=Parkinson's, SWEDD=subjects without evidence for dopaminergic deficit).\n\n\nLoad the PPMI data [06_PPMI_ClassificationValidationData_Short.csv](https://umich.instructure.com/files/330400/download?download_frd=1).\n\n\nBinarize the Dx (clinical diagnoses) classes.\n\n\nObtain a model-free predictive analytics, e.g., AdaBoost classification, and report the results.\n\n\nRecall from [Chapter 2](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Visualization.html) that when group sizes are imbalanced, we may need to rebalance them to avoid potential biases of dominant cohorts. In this case, we will re-balance the groups using the package `SMOTE` [Synthetic Minority Oversampling Technique](https://cran.r-project.org/web/packages/unbalanced/). `SMOTE` may be used to handle class imbalance in binary or multinomial (multiclass) classification.\n\n\nNext, we'll check the re-balanced cohort sizes.\n\n\n\n\nThe next step will be the actual `Cross validation`.\n\n\n\nAs we can see from the reported metrics, the overall averaged AdaBoost-based diagnostic predictions are quite good.\n\n#### Example 2: Sleep dataset\n\nThese data contain the effect of two soporific drugs to increase hours of sleep (treatment-compared design) on 10 patients. The data is available by default (`sleep {datasets}`)\n\nFirst, load the data and report some graphs and summaries.\n\n\nNext, we will define a new LDA (linear discriminant analysis) predicting function and perform the `Cross-validation` (CV) on the resulting predictor.\n\n\n\nExecute the above code and interpret the diagnostic results measuring the performance of the LDA prediction.\n\n#### Example 3: Model-based (linear regression) prediction using the `attitude` dataset\n\nThis data represents the average survey responses from about 35 employees from 30 (randomly selected) departments in a large organization. The data captures the proportion of favorable responses to 7 questions in each department.\n\nLet's load and summarize the data, which is available by default in `attitude {datasets}`.\n\n\nWe will demonstrate model-based analytics using `lm` and `lda`, and will validate the forecasting using CV.\n\n  \n#### Example 4: Parkinson's data (`ppmi_data`) \n\nLet's go back to the more elaborate Parkinson's disease (PD/PPMI) case, load and preprocess the [derived-PPMI data](https://github.com/SOCR/PBDA).\n\n\n\nApply `Cross-validation` to assess the performance of the linear model.\n\n\n\n### Summary of CV output\n\nThe cross-validation (CV) output object includes the following components:\n  \n -\t`stat.cv`: Vector of statistics returned by *predfun* for each cross validation run\n -\t`stat`:  statistic returned by *predfun* averaged over all cross validation runs\n -\t`stat.se`: variability capturing the corresponding standard error across cv-iterations.\n\n### Alternative predictor functions\t\n\nWe have already seen a number of `predict()` functions, e.g., [Chapter 3](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html). Below, we will add to the collection of predictive analytics and forecasting functions using the [PPMI dataset (06_PPMI_ClassificationValidationData_Short.csv)](https://umich.instructure.com/courses/38100/files/search?search_term=PPMI).\n\n#### Logistic Regression\t\n\nWe will learn more about the *logit* model in [Chapter 11](https://socr.umich.edu/DSPA2/DSPA2_notes/11_FeatureSelection.html). Now, we will demonstrate the use of a logit-predictor function to forecast a binary diagnostic outcome (patient or control) using the Parkinson's disease study, [06_PPMI_ClassificationValidationData_Short.csv](https://umich.instructure.com/files/330400/download?download_frd=1).\n\n  \nNote that the predicted values are in $log$ terms, so they need to be *exponentiated* to interpret them correctly.\n\n\n**Caution**: Note that if you forget to exponentiate the predicted logistic model values (see *ynew2* in `predict.logit`), you will get nonsense results, e.g., all cases may be predicted to be in one class, trivial sensitivity or negative predictive power (NPP).  \n\n#### Quadratic Discriminant Analysis (QDA)\n\nIn [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html) we discussed the *linear* and *quadratic* discriminant analysis models. Let's now introduce a `predfun.qda()` function.\n\n\nThis error message: \"**Error in qda.default(x, grouping, ...) : rank deficiency in group 1**\" indicates that there is a rank deficiency, i.e. some variables are collinear and one or more covariance matrices cannot be inverted to obtain the estimates in group 1 (Controls)!  \n  \n*Removing the strongly correlated data elements (\"R_fusiform_gyrus_Volume\", \"R_fusiform_gyrus_ShapeIndex\", and \"R_fusiform_gyrus_Curvedness\"), resolves the rank-deficiency problem!* \n\n\nIt makes sense to contrast the QDA and GLM/Logit predictions.\n\n\nClearly, both the QDA and Logit model predictions are quite similar and reliable.\n\n### Foundation of LDA and QDA for prediction, dimensionality reduction, or forecasting\n\nPreviously, in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html) we saw some examples of LDA/QDA methods. Now, we'll talk more about the details. Both LDA (Linear Discriminant Analysis) and QDA (Quadratic Discriminant Analysis) use probabilistic models of the class conditional distribution of the data $P(X \\mid Y=k)$ for each class $k$. Their predictions are obtained by using [Bayesian theorem](https://wiki.socr.umich.edu/index.php/SMHS_BayesianInference#Bayesian_Rule), see [Appendix](https://socr.umich.edu/DSPA2/DSPA2_notes/DSPA_Appendix_01_BayesianInference_MCMC_Gibbs.html),   \n\n$$P(Y=k \\mid X)= \\frac{P(X \\mid Y=k)P(Y=k)}{P(X)}=\\frac{P(X \\mid Y=k)P(Y=k)}{\\sum_{l=0}^{\\infty}P(X \\mid Y=l)P(Y=l)},$$\n\nand we select the class $k$, which **maximizes** this conditional probability (maximum likelihood estimation). In linear and quadratic discriminant analysis, $P(X \\mid Y)$ is modeled as a multivariate Gaussian distribution with density \n\n$$P(X \\mid Y=k)= \\frac{1}{{(2 \\pi)}^{\\frac{n}{2}} {\\vert{\\Sigma_{k}}\\vert}^{\\frac{1}{2}}}\\times e^{(-\\frac{1}{2}{(X- \\mu_k)}^T \\Sigma_k^{-1}(X- \\mu_k))}.$$\n  \nThis model can be used to classify data by using the training data to **estimate**:  \n  \n - the class prior probabilities $P(Y=k)$ by counting the proportion of observed instances of class $k$,   \n - the class means $\\mu_k$ by computing the empirical sample class means, and   \n - the covariance matrices by computing either the empirical sample class covariance matrices, or by using a regularized estimator, e.g., LASSO).   \n  \nIn the **linear case** (LDA), the Gaussian components for each class are assumed to share the same covariance matrix  \n$\\Sigma_k=\\Sigma$ for each class $k$. This leads to linear decision surfaces between classes. This is clear from comparing the log-probability ratios of 2 classes ($k$ and $l$):   \n$LOR=\\log\\left (\\frac{P(Y=k \\mid X)}{P(Y=l \\mid X)}\\right )$, LOR=0 $\\Longleftrightarrow$ the two probabilities are identical, i.e., yield same class label.\n\n$$LOR=\\log\\left (\\frac{P(Y=k \\mid X)}{P(Y=l \\mid X)}\\right )=0\\ \\  \\Longleftrightarrow\\ \\ {(\\mu_k-\\mu_l)}^T \\Sigma^{-1} (\\mu_k-\\mu_l)= \\frac{1}{2}(\\mu_k^T \\Sigma^{-1} \\mu_k-\\mu_l^T \\Sigma^{-1} \\mu_l). $$\n\nBut, in the more general **quadratic case** (QDA), these assumptions about the covariance matrices $\\Sigma_k$ of the Gaussian components are relaxed leading to quadratic decision surfaces.  \n  \n#### LDA (Linear Discriminant Analysis)\n\nLDA is similar to the generalized linear model (GLM) used in ANOVA and regression analyses. LDA also attempts to express one dependent variable as a linear combination of other features or data elements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas LDA has continuous independent variables and a categorical dependent variable (i.e. Dx/class label). Logistic regression `logit` and `probit` regression are more similar to LDA than ANOVA, as they also explain a categorical variable by the values of continuous independent variables.\n\n\n#### QDA (Quadratic Discriminant Analysis)\n\nSimilarly to LDA, the QDA prediction function can be defined by:\n\n\n#### Neural Network\n\nWe saw Neural Networks (NNs) in [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html). Applying NNs is not straightforward. We have to create a design matrix with an indicator column for the response feature. In addition, we need to write a *predict function* to translate the output of `neuralnet()` into analytical forecasts.\n\n\n#### SVM \n\nWe also saw SVM in [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html). Let's try cross validation on Linear and Gaussian (radial) kernel SVM. We can expect that linear SVM should achieve a close result to Gaussian or even better than Gaussian since this dataset have a large $k$ (# features) compared with $n$ (# cases), which we have studied in detail in [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html).\n\n\nIndeed, both types of kernels yield good quality predictors according to the assessment metrics reported by the `diagnosticErrors()` method. \n\n#### k-Nearest Neighbors algorithm (k-NN)\n\nAs we saw in [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html), *k-NN* is a non-parametric method for either classification or regression, where the **input** consists of the k closest **training examples** in the feature space, but the **output** depends on whether k-NN is used for classification or regression:  \n  \n - In *k-NN classification*, the output is a class membership (labels). Objects in the testing data are classified by a majority vote of their neighbors. Each object is assigned to a class that is most common among its k nearest neighbors ($k$ is always a small positive integer). When $k=1$, then an object is assigned to the class of its single nearest neighbor.\n - In *k-NN regression*, the output is the property value for the object representing the average of the values of its $k$ nearest neighbors.\n\nLet's now build the corresponding `predfun.knn()` method.\n\n  \nWe can also examine the performance of k-NN prediction on the PPMI (Parkinson's disease) data. Start by partitioning the data into `training` and `testing` sets. \n\n\nThen fit the k-NN model and report the results.\n\n\n#### k-Means Clustering (k-MC)  \n\nIn [Chapter 8](https://socr.umich.edu/DSPA2/DSPA2_notes/08_Unsupervised_Clustering.html), we showed that k-MC aims to partition $n$ observations into $k$ clusters where each observation belongs to the cluster with the nearest mean which acts as a prototype of a cluster. The k-MC partitions the data space into [Voronoi cells](https://en.wikipedia.org/wiki/Voronoi_diagram). In general, there is no computationally tractable solution for this, i.e., the problem is [NP-hard](https://en.wikipedia.org/wiki/NP-hardness). However, there are efficient algorithms that converge quickly to local optima, e.g., *expectation-maximization* algorithm for Gaussian mixture distribution modeling that we saw in [Chapter 8](https://socr.umich.edu/DSPA2/DSPA2_notes/08_Unsupervised_Clustering.html).  \n\n\nWe get an empty cluster instead of two clusters when we randomly select two points as the initial centers. The way to solve this problem is using [k-means++](https://dl.acm.org/doi/10.5555/1283383.1283494).\n\n\nNow let's evaluate the model. The first step is to justify the selection of `k=2`. We use `silhouette()` in the package `cluster`. Recall from [Chapter 8](https://socr.umich.edu/DSPA2/DSPA2_notes/08_Unsupervised_Clustering.html) that the *silhouette value* is in the range $[-1,1]$ with negative and positive values corresponding to (mostly) \"mis-labeled\" and \"correctly-labeled\" cases, respectively.\n\n\nThe result is pretty good. Only a very small number of samples are \"mis-clustered\" (having negative silhouette value).\n\nFurther, you can observe that when `k=3` or `k=4`, the overall silhouette is smaller.\n\n\nThen, let's calculate the unsupervised classification error. Here, $p$ represents the percentage of class $0$ cases, which provides the weighting factor for labeling each cluster. \n\n\nIt achieves $75\\%$ accuracy, which is reasonable for unsupervised classification. \n\nFinally, let's visualize the results by superimposing the data into the first two multidimensional scaling (MDS) dimensions.\n\n\n#### Spectral Clustering\t\n\nExpanding on the *spectral clustering* approach we covered in the previous [Chapter 8](https://socr.umich.edu/DSPA2/DSPA2_notes/08_Unsupervised_Clustering.html), suppose the multivariate dataset is represented as a set of data points $A$. We can define a similarity matrix $S={s_{(i, j)}}$, where $s_{(i, j)}$ represents a measure of the similarity between points $i, j\\in A.$  Spectral clustering uses the spectrum of the similarity matrix of the high-dimensional data and perform dimensionality reduction for clustering into fewer dimensions. The spectrum of a matrix is the set of its eigenvalues. In general, if $T:\\Omega \\stackrel{\\text{linear operator}}{\\rightarrow} \\Omega$ maps a vector space $\\Omega$ into itself, its spectrum is the set of scalars $\\lambda=\\{\\lambda_i\\}$ such that $(T-\\lambda I)v=0$, where $I$ is the identity matrix and $v$ are the eigen-vectors (or eigen-functions) for the operator $T$. The **determinant** of the matrix equals the product of its eigenvalues, i.e., $det(T)=\\Pi_i \\lambda_i$ , the **trace** of the matrix $tr(T)=\\Sigma_i \\lambda_i$, and the **pseudo-determinant** for a singular matrix is the product of its nonzero eigenvalues, $pseudo_{det}(T)=\\Pi_{\\lambda_i \\neq 0}\\lambda_i.$    \n\nTo partition the data points into two sets $(S_1, S_2)$ suppose $v$ is the second-smallest eigenvector of the Laplacian matrix:\n\n$$L = I - D^{-\\frac{1}{2}}SD^{\\frac{1}{2}}$$\n\nof the similarity matrix $S$, where $D$ is the diagonal matrix $D_{i, i}=\\Sigma_j S_{i, j}.$   \n  \nThis actual $(S_1, S_2)$ partitioning of the cases in the data may be done in different ways. For instance, $S_1$ may use the median $m$ of the components in $v$ and group all data points whose component in $v$ is greater than $m$. Then the remaining cases can be labeled as part of $S_2$. This approach may be used iteratively for *hierarchical clustering* by repeatedly partitioning the subsets.  \n  \nThe $specc$ method in the $kernlab$ package implements a spectral clustering algorithm where the data-clustering is performed by embedding the data into the subspace of the eigenvectors of an affinity matrix.\n\n\nLet's look at a few simple cases of spectral clustering. We are suppressing some of the outputs to save space (e.g., `#plot(my_data, col= data_sc)`).\n\n#### Iris petal data\n\nLet's look at the *iris* dataset we saw in [Chapter 2](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Visualization.html).\n\n\n#### Spirals data\n\nLet's look at another simple example using the `kernlab::spirals` dataset.\n\n\n#### Income data\n\nA `kernlab::income` datasets contains customer *income* information from a marketing survey.\n\n\n### Comparing multiple classifiers\n\nNow let's compare all eight classifiers (`AdaBoost, LDA, QDA, knn, logit, Neural Network, linear SVM` and `Gaussian SVM`) we presented above using the [PPMI dataset (06_PPMI_ClassificationValidationData_Short.csv)](https://umich.instructure.com/courses/38100/files/search?search_term=PPMI).\n\n\nLeaving `knn`, `kmeans` and `specc` aside, the other methods achieve pretty good results. With these data, the reason for the suboptimal results of some clustering methods may be rooted in lack of training (e.g., `specc` and `kmeans`) or the curse of (high) dimensionality, which we saw in [Chapter 4](https://socr.umich.edu/DSPA2/DSPA2_notes/04_DimensionalityReduction.html). As the data is super sparse, predicting from the nearest neighbors may not be too reliable.\n\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 9773
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    self_contained: yes\n---",
      "row_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "content": "Inference | Outcome     | Evaluation Metrics  | R functions\n----------|-------------|---------------------|------------------------\nClassification & Prediction | Binary | Accuracy, Sensitivity, Specificity, PPV/Precision, NPV/Recall, LOR |  `caret::confusionMatrix`, `gmodels::CrossTable`, `cluster::silhouette`\nClassification & Prediction | Categorical | Accuracy, Sensitivity/Specificity, PPV, NPV, LOR, Silhouette Coefficient  |  `caret::confusionMatrix`, `gmodels::CrossTable`, `cluster::silhouette`\nRegression Modeling | Real Quantitative | correlation coefficient, $R^2$, RMSE, Mutual Information, Homogeneity and Completeness Scores |  `cor`, `metrics::mse`",
      "row_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "content": "| Predicted \\\\ Observed | Minor  | Severe  | Row Sum  |\n|---|---|---|---|\n| Minor  | $A=143$  | $B=71$  | $A+B=214$ |\n| Severe | $C=72$ | $D=157$ | $C+D=229$ |\n| Column Sum  | $A+C=215$  | $B+D=228$ | $A+B+C+D=443$ | \t",
      "row_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "content": "**Model**                   | **Learning Task**   | **Method** | **Parameters**\n------------------------|-----------------|--------|----------------\nKNN\t|Classification|\t`class::knn`|\t`data, k`\nK-Means\t|Classification|\t`stats::kmeans`|\t`data, k`\nNaive Bayes|\tClassification|\t`e1071::naiveBayes`|\t`train, class, laplace`\nDecision Trees|\tClassification|\t`C50::C5.0`|\t`train, class, trials, costs`\nOneR Rule Learner|\tClassification|\t`RWeka::OneR`|\t`class~predictors, data`\nRIPPER Rule Learner|\tClassification|\t`RWeka::JRip`|\t`formula, data, subset, na.action, control, options`\nLinear Regression|\tRegression|\t`stats::lm`|\t`formula, data, subset, weights, na.action, method`\nRegression Trees|\tRegression|\t`rpart::rpart`|\t`dep_var ~ indep_var, data`\nModel Trees|\tRegression|\t`RWeka::M5P`|\t`formula, data, subset, na.action, control`\nNeural Networks|\tDual use|\t`nnet::nnet`|\t`x, y, weights, size, Wts, mask,linout, entropy, softmax, censored, skip, rang, decay, maxit, Hess, trace, MaxNWts, abstol, reltol`\nSupport Vector Machines (Polynomial Kernel)|\tDual use|\t`caret::train::svmLinear`|\t`C`\nSupport Vector Machines (Radial Basis Kernel)|\tDual use|\t`caret::train::svmRadial`|\t`C, sigma`\nSupport Vector Machines (general)|\tDual use|\t`kernlab::ksvm`|\t`formula, data, kernel`\nRandom Forests|\tDual use|\t`randomForest::randomForest`|\t`formula, data`",
      "row_count": 16
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "content": "**Resampling method** |\t**Method name**\t| **Additional options and default values**\n----------------------|-----------------|-------------------------------------\nHoldout sampling|\t`LGOCV`|\t`p = 0.75` (training data proportion)\nk-fold cross-validation|\t`cv`|\t`number = 10` (number of folds)\nRepeated k-fold cross validation|\t`repeatedcv`|\t`number = 10` (number of folds), `repeats = 10` (number of iterations)\nBootstrap sampling|\t`boot`|\t`number = 25` (resampling iterations)\n0.632 bootstrap|\t`boot632`|\t`number = 25` (resampling iterations)\nLeave-one-out cross-validation|\t`LOOCV`|\tNone",
      "row_count": 8
    }
  ],
  "r_code": [
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "hn_med <- read.csv(\"https://umich.instructure.com/files/1614350/download?download_frd=1\",\n                   stringsAsFactors = FALSE)\nhn_med$seer_stage <- factor(hn_med$seer_stage)\nlibrary(tm)\nhn_med_corpus <- Corpus(VectorSource(hn_med$MEDICATION_SUMMARY))\n corpus_clean <- tm_map(hn_med_corpus, tolower)\n corpus_clean <- tm_map(corpus_clean, removePunctuation)\n corpus_clean <- tm_map(corpus_clean, stripWhitespace)\n corpus_clean <- tm_map(corpus_clean, removeNumbers)\n hn_med_dtm   <- DocumentTermMatrix(corpus_clean)\n hn_med_train <- hn_med[1:562, ]\n hn_med_test  <- hn_med[563:662, ]\n hn_med_dtm_train <- hn_med_dtm[1:562, ]\n hn_med_dtm_test <- hn_med_dtm[563:662, ]\n corpus_train <- corpus_clean[1:562]\n corpus_test <- corpus_clean[563:662]\n hn_med_train$stage <- hn_med_train$seer_stage %in% c(4, 5, 7)\n hn_med_train$stage <- factor(hn_med_train$stage, levels=c(F, T), \n                              labels = c(\"early_stage\", \"later_stage\"))\n hn_med_test$stage <- hn_med_test$seer_stage %in% c(4, 5, 7)\n hn_med_test$stage <- factor(hn_med_test$stage, levels=c(F, T), \n                             labels = c(\"early_stage\", \"later_stage\"))\n \nconvert_counts <- function(x) {\n  x <- ifelse(x > 0, 1, 0)\n  x <- factor(x, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\n  return(x)\n}\n\nhn_med_dict <- as.character(findFreqTerms(hn_med_dtm_train, 5))\nhn_train <- DocumentTermMatrix(corpus_train, list(dictionary=hn_med_dict))\nhn_test  <- DocumentTermMatrix(corpus_test, list(dictionary=hn_med_dict))\nhn_train <- apply(hn_train, MARGIN = 2, convert_counts)\nhn_test  <- apply(hn_test, MARGIN = 2, convert_counts)\nlibrary(e1071)\nhn_classifier <- naiveBayes(hn_train, hn_med_train$stage)",
      "line_count": 36
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "pred_raw <- predict(hn_classifier, hn_test, type=\"raw\")\nhead(pred_raw)",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "pred_nb <- predict(hn_classifier, hn_test)\nhead(stats::ftable(pred_nb))",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "qol <- read.csv(\"https://umich.instructure.com/files/481332/download?download_frd=1\")\nqol <- qol[!qol$CHRONICDISEASESCORE==-9, ]\nqol$cd <- qol$CHRONICDISEASESCORE>1.497\nqol$cd <- factor(qol$cd, levels=c(F, T), labels = c(\"minor_disease\", \"severe_disease\"))\nqol <- qol[order(qol$ID), ]\n\n# Remove ID (col=1) # the clinical Diagnosis (col=41) will be handled later\nqol <- qol[ , -1]\n\n# 80-20%  training-testing data split\nset.seed(1234)\ntrain_index <- sample(seq_len(nrow(qol)), size = 0.8*nrow(qol))\nqol_train <- qol[train_index, ]\nqol_test <- qol[-train_index, ]\n\nlibrary(C50)\nset.seed(1234)\nqol_model <- C5.0(qol_train[,-c(40, 41)], qol_train$cd)",
      "line_count": 18
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "pred_prob <- predict(qol_model, qol_test, type=\"prob\")\nhead(pred_prob)",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "pred_tree <- predict(qol_model, qol_test)\nhead(pred_tree); head(stats::ftable(pred_tree))",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(knitr)\nitem_table = data.frame(predict_T = c(\"TP\",\"FP\"),predict_F = c(\"TN\",\"FN\"))\nrownames(item_table) = c(\"TRUE\",\"FALSE\")\nkable(item_table,caption = \"cross table\")",
      "line_count": 4
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "hn_test_pred <- predict(hn_classifier, hn_test)\ntable(hn_test_pred, hn_med_test$stage)",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(gmodels)\nCrossTable(hn_test_pred, hn_med_test$stage)",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "accuracy <- (73+0)/100\naccuracy\nerror_rate <- (23+4)/100\nerror_rate\n1-accuracy",
      "line_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(caret)\nqol_pred <- predict(qol_model, qol_test)\nconfusionMatrix(table(qol_pred, qol_test$cd), positive=\"severe_disease\")",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "table(qol_pred, qol_test$cd)",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "A=143; B=71; C=72; D=157\n# A+B+ C+D # 443\n# ((A+B)*(A+C)+(C+D)*(B+D))/(A+B+C+D)   # 221.7201\nEA=((A+B)*(A+C)+(C+D)*(B+D))/(A+B+C+D) # Expected accuracy\nOA=A+D; OA    # Observed accuracy\nk=(OA-EA)/(A+B+C+D - EA); k # 0.3537597\n\n# Compare against the official kappa score\nconfusionMatrix(table(qol_pred, qol_test$cd), positive=\"severe_disease\")$overall[2] # report official Kappa",
      "line_count": 9
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "round(confusionMatrix(table(qol_pred, qol_test$cd), positive=\"severe_disease\")$overall[2], 2) # report official Kappa",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# install.packages(vcd)\nlibrary(vcd)\nKappa(table(qol_pred, qol_test$cd))",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "Kappa(table(qol_pred, qol_test$cd),weights = matrix(c(1,10,1,10),nrow=2))",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "table(qol_pred, qol_test$cd)",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "sens <- 131/(131+89)\nsens\nspec <- 149/(149+74)\nspec",
      "line_count": 4
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(caret)\nsensitivity(qol_pred, qol_test$cd, positive=\"severe_disease\")\n# specificity(qol_pred, qol_test$cd)\nconfusionMatrix(table(qol_pred, qol_test$cd), positive=\"severe_disease\")$byClass[1] # another way to report the sensitivity\n# confusionMatrix(table(qol_pred, qol_test$cd), positive=\"severe_disease\")$byClass[2] # another way to report the specificity",
      "line_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "prec   <- 157/(157+72); prec\nrecall <- 157/(157+71); recall\n\nlibrary (ROCR)\nlibrary(plotly)\nqol_pred<-predict(qol_model, qol_test)\nqol_pred <- predict(qol_model, qol_test, type = 'prob')\n\npred <- prediction( qol_pred[,2], qol_test$cd)\nPrecRec <- performance(pred, \"prec\", \"rec\")\nPrecRecAUC <- performance(pred, \"auc\")\npaste0(\"AUC=\", round(as.numeric(PrecRecAUC@y.values), 2))\n\n# plot(PrecRec)\nplot_ly(x = ~PrecRec@x.values[[1]][2:length(PrecRec@x.values[[1]])], \n        y = ~PrecRec@y.values[[1]][2:length(PrecRec@y.values[[1]])], \n        name = 'Recall-Precision relation', type='scatter', mode='markers+lines') %>%\n  layout(title=paste0(\"Precision-Recall Plot, AUC=\",\n                      round(as.numeric(PrecRecAUC@y.values[[1]]), 2)),\n         xaxis=list(title=\"Recall\"), yaxis=list(title=\"Precision\"))\n\n# PrecRecAUC <- performance(pred, \"auc\")\n# paste0(\"AUC=\", round(as.numeric(PrecRecAUC@y.values), 2))",
      "line_count": 23
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "qol_pred <- predict(qol_model, qol_test)\nposPredValue(qol_pred, qol_test$cd, positive=\"severe_disease\")",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "error1<-1-prec; error1\nerror2<-1-recall; error2",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "f1 <- (2*prec*recall)/(prec+recall)\nf1",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "precision <- posPredValue(qol_pred, qol_test$cd, positive=\"severe_disease\")\nrecall <- sensitivity(qol_pred, qol_test$cd, positive=\"severe_disease\")\nF1 <- (2 * precision * recall) / (precision + recall); F1",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# install.packages(\"ROCR\")\nlibrary(ROCR)\npred <- ROCR::prediction(predictions=pred_prob[, 2], labels=qol_test$cd) \n# avoid naming collision (ROCR::prediction), as\n# there is another prediction function in the neuralnet package.",
      "line_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# curve(log(x), from=0, to=100, xlab=\"False Positive Rate\", ylab=\"True Positive Rate\", main=\"ROC curve\", col=\"green\", lwd=3, axes=F)\n# Axis(side=1, at=c(0, 20, 40, 60, 80, 100), labels = c(\"0%\", \"20%\", \"40%\", \"60%\", \"80%\", \"100%\"))\n# Axis(side=2, at=0:5, labels = c(\"0%\", \"20%\", \"40%\", \"60%\", \"80%\", \"100%\"))\n# segments(0, 0, 110, 5, lty=2, lwd=3)\n# segments(0, 0, 0, 4.7, lty=2, lwd=3, col=\"blue\")\n# segments(0, 4.7, 107, 4.7, lty=2, lwd=3, col=\"blue\")\n# text(20, 4, col=\"blue\", labels = \"Perfect Classifier\")\n# text(40, 3, col=\"green\", labels = \"Test Classifier\")\n# text(70, 2, col=\"black\", labels= \"Classifier with no predictive value\")\n\nx <- seq(from=0, to=1.0, by=0.01) + 0.001\nplot_ly(x = ~x, y = (log(100*x)+2.3)/(log(100*x[101])+2.3), line=list(color=\"lightgreen\"),\n        name = 'Test Classifier', type='scatter', mode='lines', showlegend=T) %>%\n  add_lines(x=c(0,1), y=c(0,1), line=list(color=\"black\", dash='dash'),\n               name=\"Classifier with no predictive value\") %>%\n  add_segments(x=0, xend=0, y=0, yend = 1, line=list(color=\"blue\"), \n               name=\"Perfect Classifier\") %>%\n  add_segments(x=0, xend=1, y=1, yend = 1, line=list(color=\"blue\"), \n               name=\"Perfect Classifier 2\", showlegend=F) %>%\n  layout(title=\"ROC curve\", legend = list(orientation = 'h'),\n         xaxis=list(title=\"False Positive Rate\", scaleanchor=\"y\", range=c(0,1)), \n         yaxis=list(title=\"True Positive Rate\", scaleanchor=\"x\"))",
      "line_count": 22
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "roc <- performance(pred, measure=\"tpr\", x.measure=\"fpr\")",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# plot(roc, main=\"ROC curve for Quality of Life Model\", col=\"blue\", lwd=3)\n# segments(0, 0, 1, 1, lty=2)\n\nplot_ly(x = ~roc@x.values[[1]], y = ~roc@y.values[[1]], \n        name = 'ROC Curve', type='scatter', mode='markers+lines') %>%\n  add_lines(x=c(0,1), y=c(0,1), line=list(color=\"black\", dash='dash'),\n               name=\"Classifier with no predictive value\") %>%\n  layout(title=\"ROC Curve for Quality of Life C5.0 classification Tree Model\", \n         legend = list(orientation = 'h'),\n         xaxis=list(title=\"False Positive Rate\", scaleanchor=\"y\", range=c(0,1)), \n         yaxis=list(title=\"True Positive Rate\", scaleanchor=\"x\"),\n         annotations = list(text=paste0(\"AUC=\",\n                      round(as.numeric(performance(pred, \"auc\")@y.values[[1]]), 2)),  \n                            x=0.6, y=0.4, textangle=0,\n                            font=list(size=15, color=\"blue\", showarrow=FALSE)))",
      "line_count": 15
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "roc_auc <- performance(pred, measure=\"auc\")",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "str(roc_auc)",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "roc_auc@y.values",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "sub <- sample(nrow(google_norm), floor(nrow(google_norm)*0.75))\ngoogle_train <- google_norm[sub, ]\ngoogle_test <- google_norm[-sub, ]",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "sub <- caret::createDataPartition(google_norm$RealEstate, p=0.75, list = F)\ngoogle_train <- google_norm[sub, ]\ngoogle_test <- google_norm[-sub, ]",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "google <- read.csv(\"https://umich.instructure.com/files/416274/download?download_frd=1\", \n                   stringsAsFactors = F)\ngoogle <- google[, -c(1, 2)]\nnormalize <- function(x) {\n   return((x - min(x)) / (max(x) - min(x)))\n}\ngoogle_norm <- as.data.frame(lapply(google, normalize))",
      "line_count": 7
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "sub <- sample(nrow(google_norm), floor(nrow(google_norm)*0.50))\ngoogle_train <- google_norm[sub, ]\ngoogle_test <- google_norm[-sub, ]\nsub1 <- sample(nrow(google_test), floor(nrow(google_test)*0.5))\ngoogle_test1 <- google_test[sub1, ]\ngoogle_test2 <- google_test[-sub1, ]\nnrow(google_norm)\nnrow(google_train) # training\nnrow(google_test1) # testing: internal cross validation\nnrow(google_test2) # testing: out of bag validation",
      "line_count": 10
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(\"caret\")\nset.seed(1234)\nfolds <- createFolds(google_norm$RealEstate, k=10)\nstr(folds)",
      "line_count": 4
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# install.packages(\"sparsediscrim\")\nlibrary(sparsediscrim)\nfolds2 = cv_partition(1:nrow(google_norm), num_folds=10)",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "str(folds2)",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(neuralnet)\nfold_cv <- lapply(folds, function(x){\n  google_train <- google_norm[-x, ]\n  google_test <- google_norm[x, ]\n  google_model <- neuralnet(RealEstate~Unemployment+Rental+Mortgage+Jobs+Investing+DJI_Index+StdDJI,\n                            data=google_train)\n  google_pred <- compute(google_model, google_test[, c(1:2, 4:8)])\n  pred_results <- google_pred$net.result\n  pred_cor <- cor(google_test$RealEstate, pred_results)\n  return(pred_cor)\n})\nstr(fold_cv)",
      "line_count": 12
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "mean(unlist(fold_cv))",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# define the total data size\nn <- 500\n# define number of Bootstrap iterations\nN=2000\n#define the resampling function and compute the proportion of uniquely selected elements\nuniqueProportions <- function(myDataSet, sampleSize){\n    indices <- sample(1:sampleSize, sampleSize, replace=TRUE) # sample With Replacement\n    length(unique(indices))/sampleSize\n}\n# compute the N proportions of unique elements (could also use a for loop)\nproportionsVector <- c(lapply(1:N, uniqueProportions, sampleSize=n), recursive=TRUE)\n# report the Expected (mean) proportion of unique elements in the bootstrapping samples of n\nmean(proportionsVector)",
      "line_count": 13
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# Recall: qol_model <- C5.0(qol_train[,-c(40, 41)], qol_train$cd)\n# predict labels of testing data\nqol_pred <- predict(qol_model, qol_test)\n# compute matches and mismatches of Predicted and Observed class labels\npredObsEqual <- qol_pred == qol_test$cd\npredObsTF <- c(table(predObsEqual)[1], table(predObsEqual)[2]); predObsTF\n\n# training error rate\ntrain.err <- as.numeric(predObsTF[1]/(predObsTF[1]+predObsTF[2]))\n\n# testing error rate, leave-one-out Bootstrap (LOOB) Cross-Validation\nB <- 10 \nloob.err <- NULL\nN <- dim(qol_test)[1]   # size of test-dataset\nfor (b in 1:B) {\n   bootIndices <- sample(1:N, N*0.9, replace=T)\n   train <- qol_test[bootIndices, ]\n   qol_modelBS <- C5.0(train[,-c(40, 41)], train$cd)\n   inner.err <- NULL\n\n   # for current iteration extract the appropriate testing cases for testing\n   i <- (1:length(bootIndices))\n   i <- i[is.na(match(i, bootIndices))]\n   test <- qol_test[i, ]\n  \n   # predict using model at current iteration\n   qol_modelBS_pred <- predict(qol_modelBS, test)\n  \n   predObsEqual <- qol_modelBS_pred == test$cd\n   predObsTF <- c(table(predObsEqual)[1], table(predObsEqual)[2]); predObsTF\n   # training error rate\n   inner.err <- as.numeric(predObsTF[1]/(predObsTF[1]+predObsTF[2]))\n   loob.err <- c(loob.err, mean(inner.err))\n}\n\ntest.err <- ifelse(is.null(loob.err), NA, mean(loob.err))\n\n# 0.632 Bootstrap error\nboot.632 <- 0.368 * train.err + 0.632 * test.err; boot.632",
      "line_count": 39
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(randomForest)\nboystown <- read.csv(\"https://umich.instructure.com/files/399119/download?download_frd=1\", sep=\" \")\nboystown$sex <- boystown$sex-1\nboystown$dadjob <- -1*(boystown$dadjob-2)\nboystown$momjob <- -1*(boystown$momjob-2)\nboystown <- boystown[, -1]\ntable(boystown$gpa)\nboystown$grade <- boystown$gpa %in% c(3, 4, 5)\nboystown$grade <- factor(boystown$grade, levels=c(F, T), labels = c(\"above_avg\", \"avg_or_below\"))\nnormalize <- function(x){\n  return((x-min(x))/(max(x)-min(x)))\n}\nboystown_n <- as.data.frame(lapply(boystown[, -11], normalize))",
      "line_count": 13
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "str(boystown_n)\nboystown_n <- cbind(boystown_n, boystown[, 11])\nstr(boystown_n)\ncolnames(boystown_n)[11] <- \"grade\"",
      "line_count": 4
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(caret)\nset.seed(123)\nkNN_mod <- train(grade~., data=boystown_n, method=\"knn\")\nkNN_mod; summary(kNN_mod)",
      "line_count": 4
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "set.seed(1234)\npred <- predict(kNN_mod, boystown_n)\ntable(pred, boystown_n$grade)",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "head(predict(kNN_mod, boystown_n, type = \"prob\"))",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "ctrl <- trainControl(method = \"boot632\", number=25, selectionFunction = \"oneSE\")\ngrid <- expand.grid(k=c(1, 3, 5, 7, 9)) \n# Creates a data frame from all combinations of the supplied factors",
      "line_count": 3
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "set.seed(123)\nkNN_mod2 <-train(grade ~ ., data=boystown_n, method=\"knn\", \n         metric=\"Kappa\", \n         trControl=ctrl, \n         tuneGrid=grid)\nkNN_mod2",
      "line_count": 6
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# install.packages(fastAdaboost)\nlibrary(fastAdaboost)\nlibrary(caret)    # for modeling\nlibrary(lattice)  # for plotting\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\n\n## Run all subsequent models in parallel\nlibrary(adabag)\nlibrary(doParallel)\ncl <- makePSOCKcluster(5)\nregisterDoParallel(cl)\nsystem.time({\n  rf.fit        <- train(cd~., data=qol[, -37], method=\"rf\", trControl=control);\n  knn.fit       <- train(cd~., data=qol[, -37], method=\"knn\", trControl=control);\n  svm.fit       <- train(cd~., data=qol[, -37], method=\"svmRadialWeights\", trControl=control);\n  adabag.fit    <- train(cd~., data=qol[, -37], method=\"AdaBag\", trControl=control);\n  adaboost.fit  <- train(cd~., data=qol[, -37], method=\"adaboost\", trControl=control)\n})\nstopCluster(cl) # close multi-core cluster\nrm(cl)\n\nresults <- resamples(list(RF=rf.fit, kNN=knn.fit, SVM=svm.fit, Bag=adabag.fit, Boost=adaboost.fit))\n\n# summary of model differences\nsummary(results)\n\n# Plot Accuracy Summaries\n# scales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\n# bwplot(results, scales=scales)                 # Box plots of accuracy\n\n# Convert (results) data-frame from wide to long format\n# The arguments to gather():\n# - data: Data object\n# - key: Name of new key column (made from names of data columns)\n# - value: Name of new value column\n# - ...: Names of source columns that contain values\n# - factor_key: Treat the new key column as a factor (instead of character vector)\nlibrary(tidyr)\nresults_long <- gather(results$values[, -1], method, measurement, factor_key=TRUE) %>%\n  separate(method, c(\"Technique\", \"Metric\"), sep = \"~\")\n\n# Compare original wide format to transformed long format\nresults$values[, -1]\nhead(results_long)\n\nlibrary(plotly)\nplot_ly(results_long, x=~Technique, y = ~measurement, color = ~Metric, type = \"box\")\n\n#densityplot(results, scales=scales, pch = \"|\") # Density plots of accuracy\n\ndensityModels <- with(results_long[which(results_long$Metric=='Accuracy'), ],\n                      tapply(measurement, INDEX = Technique, density))\ndf <- data.frame(\n  x = unlist(lapply(densityModels, \"[[\", \"x\")),\n  y = unlist(lapply(densityModels, \"[[\", \"y\")),\n  method = rep(names(densityModels), each = length(densityModels[[1]]$x))\n)\n\nplot_ly(df, x = ~x, y = ~y, color = ~method) %>% add_lines() %>%\n  layout(title=\"Performance Density Plots (Accuracy)\", legend = list(orientation='h'),\n         xaxis=list(title=\"Accuracy\"), yaxis=list(title=\"Density\"))\n\ndensityModels <- with(results_long[which(results_long$Metric=='Kappa'), ],\n                      tapply(measurement, INDEX = Technique, density))\ndf <- data.frame(\n  x = unlist(lapply(densityModels, \"[[\", \"x\")),\n  y = unlist(lapply(densityModels, \"[[\", \"y\")),\n  method = rep(names(densityModels), each = length(densityModels[[1]]$x))\n)\n\nplot_ly(df, x = ~x, y = ~y, color = ~method) %>% add_lines() %>%\n  layout(title=\"Performance Density Plots (Kappa)\", legend = list(orientation='h'),\n         xaxis=list(title=\"Kappa\"), yaxis=list(title=\"Density\"))\n\n# dotplot(results, scales=scales)                # Dot plots of Accuracy & Kappa\n#splom(results)      # contrast pair-wise model scatterplots of prediction accuracy (Trellis Scatterplot matrices)\n\n# Pairs - Accuracy\nresults_wide <- results_long[which(results_long$Metric=='Accuracy'), -2] %>%\n  pivot_wider(names_from = Technique, values_from = measurement)\n\ndf = data.frame(cbind(RF=results_wide$RF[[1]], kNN=results_wide$kNN[[1]], SVM=results_wide$SVM[[1]], Bag=results_wide$Bag[[1]], Boost=results_wide$Boost[[1]]))\n\ndims <- dplyr::select_if(df, is.numeric)\ndims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))\nplot_ly(type = \"splom\", dimensions = setNames(dims, NULL), \n        showupperhalf = FALSE, diagonal = list(visible = FALSE)) %>%\n  layout(title=\"Performance Pairs Plot (Accuracy)\")\n\n# Pairs - Accuracy\nresults_wide <- results_long[which(results_long$Metric=='Kappa'), -2] %>%\n  pivot_wider(names_from = Technique, values_from = measurement)\n\ndf = data.frame(cbind(RF=results_wide$RF[[1]], kNN=results_wide$kNN[[1]], SVM=results_wide$SVM[[1]], Bag=results_wide$Bag[[1]], Boost=results_wide$Boost[[1]]))\n\ndims <- dplyr::select_if(df, is.numeric)\ndims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))\nplot_ly(type = \"splom\", dimensions = setNames(dims, NULL), \n        showupperhalf = FALSE, diagonal = list(visible = FALSE)) %>%\n  layout(title=\"Performance Pairs Plot (Kappa)\")",
      "line_count": 100
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# update packages\n# update.packages()",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "ppmi_data <-read.csv(\"https://umich.instructure.com/files/330400/download?download_frd=1\", header=TRUE)",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# binarize the Dx classes\nppmi_data$ResearchGroup <- ifelse(ppmi_data$ResearchGroup == \"Control\", \"Control\", \"Patient\")\nattach(ppmi_data)\n\nhead(ppmi_data)\n# View (ppmi_data)",
      "line_count": 6
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# Model-free analysis, classification\n# install.packages(\"crossval\")\n# install.packages(\"ada\")\n# library(\"crossval\")\nlibrary(crossval)\nlibrary(ada)\n# set up adaboosting prediction function\n\n# Define a new AdaBoost classification result-reporting function\nmy.ada <- function (train.x, train.y, test.x, test.y, negative, formula){\n  ada.fit <- ada(train.x, train.y)\n  predict.y <- predict(ada.fit, test.x)\n  #count TP, FP, TN, FN, Accuracy, etc.\n  out <- confusionMatrix(test.y, predict.y, negative = negative)\n # negative\t is the label of a negative \"null\" sample (default: \"control\").\n  return (out)\n}",
      "line_count": 17
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# balance cases\n# SMOTE: Synthetic Minority Oversampling Technique to handle class imbalance in binary classification.\nset.seed(1000)\n# install.packages(\"unbalanced\") to deal with unbalanced group data\n# https://cran.r-project.org/src/contrib/Archive/unbalanced/\nlibrary(unbalanced)\nppmi_data$PD <- ifelse(ppmi_data$ResearchGroup==\"Control\", 1, 0) \nuniqueID <- unique(ppmi_data$FID_IID) \nppmi_data <- ppmi_data[ppmi_data$VisitID==1, ]\nppmi_data$PD <- factor(ppmi_data$PD)\n\ncolnames(ppmi_data)\n# ppmi_data.1<-ppmi_data[, c(3:281, 284, 287, 336:340, 341)]\nn <- ncol(ppmi_data)\noutput.1 <- ppmi_data$PD\n\n# ppmi_data$PD <- ifelse(ppmi_data$ResearchGroup==\"Control\", 1, 0) \n# remove Default Real Clinical subject classifications! \ninput <- ppmi_data[ , -which(names(ppmi_data) %in% c(\"ResearchGroup\", \"PD\", \"X\", \"FID_IID\"))]\n# output <- as.matrix(ppmi_data[ , which(names(ppmi_data) %in% {\"PD\"})])\noutput <- as.factor(ppmi_data$PD)\nc(dim(input), dim(output))\n\n#balance the dataset\nset.seed(123)\ndata.1 <- ubBalance(X= input, Y=output, type=\"ubSMOTE\", percOver=300, percUnder=150, verbose=TRUE)\nbalancedData <- cbind(data.1$X, data.1$Y)\ntable(data.1$Y)\n\nnrow(data.1$X); ncol(data.1$X)\nnrow(balancedData); ncol(balancedData)\nnrow(input); ncol(input)\n\ncolnames(balancedData) <- c(colnames(input), \"PD\")",
      "line_count": 34
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(plotly)\n\n###Check balance\n## T test\nalpha.0.05 <- 0.05\ntest.results.bin <- NULL\t\t# binarized/dichotomized p-values\ntest.results.raw <- NULL\t\t# raw p-values\n\n# get a better error-handling t.test function that gracefully handles NA's and trivial variances\nmy.t.test.p.value <- function(input1, input2) {\nobj <- try(t.test(input1, input2), silent=TRUE)\nif (is(obj, \"try-error\")) \n  return(NA) \nelse \n  return(obj$p.value)\n} \n\nfor (i in 1:ncol(balancedData)) \n{\t\ntest.results.raw[i]  <- my.t.test.p.value(input[, i], balancedData [, i])\n  \ttest.results.bin[i] <- ifelse(test.results.raw[i]  > alpha.0.05, 1, 0)   \n# binarize the p-value (0=significant, 1=otherwise) \n\tprint(c(\"i=\", i, \"var=\", colnames(balancedData[i]), \"t-test_raw_p_value=\", test.results.raw[i]))\n}\n\n# we can also employ (e.g., FDR, Bonferroni) correction for multiple testing!\n\t# test.results.corr <- stats::p.adjust(test.results.raw, method = \"fdr\", n = length(test.results.raw)) \n\t# where methods are \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", \"fdr\", \"none\")\n\t# plot(test.results.raw, test.results.corr)\n\t# sum(test.results.raw < alpha.0.05, na.rm=T)/length(test.results.raw)  #check proportion of inconsistencies\n# sum(test.results.corr < alpha.0.05, na.rm =T)/length(test.results.corr)\n\nQQ <- qqplot(input[, 5], balancedData [, 5], plot=F) # check visually for differences between the distributions of the raw (input) and rebalanced data (for only one variable, in this case)\nplot_ly(x=~QQ$x, y=~QQ$y, type=\"scatter\", mode=\"markers\", name=\"Data\") %>%\n  add_trace(x=c(0,1), y=c(0,1), name=\"Ideal Distribution Match\", type=\"scatter\", mode=\"lines\") %>%\n  layout(title=paste0(\"QQ-Plot Original vs. Rebalanced Data (Feature=\",colnames(input)[5], \")\"),\n         xaxis=list(title=\"Original Data\"), yaxis=list(title=\"Rebalanced Data\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))\n\n# Now, check visually for differences between the distributions of the raw (input) and rebalanced data.\n# par(mar=c(1,1,1,1))\n# par(mfrow=c(10,10))\n# for(i in c(1:62,64:101)){ qqplot(balancedData [, i],input[, i]) } #except VisitID\n\n# as the sample-size is changed:\nlength(input[, 5]); length(balancedData [, 5])\n# to plot raw vs. rebalanced pairs (e.g., var=\"L_insular_cortex_Volume\"), we need to equalize the lengths\n#plot (input[, 5] +0*balancedData [, 5], balancedData [, 5])   # [, 5] == \"L_insular_cortex_Volume\"\n\n# print(c(\"T-test results: \", test.results))\n# zeros (0) are significant independent between-group T-test differences, ones (1) are insignificant\n\nfor (i in 1:(ncol(balancedData)-1)) \n{\n\ttest.results.raw [i]  <- wilcox.test(input[, i], balancedData [, i])$p.value\n  \ttest.results.bin [i] <- ifelse(test.results.raw [i] > alpha.0.05, 1, 0)\n\tprint(c(\"i=\", i, \"Wilcoxon-test=\", test.results.raw [i]))\n}\nprint(c(\"Wilcoxon test results: \", test.results.bin))\n# test.results.corr <- stats::p.adjust(test.results.raw, method = \"fdr\", n = length(test.results.raw)) \n# where methods are \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", \"fdr\", \"none\")\n# plot(test.results.raw, test.results.corr)",
      "line_count": 62
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "QQ <- qqplot(input[, 5], balancedData [, 5], plot=F) # check visually for differences between the distributions of the raw (input) and rebalanced data (for only one variable, in this case)\nplot_ly(x=~QQ$x, y=~QQ$y, type=\"scatter\", mode=\"markers\", name=\"Data\") %>%\n  add_trace(x=c(0,1), y=c(0,1), name=\"Ideal Distribution Match\", type=\"scatter\", mode=\"lines\") %>%\n  layout(title=paste0(\"QQ-Plot Original vs. Rebalanced Data (Feature=\",colnames(input)[5], \")\"),\n         xaxis=list(title=\"Original Data\"), yaxis=list(title=\"Rebalanced Data\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 6
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# using raw data:\nX <- as.data.frame(input); Y <- output\nneg <- \"1\"   # \"Control\" == \"1\"\n\n# using Rebalanced data: \nX <- as.data.frame(data.1$X); Y <- data.1$Y\n# balancedData<-cbind(data.1$X, data.1$Y); dim(balancedData)\n\n# Side note: There is a function name collision for \"crossval\", the same method is present in the \"mlr\" (machine Learning in R) package and in the \"crossval\" package.  \n# To specify a function call from a specific package do:  packagename::functionname()\n\nset.seed(115) \ncv.out <- crossval::crossval(my.ada, X, Y, K = 5, B = 1, negative = neg)\n\t# the label of a negative \"null\" sample (default: \"control\")\nout <- diagnosticErrors(cv.out$stat) ",
      "line_count": 15
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "print(cv.out$stat)\nprint(out)",
      "line_count": 2
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "data(sleep); str(sleep)\nX = as.matrix(sleep[, 1, drop=FALSE]) # increase in hours of sleep, \n\t# drop is logical, if TRUE the result is coerced to the lowest possible dimension. \n# The default is to drop if only one column is left, but not to drop if only one row is left.\nY = sleep[, 2] # drug given \n# plot(X ~ Y)\n\nplot_ly(data=sleep, x=~group, y=~extra, color=~group, type=\"box\", name=\"Hours of Sleep\") %>%\n  layout(title=\"Hours of Extra Sleep\", legend = list(orientation='h'))\n\nlevels(Y) # \"1\" \"2\"\ndim(X) # 20  1",
      "line_count": 12
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(\"MASS\") # for lda function \n\npredfun.lda = function(train.x, train.y, test.x, test.y, negative)\n{\tlda.fit = lda(train.x, grouping=train.y)\n  \tynew = predict(lda.fit, test.x)$class\n# count TP, FP etc.\n  \tout = confusionMatrix(test.y, ynew, negative=negative)\nreturn( out )\n}",
      "line_count": 9
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# install.packages(\"crossval\")\nlibrary(\"crossval\")\n\nset.seed(123456)\ncv.out <- crossval::crossval(predfun.lda, X, Y, K=5, B=20, negative=\"1\", verbose=FALSE)\ncv.out$stat\ndiagnosticErrors(cv.out$stat)",
      "line_count": 7
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# ?attitude, colnames(attitude)\n# Note: when using a data frame, a time-saver is to use \".\" to indicate \"include all covariates\" in the DF.  \n# E.g., fit <- lm(Y ~ ., data = D)\n\ndata(\"attitude\")\ny = attitude[, 1] \t\t# rating variable\nx = attitude[, -1] \t# date frame with the remaining variables\nis.factor(y)\nsummary( lm(y ~ . , data=x) ) \t# R-squared:  0.7326\n# set up lm prediction function",
      "line_count": 10
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "predfun.lm = function(train.x, train.y, test.x, test.y) {     \n  lm.fit = lm(train.y ~ . , data=train.x)\n  ynew = predict(lm.fit, test.x ) \n  # compute squared error risk (MSE)\n  out = mean( (ynew - test.y)^2)\n  # note that, in general, when fitting linear model to continuous outcome variable (Y), \n  # we can't use the out<-confusionMatrix(test.y, ynew, negative=negative), as it requires a binary outcome\n  # this is why we use the MSE as an estimate of the discrepancy between observed & predicted values\n  return(out)\n}\n\n# library(\"MASS\")\n# predfun.lda = function(train.x, train.y, test.x, test.y, negative) {\t\n#  lda.fit = lda(train.x, grouping=train.y)\n#  \tynew = predict(lda.fit, test.x)$class\n#   # count TP, FP etc.\n#  \tout = confusionMatrix(test.y, ynew, negative=negative)\n#   return( out )\n# }\n\n# prediction MSE using all variables\nset.seed(123456)\ncv.out.lm = crossval::crossval(predfun.lm, x, y, K=5, B=20, verbose=FALSE)\nc(cv.out.lm$stat, cv.out.lm$stat.se) \t\t\t# 72.581198  3.736784\n# reducing to using only two variables\ncv.out.lm = crossval::crossval(predfun.lm, x[, c(1, 3)], y, K=5, B=20, verbose=FALSE)\nc(cv.out.lm$stat, cv.out.lm$stat.se)\t\t\n# 52.563957  2.015109 ",
      "line_count": 28
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# ppmi_data <-read.csv(\"https://umich.instructure.com/files/330400/download?download_frd=1\", header=TRUE)\n# ppmi_data$ResearchGroup <- ifelse(ppmi_data$ResearchGroup == \"Control\", \"Control\", \"Patient\")\n# attach(ppmi_data); head(ppmi_data)\n# install.packages(\"crossval\")\n# library(\"crossval\")\n# ppmi_data$PD <- ifelse(ppmi_data$ResearchGroup==\"Control\", 1, 0) \n# input <- ppmi_data[ , -which(names(ppmi_data) %in% c(\"ResearchGroup\", \"PD\", \"X\", \"FID_IID\"))]\n# output <- as.factor(ppmi_data$PD)\n\n# remove the irrelevant variables (e.g., visit ID)\noutput <- as.factor(ppmi_data$PD)\ninput <- ppmi_data[, -which(names(ppmi_data) %in% c(\"ResearchGroup\", \"PD\", \"X\", \"FID_IID\", \"VisitID\"))]\nX = as.matrix(input)\t# Predictor variables\nY = as.matrix(output) \t#  Actual PD clinical assessment \ndim(X); \ndim(Y)",
      "line_count": 16
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "fit <- lm(Y~X); \n\n# layout(matrix(c(1, 2, 3, 4), 2, 2)) \t\t# optional 4 graphs/page\n# plot(fit)\t\t\t# plot the fit \n\nxResid <- scale(fit$residuals)\nQQ <- qqplot(xResid, rnorm(1000), plot=F) # check visually for differences between the distributions of the raw (input) and rebalanced data (for only one variable, in this case)\nplot_ly(x=~QQ$x, y=~QQ$y, type=\"scatter\", mode=\"markers\", name=\"Data\") %>%\n  add_trace(x=c(-4,4), y=c(-4,4), name=\"Ideal Distribution Match\", type=\"scatter\", mode=\"lines\") %>%\n  layout(title=\"QQ Normal Plot of Model Residuals\",\n         xaxis=list(title=\"Model Residuals\"), yaxis=list(title=\"Normal Residuals\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))\n\nlevels(as.factor(Y)) \t\t\t# \"0\" \"1\"\nc(dim(X), dim(Y))\t\t\t# 1043  103",
      "line_count": 15
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "set.seed(12345)\n# cv.out.lm = crossval::crossval(predfun.lm, as.data.frame(X), as.numeric(Y), K=5, B=20)\n\ncv.out.lda = crossval::crossval(predfun.lda, X, Y, K=5, B=20, negative=\"1\", verbose=FALSE)\n# K=Number of folds; B=Number of repetitions.",
      "line_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# Results\n#cv.out.lda$stat; \n#cv.out.lda; \ndiagnosticErrors(cv.out.lda$stat)\n#cv.out.lm$stat; \n#cv.out.lm; \n#diagnosticErrors(cv.out.lm$stat)",
      "line_count": 7
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# ppmi_data <-read.csv(\"https://umich.instructure.com/files/330400/download?download_frd=1\", header=TRUE)\n# ppmi_data$ResearchGroup <- ifelse(ppmi_data$ResearchGroup == \"Control\", \"Control\", \"Patient\")\n# install.packages(\"crossval\"); library(\"crossval\")\n# ppmi_data$PD <- ifelse(ppmi_data$ResearchGroup==\"Control\", 1, 0) \n\n# remove the irrelevant variables (e.g., visit ID)\noutput <- as.factor(ppmi_data$PD)\ninput <- ppmi_data[, -which(names(ppmi_data) %in% c(\"ResearchGroup\", \"PD\", \"X\", \"FID_IID\", \"VisitID\"))]\nX = as.matrix(input)\t# Predictor variables\nY = as.matrix(output) \t",
      "line_count": 10
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "lm.logit <- glm(as.numeric(Y) ~ ., data = as.data.frame(X), family = \"binomial\")\nynew <- predict(lm.logit, as.data.frame(X)); #plot(ynew)\nynew2 <- ifelse(exp(ynew)<0.5, 0, 1); # plot(ynew2)\n\npredfun.logit = function(train.x, train.y, test.x, test.y, neg)\n{ \tlm.logit <- glm(train.y ~ ., data = train.x, family = \"binomial\")\n  \tynew = predict(lm.logit, test.x )\n # compute TP, FP, TN, FN\nynew2 <- ifelse(exp(ynew)<0.5, 0, 1)\n  \tout = confusionMatrix(test.y, ynew2, negative=neg)\t# Binary outcome, we can use confusionMatrix\nreturn( out )\n}\n# Reduce the bag of explanatory variables, purely to simplify the interpretation of the analytics in this example!\ninput.short <- input[, which(names(input) %in% c(\"R_fusiform_gyrus_Volume\",                                                            \n \t\t\t\"R_fusiform_gyrus_ShapeIndex\", \"R_fusiform_gyrus_Curvedness\", \n \"Sex\",  \"Weight\", \"Age\" , \"chr12_rs34637584_GT\", \"chr17_rs11868035_GT\",                                                              \n \t\t\t\"UPDRS_Part_I_Summary_Score_Baseline\", \"UPDRS_Part_I_Summary_Score_Month_03\",                                             \n\"UPDRS_Part_II_Patient_Questionnaire_Summary_Score_Baseline\",                        \n \t\t\t\"UPDRS_Part_III_Summary_Score_Baseline\",                                             \n \t\t\t\"X_Assessment_Non.Motor_Epworth_Sleepiness_Scale_Summary_Score_Baseline\"     \n))]\nX = as.matrix(input.short)\n\ncv.out.logit = crossval::crossval(predfun.logit, as.data.frame(X), as.numeric(Y), K=5, B=2, neg=\"1\", verbose=FALSE)\ncv.out.logit$stat.cv\ndiagnosticErrors(cv.out.logit$stat)",
      "line_count": 26
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "predfun.qda = function(train.x, train.y, test.x, test.y, negative) {\n  library(\"MASS\") # for lda function\n  qda.fit = qda(train.x, grouping=train.y)\n  ynew = predict(qda.fit, test.x)$class\n  out.qda = confusionMatrix(test.y, ynew, negative=negative)\n  return( out.qda )\n}  \n  \ncv.out.qda = crossval::crossval(predfun.qda, as.data.frame(input.short), as.factor(Y), K=5, B=20, neg=\"1\")\ndiagnosticErrors(cv.out.lda$stat); diagnosticErrors(cv.out.qda$stat);",
      "line_count": 10
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "input.short2 <- input[, which(names(input) %in% c(\"R_fusiform_gyrus_Volume\", \"Sex\",\n                       \"Weight\", \"Age\" , \"chr17_rs11868035_GT\", \"UPDRS_Part_I_Summary_Score_Baseline\",\n                       \"UPDRS_Part_II_Patient_Questionnaire_Summary_Score_Baseline\",\n                       \"UPDRS_Part_III_Summary_Score_Baseline\", \n                       \"X_Assessment_Non.Motor_Epworth_Sleepiness_Scale_Summary_Score_Baseline\"))]\nX = as.matrix(input.short2)\ncv.out.qda = crossval::crossval(predfun.qda, as.data.frame(X), as.numeric(Y), K=5, B=2, neg=\"1\")",
      "line_count": 7
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "diagnosticErrors(cv.out.qda$stat); diagnosticErrors(cv.out.logit$stat)",
      "line_count": 1
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "predfun.lda = function(train.x, train.y, test.x, test.y, neg) {\n  library(\"MASS\")\n  lda.fit = lda(train.x, grouping=train.y)\n  ynew = predict(lda.fit, test.x)$class\n  out.lda = confusionMatrix(test.y, ynew, negative=neg)\n  return( out.lda )\n}",
      "line_count": 7
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "predfun.qda = function(train.x, train.y, test.x, test.y, neg) {\n  library(\"MASS\") # for lda function\n  qda.fit = qda(train.x, grouping=train.y)\n  ynew = predict(qda.fit, test.x)$class\n  out.qda = confusionMatrix(test.y, ynew, negative=neg)\n  return( out.qda )\n}",
      "line_count": 7
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# predict nn\nlibrary(\"neuralnet\")\npred = function(nn, dat) {\n  yhat = compute(nn, dat)$net.result\n  yhat = apply(yhat, 1, which.max)-1\n  return(yhat)\n}\n\nmy.neural <- function (train.x, train.y, test.x, test.y,method,layer=c(5,5)) {\n  train.x <- as.data.frame(train.x)\n  train.y <- as.data.frame(train.y)\n  colnames(train.x) <- paste0('V', 1:ncol(X)) \n  colnames(train.y) <- \"V1\"\n  train_y_ind = model.matrix(~factor(train.y$V1)-1)\n  colnames(train_y_ind) = paste0('out', 0:1)\n  train = cbind(train.x, train_y_ind)\n  y_names = paste0('out', 0:1)\n  x_names = paste0('V', 1:ncol(train.x))\n  nn = neuralnet(\n    paste(paste(y_names, collapse='+'),\n          '~',\n          paste(x_names, collapse='+')),\n    train,\n    hidden=layer,\n      linear.output=FALSE,\n    lifesign='full', lifesign.step=1000)\n  #predict\n  predict.y <- pred(nn, test.x)\n  out <- crossval::confusionMatrix(test.y, predict.y,negative = 0)\n  return (out)\n}\n\nset.seed(1234) \ncv.out.nn <- crossval::crossval(my.neural, scale(X), Y, K = 5, B = 1,layer=c(20,20),verbose = F) # scaled predictors are necessary.\ncrossval::diagnosticErrors(cv.out.nn$stat)",
      "line_count": 35
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(\"e1071\")\nmy.svm <- function (train.x, train.y, test.x, test.y,method,cost=1,gamma=1/ncol(dx_norm),coef0=0,degree=3) {\n  svm_l.fit <- svm(x = train.x, y=as.factor(train.y), kernel = method)\n  predict.y <- predict(svm_l.fit, test.x)\n  out <- crossval::confusionMatrix(test.y, predict.y, negative = 0)\n  return (out)\n}\n\n# Linear kernel\nset.seed(123)\ncv.out.svml <- crossval::crossval(my.svm, as.data.frame(X), Y, K = 5, B = 1,\n                                  method = \"linear\",cost=tune_svm$best.parameters$cost,verbose = F) \ndiagnosticErrors(cv.out.svml$stat)\n\n# Gaussian kernel\nset.seed(123)\ncv.out.svmg <- crossval::crossval(my.svm, as.data.frame(X), Y, K = 5, B = 1,\n                                  method = \"radial\",cost=tune_svmg$best.parameters$cost,\n                                  gamma=tune_svmg$best.parameters$gamma,verbose = F)\ndiagnosticErrors(cv.out.svmg$stat)",
      "line_count": 20
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# X = as.matrix(input)\t# Predictor variables X = as.matrix(input.short2)\n#  Y = as.matrix(output) \t# Outcome\n# KNN (k-nearest neighbors)\nlibrary(\"class\")\n# knn.fit.test <- knn(X, X, cl = Y, k=3, prob=F); predict(as.matrix(knn.fit.test), X)$class\n# table(knn.fit.test, Y); confusionMatrix(Y, knn.fit.test, negative=\"1\")\n# This can be used for polytomous variable (multiple classes)\n\npredfun.knn = function(train.x, train.y, test.x, test.y, neg) {\n  library(\"class\")\n  knn.fit = knn(train.x, test.x, cl = train.y, prob=T) \t# knn is already a prediction function!!!\n  # ynew = predict(knn.fit, test.x)$class\t\t\t# no need of another prediction, in this case\n  out.knn = confusionMatrix(test.y, knn.fit, negative=neg)\n  return( out.knn )\n}\t\n# cv.out.knn = crossval::crossval(predfun.knn, X, Y, K=5, B=2, neg=\"1\")\ncv.out.knn = crossval::crossval(predfun.knn, X, Y, K=5, B=2, neg=\"1\")\n#Compare all 3 classifiers (lda, qda, knn, and logit)\ndiagnosticErrors(cv.out.lda$stat); diagnosticErrors(cv.out.qda$stat); diagnosticErrors(cv.out.qda$stat); diagnosticErrors(cv.out.logit$stat); ",
      "line_count": 19
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "\n# TRAINING: 75% of the sample size\nsample_size <- floor(0.75 * nrow(input))\n## set the seed to make your partition reproducible\nset.seed(1234)\ninput.train.ind <- sample(seq_len(nrow(input)), size = sample_size)\ninput.train <- input[input.train.ind, ]\noutput.train <- as.matrix(output)[input.train.ind, ]\n\n# TESTING DATA\ninput.test <- input[-input.train.ind, ]\noutput.test <- as.matrix(output)[-input.train.ind, ]",
      "line_count": 12
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(\"class\")\nknn_model <- knn(train= input.train, input.test, cl=as.factor(output.train), k=2)\n#plot(knn_model)\nsummary(knn_model)\nattributes(knn_model)\n\n# cross-validation\nknn_model.cv <- knn.cv(train= input.train, cl=as.factor(output.train), k=2)\nsummary(knn_model.cv)",
      "line_count": 9
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "kmeans_model <- kmeans(input.train, 2)\ntable (output.train, kmeans_model$cluster-1)\n\nlayout(matrix(1, 1))\n# tiff(\"C:/Users/Dinov/Desktop/test.tiff\", width = 10, height = 10, units = 'in', res = 300)\nfpc::plotcluster(input.train, output.train, col = kmeans_model$cluster)\nlegend(\"topright\", legend=c(\"0(trueLabel)+0(kmeansLabel)\", \"1(trueLabel)+0(kmeansLabel)\", \"0(trueLabel)+1(kmeansLabel)\", \"1(trueLabel)+1(kmeansLabel)\"),\n       col=c(\"red\", \"black\", \"black\", \"red\"), \n       text.col=c(\"red\", \"black\", \"black\", \"red\"), \n       pch = 15 , y.intersp=0.6, x.intersp=0.7,\n       text.width = 5, cex=1)\n\n# plot_ly(x=~input.train, y=~output.train, color = ~as.factor(kmeans_model$cluster), type=\"scatter\", mode=\"marker\")\n\ninput.train <- input.train[ , apply(input.train, 2, var, na.rm=TRUE) != 0]  # remove constant columns\n\ncluster::clusplot(input.train, kmeans_model$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)\n\n# par(mfrow=c(10,10))\n# # the next figure is very large and will not render in RStudio, you may need to save it as a PDF file!\n# # pdf(\"C:/Users/Dinov/Desktop/test.pdf\", width = 50, height = 50)\n# # with(ppmi_data[,1:10], pairs(input.train[,1:10], col=c(1:2)[kmeans_model$cluster])) \n# # dev.off()\n# with(ppmi_data[,1:10], pairs(input.train[,1:10], col=c(1:2)[kmeans_model$cluster])) \n\n# Convert all Income features to factors and construct SPLOM plot\ndf2 <- input.train[,1:10]\ncol_names <- names(df2)\ndims <- as.data.frame(lapply(df2[col_names], factor))\n\ndims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))\nplot_ly(type = \"splom\", dimensions = setNames(dims, NULL), \n        showupperhalf = FALSE, diagonal = list(visible = FALSE))\n\n# plot(input.train, col = kmeans_model$cluster) \n# points(kmeans_model$centers, col = 1:2, pch = 8, cex = 2)\n\n## cluster centers \"fitted\" to each obs.:\nfitted.kmeans <- fitted(kmeans_model);  head(fitted.kmeans)\nresid.kmeans <- (input.train - fitted(kmeans_model))\n\n# define the sum of squares function\nss <- function(data) sum(scale(data, scale = FALSE)^2)\n\n## Equalities \ncbind(kmeans_model[c(\"betweenss\", \"tot.withinss\", \"totss\")], \t# the same two columns\n         \t\tc (ss(fitted.kmeans), ss(resid.kmeans),  ss(input.train)))\n\n# validation\nstopifnot(all.equal(kmeans_model$totss, ss(input.train)), all.equal(kmeans_model$tot.withinss, ss(resid.kmeans)), \n      ## these three are the same:\n\t    all.equal(kmeans_model$betweenss, ss(fitted.kmeans)), \n\t    all.equal(kmeans_model$betweenss, kmeans_model$totss - kmeans_model$tot.withinss), \n      ## and hence also\n\t    all.equal(ss(input.train), ss(fitted.kmeans) + ss(resid.kmeans))\n)\n# kmeans(input.train, 1)$withinss \t\t\n# trivial one-cluster, (its W.SS == ss(input.train))\nclust_kmeans2 = kmeans(scale(X), center=X[1:2,],iter.max=100, algorithm='Lloyd')",
      "line_count": 59
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# k++ initialize\nkpp_init = function(dat, K) {\n  x = as.matrix(dat)\n  n = nrow(x)\n  # Randomly choose a first center\n  centers = matrix(NA, nrow=K, ncol=ncol(x))\n  centers[1,] = as.matrix(x[sample(1:n, 1),])\n  for (k in 2:K) {\n    # Calculate dist^2 to closest center for each point\n    dists = matrix(NA, nrow=n, ncol=k-1)\n    for (j in 1:(k-1)) {\n      temp = sweep(x, 2, centers[j,], '-')\n      dists[,j] = rowSums(temp^2)\n    }\n    dists = rowMeans(dists)\n    # Draw next center with probability proportional to dist^2\n    cumdists = cumsum(dists)\n    prop = runif(1, min=0, max=cumdists[n])\n    centers[k,] = as.matrix(x[min(which(cumdists > prop)),])\n  }\n  return(centers)\n}\nclust_kmeans2_plus = kmeans(scale(X), kpp_init(scale(X), 2), iter.max=100, algorithm='Lloyd')",
      "line_count": 23
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "clust_k2 = clust_kmeans2_plus$cluster\nlibrary(cluster)\n# X = as.matrix(input.short2)\n# as the data is too large for the silhouette plot, we'll just subsample and plot 100 random cases\nsubset_int <- sample(nrow(X),100)  #100 cases from 661 total cases\ndis = dist(as.data.frame(scale(X[subset_int,])))\nsil_k2 = silhouette(clust_k2[subset_int],dis) #best\nplot(sil_k2, col=c(1:length(clust_kmeans2_plus$size)))\nsummary(sil_k2)\nmean(sil_k2<0)",
      "line_count": 10
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "dis = dist(as.data.frame(scale(X)))\nclust_kmeans3_plus = kmeans(scale(X), kpp_init(scale(X), 3), iter.max=100, algorithm='Lloyd')\nsummary(silhouette(clust_kmeans3_plus$cluster,dis))\nclust_kmeans4_plus = kmeans(scale(X), kpp_init(scale(X), 4), iter.max=100, algorithm='Lloyd')\nsummary(silhouette(clust_kmeans4_plus$cluster,dis))",
      "line_count": 5
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "mat = matrix(1,nrow = length(Y))\np = sum(Y==0)/length(Y)\nfor (i in 1:2) {\n  id = which(clust_k2==i)\n  if(sum(Y[id]==0)>length(id)*p)  mat[id] = 0\n}\ncaret::confusionMatrix(factor(Y), factor(mat))\n#table(Y, mat)\n# caret::confusionMatrix(factor(Y, levels=0:1), factor(mat, levels=0:1))",
      "line_count": 9
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(\"ggplot2\")\nmds = as.data.frame(cmdscale(dis, k=2))\nmds_temp = cbind( mds, as.factor(clust_k2))\nnames(mds_temp) = c('V1', 'V2', 'cluster k=2')\n\n# gp_cluster = ggplot(mds_temp, aes(x=V2, y=V1, color=as.factor(clust_k2))) +\n#   geom_point(aes(shape = as.factor(Y))) + theme()\n# gp_cluster\n\nplot_ly(data=mds_temp, x=~V1, y=~V2, color=~mds_temp[,3], symbol=~as.factor(Y), type=\"scatter\", mode=\"markers\",\n        marker=list(size=15)) %>%\n    layout(title = \"MDS Performance\",\n\t\t\t\t\t\t\thovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 13
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# install.packages(\"kernlab\")\nlibrary(\"kernlab\")\n\n# review and choose a dataset (for example the Iris data\ndata()\n#plot(iris)",
      "line_count": 6
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "library(dplyr)\nlibrary(plotly)\nmy_data <- as.data.frame(iris); dim(my_data)\nmy_data <- as.matrix(mutate_all(my_data[, -5], function(x) as.numeric(as.character(x))))\n\nlibrary(kernlab)\nnum_clusters <- 3\ndata_sc <- specc(my_data, centers= num_clusters)\ndata_sc\ncenters(data_sc)\nwithinss(data_sc)\n#plot(my_data, col= data_sc)\n# plot(iris[,1:2], col=data_sc, pch=as.numeric(iris$Species), lwd=2)\n# legend(\"topright\", legend=c(\"setosa\", \"versicolor\", \"virginica\"), col=c(\"black\", \"red\", \"green\"), \n#       lty=as.numeric(iris$Species), cex=1.2)\n# legend(\"bottomright\", legend=c(\"Clust1\", \"Clust2\", \"Clust3\"), pch=unique(as.numeric(iris$Species)), \n#        cex=1.2)\nlegend <- c(\"setosa\", \"versicolor\", \"virginica\")\nplot_ly(x=~iris[,1], y=~iris[,2], type=\"scatter\", mode=\"markers\", color=~as.factor(data_sc@.Data), \n        marker=list(size=15), symbol=~as.numeric(iris$Species)) %>%\n  layout(title=\"Spectral Clustering (Iris Data)\", xaxis=list(title=colnames(iris)[1]), yaxis=list(title=colnames(iris)[2]))",
      "line_count": 21
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# library(\"kernlab\")\ndata(spirals)\nnum_clusters <- 2\ndata_sc <- specc(spirals, centers= num_clusters)\ndata_sc\ncenters(data_sc)\nwithinss(data_sc)\n\n# plot(spirals, col= data_sc)\n\nclusterNames <- paste0(\"Cluster \", data_sc@.Data)\n\nplot_ly(x=~spirals[,1], y=~spirals[,2], type=\"scatter\", mode=\"markers\", color = ~data_sc@.Data, name=clusterNames) %>% hide_colorbar()",
      "line_count": 13
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "data(income)\nnum_clusters <- 2\ndata_sc <- specc(income, centers= num_clusters)\ndata_sc\ncenters(data_sc)\nwithinss(data_sc)\n\n# plot(income, col= data_sc)\n\n# Convert all Income features to factors and construct SPLOM plot\ndf2 <- income\ncol_names <- names(df2)\ndims <- as.data.frame(lapply(df2[col_names], factor))\n\ndims <- purrr::map2(dims, names(dims), ~list(values=.x, label=.y))\nplot_ly(type = \"splom\", dimensions = setNames(dims, NULL), \n        showupperhalf = FALSE, diagonal = list(visible = FALSE))",
      "line_count": 17
    },
    {
      "section": "Model Performance Assessment, Validation, and Improvement",
      "code": "# ppmi_data <-read.csv(\"https://umich.instructure.com/files/330400/download?download_frd=1\", header=TRUE)\n# get AdaBoost CV results\nset.seed(123)\ncv.out.ada <- crossval::crossval(my.ada, as.data.frame(X), Y, K = 5, B = 1, negative = neg)\n\n# get k-Means CV results\nmy.kmeans <- function (train.x, train.y, test.x, test.y, negative, formula){\n  kmeans.fit <- kmeans(scale(test.x), kpp_init(scale(test.x), 2), iter.max=100, algorithm='Lloyd')\n  predict.y <- kmeans.fit$cluster\n  #count TP, FP, TN, FN, Accuracy, etc.\n  out <- confusionMatrix(test.y, predict.y, negative = negative)\n # negative\t is the label of a negative \"null\" sample (default: \"control\").\n  return (out)\n}\nset.seed(123)\ncv.out.kmeans <- crossval::crossval(my.kmeans, as.data.frame(X), Y, K = 5, B = 2, negative = neg)\n\n# get spectral clustering CV results\nmy.sc <- function (train.x, train.y, test.x, test.y, negative, formula){\n  sc.fit <- specc(scale(test.x), centers= 2)\n  predict.y <- sc.fit@.Data\n  #count TP, FP, TN, FN, Accuracy, etc.\n  out <- confusionMatrix(test.y, predict.y, negative = negative)\n # negative\t is the label of a negative \"null\" sample (default: \"control\").\n  return (out)\n}\nset.seed(123)\ncv.out.sc <- crossval::crossval(my.sc, as.data.frame(X), Y, K = 5, B = 2, negative = neg)\n\nlibrary(knitr)\nres_tab=rbind(diagnosticErrors(cv.out.ada$stat),diagnosticErrors(cv.out.lda$stat),diagnosticErrors(cv.out.qda$stat),diagnosticErrors(cv.out.knn$stat),diagnosticErrors(cv.out.logit$stat),diagnosticErrors(cv.out.nn$stat),diagnosticErrors(cv.out.svml$stat),diagnosticErrors(cv.out.svmg$stat),diagnosticErrors(cv.out.kmeans$stat),diagnosticErrors(cv.out.sc$stat))\nrownames(res_tab) <- c(\"AdaBoost\", \"LDA\", \"QDA\", \"knn\", \"logit\", \"Neural Network\", \"linear SVM\", \"Gaussian SVM\", \"k-Means\", \"Spectral Clustering\")\nkable(res_tab,caption = \"Compare Result\")",
      "line_count": 33
    }
  ]
}