{
  "metadata": {
    "created_at": "2024-11-30T13:46:17.285418",
    "total_sections": 3,
    "total_code_chunks": 4,
    "total_tables": 1,
    "r_libraries": [
      "deepnet",
      "plotly"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<u>Appendix 12: Stochastic Neural Networks - Restricted Boltzmann Machine (RBM)</u>\"\nauthor: \"SOCR/MIDAS (Ivo Dinov)\"\ndate: \"`r format(Sys.time(), '%B %d, %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\nExpanding on [DSPA Chapter 6 (Neural Networks and Supervised Machine Learning)](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html),\nthis [DSPA2 appendix](https://www.socr.umich.edu/DSPA2/) covers a type of stochastic neural networks, called a *Restricted Boltzmann Machine (RBM)*. RBMs are used for tasks such as dimensionality reduction, classification, regression, collaborative filtering, feature learning, and generative modeling. Its foundations span mathematics, statistics, and physics, with each field contributing essential principles to the structure and function of RBMs.",
      "word_count": 122
    },
    {
      "title": "Stochastic neural networks",
      "content": "Stochastic neural networks are a special kind of neural network where certain components (such as the neurons (perceptrons), connections, or activation functions) are governed by probabilistic or stochastic processes. Unlike traditional deterministic neural networks, where the output is fully determined by the input and the learned weights, stochastic neural networks introduce *randomness* into the learning process, making them better suited for handling uncertainty, modeling distributions, and performing tasks that require probabilistic reasoning, such as generative modeling and reinforcement learning. The key features of stochastic \nneural networks include\n\n - *Randomness in Neuron Activation*: In stochastic neural networks, neurons can activate probabilistically based on the input they receive. For example, a neuron's output might be a probabilistic function (such as a *sigmoid* or *softmax*) of the input, and its state can be randomly determined (e.g., binary state $0$ or $1$ based on some probability distribution).\n - *Probabilistic Weight Updates*: Stochastic neural networks may use probabilistic rules for updating weights during training, often utilizing techniques like Gibbs sampling, Markov Chain Monte Carlo (MCMC), or variational inference.\n - *Modeling Uncertainty*: These networks are particularly useful for tasks involving uncertainty, such as generative modeling, where the goal is to sample from a learned distribution, and reinforcement learning, where the environment is often stochastic.\n - *Learning and Sampling*: Stochastic neural networks often rely on sampling-based learning algorithms, such as *contrastive divergence* (for Restricted Boltzmann Machines) or *variational inference* (for Variational Autoencoders), which approximate the gradient of the likelihood function by drawing samples from a probability distribution.\n\nHere are some examples of Stochastic Neural Networks:\n\n - *Boltzmann Machines (BM)*: A type of stochastic neural network where the neurons have binary states and stochastic activation functions. BMs learn the probability distribution over the data by minimizing an energy function.\n - *Restricted Boltzmann Machines (RBM)*: A specific type of Boltzmann Machine with restricted connectivity (no intra-layer connections). RBMs are widely used for feature extraction and dimensionality reduction.\n - *Deep Belief Networks (DBN)*: A stack of multiple RBMs trained in a greedy layer-wise fashion.\n - *Variational Autoencoders (VAE)*: A type of stochastic neural network where the latent variables are modeled by a probabilistic distribution, and both the encoder and decoder are trained to maximize a variational bound on the likelihood of the data.\n\nIn this appendix, we'll focus on the *Restricted Boltzmann Machines (RBMs)*, as a specific class of *stochastic neural networks* that have the following characteristics\n\n - *Bipartite Graph Structure*: The RBM consists of two layers of neuronsâ€”*visible units* (representing the input data) and *hidden units* (representing *latent features*) with stochastic, probabilistic activations. The visible and hidden units are fully connected, but there are *no intra-layer connections* (i.e., no connections between units within the visible layer or within the hidden layer), giving the \"restricted\" property to the network.\n - *Stochastic Activations*: In a RBM, the states of the visible and hidden units are determined probabilistically (1) the hidden units are activated based on a probability that depends on the visible units and the learned weights; and (2) the visible units can also be probabilistically reconstructed from the hidden units.\n\n\nThe activation of the hidden units is stochastic, meaning that, given a visible vector \\( v \\), the state of each hidden unit \\( h_j \\) is sampled from a probability distribution\n\\[P(h_j = 1 | v) = \\sigma \\left( \\sum_i W_{ij} v_i + c_j \\right) , \\]\n\nwhere \\( \\sigma(x) \\) is the sigmoid function, \\( W_{ij} \\) is the weight connecting visible unit \\( v_i \\) and hidden unit \\( h_j \\), and \\( c_j \\) is the bias for hidden unit \\( h_j \\).\n\n - *Energy-Based Model*: RBMs are *energy-based models* where the joint probability distribution of the visible and hidden units is modeled using an energy function. The energy of a configuration (state of the visible and hidden units) is\n\\[E(v, h) = - \\sum_{i,j} v_i W_{ij} h_j - \\sum_i b_i v_i - \\sum_j c_j h_j . \\]\n\nThe lower the energy of a configuration, the more likely it is under the model. The goal of training an RBM is to find weights and biases that assign low energy to configurations corresponding to the data.\n\n - *Learning via Contrastive Divergence*: RBMs use a stochastic learning algorithm called *Contrastive Divergence (CD)* to approximate the gradient of the log-likelihood function and update the model parameters. CD works by (1) sampling from the distribution of the hidden units given the visible units (positive phase); (2) reconstructing the visible units from the hidden units (negative phase); and (3) updating the weights based on the difference between the positive and negative phase statistics.\n\n - *Generative and Feature Learning*: Since RBMs model the probability distribution of the input data, they are *generative models*, meaning they can generate new samples by sampling from the learned distribution. They are also powerful for *feature learning*, as the hidden units capture latent features of the data that can be used for tasks like classification, clustering, or dimensionality reduction.\n\nThe hidden and visible units in a RBM have stochastic binary states. For instance, given the visible units, the hidden units are not deterministically set to 0 or 1 but rather are probabilistically sampled based on their activation probability. This probabilistic behavior captures uncertainty and allows the model to represent distributions, rather than fixed mappings, between the input and output.\n\nSince RBMs are stochastic, they rely on *Gibbs sampling* for inference and learning. Gibbs sampling is a *Markov Chain Monte Carlo (MCMC)* method where the states of the visible and hidden units are alternately sampled until the system reaches a stationary distribution. The stochastic nature of the model allows it to explore various configurations during learning.\n\nRBMs are useful for (1) dimensionality reduction, since RBMs learn a compact representation of the data in the hidden layer; (2) feature extraction, as hidden units in a RBM learn abstract features of the input data; and (3) generative modeling, as RBMs can simulate/generate new samples from the l*earned distribution* by sampling from the hidden layer and reconstructing the visible layer.\n\nIn a general Boltzmann Machine, all units (both visible and hidden) can be connected to one another, resulting in a much more complex network with higher computational costs. In contrast, RBMs are restricted, meaning no connections exist within the visible or hidden layers. This makes training RBMs more tractable, especially with methods like Contrastive Divergence.\n\nVAEs differ from RBMs in that they use continuous latent variables (instead of binary) and employ variational inference to approximate the posterior distribution. VAEs are more common for tasks like generating images or text, while RBMs are simpler and can be applied to binary or discrete data.",
      "word_count": 1101
    },
    {
      "title": "Restricted Boltzmann Machines (RBMs)",
      "content": "RBMs represent a type of stochastic neural network used for tasks such as dimensionality reduction, classification, regression, collaborative filtering, feature learning, and generative modeling. Its foundations span *mathematics*, *statistics*, and *physics*. The core of an RBM is the idea of representing and learning probability distributions through the interactions of neurons. An RBM consists of two layers of neurons\n\n - *Visible layer*: Represents observed data (*input*).\n - *Hidden layer*: Represents *latent* variables or features to learn.\n\nThese layers are fully connected, but connections *within each layer are restricted*, meaning neurons in the visible layer do not interact with each other, and neurons in the hidden layer do not interact with each other.\nThe key mathematical formulation of an RBM is an *energy function*, which assigns an energy to each configuration of the visible and hidden neurons. The lower the energy, the more likely the configuration. The joint probability distribution of the visible and hidden units is defined using a *Boltzmann distribution*:\n\n\\[P(v, h) = \\frac{1}{Z} \\exp(-E(v, h)) ,\\]\n\nwhere \\( v \\) and \\( h \\) are vectors representing the states of the visible and hidden units, respectively, \\( E(v, h) \\) is the energy function that measures the interaction between the visible and hidden units, and \\( Z \\) is the partition function, ensuring that the probabilities sum to 1, i.e., \n\\(Z = \\sum_{v, h} \\exp(-E(v, h)) .\\)\n\n## RBM Energy Function\n\nThe energy function of the RBM is typically defined by\n\\[E(v, h) = -\\sum_{i} b_i v_i - \\sum_{j} c_j h_j - \\sum_{i,j} v_i W_{ij} h_j, \\]\n\nwhere \\( b_i \\) and \\( c_j \\) are biases for the visible and hidden units, \\( W_{ij} \\) are the weights connecting visible unit \\( i \\) and hidden unit \\( j \\), and \n\\( v_i \\) and \\( h_j \\) are the states of visible unit \\( i \\) and hidden unit \\( j \\), respectively.\n\nThe objective is to *minimize the energy function*, which is equivalent to\n*maximizing the likelihood of the data.* This leads to the optimal configuration of weights and biases that define the learned model.\nThe RBM's *marginal probability* over the visible units \\(v\\) is obtained by summing over all possible hidden states \\(P(v) = \\frac{1}{Z} \\sum_h \\exp(-E(v, h)) .\\)\nThis represents a powerful model for representing complex distributions because the hidden units act as latent variables that can capture higher-order correlations in the visible data.\n\nRBMs are closely tied to *probabilistic graphical models* and use principles from *statistical inference* and *statistical/machine learning theory*.\nAn RBM can be represented by an *undirected graphical model* (Markov Random Field) with the visible and hidden units forming a bipartite graph. The lack of connections within each layer simplifies inference and learning, which can be challenging in fully connected graphical models.\nTo train RBMs (estimate the model parameters), we need to sample from the joint distribution \\(P(v, h)\\). This is challenging due to the partition function \\(Z\\), which is computationally intractable for large models. Instead, *Gibbs sampling* is used, where we alternate between sampling the hidden units given the visible units and sampling the visible units given the hidden units - (1) sample \\( h_j \\) from \\( P(h_j | v) \\), and then (2) sample \\( v_i \\) from \\( P(v_i | h) \\).\n\nThis iterative procedure is based on *Markov Chain Monte Carlo (MCMC)* methods, which allow us to approximate the distribution without directly calculating \\(Z\\).\nSince exact inference is computationally expensive, RBMs often use *Contrastive Divergence (CD)* to approximate the gradient of the likelihood function. The CD algorithm updates the weights using the following steps:\n\n 1. Sample the hidden units \\( h_j \\) based on the data \\( v_i \\).\n 2.  Reconstruct the visible units \\( v_i' \\) from the hidden units.\n 3. Update the weights and biases using the difference between the observed data and the reconstruction\n\\[\\Delta W_{ij} = \\eta \\left( \\langle v_i h_j \\rangle_{\\text{data}} - \\langle v_i h_j \\rangle_{\\text{reconstruction}} \\right) ,\\]\nwhere \\( \\eta \\) is the learning rate, and the angle brackets denote expectations over the distribution of \\( v_i \\) and \\( h_j \\).\n\nThe original RBM formulation is deeply connected to concepts from *statistical mechanics*, particularly the *Boltzmann distribution* and *energy-based models* in physics.\n\nThe *Boltzmann distribution* describes the probability of a system being in a certain state based on its energy. In a RBM, this is analogous to the joint distribution of visible and hidden units \\(P(v, h) = \\frac{1}{Z} \\exp(-E(v, h)) .\\)\nThis distribution comes from thermodynamics, where systems tend to configurations with lower energy. In the context of an RBM, the system (data) seeks the configuration (parameter settings) that minimizes energy, corresponding to the most likely states for the model.\n\nThe *partition function* \\( Z \\) in the RBM is borrowed directly from statistical mechanics, where it serves to normalize the probability distribution. It is defined as the sum over all possible configurations (microstates) of the system. In practice, computing \\( Z \\) is computationally expensive, as it requires summing over an exponentially large number of states. Techniques like *Gibbs sampling* and *Contrastive Divergence* circumvent the need for direct computation.\n\nThe RBM learning process (*RBM optimization*) is analogous to the thermodynamic principle of *minimizing free energy* in a physical system. The goal is to adjust the model parameters (weights and biases) to minimize the energy function, thereby maximizing the likelihood of the observed data.\nThe free energy of a configuration \\(v\\) in a RBM is \\(F(v) = -\\log \\sum_h \\exp(-E(v, h)). \\) Minimizing this free energy is equivalent to *maximizing the likelihood of the observed data*. This parallels the behavior of physical systems that seek equilibrium states with minimum free energy.\n\n\n## Restricted vs. General (Unrestricted) Boltzmann Machines\n\nThe *restricted* adjective in the RBM name refers to the restriction on the networkâ€™s connectivity -- *no direct interactions* within the observed (data) layer or within\nthe subsequent hidden layer. This restriction of connections only between-layers, i.e., between the visible (data) units and the hidden units, constrains the network architecture making it distinct from its more general counterpart, the *Boltzmann Machine*, where units can have connections to other units in the same layer.\n\nThe RBM restriction that only allows interactions between visible and hidden units (and no connections between units within the same layer) has both mathematical and statistical motivations:\n\n - Mathematical Motivation: Simplifying Inference and Learning: Without the restriction, a full Boltzmann Machine becomes *computationally intractable* for inference and learning, particularly for large datasets. This is because the *partition function* \\( Z \\), which normalizes the probability distribution, is exponentially difficult to compute when connections within a layer exist.\n - *General Boltzmann Machines*: In a fully connected Boltzmann Machine, each unit (visible or hidden) can interact with every other unit. As a result, computing the exact likelihood of the data (which requires marginalizing over all possible hidden configurations) becomes exponentially complex because interactions within layers create loops in the graphical model. These loops make it challenging to compute the conditional distributions and marginal likelihood.\n - *Restricted Boltzmann Machines*: By *restricting the network* so that there are no connections within the visible layer or within the hidden layer, the model becomes a *bipartite graph*, which is free of cycles. This restriction allows for an efficient *block Gibbs sampling* procedure, where the hidden units can be sampled independently given the visible units, and vice versa. This *simplifies both inference and learning* considerably.\n\nMathematically, this restriction allows the conditional distributions \\( P(v | h) \\) and \\( P(h | v) \\) to factorize \\( P(v | h) = \\prod_i P(v_i | h) \\) and \\( P(h | v) = \\prod_j P(h_j | v) \\). This factorization means that each visible unit can be updated independently given the hidden units, and vice versa. This is crucial for efficient *Gibbs sampling* and for making the model computationally feasible to train.\n\nA statistical motivation relies on simplified interdependencies. The restriction that only allows connections between visible and hidden units also reflects certain statistical assumptions about the data. In a RBM, the visible units (features) are conditionally independent given the hidden units (latent variables). This reflects the assumption that once the hidden layer explains the data, the remaining variability in the visible units is independent of one another.\nThis assumption is often reasonable in high-dimensional data: many complex dependencies in the visible layer can be captured by introducing hidden variables that encode patterns or latent features. The restriction forces the hidden layer to capture the most important patterns in the data because the visible units can only interact with each other *indirectly* through the hidden units.\n\n*Example*: Consider an image dataset where the visible units represent pixel intensities. Without the hidden units, modeling pixel correlations directly would be very complex. However, with hidden units that represent abstract features (like edges, textures, or object parts), the visible units (pixels) become conditionally independent given these hidden features. Thus, the hidden layer effectively captures correlations between visible units, but through a simplified, abstract representation.\n\n - *No Correlations Between Hidden Units*: The restriction also assumes that the hidden units do not directly interact with each other. This means that each hidden unit represents an independent latent feature. In practice, this independence assumption is relaxed somewhat during learning because the hidden units share statistical dependencies through their connections to the visible units. However, the lack of direct connections between hidden units simplifies the learning process.\n\nThe restricted structure of the RBM allows for the efficient use of *Contrastive Divergence (CD)* to approximate the gradient of the likelihood function. CD works by sampling from the conditionally independent hidden and visible layers, avoiding the need for fully sampling over all configurations of the network. The restricted connectivity ensures that:\n\n - The visible units can be updated independently given the hidden units.\n - The hidden units can be updated independently given the visible units.\n\nThis leads to a *closed-form update rule* for the weights that connect visible and hidden units, enabling efficient learning with gradient-based methods.\nThe restriction that only interactions between hidden and visible units are allowed is necessary for the following reasons:\n\n - *Conditional Independence for Efficient Sampling*: The restriction enforces conditional independence within each layer, simplifying the *Gibbs sampling* procedure and making it computationally feasible. Without the restriction, interactions between units in the same layer would make the model's likelihood intractable to compute because we would no longer have conditional independence between the variables.\n - *Modeling Complex Correlations with Simpler Structure*: The hidden units act as latent variables that *capture complex dependencies* between the visible units (the data). Even though the visible units are not directly connected, the hidden units model higher-order interactions between them. This simplifies the modeling process while still allowing for the representation of complex patterns in the data.\n\nIf visible units (features) were directly connected, we would face the following issues (1) *Complexity in Conditional Probabilities*, with direct connections between visible units, the conditional probability \\( P(v | h) \\) would not factorize into independent terms, making inference and learning computationally expensive; and (2) *Difficulties in Training*, the learning process would require dealing with the complex dependencies between visible units. This introduces loops in the modelâ€™s graph, making exact inference and gradient-based learning much harder due to the need to sample from a much more complex distribution.\nIf hidden units were allowed to interact directly, the hidden layer would not be able to act as a simple set of independent feature detectors. Instead, allowing interactions between hidden units would introduce dependencies between the features they represent. This would complicate the learning process and reduce the interpretability of the hidden units as independent latent factors or features.\nAlso, without the restriction, sampling and inference become much more complex because we lose the conditional independence between the hidden units. As a result, the process of sampling from the hidden layer given the visible layer becomes intractable.\n\n## Examples of Restricted Boltzmann Machines\n\nLet's work through two RBM examples. One with a *Restricted Boltzmann Machine (RBM)* and one with a *general (unrestricted) Boltzmann Machine (BM)*â€”to demonstrate the calculations involved and highlight the differences in complexity and approach.\n\n### Example 1: Restricted Boltzmann Machine (RBM)\nConsider a simple RBM with\n\n - *3 visible units*: \\( v_1, v_2, v_3 \\)\n - *2 hidden units*: \\( h_1, h_2 \\)\n - The weights connecting the visible and hidden units are given by a matrix \\( W \\), and each unit has a bias (denoted by \\( b \\) for visible units and \\( c \\) for hidden units).\n\nAssume the following *weights* and *biases*\n\\[W = \\begin{bmatrix} \n0.5 & -0.3 \\\\\n0.7 & 0.1 \\\\\n-0.2 & 0.4 \\\\\n\\end{bmatrix}\n\\quad b = \\begin{bmatrix} 0.2 \\\\ 0.4 \\\\ 0.1 \\end{bmatrix}\n\\quad c = \\begin{bmatrix} 0.6 \\\\ -0.2 \\end{bmatrix} ,\\]\nwhere, \\( W_{ij} \\) represents the weight between visible unit \\( v_i \\) and hidden unit \\( h_j \\), and \\( b_i \\) and \\( c_j \\) represent biases for the visible and hidden units, respectively.\n\nThe *energy function* for the RBM is\n\\[E(v, h) = - \\sum_i b_i v_i - \\sum_j c_j h_j - \\sum_{i,j} v_i W_{ij} h_j ,\\]\nwhere \\( v_i \\) and \\( h_j \\) represent the states of the visible and hidden units, respectively.\n\nLet's compute the energy for a particular state of the visible and hidden units. \nAssume \\( v = [1, 0, 1] \\) (binary values for visible units) and \\( h = [1, 0] \\) (binary values for hidden units).\n\n - *First term*: \\( \\sum_i b_i v_i = 0.2 \\cdot 1 + 0.4 \\cdot 0 + 0.1 \\cdot 1 = 0.2 + 0.1 = 0.3 .\\)\n\n - *Second term*: \\( \\sum_j c_j h_j = 0.6 \\cdot 1 + (-0.2) \\cdot 0 = 0.6 .\\)\n\n - *Third term*: \\( \\sum_{i,j} v_i W_{ij} h_j = (1 \\cdot 0.5 \\cdot 1) + (0 \\cdot 0.7 \\cdot 1) + (1 \\cdot -0.2 \\cdot 1) = 0.5 + 0 + (-0.2) = 0.3 . \\)\n\nThen, the *total energy* for this state \\( v = [1, 0, 1] \\) and \\( h = [1, 0] \\) is\n\\(E(v, h) = -(0.3 + 0.6 + 0.3) = -1.2.\\)\n\nTo train the RBM, we need to sample from the *conditional distributions* \\( P(h_j | v) \\) and \\( P(v_i | h) \\). The conditional probability of hidden unit \\( h_j \\) given the visible units \\( v \\) is\n\\(P(h_j = 1 | v) = \\sigma \\left( c_j + \\sum_i W_{ij} v_i \\right) ,\\)\nwhere \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\) is the sigmoid function.\n\nFor hidden unit \\( h_1 \\)\n\\[P(h_1 = 1 | v) = \\sigma \\left( 0.6 + (1 \\cdot 0.5) + (0 \\cdot 0.7) + (1 \\cdot -0.2) \\right) = \\sigma(0.9) = \\frac{1}{1 + e^{-0.9}} \\approx 0.71 .\\]\n\nTherefore, \\( h_1 = 1 \\) with probability $0.71$.\nSimilarly, for visible units given hidden units\n\\(P(v_i = 1 | h) = \\sigma \\left( b_i + \\sum_j W_{ij} h_j \\right) .\\)\n\nIn a RBM, the lack of intra-layer connections allows for efficient Gibbs sampling because the units within each layer are conditionally independent given the other layer. Also, we can sample all hidden units in parallel and all visible units in parallel because of the restricted bipartite structure.\n\n### Example 2: General (Unrestricted) Boltzmann Machine (BM)\n\nNext, consider a general Boltzmann Machine with *3 visible units* \n\\( v_1, v_2, v_3 \\) and *2 hidden units* \\( h_1, h_2 \\), but this time \n*we allow intra-layer connections* (i.e., visible units can interact with each other, and hidden units can interact with each other).\n\nAssume the following *weights* and *biases*, but now include intra-layer connections\n\\[W = \\begin{bmatrix}\n0.5 & -0.3 & 0.2 \\\\\n0.7 & 0.1 & 0.0 \\\\\n-0.2 & 0.4 & -0.1 \\\\\n\\end{bmatrix}\n\\quad b = \\begin{bmatrix} 0.2 \\\\ 0.4 \\\\ 0.1 \\end{bmatrix}\n\\quad c = \\begin{bmatrix} 0.6 \\\\ -0.2 \\end{bmatrix} .\\]\n\nFor simplicity, assume the intra-layer connections are defined by\n\\[U_{\\text{visible}} = \\begin{bmatrix}\n0 & 0.1 & -0.05 \\\\\n0.1 & 0 & 0.07 \\\\\n-0.05 & 0.07 & 0\n\\end{bmatrix}\n\\quad U_{\\text{hidden}} = \\begin{bmatrix}\n0 & 0.2 \\\\\n0.2 & 0\n\\end{bmatrix} .\\]\n\nIn this case, the *energy function* in a general Boltzmann Machine is more complex because it includes intra-layer terms\n\\[E(v, h) = - \\sum_i b_i v_i - \\sum_j c_j h_j - \\sum_{i,j} v_i W_{ij} h_j - \\frac{1}{2} \\sum_{i, i'} U_{ii'} v_i v_{i'} - \\frac{1}{2} \\sum_{j, j'} U_{jj'} h_j h_{j'} .\\]\n\nAs before, we can compute the energy for the same state \\( v = [1, 0, 1] \\) and \\( h = [1, 0] \\).\n\n - *First term*: \\( \\sum_i b_i v_i = 0.2 \\cdot 1 + 0.4 \\cdot 0 + 0.1 \\cdot 1 = 0.3.\\)\n - *Second term*: \\( \\sum_j c_j h_j = 0.6 \\cdot 1 + (-0.2) \\cdot 0 = 0.6 .\\)\n - *Third term*: \\( \\sum_{i,j} v_i W_{ij} h_j = (1 \\cdot 0.5 \\cdot 1) + (0 \\cdot 0.7 \\cdot 1) + (1 \\cdot -0.2 \\cdot 1) = 0.5 + 0 + (-0.2) = 0.3. \\)\n - *Intra-layer visible term*: \\( \\frac{1}{2} \\sum_{i, i'} U_{ii'} v_i v_{i'} = \\frac{1}{2} \\left( 1 \\cdot 0.1 \\cdot 0 + 1 \\cdot (-0.05) \\cdot 1 + 0 \\cdot 0.07 \\cdot 1 \\right) = -0.025 .\\)\n - *Intra-layer hidden term*: \\( \\frac{1}{2} \\sum_{j, j'} U_{jj'} h_j h_{j'} = \\frac{1}{2} \\left( 1 \\cdot 0.2 \\cdot 0 \\right) = 0 . \\)\n\nThus, the total energy is \\(E(v, h) = -(0.3 + 0.6 + 0.3 + (-0.025) + 0) = -1.175.\\)\n\nIn this *unrestricted BM*, Gibbs sampling is more complex because *the visible units and hidden units are no longer conditionally independent* given the other layer. This introduces dependencies within the layers, so the conditional distributions \\( P(v | h) \\) and \\( P(h | v) \\) cannot be factorized easily. To sample \\( v_i \\), we must now consider not only its connections to the hidden units but also the influence of its neighboring visible units. Similarly, the hidden units depend on each other due to the intra-layer connections. This increases the complexity of both inference and learning.\n\nKey differences between RBMs and General BMs\n\n - *Conditional Independence*: In RBMs, the visible units are conditionally independent given the hidden units, and vice versa. This allows efficient block sampling of all units in one layer, given the other. In general BMs, there is no conditional independence within layers, leading to more complex sampling and inference. Intra-layer dependencies introduce loops in the graphical model.\n - *Complexity*: The restricted structure simplifies both inference and learning. Contrastive Divergence (CD) can be applied to approximate the maximum likelihood gradients. Whereas the general BM requires more complex algorithms, such as full MCMC or other approximations, due to the added interactions between units within the same layer.\n - *Inference*: Gibbs sampling in a RBM is more tractable due to the restriction. The updates for each unit are simpler. Whereas, Gibbs sampling in a general BM is slower and more difficult due to the need to consider interactions between all units in the same layer.\n - *Learning*: Learning is easier in RBMs with CD, where we sample the hidden and visible units alternately, exploiting the bipartite graph structure. Learning is much more computationally expensive in BMs due to the full interactions between all units, making CD less effective.\n\n## `R` Demonstration - Function estimation \\( ( f(x) = x - \\sin(x) ) \\) \n\nLet's develop an `R` function demonstrating the architecture of a Restricted Boltzmann Machine (RBM), its optimization and training process, and finally apply it to estimate the behavior of the function \\( f(x) = x - \\sin(x) \\) over the range \\([-10, 10]\\). The RBM can be designed with visible units corresponding to the input space and hidden units for latent features. We will train it to approximate the target function by reconstructing the input-output relationship through the hidden units. Specifically, the goal is to capture the behavior of \\( f(x) \\) by minimizing the reconstruction error over samples from the function.\n\nHere are the basic steps in a RBM implementation:\n\n - *RBM Architecture*: Define the visible and hidden units and initialize weights and biases randomly.\n - *Optimization and Training*: Use *Contrastive Divergence (CD)* to train the RBM and apply the *Gibbs sampling* procedure to update the weights and biases.\n - *Application to Function Estimation*: After completing RBM training, use the RBM to approximate the function \\( f(x) = x - \\sin(x) \\), and evaluate the RBM's ability to reconstruct the target function.\n\n## Manual RBM Implementation\n\nBelow is an example `R` function `train_RBM()` depicting the formulation, training, and application of an RBM for this task. The protocol involves the following steps:\n\n - Matrix Multiplication Conformability: the function `apply_RBM()` ensures that *x_values* and *weights* are compatible for matrix multiplication. We use the shape of x_values (i.e., the number of rows of x_values) and properly adjust the biases using matrix(..., byrow = TRUE) to make sure dimensions conform. The *weights* matrix is applied properly in both the forward pass (hidden layer) and the reconstruction step (visible layer).\n\n - Sampling Hidden and Visible Units:  We ensured that `prob_hidden_given_visible` and `prob_visible_given_hidden` are properly sampled and used in Gibbs sampling, with dimensionality adjustments made at each step.\n\n - To train the RBM, we first initialize the process by setting up the weights and biases, and applying Contrastive Divergence over multiple epochs. The visible units (representing xx and f(x)f(x)) are sampled, and the hidden units are updated stochastically. The parameters are updated iteratively. Following training, the RBM is applied to the xx values to reconstruct the function approximation. Finally, the original function $f(x)=xâˆ’\\sin(x)$ is compared to the RBM's approximation, and both are plotted.\n    \n\n\n<!-- Potential Improvement ... -->\n\n\n## Automated RBM Implementation (using package `deepnet`)\n\nCompare this simpe manual RBM implementation against the function\n`deepnet::rbm.train()`, which as expected produces a much more accurate \nprediction of the nonlinear function estimation for $f(x)=x-\\sin(x)$. In this case, training the RBM uses a custom function `train_RBM_deepnet()`, which utilizes\n`deepnet::rbm.train()` to train a RBM with the specified number of hidden units, epochs, learning rate, and batch size. Also, the `predict_RBM()` function uses the trained RBM to generate predictions by first calculating the hidden layer activations with `deepnet::rbm.up()` and then reconstructs the visible layer (output) with `deepnet::rbm.down()`.\n\n\n\n\n\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 3797
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "      smooth_scroll: true\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "",
      "line_count": 1
    },
    {
      "section": "Restricted Boltzmann Machines (RBMs)",
      "code": "library(plotly)\n\n# Define the sigmoid activation function\nsigmoid <- function(x) {\n  1 / (1 + exp(-x))\n}\n\n# Define the RBM architecture, optimization, and training process\ntrain_RBM <- function(data, n_hidden = 10, n_epochs = 4000, learning_rate = 0.2, batch_size = 20) {\n  \n  # Initialize RBM parameters\n  n_visible <- ncol(data)  # Number of visible units (input features)\n  \n  # Weight matrix (randomly initialized) and bias terms\n  weights <- matrix(rnorm(n_visible * n_hidden, 0, 0.1), n_visible, n_hidden)\n  visible_bias <- rep(0, n_visible)\n  hidden_bias <- rep(0, n_hidden)\n  \n  # Function to perform Gibbs sampling (updating hidden given visible and vice versa)\n  sample_hidden <- function(visible) {\n    prob_hidden <- sigmoid(visible %*% weights + matrix(hidden_bias, nrow = nrow(visible), ncol = n_hidden, byrow = TRUE))\n    prob_hidden\n  }\n  \n  sample_visible <- function(hidden) {\n    prob_visible <- sigmoid(hidden %*% t(weights) + matrix(visible_bias, nrow = nrow(hidden), ncol = n_visible, byrow = TRUE))\n    prob_visible\n  }\n  \n  # Contrastive Divergence (CD-k where k=1)\n  for (epoch in 1:n_epochs) {\n    \n    # Shuffle the data\n    data <- data[sample(1:nrow(data)), ]\n    \n    for (i in seq(1, nrow(data), by = batch_size)) {\n      batch <- data[i:min(i + batch_size - 1, nrow(data)), ]\n      \n      # Positive phase\n      prob_hidden_given_visible <- sample_hidden(batch)\n      hidden_states <- (prob_hidden_given_visible > matrix(runif(nrow(batch) * n_hidden), nrow = nrow(batch), ncol = n_hidden)) * 1\n      \n      # Negative phase (reconstruction)\n      prob_visible_given_hidden <- sample_visible(hidden_states)\n      visible_reconstructed <- (prob_visible_given_hidden > matrix(runif(nrow(hidden_states) * n_visible), nrow = nrow(hidden_states), ncol = n_visible)) * 1\n      \n      prob_hidden_given_visible_reconstructed <- sample_hidden(visible_reconstructed)\n      \n      # Update weights and biases using contrastive divergence\n      weights <- weights + learning_rate * ((t(batch) %*% prob_hidden_given_visible) - \n                                            (t(visible_reconstructed) %*% prob_hidden_given_visible_reconstructed)) / nrow(batch)\n      visible_bias <- visible_bias + learning_rate * colSums(batch - visible_reconstructed) / nrow(batch)\n      hidden_bias <- hidden_bias + learning_rate * colSums(prob_hidden_given_visible - prob_hidden_given_visible_reconstructed) / nrow(batch)\n    }\n    \n    # Optionally print progress\n    if (epoch %% 1000 == 0) {\n      cat(\"Epoch:\", epoch, \"complete\\n\")\n    }\n  }\n  \n  # Return the trained RBM parameters\n  return(list(weights = weights, visible_bias = visible_bias, hidden_bias = hidden_bias))\n}\n\n# Define the function to apply the trained RBM to estimate the function f(x) = x - sin(x)\napply_RBM <- function(rbm, x_values) {\n  # Input for the RBM is x_values\n  weights <- rbm$weights\n  visible_bias <- rbm$visible_bias\n  hidden_bias <- rbm$hidden_bias\n  \n  # Get hidden representations (features)\n  prob_hidden_given_visible <- sigmoid(as.matrix(x_values) %*% weights + matrix(hidden_bias, nrow = nrow(x_values), ncol = ncol(weights), byrow = TRUE))\n  hidden_states <- (prob_hidden_given_visible > matrix(runif(nrow(x_values) * ncol(weights)), nrow = nrow(x_values), ncol = ncol(weights))) * 1\n  \n  # Reconstruct the visible units (the function approximation)\n  reconstructed_visible <- sigmoid(hidden_states %*% t(weights) + matrix(visible_bias, nrow = nrow(hidden_states), ncol = length(visible_bias), byrow = TRUE))\n  \n  return(reconstructed_visible)\n}\n\n# Generate training data for the function f(x) = x - sin(x)\ngenerate_data <- function(x_range) {\n  x_values <- seq(x_range[1], x_range[2], length.out = 100)\n  y_values <- x_values - sin(x_values)\n  \n  # Normalize the data to fit into [0, 1] for the RBM\n  x_min <- min(x_values)\n  x_max <- max(x_values)\n  y_min <- min(y_values)\n  y_max <- max(y_values)\n  \n  x_normalized <- (x_values - x_min) / (x_max - x_min)\n  y_normalized <- (y_values - y_min) / (y_max - y_min)\n  \n  data <- cbind(x_normalized, y_normalized)\n  \n  return(list(data = data, x_values = x_values, y_values = y_values, \n              x_min = x_min, x_max = x_max, y_min = y_min, y_max = y_max))\n}\n\n# Train the RBM and apply it to approximate f(x) = x - sin(x)\nrun_RBM_demo <- function() {\n  # Generate data for the function f(x) = x - sin(x)\n  training_data <- generate_data(c(-10, 10))\n  \n  # Train the RBM on the data\n  rbm <- train_RBM(training_data$data, n_hidden = 20, n_epochs = 4000, learning_rate = 0.2, batch_size = 20)\n  \n  # Apply the trained RBM to approximate the function\n  reconstructed <- apply_RBM(rbm, training_data$data) # [, 1, drop = FALSE])  # Only use the x-values for prediction\n  \n  # Rescale the reconstructed output to the original scale\n  y_pred <- reconstructed * (training_data$y_max - training_data$y_min) + training_data$y_min\n  \n  # Plot the results\n  plot(training_data$x_values, training_data$y_values, type = \"l\", col = \"blue\", lwd = 2, ylab = \"f(x)\", xlab = \"x\",\n       main = \"Function Approximation using RBM\")\n  lines(training_data$x_values, 20*y_pred[,2], col = \"red\", lwd = 2)\n  legend(\"topright\", legend = c(\"True f(x) = x - sin(x)\", \"RBM Approximation\"), col = c(\"blue\", \"red\"), lwd = 2)\n}\n\n# Run the RBM demo\nrun_RBM_demo()",
      "line_count": 125
    },
    {
      "section": "Restricted Boltzmann Machines (RBMs)",
      "code": "# See if we can improve this block to obtain better RBM predictions.\n# Define the sigmoid activation function\nsigmoid <- function(x) {\n  1 / (1 + exp(-x))\n}\n\n# Define the RBM architecture, optimization, and training process\ntrain_RBM <- function(data, n_hidden = 20, n_epochs = 5000, learning_rate = 0.01, batch_size = 10, momentum = 0.9, lambda = 0.0001) {\n  \n  # Initialize RBM parameters\n  n_visible <- ncol(data)  # Number of visible units (input features)\n  \n  # Weight matrix (randomly initialized) and bias terms\n  weights <- matrix(rnorm(n_visible * n_hidden, 0, 0.1), n_visible, n_hidden)\n  visible_bias <- rep(0, n_visible)\n  hidden_bias <- rep(0, n_hidden)\n  \n  # Initialize momentum terms for weight updates\n  weight_update <- matrix(0, nrow = n_visible, ncol = n_hidden)\n  visible_bias_update <- rep(0, n_visible)\n  hidden_bias_update <- rep(0, n_hidden)\n  \n  # Function to perform Gibbs sampling (updating hidden given visible and vice versa)\n  sample_hidden <- function(visible) {\n    prob_hidden <- sigmoid(visible %*% weights + matrix(hidden_bias, nrow = nrow(visible), ncol = n_hidden, byrow = TRUE))\n    prob_hidden\n  }\n  \n  sample_visible <- function(hidden) {\n    prob_visible <- sigmoid(hidden %*% t(weights) + matrix(visible_bias, nrow = nrow(hidden), ncol = n_visible, byrow = TRUE))\n    prob_visible\n  }\n  \n  # Contrastive Divergence (CD-k where k=1)\n  for (epoch in 1:n_epochs) {\n    \n    # Shuffle the data\n    data <- data[sample(1:nrow(data)), ]\n    \n    for (i in seq(1, nrow(data), by = batch_size)) {\n      batch <- data[i:min(i + batch_size - 1, nrow(data)), ]\n      \n      # Positive phase\n      prob_hidden_given_visible <- sample_hidden(batch)\n      hidden_states <- (prob_hidden_given_visible > matrix(runif(nrow(batch) * n_hidden), nrow = nrow(batch), ncol = n_hidden)) * 1\n      \n      # Negative phase (reconstruction)\n      prob_visible_given_hidden <- sample_visible(hidden_states)\n      visible_reconstructed <- (prob_visible_given_hidden > matrix(runif(nrow(hidden_states) * n_visible), nrow = nrow(hidden_states), ncol = n_visible)) * 1\n      \n      prob_hidden_given_visible_reconstructed <- sample_hidden(visible_reconstructed)\n      \n      # Update weights and biases using contrastive divergence with momentum and weight decay (L2 regularization)\n      weight_update <- momentum * weight_update + learning_rate * ((t(batch) %*% prob_hidden_given_visible) - \n                                            (t(visible_reconstructed) %*% prob_hidden_given_visible_reconstructed)) / nrow(batch) - lambda * weights\n      weights <- weights + weight_update\n      \n      visible_bias_update <- momentum * visible_bias_update + learning_rate * colSums(batch - visible_reconstructed) / nrow(batch)\n      visible_bias <- visible_bias + visible_bias_update\n      \n      hidden_bias_update <- momentum * hidden_bias_update + learning_rate * colSums(prob_hidden_given_visible - prob_hidden_given_visible_reconstructed) / nrow(batch)\n      hidden_bias <- hidden_bias + hidden_bias_update\n    }\n    \n    # Optionally print progress every 500 epochs\n    if (epoch %% 500 == 0) {\n      cat(\"Epoch:\", epoch, \"complete\\n\")\n    }\n  }\n  \n  # Return the trained RBM parameters\n  return(list(weights = weights, visible_bias = visible_bias, hidden_bias = hidden_bias))\n}\n\n# Define the function to apply the trained RBM to estimate the function f(x) = x - sin(x)\napply_RBM <- function(rbm, x_values) {\n  # Input for the RBM is x_values\n  weights <- rbm$weights\n  visible_bias <- rbm$visible_bias\n  hidden_bias <- rbm$hidden_bias\n  \n  # Get hidden representations (features)\n  prob_hidden_given_visible <- sigmoid(as.matrix(x_values) %*% weights + matrix(hidden_bias, nrow = nrow(x_values), ncol = ncol(weights), byrow = TRUE))\n  hidden_states <- (prob_hidden_given_visible > matrix(runif(nrow(x_values) * ncol(weights)), nrow = nrow(x_values), ncol = ncol(weights))) * 1\n  \n  # Reconstruct the visible units (the function approximation)\n  reconstructed_visible <- sigmoid(hidden_states %*% t(weights) + matrix(visible_bias, nrow = nrow(hidden_states), ncol = length(visible_bias), byrow = TRUE))\n  \n  return(reconstructed_visible)\n}\n\n# Generate training data for the function f(x) = x - sin(x)\ngenerate_data <- function(x_range) {\n  x_values <- seq(x_range[1], x_range[2], length.out = 100)\n  y_values <- x_values - sin(x_values)\n  \n  # Standardize the data to have mean 0 and variance 1\n  x_standardized <- scale(x_values)\n  y_standardized <- scale(y_values)\n  \n  data <- cbind(x_standardized, y_standardized)\n  \n  return(list(data = data, x_values = x_values, y_values = y_values, \n              x_min = min(x_values), x_max = max(x_values), \n              y_min = min(y_values), y_max = max(y_values)))\n}\n\n# Train the RBM and apply it to approximate f(x) = x - sin(x)\nrun_RBM_demo <- function() {\n  # Generate data for the function f(x) = x - sin(x)\n  training_data <- generate_data(c(-10, 10))\n  \n  # Train the RBM on the data with improved parameters\n  rbm <- train_RBM(training_data$data, n_hidden = 20, n_epochs = 5000, learning_rate = 0.1, batch_size = 10, momentum = 0.9, lambda = 0.0001)\n  \n  # Apply the trained RBM to approximate the function\n  reconstructed <- apply_RBM(rbm, training_data$data)  # Only use the x-values for prediction\n  \n  # Rescale the reconstructed output to the original scale\n  y_pred <- reconstructed * attr(training_data$data, \"scaled:center\")[2] + attr(training_data$data, \"scaled:scale\")[2]\n  \n  # Plot the results\n  plot(training_data$x_values, training_data$y_values, type = \"l\", col = \"blue\", lwd = 2, ylab = \"f(x)\", xlab = \"x\",\n       main = \"Function Approximation using Improved RBM\")\n  lines(training_data$x_values, 20*y_pred[,2], col = \"red\", lwd = 2)\n  legend(\"topright\", legend = c(\"True f(x) = x - sin(x)\", \"RBM Approximation\"), col = c(\"blue\", \"red\"), lwd = 2)\n}\n\n# Run the RBM demo with the improved training procedure\nrun_RBM_demo()",
      "line_count": 130
    },
    {
      "section": "Restricted Boltzmann Machines (RBMs)",
      "code": "# Load the required package\n# install.packages(\"deepnet\")\nlibrary(deepnet)\n\n# Generate the dataset for f(x) = x - sin(x)\ngenerate_data <- function(x_range) {\n  x_values <- seq(x_range[1], x_range[2], length.out = 100)\n  y_values <- x_values - sin(x_values)\n  \n  # Normalize the data between 0 and 1 for the RBM\n  x_min <- min(x_values)\n  x_max <- max(x_values)\n  y_min <- min(y_values)\n  y_max <- max(y_values)\n  \n  x_normalized <- (x_values - x_min) / (x_max - x_min)\n  y_normalized <- (y_values - y_min) / (y_max - y_min)\n  \n  data <- cbind(x_normalized, y_normalized)\n  \n  return(list(data = data, x_values = x_values, y_values = y_values, \n              x_min = x_min, x_max = x_max, y_min = y_min, y_max = y_max))\n}\n\n# Train a RBM to approximate f(x) = x - sin(x)\ntrain_RBM_deepnet <- function(data, hidden_units = 10, num_epochs = 1000, learning_rate = 0.1, batch_size = 10) {\n  \n  # Train the RBM using deepnet's rbm.train function\n  rbm_model <- rbm.train(x = data, hidden = hidden_units, \n                         numepochs = num_epochs, learningrate = learning_rate, batchsize = batch_size)\n  \n  return(rbm_model)\n}\n\n# Use the trained RBM to generate predictions\npredict_RBM <- function(rbm_model, x_values_normalized) {\n  \n  # Use the rbm.up function from deepnet to get hidden representation, then rbm.down to get reconstruction\n  hidden_representation <- rbm.up(rbm_model, x_values_normalized)\n  reconstructed_output <- rbm.down(rbm_model, hidden_representation)\n  \n  return(reconstructed_output)\n}\n\n# Plot the results comparing true f(x) with RBM approximations\nrun_RBM_demo_deepnet <- function() {\n  \n  # Generate the data for f(x) = x - sin(x)\n  training_data <- generate_data(c(-10, 10))\n  \n  # Train the RBM with deepnet (normalized data)\n  rbm_model <- train_RBM_deepnet(training_data$data, hidden_units = 30, num_epochs = 10000, learning_rate = 0.1, batch_size = 10)\n  \n  # Get predictions from the trained RBM (use the normalized x values)\n  reconstructed_output <- predict_RBM(rbm_model, training_data$data)\n  \n  # Rescale the RBM predictions back to the original scale\n  # y_pred <- reconstructed_output * (training_data$y_max - training_data$y_min) + training_data$y_min\n  # Plot the original function and RBM's approximation\n  # plot(training_data$x_values, training_data$y_values, type = \"l\", col = \"blue\", lwd = 2, \n  #      ylab = \"f(x)\", xlab = \"x\", main = \"RBM Approximation of f(x) = x - sin(x)\")\n  # lines(training_data$x_values, y_pred[, 1], col = \"red\", lwd = 2)\n  # \n  # legend(\"topright\", legend = c(\"True f(x)\", \"RBM Approximation\"), col = c(\"blue\", \"red\"), lwd = 2)\n  \n  # Rescale the RBM predictions back to the original scale\n  y_pred <- reconstructed_output[, 2] * (training_data$y_max - training_data$y_min) +\n            training_data$y_min\n  \n  # expend testing data outside the domain of the training data [-10,10] --> [-20,20]\n  training_data <- generate_data(c(-20, 20))\n  reconstructed_output <- predict_RBM(rbm_model, training_data$data)\n  y_pred <- reconstructed_output[, 2] * (training_data$y_max - training_data$y_min) +\n            training_data$y_min\n  \n  # Create an interactive plot using plot_ly\n  # vertical lines delineating the training domain\n  vline <- function(x = 0, color = \"green\") {\n    list(type = \"line\", y0 = 0, y1 = 1, yref = \"paper\", x0 = x, x1 = x,\n         line = list(color = color, dash=\"dot\"))\n  }\n  \n  fig <- plot_ly() %>%\n    add_lines(x = training_data$x_values, y = training_data$y_values, \n              name = \"True f(x)\", line = list(color = 'blue')) %>%\n    add_lines(x = training_data$x_values, y = y_pred, \n              name = \"RBM Approximation\", line = list(color = 'red')) %>%\n    layout(title = \"RBM Approximation of f(x) = x - sin(x)\",\n           xaxis = list(title = \"x\"),\n           yaxis = list(title = \"f(x)\"),\n           legend = list(title=\"Predicting Covariates\", orientation = 'h'), \n           # legend = list(x = 0.1, y = 0.9),\n           shapes = list(vline(-10), vline(10)))\n  \n  # Display the plot\n  fig\n}\n\n# Run the RBM demo\nrun_RBM_demo_deepnet()",
      "line_count": 100
    }
  ]
}