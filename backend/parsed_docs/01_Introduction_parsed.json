{
  "metadata": {
    "created_at": "2024-11-30T13:46:16.005830",
    "total_sections": 8,
    "total_code_chunks": 67,
    "total_tables": 18,
    "r_libraries": [
      "base",
      "ggplot2",
      "gtrendsR",
      "plotly",
      "prophet",
      "reshape2",
      "rvest",
      "scales",
      "tidyr"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Chapter 1: Introduction</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    code_folding: show\n    self_contained: yes\neditor_options: \n  markdown: \nWe start the DSPA journey with an overview of the mission and objectives\nof this textbook. Some early examples of driving motivational problems\nand challenges provide context into the common characteristics of big\n(biomedical and health) data. We will define data science and predictive\nanalytics and emphasize the importance of their ethical, responsible,\nand reproducible practical use. This chapter also covers the foundations\nof R, contrasts R against other languages and computational data science\nplatforms and introduces basic functions and data objects, formats, and\nsimulation.",
      "word_count": 142
    },
    {
      "title": "Motivation",
      "content": "Let's start with a quick overview illustrating some common data science\nchallenges, qualitative descriptions of the fundamental principles, and\nawareness about the power and potential pitfalls of modern data-driven\nscientific inquiry.\n\n## DSPA Mission and Objectives\n\nThe second edition of this textbook\n([DSPA2](https://dspa2.predictive.space))is based on the [HS650: Data\nScience and Predictive Analytics (DSPA)\ncourse](https://predictive.space/) I teach at the University of Michigan\nand the [first DSPA edition](https://dspa.predictive.space/). These\nmaterials collectively aim to provide learners with a deep understanding\nof the challenges, appreciation of the enormous opportunities, and a\nsolid methodological foundation for designing, collecting, managing,\nprocessing, interrogating, analyzing and interpreting complex health and\nbiomedical data. Readers that finish this course of training and\nsuccessfully complete the examples and assignments included in the book\nwill gain unique skills and acquire a tool-chest of methods, software\ntools, and protocols that can be applied to a broad spectrum of Big Data\nproblems.\n\n-   **Vision**: Enable active-learning by integrating driving\n    motivational challenges with mathematical foundations, computational\n    statistics, and modern scientific inference\n-   **Values**: Effective, reliable, reproducible, and transformative\n    data-driven discovery supporting open-science\n-   **Strategic priorities**: Trainees will develop scientific\n    intuition, computational skills, and data-wrangling abilities to\n    tackle Big biomedical and health data problems. Instructors will\n    provide well-documented R-scripts and software recipes implementing\n    atomic data-filters as well as complex end-to-end predictive big\n    data analytics solutions.\n\nBefore diving into the mathematical algorithms, statistical computing\nmethods, software tools, and health analytics covered in the remaining\nchapters, we will discuss several *driving motivational problems*. These\nwill ground all the subsequent scientific discussions, data modeling,\nand computational approaches.\n\n## Examples of driving motivational problems and challenges\n\nFor each of the studies below, we illustrate several clinically-relevant\nscientific questions, identify appropriate data sources, describe the\ntypes of data elements, and pinpoint various complexity challenges.\n\n### Alzheimer's Disease\n\n-   Identify the relation between observed clinical phenotypes and\n    expected behavior;\n-   Prognosticate future cognitive decline (3-12 months prospectively)\n    as a function of imaging data and clinical assessment (both\n    model-based and model-free machine learning prediction methods will\n    be used);\n-   Derive and interpret the classifications of subjects into clusters\n    using the harmonized and aggregated data from multiple sources.\n\n### Parkinson's Disease\n\n-   Predict the clinical diagnosis of patients using all available data\n    (with and without the UPDRS clinical assessment, which is the basis\n    of the clinical diagnosis by a physician);\n-   Compute derived neuroimaging and genetics biomarkers that can be\n    used to model the disease progression and provide automated clinical\n    decisions support;\n-   Generate decision trees for numeric and categorical responses\n    (representing clinically relevant outcome variables) that can be\n    used to suggest an appropriate course of treatment for specific\n    clinical phenotypes.\n\n### Drug and substance use\n\n-   Is the Risk for Alcohol Withdrawal Syndrome (RAWS) screen a valid\n    and reliable tool for predicting alcohol withdrawal in an adult\n    medical inpatient population?\n-   What is the optimal cut-off score from the AUDIT-C to predict\n    alcohol withdrawal based on RAWS screening?\n-   Should any items be deleted from, or added to, the RAWS screening\n    tool to enhance its performance in predicting the emergence of\n    alcohol withdrawal syndrome in an adult medical inpatient\n    population?\n\n### Amyotrophic lateral sclerosis\n\n-   Identify the most highly-significant variables that have power to\n    jointly predict the progression of ALS (in terms of clinical\n    outcomes like\n    [ALSFRS](http://www.outcomes-umassmed.org/ALS/sf12.aspx) and muscle\n    function)\n-   Provide a decision tree prediction of adverse events based on\n    subject phenotype and 0-3 month clinical assessment changes\n\n### Normal Brain Visualization\n\nThe [SOCR Brain Visualization\nApp](https://socr.umich.edu/HTML5/BrainViewer) has preloaded sMRI, ROI\nlabels, and fiber track models for a normal brain. It also allows users\nto drag-and-drop their data into the browser to visualize and navigate\nthrough the stereotactic data (including imaging, parcellations and\ntractography).\n\n![](http://socr.umich.edu/Udall_PD/images/UdallPD_Carousel_01.png)\n![](http://socr.umich.edu/Udall_PD/images/UdallPD_Carousel_02.png)\n\n### Neurodegeneration\n\nA recent study of [Structural Neuroimaging in Alzheimer's\nDisease](https://www.ncbi.nlm.nih.gov/pubmed/26444770) illustrates the\nBig Data challenges in modeling complex neuroscientific data.\nSpecifically, 808 [ADNI](https://adni.loni.usc.edu/) subjects were\ndivided into 3 groups: 200 subjects with Alzheimer's disease (AD), 383\nsubjects with mild cognitive impairment (MCI), and 225 asymptomatic\nnormal controls (NC). Their sMRI data were parcellated using\n[BrainParser](https://www.loni.usc.edu/Software/BrainParser), and the 80\nmost important neuroimaging biomarkers were extracted using the global\nshape analysis Pipeline workflow. Using a pipeline implementation of\nPlink, the authors obtained 80 SNPs highly-associated with the imaging\nbiomarkers. The authors observed significant correlations between\ngenetic and neuroimaging phenotypes in the 808 ADNI subjects. These\nresults suggest that differences between AD, MCI, and NC cohorts may be\nexamined by using powerful joint models of morphometric, imaging and\ngenotypic data.\n\n![](http://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/images/AD_Moon_Dinov_2016_F1.png)\n\n### Genomics computing\n\n#### Genetic Forensics - 2013-2016 Ebola Outbreak\n\nThis [HHMI disease detective\nactivity](https://www.hhmi.org/biointeractive/ebola-disease-detectives)\nillustrates genetic analysis of sequences of Ebola viruses isolated from\npatients in Sierra Leone during the Ebola outbreak of 2013-2016.\nScientists track the spread of the virus using the fact that most of the\ngenome is identical among individuals of the same species, most similar\nfor genetically related individuals, and more different as the\nhereditary distance increases. DNA profiling capitalizes on these\ngenetic differences. In particular, in regions of noncoding DNA, which\nis DNA that is not transcribed and translated into a protein. Variations\nin noncoding regions impact less individual's traits. Such changes in\nnoncoding regions may be immune to natural selection. DNA variations\ncalled **short tandem repeats (STRs)** are short bases, typically 2-5\nbases long, that repeat multiple times. The repeat units are found at\ndifferent locations, or loci, throughout the genome. Every STR has\nmultiple alleles. These allele variants are defined by the **number of\nrepeat units** present or by the **length of the repeat sequence**. STR\nare surrounded by non-variable segments of DNA known as flanking\nregions. The STR allele in the Figure below could be denoted by \"6\", as\nthe repeat unit (GATA) repeats 6 times, or as 70 base pairs (bps)\nbecause its length is 70 bases in length, including the starting/ending\nflanking regions. Different alleles of the same STR may correspond to\ndifferent numbers of GATA repeats, with the same flanking regions.\n\n::: {style=\"width:65%; margin:0 auto;\"}\n#### Next Generation Sequence (NGS) Analysis\n\nWhole-genome and exome sequencing include essential clues for\nidentifying genes responsible for simple Mendelian inherited disorders.\nThis [paper proposed methods can be applied to complex disorders based\non population genetics](http://www.mdpi.com/2073-4425/3/3/545). Next\ngeneration sequencing (NGS) technologies include bioinformatics\nresources to analyze the dense and complex sequence data. The Graphical\nPipeline for Computational Genomics (GPCG) performs the computational\nsteps required to analyze NGS data. The GPCG implements flexible\nworkflows for basic sequence alignment, sequence data quality control,\nsingle nucleotide polymorphism analysis, copy number variant\nidentification, annotation, and visualization of results. Applications\nof NGS analysis provide clinical utility for identifying miRNA\nsignatures in diseases. Enabling hypotheses testing about the functional\nrole of variants in the human genome will help to pinpoint the genetic\nrisk factors of many diseases (e.g., neuropsychiatric disorders).\n\n![](http://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/images/GPCG_Torri_Dinov.png)\n\n#### Neuroimaging-genetics\n\nA [computational infrastructure for high-throughput\nneuroimaging-genetics (doi:\n10.3389/fninf.2014.00041)](http://journal.frontiersin.org/article/10.3389/fninf.2014.00041/full)\nfacilitates the data aggregation, harmonization, processing and\ninterpretation of multisource imaging, genetics, clinical and cognitive\ndata. A unique feature of this architecture is the graphical user\ninterface to the Pipeline environment. Through its client-server\narchitecture, the Pipeline environment provides a graphical user\ninterface for designing, executing, monitoring, validating, and\ndisseminating complex protocols that utilize diverse suites of software\ntools and web-services. These pipeline workflows are represented as\nportable XML objects, which transfer the execution instructions and user\nspecifications from the client user machine to remote pipeline servers\nfor distributed computing. Using Alzheimer's and Parkinson's data, this\nstudy provides examples of translational applications using this\ninfrastructure\n\n![](http://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/images/LONI_Cranium_2014.png)\n\n## Common Characteristics of *Big (Biomedical and Health) Data*\n\nSoftware developments, student training, utilization of Cloud or IoT\nservice platforms, and methodological advances associated with Big Data\nDiscovery Science all present existing opportunities for learners,\neducators, researchers, practitioners and policy makers, alike. A review\nof many biomedical, health informatics and clinical studies suggests\nthat there are indeed common characteristics of complex big data\nchallenges. For instance, imagine analyzing observational data of\nthousands of Parkinson's disease patients based on tens-of-thousands of\nsignature biomarkers derived from multi-source imaging, genetics,\nclinical, physiologic, phenomics and demographic data elements. IBM had\ndefined the qualitative characteristics of Big Data as 4V's: **Volume,\nVariety, Velocity** and **Veracity** (there are additional V-qualifiers\nthat can be added).\n\nMore recently\n([PMID:26998309](https://www.ncbi.nlm.nih.gov/pubmed/26998309)) we\ndefined a constructive characterization of Big Data that clearly\nidentifies the methodological gaps and necessary tools:\n\n## Data Science\n\n*Data science* is an emerging new field that (1) is extremely\ntransdisciplinary - bridging between the theoretical, computational,\nexperimental, and biosocial areas, (2) deals with enormous amounts of\ncomplex, incongruent and dynamic data from multiple sources, and (3)\naims to develop algorithms, methods, tools and services capable of\ningesting such datasets and generating semi-automated decision support\nsystems. The latter can mine the data for patterns or motifs, predict\nexpected outcomes, suggest clustering or labeling of retrospective or\nprospective observations, compute data signatures or fingerprints,\nextract valuable information, and offer evidence-based actionable\nknowledge. Data science techniques often involve data manipulation\n(wrangling), data harmonization and aggregation, exploratory or\nconfirmatory data analyses, predictive analytics, validation and\nfine-tuning.\n\n## Predictive Analytics\n\n*Predictive analytics* is the process of utilizing advanced mathematical\nformulations, powerful statistical computing algorithms, efficient\nsoftware tools and services to represent, interrogate and interpret\ncomplex data. As its name suggests, a core aim of predictive analytics\nis to forecast trends, predict patterns in the data, or prognosticate\nthe process behavior either within the range or outside the range of the\nobserved data (e.g., in the future, or at locations where data may not\nbe available). In this context, *process* refers to a natural phenomenon\nthat is being investigated by examining proxy data. Presumably, by\ncollecting and exploring the intrinsic data characteristics, we can\ntrack the behavior and unravel the underlying mechanism of the system.\n\nThe fundamental goal of predictive analytics is to identify\nrelationships, associations, arrangements or motifs in the dataset, in\nterms of space, time, features (variables) that may reduce the\ndimensionality of the data, i.e., its complexity. Using these process\ncharacteristics, predictive analytics may predict unknown outcomes,\nproduce estimations of likelihoods or parameters, generate\nclassification labels, or contribute other aggregate or individualized\nforecasts. We will discuss how the outcomes of these predictive\nanalytics can be refined, assessed and compared, e.g., between\nalternative methods. The underlying assumptions of the specific\npredictive analytics technique determine its usability, affect the\nexpected accuracy, and guide the (human) actions resulting from the\n(machine) forecasts. In this textbook, we will discuss supervised and\nunsupervised, model-based and model-free, classification and regression,\nas well as deterministic, stochastic, classical and machine\nlearning-based techniques for predictive analytics. The type of the\nexpected outcome (e.g., binary, polytomous, probability, scalar, vector,\ntensor, etc.) determines if the predictive analytics strategy provides\nprediction, forecasting, labeling, likelihoods, grouping or motifs.\n\n## High-throughput Big Data Analytics\n\nThe [Pipeline Environment](https://pipeline.loni.usc.edu) provides a\nlarge tool chest of software and services that can be integrated, merged\nand processed. The [Pipeline workflow\nlibrary](https://pipeline.loni.usc.edu/explore/library-navigator/) and\nthe [workflow\nminer](https://pipeline.loni.usc.edu/products-services/workflow-miner/)\nillustrate much of the functionality that is available. A\n[Java-based](https://pipeline.loni.usc.edu/products-services/pws/) and\nan [HTML5 webapp\nbased](https://pipeline.loni.usc.edu/products-services/webapp/)\ngraphical user interfaces provide access to a powerful 4,000 core grid\ncompute server.\n\n![](http://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/images/PPMI_BDDS_GSA_PipeProtocol.png)\n\n## Examples of data repositories, archives and services\n\nThere are many sources of data available on the Internet. A number of\nthem provide open-access to the data based on FAIR (Findable,\nAccessible, Interoperable, Reusable) principles. Below are examples of\nopen-access data sources that can be used to test the techniques\npresented in the textbook. We demonstrate the tasks of retrieval,\nmanipulation, processing, analytics and visualization using example\ndatasets from these archives.\n\n-   [SOCR Wiki Data](https://wiki.socr.umich.edu/index.php/SOCR_Data)\n-   [SOCR Canvas\n    datasets](https://umich.instructure.com/courses/38100/files/folder/data)\n-   [SOCR\n    Case-Studies](https://umich.instructure.com/courses/38100/files/folder/Case_Studies)\n-   [XNAT](https://central.xnat.org/)\n-   [IDA](https://ida.loni.usc.edu/)\n-   [NIH dbGaP](https://dbgap.ncbi.nlm.nih.gov/)\n-   [Data.gov](https://data.gov/)\n\n## Responsible Data Science and Ethical Predictive Analytics\n\nIn addition to being data-literate and skilled artisans, all data\nscientists, quantitative analysts, and informaticians need to be aware\nof certain global societal norms and exhibit professional work ethics\nthat ensure the appropriate use, result reproducibility, unbiased\nreporting, as well as expected and unanticipated interpretations of\ndata, analytical methods, and novel technologies. Examples of this basic\netiquette include (1) promoting FAIR (findable, accessible,\ninteroperable, reusable, and reproducible resource) sharing principles,\n(2) ethical conduct of research, (3) balancing and explicating potential\nbenefits and probable detriments of findings, (4) awareness of relevant\nlegislation, codes of practice, and respect for privacy, security and\nconfidentiality of sensitive data, and (5) document provenance,\nattributions and longevity of resources.\n\n### Promoting FAIR resource sharing\n\nThe FAIR (findable, accessible, interoperable, reusable, and\nreproducible) resource sharing principles provide guiding essentials for\nappropriate development, deployment, use, management, and stewardship of\ndata, techniques, tools, services, and information dissemination.\n\n### Research ethics\n\nEthical data science and predictive analytics research demands\nresponsible scientific conduct and integrity in all aspects of practical\nscientific investigation and discovery. All analysts should be aware of,\nand practice, established professional norms and ethical principles in\nplanning, designing, implementing, executing, and assessing activities\nrelated to data-driven scientific research.\n\n### Understanding the benefits and detriments of analytical findings\n\nEvidence and data-driven discovery is often bound to generate both\nquestions and answers, some of which may be unexpected, undesired, or\ndetrimental. Quantitative analysts are responsible for validating all\ntheir results, as well as for balancing and explicating all potential\nbenefits and enumerating all probable detriments of positive and\nnegative findings.\n\n### Regulatory and practical issues in handling sensitive data\n\nDecisions on security, privacy, and confidentiality of sensitive data\ncollected and managed by data governors, manipulated by quantitative\nanalysts, and interpreted by policy and decision makers is not trivial.\nThe large number of people, devices, algorithms, and services that are\nwithin arms-length of the raw data suggests a multi-tier approach for\nsensible protection and security of sensitive information like personal\nhealth, biometric, genetic, and proprietary data. Data security,\nprivacy, and confidentiality of sensitive information should always be\nprotected throughout the data life cycle. This may require preemptive,\non-going, and post-hoc analyses to identify and patch potential\nvulnerabilities. Often, there may be tradeoffs between data value\nbenefits and potential risks of blind automated information\ninterpretation. Neither of the extremes is practical, sustainable, or\neffective.\n\n### Resource provenance and longevity\n\nThe digitalization of human experiences, the growth of data science, and\nthe promise of artificial intelligence have led to enormous societal\ninvestments, excitements, and anxieties. There is a strong sentiment and\nanticipation that the vast amounts of available information will\nubiquitously translate into quick insights, useful predictions, optimal\nrisk-estimations, and cost-effective decisions. Proper recording of the\ndata, algorithmic, scientific, computational, and human factors involved\nin these forecasts represents a critical component of data science and\nits essential contributions to knowledge.\n\n### Examples of inappropriate, fake, or malicious use of resources\n\nEach of the complementary spaces of *appropriate* and *inappropriate*\nuse of data science and predictive analytics resources are vast. The\nsections above outlined some of the guiding principles for ethical,\nrespectful, appropriate, and responsible data-driven analytics and\nfactual reporting. Below are some examples illustrating inappropriate\nuse of data, resources, information, or knowledge to intentionally or\nunintentionally gain unfair advantage, spread fake news, misrepresent\nfindings, or detrimental socioeconomic effects.\n\n-   Attempts to re-identify sensitive information or circumvent\n    regulatory policies, common sense norms, or agreements. For\n    instance, [Big data and advanced analytics were employed to\n    re-identify the Massachusetts Governor William Weld's medical\n    record](https://fpf.org/wp-content/uploads/The-Re-identification-of-Governor-Welds-Medical-Information-Daniel-Barth-Jones.pdf)\n    using openly released insurance dataset stripped of direct personal\n    identifiers.\n-   Calibrated analytics that report findings relative to status-quo\n    alternatives and level setting expected and computed inference in\n    the context of the application domain. For example, ignoring placebo\n    effects, methodological assumptions, potential conflicts and biases,\n    randomization of unknown effects, and other strategies may\n    significantly impact the efficacy of data-driven studies.\n-   Unintended misuse of resource access may be common practice. In\n    2014, an [Uber employee ignored the company's policies and used his\n    access to track the location of a\n    journalist](https://nypost.com/2017/08/15/uber-settles-federal-probe-over-god-view-spy-software/)\n    who was delayed for an Uber interview. Obviously, tracking people\n    without their explicit consent is unethical, albeit it represents an\n    innovative use of the available technology to answer a good\n    question.\n-   Gaming the system for personal gain represents an intentional misuse\n    of resources. Insider trading and opportunistic wealth management\n    represent such examples. In 2015, an [analyst at Morgan Stanley\n    inappropriately downloaded 10% of their account\n    data](https://www.reuters.com/article/us-morgan-stanley-data/morgan-stanley-says-wealth-management-employee-stole-client-data-idUSKBN0KE1AY20150106),\n    which was used for personal enrichment.\n-   There are plenty of examples of [misuse of analytical strategies to\n    fake the results or strengthen a point beyond the observed\n    evidence](https://en.wikipedia.org/wiki/Misuse_of_statistics) and\n    [inappropriate use of information and advanced\n    technology](https://www.bbc.com/bitesize/guides/zt8qtfr/).\\\n-   Big data is naturally prone to *innocent errors*, e.g., selection\n    bias, methodological development and applications, computational\n    processing, empirical estimation instability, misunderstanding of\n    data formats and metadata understanding, as well as *malicious\n    manipulations*.\n-   Collecting, managing and processing irrelevant Big Data may yield\n    unnecessary details, skew the understanding of the phenomenon, or\n    distract from the main discoveries. In these situations, there may\n    be substantial socioeconomic costs, as well as negative returns\n    associated with lost opportunities.\n\n## DSPA Expectations\n\nThe heterogeneity of data science makes it difficult to identify a\ncomplete and exact list of prerequisites necessary to succeed in\nlearning all the appropriate methods. However, the reader is strongly\nencouraged to glance over the [preliminary\nprerequisites](https://www.socr.umich.edu/people/dinov/courses/DSPA_Prereqs.html),\nthe [self-assessment pretest and remediation\nmaterials](https://www.socr.umich.edu/people/dinov/courses/DSPA_Pretest.html),\nand the [outcome\ncompetencies](https://www.socr.umich.edu/people/dinov/courses/DSPA_Competencies.html).\nThroughout this journey, it is useful to *remember the following\npoints*:\n\n-   You *don't have to* satisfy all prerequisites, be versed in all\n    mathematical foundations, have substantial statistical analysis\n    expertise, or be an experienced programmer.\n-   You *don't have to complete all chapters and sections* in the order\n    they appear in the [DSPA Topics\n    Flowchart](https://www.socr.umich.edu/people/dinov/courses/DSPA_Topics.html).\n    Completing one, or several of the [suggested\n    pathways](https://www.socr.umich.edu/people/dinov/courses/DSPA_FlowChart.html)\n    may be sufficient for many readers.\n-   The *DSPA textbook aims* to expand the trainees' horizons, improve\n    the understanding, enhance the skills, and provide a set of\n    advanced, validated, and practice-oriented code, scripts, and\n    protocols.\n-   To varying degrees, readers will develop abilities to skillfully\n    utilize the **tool chest** of resources provided in the DSPA\n    textbook. These resources can be revised, improved, customized and\n    applied to other biomedicine and biosocial studies, as well as to\n    Big Data predictive analytics challenges in other disciplines.\n-   The DSPA *materials will challenge most readers*. When going gets\n    tough, seek help, engage with fellow trainees, search for help on\n    the web, communicate via DSPA discussion forum/chat, review\n    references and supplementary materials. Be proactive! Remember you\n    will gain, but it will require commitment, prolonged immersion, hard\n    work, and perseverance. If it were easy, its value would be\n    compromised.\n-   When covering some chapters, few readers may be *underwhelmed or\n    bored*. If you are familiar with certain topics, you can skim over\n    the corresponding chapters/sections and move forward to the next\n    topic. Still, it's worth reviewing some of the examples and trying\n    the assignment problems to ensure you have a firm grasp of the\n    material and your technical abilities are sound.\n-   Although the *return on investment* (e.g., time, effort) may vary\n    between readers. Those that complete the DSPA textbook will discover\n    something new, acquire some advanced skills, learn novel data\n    analytic protocols, or conceive of a cutting-edge idea.\n-   The complete `R` code (R markdown) for all examples and\n    demonstrations presented in the textbook are available as\n    [electronic supplement](https://dspa2.predictive.space/).\n-   The instructor acknowledges that these *materials may be improved*.\n    If you discover problems, typos, errors, inconsistencies, or other\n    problems, please contact us (`DSPA.info @ umich.edu`) to correct,\n    expand, or polish the resources, accordingly. If you have\n    alternative ideas, suggestions for improvements, optimized code,\n    interesting data and case-studies, or any other refinements, please\n    send these along, as well. All suggestions and critiques will be\n    carefully reviewed and potentially incorporated in revisions and new\n    editions.",
      "word_count": 3245
    },
    {
      "title": "Foundations of R",
      "content": "In this section, we will start with the foundations of `R` programming\nfor visualization, statistical computing and scientific inference.\nSpecifically, we will (1) discuss the rationale for selecting `R` as a\ncomputational platform for all DSPA demonstrations; (2) present the\nbasics of installing shell-based `R` and RStudio user-interface, (3)\nshow some simple `R` commands and scripts (e.g., translate long-to-wide\ndata format, data simulation, data stratification and subsetting), (4)\nintroduce variable types and their manipulation; (5) demonstrate simple\nmathematical functions, statistics, and matrix operators; (6) explore\nsimple data visualization; and (7) introduce optimization and model\nfitting. The chapter appendix includes references to `R` introductory\nand advanced resources, as well as a primer on debugging.\n\n## Why use `R`?\n\nThere are many different classes of software that can be used for data\ninterrogation, modeling, inference and statistical computing. Among\nthese are `R`, Python, Java, C/C++, Perl, and many others. The table\nbelow compares `R` to various other statistical analysis software\npackages and [more detailed comparison is available\nonline](https://en.wikipedia.org/wiki/Comparison_of_statistical_packages).\n\nThere exist substantial differences between different types of\ncomputational environments for data wrangling, preprocessing, analytics,\nvisualization and interpretation. The table below provides some rough\ncomparisons between some of the most popular data computational\nplatforms. With the exception of *ComputeTime*, higher scores represent\nbetter performance within the specific category. Note that these are\njust estimates and the scales are not normalized between categories.\n\n-   [UCLA Stats Software\n    Comparison](http://stats.idre.ucla.edu/other/mult-pkg/whatstat/)\n-   [Wikipedia Stats Software\n    Comparison](https://en.wikipedia.org/wiki/Comparison_of_statistical_packages)\n-   [NASA Comparison of Python, Julia, R, Matlab and\n    IDL](https://modelingguru.nasa.gov/docs/DOC-2625).\n\nLet's first look at some real peer-review publication data (1995-2015),\nspecifically comparing all published scientific reports utilizing `R`,\n`SAS` and `SPSS`, as popular tools for data manipulation and statistical\nmodeling. These data are retrieved using [GoogleScholar literature\nsearches](https://scholar.google.com).\n\n\nWe can also look at a [dynamic Google Trends\nmap](https://trends.google.com/trends/explore?date=all&q=%2Fm%2F0212jm,%2Fm%2F018fh1,%2Fm%2F02l0yf8),\nwhich provides longitudinal tracking of the number of web-searches for\neach of these three statistical computing platforms (`R`, SAS, SPSS).\nThe figure below shows one example of the evolving software interest\nover the past 15 years. You can [expand this plot by modifying the trend\nterms, expanding the search phrases, and changing the time\nperiod](https://trends.google.com/trends/explore?date=all&q=%2Fm%2F0212jm,%2Fm%2F018fh1,%2Fm%2F02l0yf8).\nStatic 2004-2018 monthly data of popularity of `SAS`, `SPSS`, and `R`\nprogramming Google searches is saved in this file\n[GoogleTrends_Data_R_SAS_SPSS_Worldwide_2004_2018.csv](https://umich.instructure.com/courses/38100/files/folder/data).\n\nThe example below shows a dynamic pull of $\\sim20$ years of Google\nqueries about `R`, `SAS`, `SPSS`, and `Python`, traced between\n`2004-01-01` and `2023-06-16`.\n\n\n\n## Getting started with `R`\n\n### Install Basic Shell-based `R`\n\n`R` is a free software that can be installed on any computer. The 'R'\nwebsite is <https://R-project.org>. There you can install a shell-based\n`R`-environment following this protocol:\n\n-   click download CRAN in the left bar\n-   choose a download site\n-   choose your operating system (e.g., Windows, Mac, Linux)\n-   select *base*\n-   choose the latest version to Download `R` (4.3, or higher (newer)\n    version for your specific operating system, e.g., Windows, Linux,\n    MacOS).\n\n### GUI based `R` Invocation (RStudio)\n\nFor many readers, its best to also install and run `R` via *RStudio*\ngraphical user interface. To install RStudio, go to:\n<https://www.rstudio.org/> and do the following:\n\n-   click Download RStudio\n-   click Download RStudio Desktop\n-   click Recommended For Your System\n-   download the appropriate executable file (e.g., .exe) and run it\n    (choose default answers for all questions).\n\n### RStudio GUI Layout\n\nThe RStudio interface consists of several windows.\n\n-   *Bottom left*: console window (also called command window). Here you\n    can type simple commands after the \"\\>\" prompt and `R` will then\n    execute your command. This is the most important window, because\n    this is where `R` actually does stuff.\n-   *Top left*: editor window (also called script window). Collections\n    of commands (scripts) can be edited and saved. When you don't get\n    this window, you can open it with File \\> New \\> `R` script. Just\n    typing a command in the editor window is not enough, it has to get\n    into the command window before `R` executes the command. If you want\n    to run a line from the script window (or the whole script), you can\n    click Run or press CTRL+ENTER to send it to the command window.\n-   *Top right*: workspace / history window. In the workspace window,\n    you can see which data and values `R` has in its memory. You can\n    view and edit the values by clicking on them. The history window\n    shows what has been typed before.\n-   *Bottom right*: files / plots / packages / help window. Here you can\n    open files, view plots (also previous plots), install and load\n    packages or use the help function.\n\nYou can change the size of the windows by dragging the gray bars between\nthe windows.\n\n### Software Updates\n\nUpdating and upgrading the `R` environment involves a three-step\nprocess:\n\n-   *Updating the R-core*: This can be accomplished either by manually\n    downloading and installing the [latest version of `R` from\n    CRAN](https://cran.r-project.org/) or by auto-upgrading to the\n    latest version of `R` using the `R` `installr` package. Type this in\n    the `R` console:\n    `install.packages(\"installr\"); library(installr); updateR()`,\n-   *Updating RStudio*: This installs new versions of\n    [RStudio](https://www.rstudio.com) using RStudio itself. Go to the\n    `Help` menu and click *Check for Updates*, and\n-   *Updating `R` libraries*: Go to the `Tools` menu and click *Check\n    for Package Updates...*.\n\nJust like any other software, services, or applications, these `R`\nupdates should be done regularly; preferably monthly or at least\nsemi-annually.\n\n### (Optional) Install `Quarto`\n\n[Quarto](https://quarto.org/) is a multi-language *next-gen R Markdown*\nfrom [Posit](https://posit.co/), the rebranded Public Benefit\nCorporation [RStudio](https://posit.co/about/). *Quarto* includes new\nfeatures and capabilities expanding existing *Rmd* files without further\nmodification. It's recommended, but not required, to install\n[Quarto](https://quarto.org/docs/get-started/), after building `R` and\n`RStudio` GUI. We still *edit* code and markdown in the RStudio IDE,\njust as we normally do with any Rmd computational protocol, as well as\n*preview* the rendered document in the *Viewer* tab dynamically.\n\nThe Quarto markdown documents have the *.qmd* extension, as opposed to\nthe classical R markdown extension (*.rmd*). Once knitted, the *.qmd*\nsource can be rendered into many different formats, PDF, MS Word, HTML5,\netc.\n\n![](https://quarto.org/docs/get-started/hello/images/rstudio-hello.png)\nQuarto allows including *executable expressions within markdown text* by\nenclosing code in `r ` expressions. For instance, we can use inline code to \nreport dynamically in the text the number of observations in a dataset, e.g.,\nthe dimensions of the *mpg* dataframe are `r dim(mpg)`.\n\nManual creation of a new *qmd* document is accomplished by mouse-clicking\n$File\\to New\\ File\\to Quarto Document$ or by using the command palette (shortcut *Ctrl+Shift+P*), search for *Create a new Quarto document* and hit return.\n\nQuarto includes native support for [Observable JS](https://quarto.org/docs/interactive/ojs/), a set of enhancements to raw JavaScript, which provides *reactive runtime* useful for interactive data exploration and analysis. [Observable JS (OJS) supports a hosted service](https://observablehq.com/) for creating and publication of Rmd/Qmd/Pyton notebooks. OJS works in any Quarto document (Rmd, Jupyter, and Knitr documents) via an `{ojs}` executable code block. \n\nThis [Posit Quarto video](https://www.youtube.com/watch?v=TnVgHE9LAiw) and the\n[acompaning QMD slidedeck](https://thomasmock.quarto.pub/python-umbrella/) offer insights into the incredible power of markdown and\ninteractive content integration across multiple programming languages.\n\n\n### Some notes\n\n-   The basic `R` environment installation comes with limited core\n    functionality. Everyone eventually will have to install more\n    packages, e.g., `reshape2`, `ggplot2`, and we will show how to\n    [expand your Rstudio\n    library](https://support.rstudio.com/hc/en-us/categories/200035113-Documentation)\n    throughout these materials.\n-   The core `R` environment also has to be upgraded occasionally, e.g.,\n    every 3-6 months to get `R` patches, to fix known problems, and to\n    add new functionality. This is also [easy to\n    do](https://cran.r-project.org/bin/).\n-   The assignment operator in `R` is `<-` (although `=` may also be\n    used), so to assign a value of $2$ to a variable $x$, we can use\n    `x <- 2` or equivalently `x = 2`.\n\n### Help\n\n`R` provides documentation for different `R` functions using the method\n`help()`. Typing `help(topic)` in the `R` console will provide detailed\nexplanations for each `R` topic or function. Another way of doing it is\nto call `?topic`, which is even easier.\n\nFor example, if I want to check the function for linear models (i.e.\nfunction `lm()`), I will use the following function.\n\n\n### Simple Wide-to-Long Data format translation\n\nBelow is a simple `R` script for melting a small dataset that\nillustrates the `R` syntax for variable definition, instantiation,\nfunction calls, and parameter setting.\n\n\nThere are specific options for the `reshape2::melt()` function, from the\n`reshape2` `R` package, that control the transformation of the original\n(wide-format) dataset `rawdata_wide` into the modified (long-format)\nobject `data_long`.\n\n\nFor an elaborate justification, detailed description, and multiple\nexamples of handling long-and-wide data, messy and tidy data, and data\ncleaning strategies see the [JSS `Tidy Data` article by Hadley\nWickham](https://www.jstatsoft.org/article/view/v059i10).\n\n### Data generation\n\nPopular data generation functions include `c()`, `seq()`, `rep()`, and\n`data.frame()`. Sometimes we use `list()` and `array()` to create data\ntoo.\n\n**c()**\n\n`c()` creates a (column) vector. With option `recursive=T`, it descends\nthrough lists combining all elements into one vector.\n\n\nWhen combined with `list()`, `c()` successfully created a vector with\nall the information in a list with three members `A`, `B`, and `C`.\n\n**seq(from, to)**\n\n`seq(from, to)` generates a sequence. Adding option `by=` can help us\nspecifies increment; Option `length=` specifies desired length. Also,\n`seq(along=x)` generates a sequence `1, 2, ..., length(x)`. This is used\nfor loops to create ID for each element in `x`.\n\n\n**rep(x, times)**\n\n`rep(x, times)` creates a sequence that repeats `x` a specified number\nof times. The option `each=` also allows us to repeat first over each\nelement of `x` a certain number of times.\n\n\nCompare this to replicating using `replicate()`.\n\n\n**data.frame()**\n\nThe function `data.frame()` creates a data frame object of named or\nunnamed arguments. We can combine multiple vectors of different types\ninto data frames with each vector stored as a column. Shorter vectors\nare automatically wrapped around to match the length of the longest\nvectors. With `data.frame()` you can mix numeric and characteristic\nvectors.\n\n\nNote that the operator `:` generates a sequence and the expression `1:4`\nyields a vector of integers, from $1$ to $4$.\n\n**list()**\n\nMuch like the column function `c()`, the function `list()` creates a\n*list* of the named or unnamed arguments - indexing rule: from $1$ to\n$n$, including $1$ and $n$. Remember that in `R` indexing of vectors,\nlists, arrays and tensors starts at $1$, not $0$, as in some other\nprogramming languages.\n\n\nAs `R` uses general *objects* to represent different constructs, object\nelements are accessible via `$`, `@`, `.`, and other delimiters,\ndepending on the object type. For instance, we can refer to a member $a$\nand index $i$ in the list of objects $l$ containing an element $a$ by\n`l$a[[i]]` . For example,\n\n\n**array(x, dim=)**\n\n`array(x, dim=)` creates an array with specific dimensions. For example,\n`dim=c(3, 4, 2)` means two 3x4 matrices. We use `[]` to extract specific\nelements in the array. `[2, 3, 1]` means the element at the 2nd row 3rd\ncolumn in the 1st page. Leaving one number in the dimensions empty would\nhelp us to get a specific row, column or page. `[2, ,1]` means the\nsecond row in the 1st page. See this image:\n![](http://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/images/R_MatrixArrays_Diagram.png):\n\n\n-   In General, multi-dimensional arrays are called \"tensors\" (of\n    order=number of dimensions).\n\nOther useful functions are:\n\n-   `matrix(x, nrow=, ncol=)`: creates matrix elements of `nrow` rows\n    and `ncol` columns.\n-   `factor(x, levels=)`: encodes a vector `x` as a factor.\n-   `gl(n, k, length=n*k, labels=1:n)`: generate levels (factors) by\n    specifying the pattern of their levels. *k* is the number of levels,\n    and *n* is the number of replications.\n-   `expand.grid()`: a data frame from all combinations of the supplied\n    vectors or factors.\n-   `rbind()` combine arguments by rows for matrices, data frames, and\n    others\n-   `cbind()` combine arguments by columns for matrices, data frames,\n    and others\n\n### Input/Output (I/O)\n\nThe first pair of functions we will talk about are `save()` and\n`load()`, which write and import objects between the current `R`\nenvironment RAM memory and long term storage, e.g., hard drive, Cloud\nstorage, SSD, etc. The script below demonstrates the basic export and\nimport operations with simple data. Note that we saved the data in\n`Rdata` (`Rda`) format.\n\n\nThere are two basic functions `data(x)` and `library(x)` that load\nspecified data sets and `R` packages, respectively. The `R` *base*\nlibrary is always loaded by default. However, add-on libraries need to\nbe *installed* first and then *imported* (loaded) in the working\nenvironment before functions and objects in these libraries are\naccessible.\n\n\n**read.table(file)** reads a file in table format and creates a data\nframe from it. The default separator `sep=\"\"` is any whitespace. Use\n`header=TRUE` to read the first line as a header of column names. Use\n`as.is=TRUE` to prevent character vectors from being converted to\nfactors. Use `comment.char=\"\"` to prevent `\"#\"` from being interpreted\nas a comment. Use `skip=n` to skip `n` lines before reading data. See\nthe help for options on row naming, NA treatment, and others.\n\nThe example below uses `read.table()` to parse and load an ASCII text\nfile containing a simple dataset, which is available on the [supporting\ncanvas data archive](https://umich.instructure.com/courses/38100/files).\n\n\nWhen using `R` to access (read/write) data on a Cloud web service, like\n[Instructure/Canvas](https://umich.instructure.com/courses/38100/files/folder/Case_Studies)\nor GoogleDrive/GDrive, mind that the direct URL reference to the raw\nfile will be different from the URL of the pointer to the file that can\nbe rendered in the browser window. For instance,\n\n-   This [GDrive TXT file, *1Zpw3HSe-8HTDsOnR-n64KoMRWYpeBBek*\n    (01a_data.txt)](https://drive.google.com/open?id=1Zpw3HSe-8HTDsOnR-n64KoMRWYpeBBek),\n-   [Can be downloaded and ingested in `R` via this separate\n    URL](https://drive.google.com/uc?export=download&id=1Zpw3HSe-8HTDsOnR-n64KoMRWYpeBBek).\n-   While the file reference is unchanged\n    (*1Zpw3HSe-8HTDsOnR-n64KoMRWYpeBBek*), note the change of syntax\n    from viewing the file in the browser, **open?id=**, to\n    auto-downloading the file for `R` processing,\n    **uc?export=download&id=**.\n\n\n**read.csv(\"filename\", header=TRUE)** is identical to `read.table()` but\nwith defaults set for reading comma-delimited files.\n\n\n**read.delim(\"filename\", header=TRUE)** is very similar to the first\ntwo. However, it has defaults set for reading tab-delimited files.\n\nAlso we have\n`read.fwf(file, widths, header=FALSE, sep=\"\\t\", as.is=FALSE)` to read a\ntable of fixed width formatted data into a data frame.\n\n**match(x, y)** returns a vector of the positions of (first) matches of\nits first argument in its second. For a specific element in `x` if no\nelement matches it in `y`, then the output would be `NA`.\n\n\n**save.image(file)** saves all objects in the current workspace.\n\n**write.table**(x, file=\"\", row.names=TRUE, col.names=TRUE, sep=\"\")\nprints x after converting to a data frame and stores it into a specified\nfile. If `quote` is TRUE, character or factor columns are surrounded by\nquotes (\"). `sep` is the field separator. `eol` is the end-of-line\nseparator. `na` is the string for missing values. Use `col.names=NA` to\nadd a blank column header to get the column headers aligned correctly\nfor spreadsheet input.\n\nMost of the I/O functions have a file argument. This can often be a\ncharacter string naming a file or a connection. `file=\"\"` means the\nstandard input or output. Connections can include files, pipes, zipped\nfiles, and `R` variables.\n\nOn windows, the file connection can also be used with\n`description = \"clipboard\"`. To read a table copied from Excel, use\n`x <- read.delim(\"clipboard\")`\n\nTo write a table to the clipboard for Excel, use\n`write.table(x, \"clipboard\", sep=\"\\t\", col.names=NA)`\n\nFor database interaction, see packages RODBC, DBI, RMySQL, RPgSQL, and\nROracle, as well as packages XML, hdf5, netCDF for reading other file\nformats. We will talk about some of them in later chapters.\n\n*Note*, an alternative library called `rio` handles import/export of\nmultiple data types with simple syntax.\n\n### Slicing and extracting data\n\nThe following table summarizes the basic vector indexing operations.\n\nIndexing lists are similar but not identical to indexing vectors.\n\nIndexing for *matrices* and higher dimensional *arrays* (*tensors*)\nderive from vector indexing.\n\n### Variable conversion\n\nThe following functions represent simple examples of convert data types:\n\n`as.array(x)`, `as.data.frame(x)`, `as.numeric(x)`, `as.logical(x)`,\n`as.complex(x)`, `as.character(x)`, ...\n\nTyping `methods(as)` in the console will generate a complete list for\nvariable conversion functions.\n\n### Variable information\n\nThe following functions verify if the input is of a specific data type:\n\n`is.na(x)`, `is.null(x)`, `is.array(x)`, `is.data.frame(x)`,\n`is.numeric(x)`, `is.complex(x)`, `is.character(x)`, ...\n\nFor a complete list, type `methods(is)` in the `R` console. The outputs\nfor these functions are objects, either single values (`TRUE` or\n`FALSE`), or objects of the same dimensions as the inputs containing a\nBoolean `TRUE` or `FALSE` element for each entry in the dataset.\n\n**length(x)** gives us the number of elements in `x`.\n\n\n**dim(x)** retrieves or sets the dimension of an array and **length(y)**\nreports the length of a list or a vector.\n\n\n**dimnames(x)** retrieves or sets the dimension names of an object. For\nhigher dimensional objects like matrices or arrays we can combine\n`dimnames()` with a list.\n\n\n**nrow(x)** and **ncol(x)** report the number of rows and number of\ncolumns or a matrix.\n\n\n**class(x)** gets or sets the class of $x$. Note that we can use\n`unclass(x)` to remove the class attribute of $x$.\n\n\n**attr(x, which)** gets or sets the attribute `which` of $x$.\n\n\nThe above script shows that applying `unclass` to $x$ sets its class to\n`NULL`.\n\n**attributes(obj)** gets or sets the list of *attributes* of an object.\n\n\n### Data selection and manipulation\n\nIn this section, we will introduce some data manipulation functions. In\naddition, tools from `dplyr` provide easy dataset manipulation routines.\n\n**which.max(x)** returns the index of the greatest element (max) of $x$,\n**which.min(x)** returns the index of the smallest element (min) of $x$,\nand **rev(x)** reverses the elements of $x$.\n\n\n**sort(x)** sorts the elements of $x$ in increasing order. To sort in\ndecreasing order we can use `rev(sort(x))`.\n\n\n**cut(x, breaks)** divides $x$ into intervals with the same length\n(sometimes factors). The optional parameter `breaks` specifies the\nnumber of cut intervals or a vector of cut points. `cut` divides the\nrange of $x$ into intervals coding the values in $x$ according to the\nintervals they fall into.\n\n\n**which(x == a)** returns a vector of the indices of $x$ if the\ncomparison operation is true. For example, it returns the value $i$, if\n$x[i]== a$ is TRUE. Thus, the argument of this function (like `x==a`)\nmust be a Boolean variable.\n\n\n**na.omit(x)** suppresses the observations with missing data (`NA`). It\nsuppresses the corresponding line if $x$ is a matrix or a data frame.\n**na.fail(x)** returns an error message if $x$ contains at least one\n`NA`.\n\n\n**unique(x)** If $x$ is a vector or a data frame, it returns a similar\nobject suppressing the duplicate elements.\n\n\n**table(x)** returns a table with the different values of $x$ and their\nfrequencies (typically used for integer or factor variables). The\ncorresponding **prop.table()** function transforms these raw frequencies\nto relative frequencies (proportions, marginal mass).\n\n\n**subset(x, ...)** returns a selection of $x$ with respect to the\nspecified criteria `...`. Typically `...` are comparisons like\n`x$V1 < 10`. If $x$ is a data frame, the option `select=` allows using a\nnegative sign $-$ to indicate values to keep or drop from the object.\n\n\n**sample(x, size)** resamples randomly, without replacement, *size*\nelements in the vector $x$. The option `replace = TRUE` allows\nresampling with replacement.\n\n\n## Mathematics, Statistics, and Optimization\n\nMany mathematical functions, statistical summaries, and function\noptimizers will be discussed throughout the book. Below are the very\nbasic functions to keep in mind.\n\n### Math Functions\n\nBasic math functions like `sin`, `cos`, `tan`, `asin`, `acos`, `atan`,\n`atan2`, `log`, `log10`, `exp` and \"set\" functions `union(x, y)`,\n`intersect(x, y)`, `setdiff(x, y)`, `setequal(x, y)`,\n`is.element(el, set)` are available in R.\n\n`lsf.str(\"package:base\")` displays all base functions built in a\nspecific `R` package (like `base`).\n\nThis table summarizes the core functions for most basic `R` for\ncalculations.\n\nNote: many math functions have a logical parameter `na.rm=TRUE` to\nspecify missing data (`NA`) removal.\n\n### Matrix Operations\n\nThe following table summarizes basic operation functions. We will\ndiscuss this topic in detail in [Chapter 3 (Linear Algebra, Matrix\nComputing, and Regression\nModeling)](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html).\n\n\n### Optimization and model fitting\n\n-   **optim(par, fn, method = c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\",\n    \"SANN\"))** general-purpose optimization; `par` is initial values,\n    `fn` is a function to optimize (normally minimize).\n-   **nlm(f, p)** minimize function fusing a Newton-type algorithm with\n    starting values p.\n-   **lm(formula)** fit linear models; `formula` is typically of the\n    form `response ~ termA + termB + ...`; use `I(x*y) + I(x^2)` for\n    terms made of nonlinear components.\n-   **glm(formula, family=)** fit generalized linear models, specified\n    by giving a symbolic description of the linear predictor and a\n    description of the error distribution; `family` is a description of\n    the error distribution and link function to be used in the model;\n    see `?family`.\n-   **nls(formula)** nonlinear least-squares estimates of the nonlinear\n    model parameters.\n-   **approx(x, y=)** linearly interpolate given data points; $x$ can be\n    an $xy$ plotting structure.\n-   **spline(x, y=)** cubic spline interpolation.\n-   **loess(formula)** (locally weighted scatterplot smoothing) fit a\n    polynomial surface using local fitting.\n\nMany of the formula-based modeling functions have several common\narguments:\n\n`data=` the data frame for the formula variables, `subset=` a subset of\nvariables used in the fit, `na.action=` action for missing values:\n`\"na.fail\"`, `\"na.omit\"`, or a function.\n\nThe following generics often apply to model fitting functions:\n\n-   `predict(fit, ...)` predictions from fit based on input data.\n-   `df.residual(fit)` returns the number of residual degrees of\n    freedom.\n-   `coef(fit)` returns the estimated coefficients (sometimes with their\n    standard-errors).\n-   `residuals(fit)` returns the residuals.\n-   `deviance(fit)` returns the deviance.\n-   `fitted(fit)` returns the fitted values.\n-   `logLik(fit)` computes the logarithm of the likelihood and the\n    number of parameters.\n-   `AIC(fit)` computes the Akaike information criterion (AIC).\n\n### Statistics\n\n-   **aov(formula)** analysis of variance model.\n-   **anova(fit, ...)** analysis of variance (or deviance) tables for\n    one or more fitted model objects.\n-   **density(x)** kernel density estimates of x.\n\nOther functions include: `binom.test()`, `pairwise.t.test()`,\n`power.t.test()`, `prop.test()`, `t.test()`, ... use\n`help.search(\"test\")` to see details.\n\n### Distributions\n\nThe [Probability Distributome Project](http://distributome.org) provides\nmany details about univariate probability distributions. The [SOCR R\nShiny Distribution\nCalculators](https://rcompute.nursing.umich.edu/DistributionCalculator/)\nand the [SOCR Bivariate and Trivariate Interactive Graphical\nCalculators](https://socr.umich.edu/HTML5/BivariateNormal/) provide\nadditional demonstrations of multivariate probability distribution.\n\nIn `R`, there are four complementary functions supporting each\nprobability distribution. For *Normal distribution*, these four\nfunctions are `dnorm()` - density, `pnorm()` - distribution function,\n`qnorm()` - quantile function, and `rnorm()` - random generating\nfunction. For *Poisson distribution*, the corresponding functions are\n`dpois()`, `ppois()`, `qpois()`, and `rpois()`.\n\nThe table below shows the invocation syntax for generating random\nsamples from a number of different probability distributions.\n\nObviously, replacing the first letter `r` with `d`, `p` or `q` would\nreference the corresponding probability density (`dfunc(x, ...)`), the\ncumulative probability density (`pfunc(x, ...)`), and the value of\nquantile (`qfunc(p, ...)`, with $0 < p < 1$).\n\n## Advanced Data Processing\n\nIn this section, we will introduce some useful functions that are useful\nin many data analytic protocols. The family of `*apply()` functions act\non lists, arrays, vectors, data frames and other objects.\n\n**apply(X, INDEX, FUN=)** returns a vector or array or list of values\nobtained by applying a function `FUN` to margins (`INDEX=1` means row,\n`INDEX=2` means column) of $X$. Additional options may be specified\nafter the `FUN` argument.\n\n\n**lapply(X, FUN)** applies `FUN` to each member of the list $X$. If $X$\nis a data frame then it will apply the `FUN` to each column and return a\nlist.\n\n\n**tapply(X, INDEX, FUN=)** applies `FUN` to each cell of a ragged array\ngiven by $X$ with indexes equals to `INDEX`. Note that $X$ is an atomic\nobject, typically a vector.\n\n\n**by(data, INDEX, FUN)** applies `FUN` to data frame data subsetted by\nINDEX. In this example, we apply the `sum` function using column 1 (`a`)\nas an index.\n\n\n**merge(a, b)** merges two data frames by common columns or row names.\nWe can use option `by=` to specify the index column.\n\n\n**xtabs(a \\~ b, data=x)** reports specific factorized contingency\ntables. The example below uses the [1973 UC Berkeley admissions\ndataset](https://www.jstor.org/stable/1739581) to report\ngender-by-status breakdown.\n\n\n**aggregate(x, by, FUN)** splits the data frame $x$ into subsets,\ncomputes summary statistics for each part, and reports the results. `by`\nis a list of grouping elements, each of which has the same length as the\nvariables in $x$. For example, we can apply the function `sum` to the\ndata frame `df3` subject to the index created by\n`list(rep(1:3, length=7))`.\n\n\n**stack(x, ...)** transforms data stored as separate columns in a data\nframe, or list, into a single column vector; **unstack(x, ...)** is the\ninverse of `stack()`.\n\n\n**reshape(x, ...)** reshapes a data frame between *wide* format, with\nrepeated measurements in separate columns of the same record, and *long*\nformat, with the repeated measurements in separate records. We can\nspecify the transformation direction, `direction=\"wide\"` or\n`direction=\"long\"`.\n\n\n```{=html}\n<!---",
      "word_count": 4028
    },
    {
      "title": "See: http://stats.idre.ucla.edu/r/faq/how-can-i-reshape-my-data-in-r/",
      "content": "",
      "word_count": 0
    },
    {
      "title": "wiki_url <- read_html(\"https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata\")",
      "content": "#html_nodes(wiki_url, \"#content\")\n#pd_data <- html_table(html_nodes(wiki_url, \"table\")[[1]])\n#head(pd_data); summary(pd_data)\nlong <- reshape(pd_data, \n  varying = \"Time\", \n  v.names = c(\"Time1\", \"Time2\", \"Time3\", \"Time4\")  # colnames(pd_data)[-33],\n  times = \"Time\", \n  direction = \"long\")",
      "word_count": 28
    },
    {
      "title": "(5) the end format for the data (direction)",
      "content": "wide <- reshape(pd_data, \n  timevar = \"Time\",\n  idvar = colnames(pd_data)[-33],\n  direction = \"wide\")",
      "word_count": 12
    },
    {
      "title": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "content": "### (1) *direction* = \"wide\" \n### (2) *timevar*, we indicate the variable that will define the multiple measurements per subject. \n### (3) *idvar* list the variables that should not vary within subject.\n--->\n```\n**Notes**\n\n-   The $x$ in this function has to be longitudinal data.\n-   The call to `rnorm` used in reshape might generate different results\n    for each call, unless `set.seed(1234)` is used to ensure\n    reproducibility of random-number generation.\n\n### Strings\n\nThe following functions are useful for handling strings in `R`.\n\n**paste(...)** and **paste0(...)** concatenate vectors after converting\nthe arguments to a vector of characters. There are several options,\n`sep=` to use a string to separate terms (a single space is the\ndefault), `collapse=` to separate \"collapsed\" results.\n\n\n**substr(x, start, stop)** substrings in a character vector. Using\n`substr(x, start, stop) <- value`, it can also assign values (with the\nsame length) to part of a string.\n\n\nNote that characters at `start` and `stop` indexes are inclusive in the\noutput.\n\n**strsplit(x, split)** splits $x$ according to the substring split. Use\n`fixed=TRUE` for non-regular expressions.\n\n\n**grep(pattern, x)** searches for pattern matches within $x$ and returns\na vector of the indices of the elements of $x$ that had a match. Use\nregular expressions for `pattern`(unless `fixed=TRUE`), see `?regex` for\ndetails.\n\n\n**gsub(pattern, replacement, x)** replaces matching patterns in $x$,\nallowing for use of regular expression matching; **sub()** is the same\nbut it only replaces the first occurrence of the matched pattern.\n\n\n**tolower(x)** converts strings to lowercase and **toupper(x)** converts\nto uppercase.\n\n**match(x, table)** yields a vector of the positions of first matches\nfor the elements of $x$ among `table`, `x %in% table` returns a logical\nvector.\n\n\n**pmatch(x, table)** reports partial matches for the elements of $x$.\n\n**Dates and Times**\n\nThe class `Date` stores calendar dates, without times. `POSIXct()` has\ndates and times, including time zones. Comparisons (e.g. $>$), `seq()`,\nand `difftime()` are useful to compare dates. `?DateTimeClasses` gives\nmore information, see also package `chron`.\n\nThe functions `as.Date(s)` and `as.POSIXct(s)` convert to the respective\nclass; `format(dt)` converts to a string representation. The default\nstring format is `2001-02-21`. These accept a second argument to specify\na format for conversion. Some common formats are:\n\nWhere leading zeros are shown they will be used on output but are\noptional on input. See `?strftime` for details.\n\n## Basic Plotting\n\nThe following functions represent the basic plotting functions in `R`.\nLater, in [Chapter 2 (Visualization &\nEDA)](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Vizualization_EDA.html),\nwe will discuss more elaborate visualization in and exploratory data\nanalytic strategies.\n\n-   **plot(x)** plot of the values of x (on the y-axis) ordered on the\n    x-axis.\n-   **plot(x, y)** bivariate plot of x (on the x-axis) and y (on the\n    y-axis).\n-   **hist(x)** histogram of the frequencies of x.\n-   **barplot(x)** histogram of the values of x. Use `horiz=FALSE` for\n    horizontal bars.\n-   **dotchart(x)** if x is a data frame, plots a Cleveland dot plot\n    (stacked plots line-by-line and column-by-column).\n-   **pie(x)** circular pie-chart.\n-   **boxplot(x)** 'box-and-whiskers' plot.\n-   **sunflowerplot(x, y)** sunflowers plot with multiple leaves\n    ('petals') such that overplotting is visualized instead of\n    accidental and invisible.\n-   **stripplot(x)** plot of the values of x on a line (an alternative\n    to `boxplot()` for small sample sizes).\n-   **coplot(x\\~y \\| z)** bivariate plot of x and y for each value or\n    interval of values of z.\n-   **interaction.plot (f1, f2, y)** if f1 and f2 are factors, plots the\n    means of y (on the y-axis) with respect to the values of f1 (on the\n    x-axis) and of f2 (different curves). The option `fun` allows you to\n    choose the summary statistic of y (by default `fun=mean`).\n-   **matplot(x, y)** bivariate plot of the first column of x vs. the\n    first one of y, the second one of x vs. the second one of y, etc.\n-   **fourfoldplot(x)** visualizes, with quarters of circles, the\n    association between two dichotomous variables for different\n    populations (x must be an array with dim=c(2, 2, k), or a matrix\n    with dim=c(2, 2) if k = 1)\n-   **assocplot(x)** Cohen's Friendly graph shows the deviations from\n    independence of rows and columns in a two dimensional contingency\n    table.\n-   **mosaicplot(x)** \"mosaic\"\" graph of the residuals from a log-linear\n    regression of a contingency table.\n-   **pairs(x)** if x is a matrix or a data frame, draws all possible\n    bivariate plots between the columns of x.\n-   **plot.ts(x)** if x is an object of class \"ts\", plot of x with\n    respect to time, x may be multivariate but the series must have the\n    same frequency and dates. Detailed examples are in **Chapter 17: Big\n    Longitudinal Data Analysis**.\n-   **ts.plot(x)** id. but if x is multivariate the series may have\n    different dates and must have the same frequency.\n-   **qqnorm(x)** quantiles of x with respect to the values expected\n    under a normal law.\n-   **qqplot(x, y)** quantiles of y with respect to the quantiles of x.\n-   **contour(x, y, z)** contour plot (data are interpolated to draw the\n    curves), x and y must be vectors and z must be a matrix so that\n    `dim(z)=c(length(x), length(y))` (x and y may be omitted).\n-   **filled.contour(x, y, z)** areas between the contours are colored,\n    and a legend of the colors is drawn as well.\n-   **image(x, y, z)** plotting actual data with colors.\n-   **persp(x, y, z)** plotting actual data in perspective view.\n-   **stars(x)** if x is a matrix or a data frame, it draws a graph with\n    segments or a star where each row of x is represented by a star and\n    the columns are the lengths of the segments.\n-   **symbols(x, y, ...)** draws, at the coordinates given by x and y,\n    symbols (circles, squares, rectangles, stars, thermometers or\n    \"boxplots\"\") whose sizes, colors... are specified by supplementary\n    arguments.\n-   **termplot(mod.obj)** plot of the (partial) effects of a regression\n    model (`mod.obj`).\n\nThe following parameters are common to many plotting functions:\n\n### QQ Normal probability plot\n\nLet's look at one simple example - quantile-quantile probability plot.\nSuppose $X\\sim N(0,1)$ and $Y\\sim Cauchy$ represent the observed/raw and\nsimulated/generated data for one feature (variable) in the data.\n\n\n### Low-level plotting commands\n\n-   **points(x, y)** adds points (the option `type=` can be used)\n-   **lines(x, y)** id. but with lines\n-   **text(x, y, labels, ...)** adds text given by labels at coordinates\n    (x, y). Typical use: `plot(x, y, type=\"n\"); text(x, y, names)`\n-   **mtext(text, side=3, line=0, ...)** adds text given by text in the\n    margin specified by side (see `axis()` below); line specifies the\n    line from the plotting area.\n-   **segments(x0, y0, x1, y1)** draws lines from points `(x0, y0)` to\n    points `(x1, y1)`\n-   **arrows(x0, y0, x1, y1, angle= 30, code=2)** id. With arrows at\n    points `(x0, y0)`, if `code=2`. The arrow is at point `(x1, y1)`, if\n    `code=1`. Arrows are at both if `code=3`. Angle controls the angle\n    from the shaft of the arrow to the edge of the arrow head.\n-   **abline(a, b)** draws a line of slope `b` and intercept `a`.\n-   **abline(h=y)** draws a horizontal line at ordinate y.\n-   **abline(v=x)** draws a vertical line at abscissa x.\n-   **abline(lm.obj)** draws the regression line given by `lm.obj`.\n    abline(h=0, col=2) #color (col) is often used\n-   **rect(x1, y1, x2, y2)** draws a rectangle whose left, right,\n    bottom, and top limits are x1, x2, y1, and y2, respectively.\n-   **polygon(x, y)** draws a polygon linking the points with\n    coordinates given by x and y.\n-   **legend(x, y, legend)** adds the legend at the point `(x, y)` with\n    the symbols given by `legend`.\n-   **title()** adds a title and optionally a subtitle.\n-   **axis(side, vect)** adds an axis at the bottom (`side=1`), on the\n    left (`side=2`), at the top (`side=3`), or on the right (`side=4`);\n    `vect` (optional) gives the abscissa (or ordinates) where tick-marks\n    are drawn.\n-   **rug(x)** draws the data x on the x-axis as small vertical lines.\n-   **locator(n, type=\"n\", ...)** returns the coordinates `(x, y)` after\n    the user has clicked n times on the plot with the mouse; also draws\n    symbols (`type=\"p\"`) or lines (`type=\"l\"`) with respect to optional\n    graphic parameters (...); by default nothing is drawn (`type=\"n\"`).\n\n### General graphics parameters\n\nThese can be set globally with **par(...)**. Many can be passed as\nparameters to plotting commands.\n\n-   **adj** controls text justification (`adj=0` left-justified,\n    `adj=0.5` centered, `adj=1` right-justified).\n-   **bg** specifies the color of the background (ex. : `bg=\"red\"`,\n    `bg=\"blue\"`, ...the list of the 657 available colors is displayed\n    with `colors()`).\n-   **bty** controls the type of box drawn around the plot. Allowed\n    values are: \"o\", \"l\", \"7\", \"c\", \"u\" ou \"]\" (the box looks like the\n    corresponding character). If `bty=\"n\"` the box is not drawn.\n-   **cex** a value controlling the size of texts and symbols with\n    respect to the default. The following parameters have the same\n    control for numbers on the axes-`cex.axis`, the axis\n    labels-`cex.lab`, the title-`cex.main`, and the subtitle-`cex.sub`.\n-   **col** controls the color of symbols and lines. Use color names:\n    \"red\", \"blue\" see `colors()` or as \"#RRGGBB\"; see `rgb()`, `hsv()`,\n    `gray()`, and `rainbow()`; as for cex there are: `col.axis`,\n    `col.lab`, `col.main`, `col.sub`.\n-   **font** an integer which controls the style of text (1: normal, 2:\n    italics, 3: bold, 4: bold italics); as for cex there are:\n    `font.axis`, `font.lab`, `font.main`, `font.sub`.\n-   **las** an integer which controls the orientation of the axis labels\n    (0: parallel to the axes, 1: horizontal, 2: perpendicular to the\n    axes, 3: vertical).\n-   **lty** controls the type of lines, can be an integer or string (1:\n    \"solid\", 2: \"dashed\", 3: \"dotted\", 4: \"dotdash\", 5: \"longdash\", 6:\n    \"twodash\", or a string of up to eight characters (between \"0\" and\n    \"9\") which specifies alternatively the length, in points or pixels,\n    of the drawn elements and the blanks, for example `lty=\"44\"` will\n    have the same effect than `lty=2`.\n-   **lwd** a numeric which controls the width of lines, default=1.\n-   **mar** a vector of 4 numeric values which control the space between\n    the axes and the border of the graph of the form\n    `c(bottom, left, top, right)`, the default values are\n    `c(5.1, 4.1, 4.1, 2.1)`.\n-   **mfcol** a vector of the form `c(nr, nc)` which partitions the\n    graphic window as a matrix of nr lines and nc columns, the plots are\n    then drawn in columns.\n-   **mfrow** plots are drawn by row-by-row.\n-   **pch** controls the type of symbol, either an integer between 1 and\n    25, or any single character within \"\".\n-   **ts.plot(x)** id. but if x is multivariate the series may have\n    different dates by x and y.\n-   **ps** an integer which controls the size in points of texts and\n    symbols.\n-   **pty** a character, which specifies the type of the plotting\n    region, \"s\": square, \"m\": maximal.\n-   **tck** a value which specifies the length of tick-marks on the axes\n    as a fraction of the smallest of the width or height of the plot; if\n    `tck=1` a grid is drawn.\n-   **tcl** a value which specifies the length of tick-marks on the axes\n    as a fraction of the height of a line of text (by default\n    `tcl=-0.5`).\n-   **xaxt** if `xaxt=\"n\"` the x-axis is set but not drawn (useful in\n    conjunction with `axis(side=1, ...)`).\n-   **yaxt** if `yaxt=\"n\"` the y-axis is set but not drawn (useful in\n    conjunction with `axis(side=2, ...)`).\n\nIn the normal Lattice formula, `y~x|g1*g2`, combinations of optional\nconditioning variables `g1` and `g2` plotted on separate panels. Lattice\nfunctions take many of the same arguments as base graphics plus also\n`data=` the data frame for the formula variables and `subset=` for\nsubsetting. Use `panel=` to define a custom panel function (see\n`apropos(\"panel\")` and `?lines`). Lattice functions return an object of\nclass trellis and have to be printed to produce the graph. Use\n`print(xyplot(...))` inside functions where automatic printing doesn't\nwork. Use `lattice.theme` and `lset` to change Lattice defaults.\n\n## Basic `R` Programming\n\nThe standard setting for our **own function** is:\n\n`function.name<-function(x) {` `expr(an expression)` `return(value)` `}`\n\nWhere $x$ is the parameter in the expression. A simple example of this\nis:\n\n\n**Conditions setting**: `if(cond) {expr}` or\n`if(cond) cons.expr else alt.expr`.\n\n\nAlternatively, `ifelse` represents a vectorized and extremely efficient\nconditional mechanism that provides one of the main advantages of `R`.\n\n**For loop**: `for(var in seq) expr`.\n\n\n**Other loops**: While loop: `while(cond) expr`, repeat: `repeat expr`.\nApplied to the innermost of nested loops: `break`, `next`. Use braces\n`{}` around statements.\n\n**ifelse(test, yes, no)** returns a value with the same shape as test,\nfilled with yes or no Boolean values.\n\n**do.call(funname, args)** executes a function call from the name of the\nfunction and a list of arguments to be passed to it.\n\n## Data Simulation Primer\n\nBefore we demonstrate how to synthetically simulate data that resembles\nclosely the characteristics of real observations from the same process,\nlet's import some observed data for initial exploratory analytics.\n\nUsing the [SOCR Parkinson's Disease\nCase-study](https://wiki.socr.umich.edu/index.php/SOCR_Simulated_HELP_Data)\navailable in the [Canvas Data\nArchive](https://umich.instructure.com/files/330397/download?download_frd=1),\nwe can import some data and extract some descriptions of the [sample\ndata\n(05_PPMI_top_UPDRS_Integrated_LongFormat1.csv)](https://umich.instructure.com/files/330397/download?download_frd=1).\n\n\n\nNext, we will simulate new synthetic data to match the\nproperties/characteristics of the observed PPMI data (using `Uniform`,\n`Normal`, and `Poisson` distributions).",
      "word_count": 2157
    },
    {
      "title": "Appendix",
      "content": "## Tidyverse\n\nThe [Tidyverse](https://r4ds.had.co.nz/introduction.html) represents a\nsuite of integrated `R` packages that provide support for `data science`\nand `Big Data analytics`, including functionality for data import\n(`readr`), data manipulation (`dplyr`), data visualization (`ggplot2`),\nexpanded data frames (`tibble`), data tidying (`tidyr`), and functional\nprogramming (`purrr`). These [learning modules provide introduction to\ntidyverse](https://m-clark.github.io/data-processing-and-visualization/tidyverse.html).\n\n## Additional `R` documentation and resources\n\n-   The Software Carpentry Foundation provides useful [Programming with\n    R](http://swcarpentry.github.io/r-novice-inflammation) and [R for\n    Reproducible Scientific\n    Analysis](http://swcarpentry.github.io/r-novice-gapminder)\n    materials.\n-   [A very gentle stats intro using `R` Book\n    (Verzani)](http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf).\n-   [Online Quick-R examples\n    (StatsMethods)](http://www.statmethods.net/index.html).\n-   [R-tutor Introduction](http://www.r-tutor.com/r-introduction).\n-   [R project\n    Introduction](http://cran.r-project.org/doc/manuals/r-release/R-intro.html).\n-   [UCLA ITS/IDRE `R` Resources](https://stats.idre.ucla.edu/r/).\n\n## HTML SOCR Data Import\n\n[SOCR Datasets](https://wiki.socr.umich.edu/index.php/SOCR_Data) can\nautomatically be downloaded into the `R` environment using the following\nprotocol, which uses the [Parkinson's Disease\ndataset](https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata)\nas an example:\n\n\nAlso see [the SMHS Simulation\nPrimer](https://wiki.socr.umich.edu/index.php/SMHS_DataSimulation).\n\n## `R` Debugging\n\nMost programs that give incorrect results are impacted by logical\nerrors. When errors (bugs, exceptions) occur, we need to explore deeper\n-- this procedure to identify and fix bugs is \"debugging\".\n\nR tools for debugging: traceback(), debug() browser() trace() recover()\n\n**traceback()**: Failing `R` functions report to the screen immediately\nthe run-time errors. Calling `traceback()` shows the place where the\nerror occurred. The `traceback()` function prints the list of functions\nthat were called before the error occurred. The stacked function calls\nare printed in reverse order.\n\n\n**debug()** - `traceback()` does not tell you where the error is. To\nfind out which line causes the error, we may step through the function\nusing `debug()`.\n\n**debug(foo)** flags the function `foo()` for debugging. `undebug(foo)`\nunflags the function. When a function is flagged for debugging, each\nstatement in the function is executed one at a time. After a statement\nis executed, the function suspends and the user can interact with the\n`R` shell. This allows us to inspect a function line-by-line.\n\n**Example**: compute sum of squared error SS.\n\n\n\nIn the debugging shell (\"Browse[1]\\>\"), users can:\n\n-   Enter **n** (next) executes the current line and prints the next\n    one;\\\n-   Typing **c** (continue) executes the rest of the function without\n    stopping;\n-   Enter **Q** quits the debugging;\\\n-   Enter **ls()** list all objects in the local environment;\\\n-   Enter an object name or print(<object name>) tells the current value\n    of an object.\n\n\nBrowse[1]\\> n\\\ndebug: d \\<- x - mu \\## the next command\\\nBrowse[1]\\> ls() \\## current environment [1] \"mu\" \"x\" \\## there is no d\\\nBrowse[1]\\> n \\## go one step debug: d2 \\<- d\\^2 \\## the next command\\\nBrowse[1]\\> ls() \\## current environment [1] \"d\" \"mu\" \"x\" \\## d has been\ncreated\\\nBrowse[1]\\> d[1:3] \\## first three elements of d [1] -1.5021924\n-0.8684688 -1.0789171\\\nBrowse[1]\\> hist(d) \\## histogram of d\\\nBrowse[1]\\> where \\## current position in call stack where 1: SS(1, x)\\\nBrowse[1]\\> n\\\ndebug: ss \\<- sum(d2)\\\nBrowse[1]\\> Q \\## quit\n\n\nYou can label a function for debugging while debugging another function.\n\n\nBrowse[1]\\> n\\\nBrowse[1]\\> n\n\nBut, we can also label g and h for debugging when we debug f\n\nf(-1)\\\nBrowse[1]\\> n\\\nBrowse[1]\\> debug(g)\\\nBrowse[1]\\> debug(h)\\\nBrowse[1]\\> n\n\nInserting a call to **browser()** in a function will pause the execution\nof a function at the point where browser() is called. Similar to using\n**debug()**, except that you can control where execution gets paused.\nHere is another example.\n\n\nBrowse[1]\\> ls() Browse[1]\\> z\\\nBrowse[1]\\> n\\\nBrowse[1]\\> n\\\nBrowse[1]\\> ls()\\\nBrowse[1]\\> c\n\nCalling **trace()** on a function allows inserting new code into a\nfunction.\n\nas.list(body(h))\\\ntrace(\"h\", quote(\\\nif(is.nan(r))\\\n{browser()}), at=3, print=FALSE)\\\nf(1)\\\nf(-1)\n\ntrace(\"h\", quote(if(z\\<0) {z\\<-1}), at=2, print=FALSE)\\\nf(-1)\\\nuntrace()\n\nDuring the debugging process, **recover()** allows checking the status\nof variables in upper level functions. recover() can be used as an error\nhandler using **options()** (e.g. `options(error=recover)`). When\nfunctions throw exceptions, execution stops at the point of failure.\nBrowsing the function calls and examining the environment may indicate\nthe source of the problem.\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t\n\t\t\t\t<img class=\"statcounter\"\n\t\t\t\t\t\t\t\tsrc=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" \n\t\t\t\t\t\t\t\talt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<!-- script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script -->\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 734
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    wrap: 72\n---",
      "row_count": 2
    },
    {
      "section": "Motivation",
      "content": "| Data Source                               | Sample Size/Data Type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Summary                                                                                                                                                                                                                |\n|-----------------|--------------------------------------|------------------|\n| [ADNI Archive](https://www.adni-info.org) | Clinical data: demographics, clinical assessments, cognitive assessments; Imaging data: sMRI, fMRI, DTI, PiB/FDG PET; Genetics data: Ilumina SNP genotyping; Chemical biomarker: lab tests, proteomics. Each data modality comes with a different number of cohorts. Generally, $200\\le N \\le 1200$. For instance, previously conducted ADNI studies with N\\>500 [ [doi: 10.3233/JAD-150335](https://www.ncbi.nlm.nih.gov/pubmed/26444770), [doi: 10.1111/jon.12252](https://www.ncbi.nlm.nih.gov/pubmed/25940587), [doi: 10.3389/fninf.2014.00041](https://www.ncbi.nlm.nih.gov/pubmed/24795619)] | ADNI provides interesting data modalities, multiple cohorts (e.g., early-onset, mild, and severe dementia, controls) that allow effective model training and validation [NACC Archive](https://www.alz.washington.edu) |",
      "row_count": 3
    },
    {
      "section": "Motivation",
      "content": "| Data Source                               | Sample Size/Data Type                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Summary                                                                                                                                                                                                                                        |\n|-----------------|--------------------------------------|------------------|\n| [PPMI Archive](https://www.ppmi-info.org) | Demographics: age, medical history, sex; Clinical data: physical, verbal learning and language, neurological and olfactory (University of Pennsylvania Smell Identification Test, UPSIT) tests), vital signs, MDS-UPDRS scores (Movement Disorder; Society-Unified Parkinson's Disease Rating Scale), ADL (activities of daily living), Montreal Cognitive Assessment (MoCA), Geriatric Depression Scale (GDS-15); Imaging data: structural MRI; Genetics data: llumina ImmunoChip (196,524 variants) and NeuroX (covering 240,000 exonic variants) with 100% sample success rate, and 98.7% genotype success rate genotyped for APOE e2/e3/e4. Three cohorts of subjects; Group 1 = {de novo PD Subjects with a diagnosis of PD for two years or less who are not taking PD medications}, N1 = 263; Group 2 = {PD Subjects with Scans without Evidence of a Dopaminergic Deficit (SWEDD)}, N2 = 40; Group 3 = {Control Subjects without PD who are 30 years or older and who do not have a first degree blood relative with PD}, N3 = 127 | The longitudinal PPMI dataset including clinical, biological and imaging data (screening, baseline, 12, 24, and 48 month follow-ups) may be used conduct model-based predictions as well as model-free classification and forecasting analyses |",
      "row_count": 3
    },
    {
      "section": "Motivation",
      "content": "| Data Source                                                                                       | Sample Size/Data Type                                                                                                                                                                                                                                         | Summary                                                                                                                                                                                                                                                       |\n|-----------------|--------------------------------------|------------------|\n| MAWS Data / UMHS EHR / [WHO AWS Data](https://apps.who.int/gho/data/node.main-amro.A1311?lang=en) | Scores from Alcohol Use Disorders Identification Test-Consumption (AUDIT-C) [49], including dichotomous variables for any current alcohol use (AUDIT-C, question 1), total AUDIT-C score \\> 8, and any positive history of alcohol withdrawal syndrome (HAWS) | \\~1,000 positive cases per year among 10,000 adult medical inpatients, % RAWS screens completed, % positive screens, % entered into MAWS protocol who receive pharmacological treatment for AWS, % entered into MAWS protocol without a completed RAWS screen |",
      "row_count": 3
    },
    {
      "section": "Motivation",
      "content": "| Data Source                                  | Sample Size/Data Type                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Summary                                                                                                                                                                                                                            |\n|-----------------|--------------------------------------|------------------|\n| [ProAct Archive](http://www.ALSdatabase.org) | Over 100 clinical variables are recorded for all subjects including: Demographics: age, race, medical history, sex; Clinical data: Amyotrophic Lateral Sclerosis Functional Rating Scale (ALSFRS), adverse events, onset_delta, onset_site, drugs use (riluzole) The PRO-ACT training dataset contains clinical and lab test information of 8,635 patients. Information of 2,424 study subjects with valid gold standard ALSFRS slopes will be used in out processing, modeling and analysis | The time points for all longitudinally varying data elements will be aggregated into signature vectors. This will facilitate the modeling and prediction of ALSFRS slope changes over the first three months (baseline to month 3) |",
      "row_count": 3
    },
    {
      "section": "Motivation",
      "content": "![](http://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/images/HHMI_BioInteractive_Ebola.png)\n:::",
      "row_count": 2
    },
    {
      "section": "Motivation",
      "content": "| BD Dimensions | Tools                                                       |\n|---------------|-------------------------------------------------------------|\n| Size          | Harvesting and management of vast amounts of data           |\n| Complexity    | Wranglers for dealing with heterogeneous data               |\n| Incongruency  | Tools for data harmonization and aggregation                |\n| Multi-source  | Transfer and joint modeling of disparate elements           |\n| Multi-scale   | Macro to meso to micro scale observations                   |\n| Time          | Techniques accounting for longitudinal patterns in the data |\n| Incomplete    | Reliable management of missing data                         |",
      "row_count": 9
    },
    {
      "section": "Foundations of R",
      "content": "| Statistical Software | Advantages                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Disadvantages                                                                                 |\n|--------------------|-----------------------------------|-----------------|\n| R                    | R is actively maintained ($\\ge 100,000$ developers, $\\ge 15K$ packages). Excellent connectivity to various types of data and other systems. Versatile for solving problems in many domains. It's free, open-source code. Anybody can access/review/extend the source code. `R` is very stable and reliable. If you change or redistribute the `R` source code, you have to make those changes available for anybody else to use. `R` runs anywhere (platform agnostic). Extensibility: `R` supports extensions, e.g., for data manipulation, statistical modeling, and graphics. Active and engaged community supports R. Unparalleled question-and-answer (Q&A) websites. `R` connects with other languages (Java/C/JavaScript/Python/Fortran) & database systems, and other programs, SAS, SPSS, etc. Other packages have add-ons to connect with R. SPSS has incorporated a link to R, and SAS has protocols to move data and graphics between the two packages. | Mostly scripting language. Steeper learning curve                                             |\n| SAS                  | Large datasets. Commonly used in business & Government                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Expensive. Somewhat dated programming language. Expensive/proprietary                         |\n| Stata                | Easy statistical analyses                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Mostly classical stats                                                                        |\n| SPSS                 | Appropriate for beginners Simple interfaces                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Weak in more cutting edge statistical procedures lacking in robust methods and survey methods |",
      "row_count": 6
    },
    {
      "section": "Foundations of R",
      "content": "| Language | OpenSource | Speed | ComputeTime | LibraryExtent | EaseOfEntry | Costs | Interoperability |\n|---------|---------|---------|---------|---------|---------|---------|---------|\n| Python   | Yes        | 16    | 62          | 80            | 85          | 10    | 90               |\n| Julia    | Yes        | 2941  | 0.34        | 100           | 30          | 10    | 90               |\n| R        | Yes        | 1     | 745         | 100           | 80          | 15    | 90               |\n| IDL      | No         | 67    | 14.77       | 50            | 88          | 100   | 20               |\n| Matlab   | No         | 147   | 6.8         | 75            | 95          | 100   | 20               |\n| Scala    | Yes        | 1428  | 0.7         | 50            | 30          | 20    | 40               |\n| C        | Yes        | 1818  | 0.55        | 100           | 30          | 10    | 99               |\n| Fortran  | Yes        | 1315  | 0.76        | 95            | 25          | 15    | 95               |",
      "row_count": 10
    },
    {
      "section": "Foundations of R",
      "content": "| Expression                       | Explanation                  |\n|----------------------------------|------------------------------|\n| `x[n]`                           | nth element                  |\n| `x[-n]`                          | all but the nth element      |\n| `x[1:n]`                         | first n elements             |\n| `x[-(1:n)]`                      | elements from n+1 to the end |\n| `x[c(1, 4, 2)]`                  | specific elements            |\n| `x[\"name\"]`                      | element named \"name\"         |\n| `x[x > 3]`                       | all elements greater than 3  |\n| `x[x > 3 & x < 5]`               | all elements between 3 and 5 |\n| `x[x %in% c(\"a\", \"and\", \"the\")]` | elements in the given set    |",
      "row_count": 11
    },
    {
      "section": "Foundations of R",
      "content": "| Expression    | Explanation                      |\n|---------------|----------------------------------|\n| `x[n]`        | list with n elements             |\n| `x[[n]]`      | nth element of the list          |\n| `x[[\"name\"]]` | element of the list named \"name\" |",
      "row_count": 5
    },
    {
      "section": "Foundations of R",
      "content": "| Expression     | Explanation                |\n|----------------|----------------------------|\n| `x[i, j]`      | element at row i, column j |\n| `x[i, ]`       | row i                      |\n| `x[, j]`       | column j                   |\n| `x[, c(1, 3)]` | columns 1 and 3            |\n| `x[\"name\", ]`  | row named \"name\"           |",
      "row_count": 7
    },
    {
      "section": "Foundations of R",
      "content": "| Expression                 | Explanation                                                                                                                                                             |\n|-------------------|-----------------------------------------------------|\n| `choose(n, k)`             | computes the combinations of k events among n repetitions. Mathematically it equals to $\\frac{n!}{[(n-k)!k!]}$                                                          |\n| `max(x)`                   | maximum of the elements of x                                                                                                                                            |\n| `min(x)`                   | minimum of the elements of x                                                                                                                                            |\n| `range(x)`                 | minimum and maximum of the elements of x                                                                                                                                |\n| `sum(x)`                   | sum of the elements of x                                                                                                                                                |\n| `diff(x)`                  | lagged and iterated differences of vector x                                                                                                                             |\n| `prod(x)`                  | product of the elements of x                                                                                                                                            |\n| `mean(x)`                  | mean of the elements of x                                                                                                                                               |\n| `median(x)`                | median of the elements of x                                                                                                                                             |\n| `quantile(x, probs=)`      | sample quantiles corresponding to the given probabilities (defaults to 0, .25, .5, .75, 1)                                                                              |\n| `weighted.mean(x, w)`      | mean of x with weights w                                                                                                                                                |\n| `rank(x)`                  | ranks of the elements of x                                                                                                                                              |\n| `var(x)` or `cov(x)`       | variance of the elements of x (calculated on n\\>1). If x is a matrix or a data frame, the variance-covariance matrix is calculated                                      |\n| `sd(x)`                    | standard deviation of x                                                                                                                                                 |\n| `cor(x)`                   | correlation matrix of x if it is a matrix or a data frame (1 if x is a vector)                                                                                          |\n| `var(x, y)` or `cov(x, y)` | covariance between x and y, or between the columns of x and those of y if they are matrices or data frames                                                              |\n| `cor(x, y)`                | linear correlation between x and y, or correlation matrix if they are matrices or data frames                                                                           |\n| `round(x, n)`              | rounds the elements of x to n decimals                                                                                                                                  |\n| `log(x, base)`             | computes the logarithm of x with base base                                                                                                                              |\n| `scale(x)`                 | if x is a matrix, centers and reduces the data. Without centering use the option `center=FALSE`. Without scaling use `scale=FALSE` (by default center=TRUE, scale=TRUE) |\n| `pmin(x, y, ...)`          | a vector whose i-th element is the minimum of x[i], y[i], . . .                                                                                                         |\n| `pmax(x, y, ...)`          | a vector whose i-th element is the maximum of x[i], y[i], . . .                                                                                                         |\n| `cumsum(x)`                | a vector which ith element is the sum from x[1] to x[i]                                                                                                                 |\n| `cumprod(x)`               | id. for the product                                                                                                                                                     |\n| `cummin(x)`                | id. for the minimum                                                                                                                                                     |\n| `cummax(x)`                | id. for the maximum                                                                                                                                                     |\n| `Re(x)`                    | real part of a complex number                                                                                                                                           |\n| `Im(x)`                    | imaginary part of a complex number                                                                                                                                      |\n| `Mod(x)`                   | modulus. `abs(x)` is the same                                                                                                                                           |\n| `Arg(x)`                   | angle in radians of the complex number                                                                                                                                  |\n| `Conj(x)`                  | complex conjugate                                                                                                                                                       |\n| `convolve(x, y)`           | compute several types of convolutions of two sequences                                                                                                                  |\n| `fft(x)`                   | Fast Fourier Transform of an array                                                                                                                                      |\n| `mvfft(x)`                 | FFT of each column of a matrix                                                                                                                                          |\n| `filter(x, filter)`        | applies linear filtering to a univariate time series or to each series separately of a multivariate time series                                                         |",
      "row_count": 37
    },
    {
      "section": "Foundations of R",
      "content": "| Expression                | Explanation                                                            |\n|----------------|--------------------------------------------------------|\n| `t(x)`                    | transpose                                                              |\n| `diag(x)`                 | diagonal                                                               |\n| `%*%`                     | matrix multiplication                                                  |\n| `solve(a, b)`             | solves `a %*% x = b` for x                                             |\n| `solve(a)`                | matrix inverse of a                                                    |\n| `rowsum(x)`               | sum of rows for a matrix-like object. `rowSums(x)` is a faster version |\n| `colsum(x)`, `colSums(x)` | id. for columns                                                        |\n| `rowMeans(x)`             | fast version of row means                                              |\n| `colMeans(x)`             | id. for columns                                                        |",
      "row_count": 11
    },
    {
      "section": "Foundations of R",
      "content": "| Expression                              | Explanation                            |\n|-----------------------------------------|----------------------------------------|\n| `rnorm(n, mean=0, sd=1)`                | Gaussian (normal)                      |\n| `rexp(n, rate=1)`                       | exponential                            |\n| `rgamma(n, shape, scale=1)`             | gamma                                  |\n| `rpois(n, lambda)`                      | Poisson                                |\n| `rweibull(n, shape, scale=1)`           | Weibull                                |\n| `rcauchy(n, location=0, scale=1)`       | Cauchy                                 |\n| `rbeta(n, shape1, shape2)`              | beta                                   |\n| `rt(n, df)`                             | Student's (t)                          |\n| `rf(n, df1, df2)`                       | Fisher's (F) (df1, df2)                |\n| `rchisq(n, df)`                         | Pearson rbinom(n, size, prob) binomial |\n| `rgeom(n, prob)`                        | geometric                              |\n| `rhyper(nn, m, n, k)`                   | hypergeometric                         |\n| `rlogis(n, location=0, scale=1)`        | logistic                               |\n| `rlnorm(n, meanlog=0, sdlog=1)`         | lognormal                              |\n| `rnbinom(n, size, prob)`                | negative binomial                      |\n| `runif(n, min=0, max=1)`                | uniform                                |\n| `rwilcox(nn, m, n)`, `rsignrank(nn, n)` | Wilcoxon's statistics                  |",
      "row_count": 19
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "content": "| Formats             | Explanations                                              |\n|------------------|------------------------------------------------------|\n| `%a, %A`            | Abbreviated and full weekday name.                        |\n| `%b, %B`            | Abbreviated and full month name.                          |\n| `%d`                | Day of the month (01 ... 31).                             |\n| `%H`                | Hours (00 ... 23).                                        |\n| `%I`                | Hours (01 ... 12).                                        |\n| `%j`                | Day of year (001 ... 366).                                |\n| `%m`                | Month (01 ... 12).                                        |\n| `%M`                | Minute (00 ... 59).                                       |\n| `%p`                | AM/PM indicator.                                          |\n| `%S`                | Second as a decimal number (00 ... 61).                   |\n| `%U`                | Week (00 ... 53); the first Sunday as day 1 of week 1.    |\n| `%w`                | Weekday (0 ... 6, Sunday is 0).                           |\n| `%W`                | Week (00 ... 53); the first Monday as day 1 of week 1.    |\n| `%y`                | Year without century (00 ... 99). Don't use it.           |\n| `%Y`                | Year with century.                                        |\n| `%z` (output only.) | Offset from Greenwich; -0800 is 8 hours west of.          |\n| `%Z` (output only.) | Time zone as a character string (empty if not available). |",
      "row_count": 19
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "content": "| Parameters     | Explanations                                                                                                                                                                                                                                                                                                |\n|----------------|--------------------------------------------------------|\n| `add=FALSE`    | if TRUE superposes the plot on the previous one (if it exists)                                                                                                                                                                                                                                              |\n| `axes=TRUE`    | if FALSE does not draw the axes and the box                                                                                                                                                                                                                                                                 |\n| `type=\"p\"`     | specifies the type of plot, \"p\": points, \"l\": lines, \"b\": points connected by lines, \"o\": id. But the lines are over the points, \"h\": vertical lines, \"s\": steps, the data are represented by the top of the vertical lines, \"S\": id. However, the data are represented at the bottom of the vertical lines |\n| `xlim=, ylim=` | specifies the lower and upper limits of the axes, for example with `xlim=c(1, 10)` or `xlim=range(x)`                                                                                                                                                                                                       |\n| `xlab=, ylab=` | annotates the axes, must be variables of mode character                                                                                                                                                                                                                                                     |\n| `main=`        | main title, must be a variable of mode character                                                                                                                                                                                                                                                            |\n| `sub=`         | subtitle (written in a smaller font)                                                                                                                                                                                                                                                                        |",
      "row_count": 9
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "content": "| Expression                    | Explanation                                                                                                                  |\n|---------------------------|---------------------------------------------|\n| **xyplot(y\\~x)**              | bivariate plots (with many functionalities).                                                                                 |\n| **barchart(y\\~x)**            | histogram of the values of y with respect to those of x.                                                                     |\n| **dotplot(y\\~x)**             | Cleveland dot plot (stacked plots line-by-line and column-by-column)                                                         |\n| **densityplot(\\~x)**          | density functions plot                                                                                                       |\n| **histogram(\\~x)**            | histogram of the frequencies of x                                                                                            |\n| **bwplot(y\\~x)**              | \"box-and-whiskers\" plot                                                                                                      |\n| **qqmath(\\~x)**               | quantiles of x with respect to the values expected under a theoretical distribution                                          |\n| **stripplot(y\\~x)**           | single dimension plot, x must be numeric, y may be a factor                                                                  |\n| **qq(y\\~x)**                  | quantiles to compare two distributions, x must be numeric, y may be numeric, character, or factor but must have two \"levels\" |\n| **splom(\\~x)**                | matrix of bivariate plots                                                                                                    |\n| **parallel(\\~x)**             | parallel coordinates plot                                                                                                    |\n| levelplot($z\\sim x*y\\|g1*g2$) | colored plot of the values of z at the coordinates given by x and y (x, y and z are all of the same length)                  |\n| wireframe($z\\sim x*y\\|g1*g2$) | 3d surface plot                                                                                                              |\n| cloud($z\\sim x*y\\|g1*g2$)     | 3d scatter plot                                                                                                              |",
      "row_count": 16
    }
  ],
  "r_code": [
    {
      "section": "Foundations of R",
      "code": "# library(ggplot2)\n# library(reshape2)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(plotly)\nData_R_SAS_SPSS_Pubs <- \n  read.csv('https://umich.instructure.com/files/2361245/download?download_frd=1', header=T)\ndf <- data.frame(Data_R_SAS_SPSS_Pubs) \n# convert to long format (http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/) \n# df <- melt(df ,  id.vars = 'Year', variable.name = 'Software') \n# ggplot(data=df, aes(x=Year, y=value, color=Software, group = Software)) + \n#   geom_line(size=4) + labs(x='Year', y='Paper Software Citations') +\n#   ggtitle(\"Manuscript Citations of Software Use (1995-2015)\") +\n#   theme(legend.position=c(0.1,0.8), \n#         legend.direction=\"vertical\",\n#         axis.text.x = element_text(angle = 45, hjust = 1),\n#         plot.title = element_text(hjust = 0.5))\n\nplot_ly(df, x = ~Year)  %>%\n  add_trace(y = ~R, name = 'R', mode = 'lines+markers') %>%\n  add_trace(y = ~SAS, name = 'SAS', mode = 'lines+markers') %>%\n  add_trace(y = ~SPSS, name = 'SPSS', mode = 'lines+markers') %>% \n  layout(title=\"Manuscript Citations of Software Use (1995-2015)\", legend = list(orientation = 'h'))",
      "line_count": 23
    },
    {
      "section": "Foundations of R",
      "code": "# require(ggplot2)\n# require(reshape2)\n# GoogleTrends_Data_R_SAS_SPSS_Worldwide_2004_2018 <- \n#   read.csv('https://umich.instructure.com/files/9310141/download?download_frd=1', header=T)\n#   # read.csv('https://umich.instructure.com/files/9314613/download?download_frd=1', header=T) # Include Python\n# df_GT <- data.frame(GoogleTrends_Data_R_SAS_SPSS_Worldwide_2004_2018) \n# \n# # convert to long format \n# # df_GT <- melt(df_GT ,  id.vars = 'Month', variable.name = 'Software') \n# # \n# # library(scales)\n# df_GT$Month <- as.Date(paste(df_GT$Month,\"-01\",sep=\"\"))\n# ggplot(data=df_GT1, aes(x=Date, y=hits, color=keyword, group = keyword)) +\n#   geom_line(size=4) + labs(x='Month-Year', y='Worldwide Google Trends') +\n#   scale_x_date(labels = date_format(\"%m-%Y\"), date_breaks='4 months') +\n#   ggtitle(\"Web-Search Trends of Statistical Software (2004-2018)\") +\n#   theme(legend.position=c(0.1,0.8),\n#         legend.direction=\"vertical\",\n#         axis.text.x = element_text(angle = 45, hjust = 1),\n#         plot.title = element_text(hjust = 0.5))\n\n\n#### Pull dynamic Google-Trends data\n# install.packages(\"prophet\")\n# install.packages(\"devtools\")\n# install.packages(\"ps\"); install.packages(\"pkgbuild\")\n# devtools::install_github(\"PMassicotte/gtrendsR\")\n\n# Potential 429 Error, see: \n#      https://github.com/PMassicotte/gtrendsR/issues/431\n#      https://github.com/trendecon/trendecon/blob/master/R/gtrends_with_backoff.R \n\nlibrary(gtrendsR)\nlibrary(ggplot2)\nlibrary(prophet)\ndf_GT1 <- gtrends(c(\"R\", \"SAS\", \"SPSS\", \"Python\"), \n                 gprop = \"web\", time = \"2004-01-01 2023-06-16\")[[1]]\n                 # geo = c(\"US\",\"CN\",\"GB\", \"EU\")\n# During repeated requests, to prevent gtrends error message \n# \"Status code was not 200. Returned status code:429\", due to multiple queries\n# we used the GoogleTrends online search to for the 4 terms\n# https://trends.google.com/trends/explore?date=all&q=R,SAS,Python,SPSS&hl=en\n# and saved the data to DSPA Canvas site:\n# https://umich.instructure.com/courses/38100/files/folder/Case_Studies/\n# https://umich.instructure.com/files/31071103/download?download_frd=1\n\n# df_GT1_wide <- spread(df_GT1, key = keyword, value = hits)\n# # colnames(df_GT1_wide)[7] <- \"R\"\n# # colnames(df_GT1_wide) <- gsub(\" \", \"\", colnames(data))\n# # dim(df_GT1_wide ) # [1] 212   9\n# \n# plot_ly(df_GT1_wide, x = ~date)  %>%\n#   add_trace(x = ~date, y = ~R, name = 'R', type = 'scatter', mode = 'lines+markers') %>%\n#   add_trace(x = ~date, y = ~SAS, name = 'SAS', type = 'scatter', mode = 'lines+markers') %>%\n#   add_trace(x = ~date, y = ~SPSS, name = 'SPSS', type = 'scatter', mode = 'lines+markers') %>%\n#   add_trace(x = ~date, y = ~Python, name = 'Python', type = 'scatter', mode = 'lines+markers') %>% \n#   layout(title=\"Monthly Web-Search Trends of Statistical Software (2004-2023)\", \n#          legend = list(orientation = 'h'),\n#          xaxis = list(title = 'Time'), \n#          yaxis = list (title = 'Relative Search Volume'))",
      "line_count": 60
    },
    {
      "section": "Foundations of R",
      "code": "# load the data\ndf_GT1 <- read.csv(\n  \"https://umich.instructure.com/files/31071103/download?download_frd=1\", \n  header=T, as.is = T) # R_SAS_SPSS_Python_GoogleTrendsSearchDate_July_2023.csv\nsummary(df_GT1)\n\nhead(df_GT1)\n\n# keywords = c(\"R\", \"SAS\", \"SPSS\", \"Python\")\n# time_period = \"2004-01-01 2023-06-16\"\n# geo = c(\"US\",\"CN\",\"GB\", \"EU\")\n# gtrends_data <- data.frame(gtrends(keyword=keywords,\n#                                    time=time_period,geo=geo)$interest_over_time)\n\nlibrary(tidyr)\n# colnames(df_GT1_wide)[7] <- \"R\"\n# colnames(df_GT1_wide) <- gsub(\" \", \"\", colnames(data))\n# dim(df_GT1_wide ) # [1] 212   9\n\nplot_ly(df_GT1, x = ~Month)  %>%\n  add_trace(x = ~Month, y = ~R, name = 'R', type = 'scatter', mode = 'lines+markers') %>%\n  add_trace(x = ~Month, y = ~SAS, name = 'SAS', type = 'scatter', mode = 'lines+markers') %>%\n  add_trace(x = ~Month, y = ~SPSS, name = 'SPSS', type = 'scatter', mode = 'lines+markers') %>%\n  add_trace(x = ~Month, y = ~Python, name = 'Python', type = 'scatter', mode = 'lines+markers') %>% \n  layout(title=\"Monthly Web-Search Trends of Statistical Software (2004-2023)\", \n         # legend = list(orientation = 'h'),\n         xaxis = list(title = 'Monthly', automargin = TRUE), \n         yaxis = list (title = 'Relative Search Volume'))",
      "line_count": 28
    },
    {
      "section": "Foundations of R",
      "code": "help(lm)\n?lm",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "rawdata_wide <- read.table(header=TRUE, text='\n CaseID Gender Age \tCondition1 \tCondition2\n       1   M    5  \t13  \t\t10.5\n       2   F    6  \t16  \t\t11.2\n       3   F    8  \t10  \t\t18.3\n       4   M    9   \t9.5\t\t18.1\n       5   M    10  \t12.1  \t\t19\n')\n# Make the CaseID column a factor\nrawdata_wide$subject <- factor(rawdata_wide$CaseID)\n\nrawdata_wide\n\nlibrary(reshape2)\n\n# Specify id.vars: the variables to keep (don't split apart on!)\nmelt(rawdata_wide, id.vars=c(\"CaseID\", \"Gender\"))",
      "line_count": 17
    },
    {
      "section": "Foundations of R",
      "code": "data_long <- melt(rawdata_wide, \n        # ID variables - all the variables to keep but not split apart on\n    id.vars=c(\"CaseID\", \"Gender\"), \n        # The source columns\n    measure.vars=c(\"Age\", \"Condition1\", \"Condition2\" ), \n        # Name of the destination column that will identify the original\n        # column that the measurement came from\n    variable.name=\"Feature\", \n    value.name=\"Measurement\"\n)\ndata_long",
      "line_count": 11
    },
    {
      "section": "Foundations of R",
      "code": "a<-c(1, 2, 3, 5, 6, 7, 10, 1, 4)\na\nc(list(A = c(Z = 1, Y = 2), B = c(X = 7), C = c(W = 7, V=3, U=-1.9)), recursive = TRUE)",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "seq(1, 20, by=0.5)\nseq(1, 20, length=9)\nseq(along=c(5, 4, 5, 6))",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "rep(c(1, 2, 3), 4)\nrep(c(1, 2, 3), each=4)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "X <- seq(along=c(1, 2, 3)); replicate(4, X+1)",
      "line_count": 1
    },
    {
      "section": "Foundations of R",
      "code": "data.frame(v=1:4, ch=c(\"a\", \"B\", \"C\", \"d\"), n=c(10, 11))",
      "line_count": 1
    },
    {
      "section": "Foundations of R",
      "code": "l<-list(a=c(1, 2), b=\"hi\", c=-3+3i)\nl\n# Note Complex Numbers a <- -1+3i; b <- -2-2i; a+b",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "l$a[[2]]\nl$b",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "ar<-array(1:24, dim=c(3, 4, 2))\nar\nar[2, 3, 1]\nar[2, ,1]",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "x <- seq(1, 10, by=0.5)\ny <- list(a = 1, b = TRUE, c = \"oops\")\nsave(x, y, file=\"xy.RData\")\nload(\"xy.RData\")",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "data(\"iris\")\nsummary(iris)\n\nlibrary(base)",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "data.txt<-read.table(\"https://umich.instructure.com/files/1628628/download?download_frd=1\", header=T, as.is = T) # 01a_data.txt\nsummary(data.txt)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "dataGDrive.txt<-read.table(\"https://drive.google.com/uc?export=download&id=1Zpw3HSe-8HTDsOnR-n64KoMRWYpeBBek\", header=T, as.is = T) # 01a_data.txt\nsummary(dataGDrive.txt)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "data.csv<-read.csv(\"https://umich.instructure.com/files/1628650/download?download_frd=1\", header = T)  # 01_hdp.csv\nsummary(data.csv)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "match(c(1, 2, 4, 5), c(1, 4, 4, 5, 6, 7))",
      "line_count": 1
    },
    {
      "section": "Foundations of R",
      "code": "x<-c(1, 3, 10, 23, 1, 3)\nlength(x)\nis.na(x)\nis.vector(x)",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "x<-1:12\nlength(x)\ndim(x)<-c(3, 4)\nx",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "dimnames(x)<-list(c(\"R1\", \"R2\", \"R3\"), c(\"C1\", \"C2\", \"C3\", \"C4\"))\nx",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "nrow(x)\nncol(x)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "class(x)\nclass(x)<-\"myclass\"\nx<-unclass(x)\nx",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "attr(x, \"class\")\nattr(x, \"dim\")<-c(2, 6)\nx",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "attributes(x) <- list(mycomment = \"really special\", dim = 3:4, \n   dimnames = list(LETTERS[1:3], letters[1:4]), names = paste(1:12))\nx",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "x<-c(1, 5, 2, 1, 10, 40, 3)\nwhich.max(x)\nwhich.min(x)\nrev(x)",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "sort(x)\nrev(sort(x))",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "x\ncut(x, 3)\ncut(x, c(0, 5, 20, 30))",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "x\nwhich(x==2)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "df<-data.frame(a=1:5, b=c(1, 3, NA, 9, 8))\ndf\nna.omit(df)",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "df1<-data.frame(a=c(1, 1, 7, 6, 8), b=c(1, 1, NA, 9, 8))\ndf1\nunique(df1)",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "v<-c(1, 2, 4, 2, 2, 5, 6, 4, 7, 8, 8)\ntable(v)\nprop.table(v)",
      "line_count": 3
    },
    {
      "section": "Foundations of R",
      "code": "sub <- subset(df1, df1$a>5)\nsub\nsub <- subset(df1, select=-a)\nsub\n\n## Subsampling\nx <- matrix(rnorm(100), ncol = 5)\ny <- c(1, seq(19))\n\nz <- cbind(x, y)\n\nz.df <- data.frame(z)\nz.df\n\nnames(z.df)\n# subsetting rows\nz.sub <- subset(z.df, y > 2 & (y<10 | V1>0))\nz.sub\n\nz.sub1 <- z.df[z.df$y == 1, ]\nz.sub1\nz.sub2 <- z.df[z.df$y %in% c(1, 4), ]\nz.sub2\n\n# subsetting columns\nz.sub6 <- z.df[, 1:2]\nz.sub6",
      "line_count": 27
    },
    {
      "section": "Foundations of R",
      "code": "df1 <- data.frame(a=c(1, 1, 7, 6, 8), b=c(1, 1, NA, 9, 8))\nsample(df1$a, 20, replace = T)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "mat1 <- cbind(c(1, -1/5), c(-1/3, 1))\nmat1.inv <- solve(mat1)\n\nmat1.identity <- mat1.inv %*% mat1\nmat1.identity\nb <- c(1, 2)\nx <- solve (mat1, b)\nx",
      "line_count": 8
    },
    {
      "section": "Foundations of R",
      "code": "df1\napply(df1, 2, mean, na.rm=T)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "lapply(df1, mean, na.rm=T)\nlapply(list(a=c(1, 23, 5, 6, 1), b=c(9, 90, 999)), median)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "# v<-c(1, 2, 4, 2, 2, 5, 6, 4, 7, 8, 8)\nv\nfac <- factor(rep(1:3, length = 11), levels = 1:3)\ntable(fac)\ntapply(v, fac, sum)",
      "line_count": 5
    },
    {
      "section": "Foundations of R",
      "code": "by(df1, df1[, 1], sum)",
      "line_count": 1
    },
    {
      "section": "Foundations of R",
      "code": "df2<-data.frame(a=c(1, 1, 7, 6, 8), c=1:5)\ndf2\ndf3<-merge(df1, df2, by=\"a\")\ndf3",
      "line_count": 4
    },
    {
      "section": "Foundations of R",
      "code": "DF <- as.data.frame(UCBAdmissions)\n##  'DF' is a data frame with a grid of the factors and the counts\n## in variable 'Freq'.\nDF\n## Nice for taking margins ...\nxtabs(Freq ~ Gender + Admit, DF)\n## And for testing independence ...\nsummary(xtabs(Freq ~ ., DF))",
      "line_count": 8
    },
    {
      "section": "Foundations of R",
      "code": "list(rep(1:3, length=7))\naggregate(df3, by=list(rep(1:3, length=7)), sum)",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "stack(df3)\nunstack(stack(df3))",
      "line_count": 2
    },
    {
      "section": "Foundations of R",
      "code": "df4 <- data.frame(school = rep(1:3, each = 4), class = rep(9:10, 6), \n                  time = rep(c(1, 1, 2, 2), 3), score = rnorm(12))\nwide <- reshape(df4, idvar = c(\"school\", \"class\"), direction = \"wide\")\nwide\n\nlong <- reshape(wide, idvar = c(\"school\", \"class\"), direction = \"long\")\nlong",
      "line_count": 7
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "a<-\"today\"\nb<-\"is a good day\"\npaste(a, b)\npaste(a, b, sep=\", \")",
      "line_count": 4
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "a<-\"When the going gets tough, the tough get going!\"\nsubstr(a, 10, 40)\n## [1] \"going gets tough, the tough get\"\nsubstr(a, 1, 9)<-\".........\"\na",
      "line_count": 5
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "strsplit(\"a.b.c\", \".\", fixed = TRUE)",
      "line_count": 1
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "letters\ngrep(\"[a-z]\", letters)",
      "line_count": 2
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "a<-c(\"e\", 0, \"kj\", 10, \";\")\ngsub(\"[a-z]\", \"letters\", a)\nsub(\"[a-z]\", \"letters\", a)",
      "line_count": 3
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "x<-c(1, 2, 10, 19, 29)\nmatch(x, c(1, 10))\nx %in% c(1, 10)",
      "line_count": 3
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "# This commended example illustrates a linear model based approach (below is a more direct QQ-plot demonstration)\n# X_norm1 <- rnorm(1000)\n# X_norm2 <- rnorm(1000, m=-75, sd=3.7)\n# X_Cauchy <- rcauchy(1000)\n# \n# # compare X to StdNormal distribution\n# # \tqqnorm(X, \n# #          \t\tmain=\"Normal Q-Q Plot of the data\", \n# #          \t\txlab=\"Theoretical Quantiles of the Normal\", \n# #          \t\tylab=\"Sample Quantiles of the X (Normal) Data\")\n# # \tqqline(X)\n# # \tqqplot(X, Y)\n# fit_norm_norm = lm(X_norm2 ~ X_norm1)\n# fit_norm_cauchy = lm(X_Cauchy ~ X_norm1)\n# \n# # Get model fitted values\n# Fitted.Values.norm_norm <-  fitted(fit_norm_norm)\n# Fitted.Values.norm_cauchy <-  fitted(fit_norm_cauchy)\n#   \n# # Extract model residuals\n# Residuals.norm_norm <-  resid(fit_norm_norm)\n# Residuals.norm_cauchy <-  resid(fit_norm_cauchy)\n# \n# # Compute the model standardized residuals from lm() object\n# Std.Res.norm_norm <- MASS::stdres(fit_norm_norm)  \n# Std.Res.norm_cauchy <- MASS::stdres(fit_norm_cauchy)  \n#   \n# # Extract the theoretical (Normal) quantiles\n# Theoretical.Quantiles.norm_norm <- qqnorm(Residuals.norm_norm, plot.it = F)$x\n# Theoretical.Quantiles.norm_cauchy <- qqnorm(Residuals.norm_cauchy, plot.it = F)$x\n#   \n# qq.df.norm_norm <- data.frame(Std.Res.norm_norm, Theoretical.Quantiles.norm_norm)\n# qq.df.norm_cauchy <- data.frame(Std.Res.norm_cauchy, Theoretical.Quantiles.norm_cauchy)\n# \n# qq.df.norm_norm %>% \n#   plot_ly(x = ~Theoretical.Quantiles.norm_norm) %>% \n#     add_markers(y = ~Std.Res.norm_norm, name=\"Normal(0,1) vs. Normal(-75, 3.7) Data\") %>%\n#     add_lines(x = ~Theoretical.Quantiles.norm_norm, y = ~Theoretical.Quantiles.norm_norm, \n#               mode = \"line\", name = \"Theoretical Normal\", line = list(width = 2)) %>% \n#     layout(title = \"Q-Q Normal Plot\", legend = list(orientation = 'h'))\n# \n# # Normal vs. Cauchy\n# qq.df.norm_cauchy %>% \n#   plot_ly(x = ~Theoretical.Quantiles.norm_cauchy) %>% \n#     add_markers(y = ~Std.Res.norm_cauchy, name=\"Normal(0,1) vs. Cauchy Data\") %>%\n#     add_lines(x = ~Theoretical.Quantiles.norm_norm, y = ~Theoretical.Quantiles.norm_norm, \n#               mode = \"line\", name = \"Theoretical Normal\", line = list(width = 2)) %>% \n#     layout(title = \"Normal vs. Cauchy Q-Q Plot\", legend = list(orientation = 'h'))\n\n# Q-Q plot data (X) vs. simulation(Y)\n# \n# myQQ <- function(x, y, ...) {\n#   #rang <- range(x, y, na.rm=T)\n#   rang <- range(-4, 4, na.rm=T)\n#   qqplot(x, y, xlim=rang, ylim=rang)\n# }\n# \n# myQQ(X, Y) # where the Y is the newly simulated data for X\n# qqline(X)\n\n# Sample different number of observations from all the 3 processes\nX_norm1 <- rnorm(500)\nX_norm2 <- rnorm(1000, m=-75, sd=3.7)\nX_Cauchy <- rcauchy(1500)\n\n# estimate the quantiles (scale the values to ensure measuring-unit invariance of both processes)\nqX_norm1 <- quantile(scale(X_norm1), probs = seq(from=0.01, to=0.99, by=0.01))\nqX_norm2 <- quantile(scale(X_norm2), probs = seq(from=0.01, to=0.99, by=0.01))\nqq.df.norm_norm <- data.frame(qX_norm1, qX_norm2)\n\n# Normal(0,1) vs. Normal(-75, 3.7)\nqq.df.norm_norm %>% \n  plot_ly(x = ~qX_norm1) %>% \n    add_markers(y = ~qX_norm2, name=\"Normal(0,1) vs. Normal(-75, 3.7) Data\") %>%\n    add_lines(x = ~qX_norm1, y = ~qX_norm1, \n              mode = \"line\", name = \"Theoretical Normal\", line = list(width = 2)) %>% \n    layout(title = \"Q-Q Normal Plot\", legend = list(orientation = 'h'))\n\n# Normal(0,1) vs. Cauchy\nqX_norm1 <- quantile(X_norm1, probs = seq(from=0.01, to=0.99, by=0.01))\nqX_Cauchy <- quantile(X_Cauchy, probs = seq(from=0.01, to=0.99, by=0.01))\nqq.df.norm_cauchy <- data.frame(qX_norm1, qX_Cauchy)\n\nqq.df.norm_cauchy %>% \n  plot_ly(x = ~qX_norm1) %>% \n    add_markers(y = ~qX_Cauchy, name=\"Normal(0,1) vs. Cauchy Data\") %>%\n    add_lines(x = ~qX_norm1, y = ~qX_norm1, \n              mode = \"line\", name = \"Theoretical Normal\", line = list(width = 2)) %>% \n    layout(title = \"Normal vs. Cauchy Q-Q Plot\", legend = list(orientation = 'h'))",
      "line_count": 89
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "adding <- function(x=0, y=0) {\n  z<-x+y\n  return(z)\n}\nadding(x=5, y=10)",
      "line_count": 5
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "x<-10\nif(x>10) z=\"T\" else z=\"F\"\nz",
      "line_count": 3
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "x<-c()\nfor(i in 1:10) x[i]=i\nx",
      "line_count": 3
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "PPMI <- read.csv(\"https://umich.instructure.com/files/330397/download?download_frd=1\")\n# summary(PPMI)\nHmisc::describe(PPMI)",
      "line_count": 3
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "# data driven age estimates\nm  = round (mean(PPMI$Age), 2)\nsd = round(sd(PPMI$Age), 2)\n\nx.norm <- rnorm(n=200, m=m, sd=sd)\n# hist(x.norm, main='N(10, 20) Histogram')\nplot_ly(x = ~x.norm, type = \"histogram\") %>% \n  layout(bargap=0.1, title=paste0('N(', m, ', ', sd, ') Histogram'))\n\nmean(PPMI$Age)\nsd(PPMI$Age)",
      "line_count": 11
    },
    {
      "section": "To reshape the PD dataset into a wide format, we will use the reshape function. The arguments we provide include",
      "code": "# age m=62, sd=10\n\n# Demographics variables\n# Define number of subjects\nNumSubj <- 282\nNumTime <- 4\n\n# Define data elements\n# Cases\nCases <- c(2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 41, 42, 43, 44, 45, 53, 55, 58, 60, 62, 67, 69, 71, 72, 74, 79, 80, 85, 87, 90, 95, 97, 99, 100, 101, 106, 107, 109, 112, 120, 123, 125, 128, 129, 132, 134, 136, 139, 142, 147, 149, 153, 158, 160, 162, 163, 167, 172, 174, 178, 179, 180, 182, 192, 195, 201, 208, 211, 215, 217, 223, 227, 228, 233, 235, 236, 240, 245, 248, 250, 251, 254, 257, 259, 261, 264, 268, 269, 272, 273, 275, 279, 288, 289, 291, 296, 298, 303, 305, 309, 314, 318, 324, 325, 326, 328, 331, 332, 333, 334, 336, 338, 339, 341, 344, 346, 347, 350, 353, 354, 359, 361, 363, 364, 366, 367, 368, 369, 370, 371, 372, 374, 375, 376, 377, 378, 381, 382, 384, 385, 386, 387, 389, 390, 393, 395, 398, 400, 410, 421, 423, 428, 433, 435, 443, 447, 449, 450, 451, 453, 454, 455, 456, 457, 458, 459, 460, 461, 465, 466, 467, 470, 471, 472, 476, 477, 478, 479, 480, 481, 483, 484, 485, 486, 487, 488, 489, 492, 493, 494, 496, 498, 501, 504, 507, 510, 513, 515, 528, 530, 533, 537, 538, 542, 545, 546, 549, 555, 557, 559, 560, 566, 572, 573, 576, 582, 586, 590, 592, 597, 603, 604, 611, 619, 621, 623, 624, 625, 631, 633, 634, 635, 637, 640, 641, 643, 644, 645, 646, 647, 648, 649, 650, 652, 654, 656, 658, 660, 664, 665, 670, 673, 677, 678, 679, 680, 682, 683, 686, 687, 688, 689, 690, 692)\n\n# Imaging Biomarkers\nL_caudate_ComputeArea <- rpois(NumSubj, 600)\nL_caudate_Volume <- rpois(NumSubj, 800)\nR_caudate_ComputeArea <- rpois(NumSubj, 893)\nR_caudate_Volume <- rpois(NumSubj, 1000)\nL_putamen_ComputeArea <- rpois(NumSubj, 900)\nL_putamen_Volume <- rpois(NumSubj, 1400)\nR_putamen_ComputeArea <- rpois(NumSubj, 1300)\nR_putamen_Volume <- rpois(NumSubj, 3000)\nL_hippocampus_ComputeArea <- rpois(NumSubj, 1300)\nL_hippocampus_Volume <- rpois(NumSubj, 3200)\nR_hippocampus_ComputeArea <- rpois(NumSubj, 1500)\nR_hippocampus_Volume <- rpois(NumSubj, 3800)\ncerebellum_ComputeArea <- rpois(NumSubj, 16700)\ncerebellum_Volume <- rpois(NumSubj, 14000)\nL_lingual_gyrus_ComputeArea <- rpois(NumSubj, 3300)\nL_lingual_gyrus_Volume <- rpois(NumSubj, 11000)\nR_lingual_gyrus_ComputeArea <- rpois(NumSubj, 3300)\nR_lingual_gyrus_Volume <- rpois(NumSubj, 12000)\nL_fusiform_gyrus_ComputeArea <- rpois(NumSubj, 3600)\nL_fusiform_gyrus_Volume <- rpois(NumSubj, 11000)\nR_fusiform_gyrus_ComputeArea <- rpois(NumSubj, 3300)\nR_fusiform_gyrus_Volume <- rpois(NumSubj, 10000)\n\nSex <- ifelse(runif(NumSubj)<.5, 0, 1)\n\nWeight <- as.integer(rnorm(NumSubj, 80, 10))\n\nAge <- as.integer(rnorm(NumSubj, 62, 10))\n\n# Diagnosis\nDx <- c(rep(\"PD\", 100), rep(\"HC\", 100), rep(\"SWEDD\", 82))\n\n# Genetics\nchr12_rs34637584_GT <- c(ifelse(runif(100)<.3, 0, 1), ifelse(runif(100)<.6, 0, 1), ifelse(runif(82)<.4, 0, 1))                              # NumSubj Bernoulli trials\n\nchr17_rs11868035_GT <- c(ifelse(runif(100)<.7, 0, 1), ifelse(runif(100)<.4, 0, 1), ifelse(runif(82)<.5, 0, 1))                              # NumSubj Bernoulli trials\n\n# Clinical          # rpois(NumSubj, 15) + rpois(NumSubj, 6)\nUPDRS_part_I <- c( ifelse(runif(100)<.7, 0, 1) + ifelse(runif(100) < .7, 0, 1), \n\nifelse(runif(100)<.6, 0, 1)+ ifelse(runif(100)<.6, 0, 1), \n\nifelse(runif(82)<.4, 0, 1)+ ifelse(runif(82)<.4, 0, 1) )\n\nUPDRS_part_II <- c(sample.int(20, 100, replace=T), sample.int(14, 100, replace=T), \n\nsample.int(18, 82, replace=T) )\n\nUPDRS_part_III <- c(sample.int(30, 100, replace=T), sample.int(20, 100, replace=T), \n\n           sample.int(25, 82, replace=T) )\n\n# Time: VisitTime - done automatically below in aggregator\n\n# Data (putting all components together)\nsim_PD_Data <- cbind(\n        rep(Cases, each= NumTime),                 # Cases\n        rep(L_caudate_ComputeArea, each= NumTime), # Imaging\n        rep(Sex, each= NumTime),                   # Demographics\n        rep(Weight, each= NumTime), \n        rep(Age, each= NumTime), \n        rep(Dx, each= NumTime),                    # Dx\n        rep(chr12_rs34637584_GT, each= NumTime),   # Genetics\n        rep(chr17_rs11868035_GT, each= NumTime), \n        rep(UPDRS_part_I, each= NumTime),          # Clinical\n        rep(UPDRS_part_II, each= NumTime), \n        rep(UPDRS_part_III, each= NumTime), \n        rep(c(0, 6, 12, 18), NumSubj)                # Time\n)\n\n\n# Assign the column names\n\ncolnames(sim_PD_Data) <- c(\n\"Cases\", \n\"L_caudate_ComputeArea\", \n\"Sex\", \"Weight\", \"Age\", \n\"Dx\", \"chr12_rs34637584_GT\", \"chr17_rs11868035_GT\", \n\"UPDRS_part_I\", \"UPDRS_part_II\", \"UPDRS_part_III\", \n\"Time\"\n)\n\n# some QC\nsummary(sim_PD_Data)\ndim(sim_PD_Data)\nhead(sim_PD_Data)\n\n# hist(PPMI$Age, freq=FALSE, right=FALSE, ylim = c(0,0.05))\n# lines(density(as.numeric(as.data.frame(sim_PD_Data)$Age)), lwd=2, col=\"blue\")\n# legend(\"topright\", c(\"Raw Data\", \"Simulated Data\"), fill=c(\"black\", \"blue\"))\n\nx <- PPMI$Age\nfit <- density(as.numeric(as.data.frame(sim_PD_Data)$Age))\n\nplot_ly(x = x, type = \"histogram\", name = \"Histogram (Raw Age)\") %>% \n    add_trace(x = fit$x, y = fit$y, type = \"scatter\", mode = \"lines\", \n              fill = \"tozeroy\", yaxis = \"y2\", name = \"Density (Simulated Age)\") %>% \n    layout(title='Observed and Simulated Ages', yaxis2 = list(overlaying = \"y\", side = \"right\"))\n\n\n# Save Results\n# Write out (save) the result to a file that can be shared\nwrite.table(sim_PD_Data, \"output_data.csv\", sep=\", \", row.names=FALSE, col.names=TRUE)",
      "line_count": 115
    },
    {
      "section": "Appendix",
      "code": "library(rvest)\n# Loading required package: xml2\nwiki_url <- read_html(\"https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata\") # UMich SOCR Data\n# wiki_url <- read_html(\"http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_PD_BiomedBigMetadata\") # UCLA SOCR Data\nhtml_nodes(wiki_url, \"#content\")\npd_data <- html_table(html_nodes(wiki_url, \"table\")[[2]])\nhead(pd_data); summary(pd_data)",
      "line_count": 7
    },
    {
      "section": "Appendix",
      "code": "f1 <- function(x) { r<- x-g1(x); `R` }\n\ng1 <- function(y) { r<-y*h1(y); `R` }\n\nh1 <- function(z) { r<-log(z); if(r<10) r^2 else   r^3}\n\nf1(-1)\n\ntraceback()   \n3:  h(y)  \n2: g(x)  \n1: f(-1)  ",
      "line_count": 12
    },
    {
      "section": "Appendix",
      "code": "## compute sum of squares   \nSS <- function(mu, x) { \n  d<-x-mu; \n  d2<-d^2; \n  ss<-sum(d2);  \n  ss \n}  \nset.seed(100);  \nx<-rnorm(100); \nSS(1, x)    ",
      "line_count": 10
    },
    {
      "section": "Appendix",
      "code": "## to debug  \ndebug(SS); SS(1, x)  ",
      "line_count": 2
    },
    {
      "section": "Appendix",
      "code": "debug(SS)\nSS(1, x)",
      "line_count": 2
    },
    {
      "section": "Appendix",
      "code": "undebug(SS)\t\t\t## remove debug label, stop debugging process  \nSS(1, x)\t\t\t\t## now call SS again will without debugging  ",
      "line_count": 2
    },
    {
      "section": "Appendix",
      "code": "f <- function(x) { \n  r<-x-g(x); \n  `R` \n}  \ng <- function(y) { \n  r<-y*h(y); \n  `R` \n}  \nh <- function(z) { \n  r<-log(z); \n  if(r<10) r^2 \n  else r^3 \n}  \n\ndebug(f)\t\t\t# ## If you only debug f, you will not go into g\nf(-1)",
      "line_count": 16
    },
    {
      "section": "Appendix",
      "code": "h <- function(z) {\n  browser() \t## a breakpoint inserted here \n  r<-log(z)\n  if(r<10)  r^2 \n  else r^3\n}\n\nf(-1)",
      "line_count": 8
    }
  ]
}