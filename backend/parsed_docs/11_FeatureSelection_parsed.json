{
  "metadata": {
    "created_at": "2024-11-30T13:46:16.553587",
    "total_sections": 10,
    "total_code_chunks": 75,
    "total_tables": 8,
    "r_libraries": [
      "Boruta",
      "DT",
      "ModelMetrics",
      "RColorBrewer",
      "arm",
      "caret",
      "doParallel",
      "ggplot2",
      "glmnet",
      "gridExtra",
      "knitr",
      "knockoff",
      "mlbench",
      "needs",
      "plotly",
      "plotmo",
      "polynom",
      "rSymPy",
      "randomForest",
      "rvest",
      "tidyr",
      "tidyverse"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Variable Importance and Feature Selection</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    code_folding: show",
      "word_count": 55
    },
    {
      "title": "Variable Importance and Feature Selection",
      "content": "As we mentioned earlier in [Chapter 4](https://socr.umich.edu/DSPA2/DSPA2_notes/04_DimensionalityReduction.html), variable selection is very important when dealing with bioinformatics, healthcare, and biomedical data where we may have more features than observations. Instead of trying to interrogate the complete data in its native high-dimensional state, we can apply variable selection, or feature selection, to focus on the most salient information contained in the observations Due to the presence of intrinsic and extrinsic noise, the volume and complexity of big health data, as well as different methodological and technological challenges, the process of identifying the salient features may resemble finding a needle in a haystack. Here, we will illustrate alternative strategies for feature selection using filtering (e.g., correlation-based feature selection), wrapping (e.g., recursive feature elimination), and embedding (e.g., variable importance via random forest classification) techniques. \n\nVariable selection relates to *dimensionality reduction*, which we saw in [Chapter 4](https://socr.umich.edu/DSPA2/DSPA2_notes/04_DimensionalityReduction.html), however there are differences between them. \n\nRelative to the lower variance estimates in *continuous dimensionality reduction*, the intrinsic characteristics of the *discrete feature selection* process yields higher variance in bootstrap estimation and cross validation.\n\nIn in this Chapter, we will also learn about another powerful technique for variable-selection using *decoy features* (knockoffs) to control for the false discovery rate of selecting inconsequential features as important.\n\n## Feature selection methods\n\nThere are three major classes of variable or feature selection techniques - filtering-based, wrapper-based, and embedded methods.\n\n### Filtering techniques\n\n - *Univariate*: Univariate filtering methods focus on selecting single features with high scores based on some statistics like $\\chi^2$ or Information Gain Ratio. Each feature is viewed as independent of the others, effectively ignoring interactions between features. \n    + Examples: $\\chi^2$, Euclidean distance, $i$-test, and Information gain.\n - *Multivariate*: Multivariate filtering methods rely on various (multivariate) statistics to select the principal features. They typically account for between-feature interactions by using higher-order statistics like correlation. The basic idea is that we iteratively triage variables that have high correlations with other features.\n    + Examples: Correlation-based feature selection, Markov blanket filter, and fast correlation-based feature selection.\n\n### Wrapper\n\n - *Deterministic*: Deterministic wrapper feature selection methods either start with no features (forward-selection) or with all features included in the model (backward-selection) and iteratively refine the set of chosen features according to some model quality measures. The iterative process of adding or removing features may rely on statistics like the Jaccard similarity coefficient.\n    + Examples: Sequential forward selection, Recursive Feature Elimination, Plus $q$ take-away $r$, and Beam search.\n - *Randomized*: Stochastic wrapper feature selection procedures utilize a binary feature-indexing vector indicating whether or not each variable should be included in the list of salient features. At each iteration, we *randomly* perturb to the binary indicators vector and compare the combinations of features before and after the random inclusion-exclusion indexing change. Finally, we pick the indexing vector corresponding with the optimal performance based on some metric like acceptance probability measures. The iterative process continues until no improvement of the objective function is observed.\n    + Examples: Simulated annealing, Genetic algorithms, Estimation of distribution algorithms.\n  \n### Embedded Techniques \n  \n - Embedded feature selection techniques are based on various classifiers, predictors, or clustering procedures. For instance, we can accomplish feature selection by using decision trees where the separation of the training data relies on features associated with the highest information gain. Further tree branching separating the data deeper may utilize *weaker* features. This process of choosing the vital features based on their separability characteristics continues until the classifier generates group labels that are mostly homogeneous within clusters/classes and largely heterogeneous across groups, and when the information gain of further tree branching is marginal. The entire process may be iterated multiple times and select the features that appear most frequently.\n    + Examples: Decision trees, random forests, weighted naive Bayes, and feature selection using weighted-SVM. \n\nThe different types of feature selection methods have their own pros and cons. In this chapter, we are going to introduce the randomized wrapper method using the `Boruta` package, which utilizes a random forest classification method to output variable importance measures (VIMs). Then, we will compare its results with Recursive Feature Elimination, a classical deterministic wrapper method.\n\n### Random Forest Feature Selection\n\nLet's start by examining random forest based feature selection, as an embedded technique. The good performance of random forest as a classification, regression, and clustering method is coupled with its ease-of-use, accurate, and robust results. Having a random forest, or more broadly a decision tree, prediction naturally leads to feature selection by using the mean decrease impurity or the mean accuracy decrease criteria.\n\nThe many decision trees captured in a random forest include explicit conditions at each branching node, which are based on single features. The intrinsic bifurcation conditions splitting the data may be based on cost function optimization using the *impurity*, see [Chapter 5](https://socr.umich.edu/DSPA2/DSPA2_notes/05_SupervisedClassification.html). We can also use other metrics information gain or entropy for classification problems. These measures capture the importance of variables by computing its impact (how much is the feature-based splitting decision decreasing the weighted impurity in a tree). In random forests, the ranking of feature importance, which is based on the average impurity decrease due to each variable, leads to effective feature selection.\n\n### Case Study - ALS\n\n*Step 1: Collecting Data*\n\nFirst things first, let's explore the dataset we will be using. Case Study 15, [Amyotrophic Lateral Sclerosis (ALS)](https://umich.instructure.com/files/1789624/download?download_frd=1), examines the patterns, symmetries, associations and causality in a rare but devastating disease, amyotrophic lateral sclerosis (ALS), also known as *Lou Gehrig disease*. This ALS case-study reflects a large clinical trial including big, multi-source and heterogeneous datasets. It would be interesting to interrogate the data and attempt to derive potential biomarkers that can be used for detecting, prognosticating, and forecasting the progression of this neurodegenerative disorder. Overcoming many scientific, technical and infrastructure barriers is required to establish complete, efficient, and reproducible protocols for such complex data. These pipeline workflows start with ingesting the raw data, preprocessing, aggregating, harmonizing, analyzing, visualizing and interpreting the findings.\n\nIn this case-study, we use the training dataset that contains 2,223 observations and 131 numeric variables. We select `ALSFRS slope` as our outcome variable, as it captures the patients' clinical decline over a year. Although we have more observations than features, this is one of the examples where multiple features are highly correlated. Therefore, we need to preprocess the variables before commencing with feature selection. \n\n*Step 2: Exploring and preparing the data*\n\nThe dataset is located in our [case-studies archive](https://umich.instructure.com/courses/38100/files/folder/Case_Studies). We can use `read.csv()` to directly import the CSV dataset into R using the URL reference.\n\n\nThere are $131$ features and some of variables represent statistics like *max*, *min* and *median* values of the same clinical measurements.\n\n*Step 3 - training a model on the data*\n\nNow let's explore the `Boruta()` function in the `Boruta` package to perform variable selection, based on random forest classification. `Boruta()` includes the following components:\n\n`vs <- Boruta(class~features, data=Mydata, pValue = 0.01, mcAdj = TRUE, maxRuns = 100, doTrace=0, getImp = getImpRfZ, ...)`\n\n - `class`: variable for class labels.\n - `features`: potential features to select from.\n - `data`: dataset containing classes and features.\n - `pValue`: confidence level. Default value is 0.01 (Notice we are applying multiple variable selection.\n - `mcAdj`: Default TRUE to apply a multiple comparisons adjustment using the Bonferroni method.\n - `maxRuns`: maximal number of importance source runs. You may increase it to resolve attributes left Tentative.\n - `doTrace`: verbosity level. Default 0 means no tracing, 1 means reporting decision about each attribute as soon as it is justified, 2 means same as 1, plus at each importance source run reporting the number of attributes. The default is 0 where we don't do the reporting.\n - `getImp`: function used to obtain attribute importance. The default is $getImpRfZ$, which runs random forest from the ranger package and gathers $Z$-scores of mean decrease accuracy measure.\n\nThe resulting `vs` object is of class `Boruta` and contains two important components:\n\n - `finalDecision`: a factor of three values: `Confirmed`, `Rejected` or `Tentative`, containing the final results of the feature selection process.\n - `ImpHistory`: a data frame of importance of attributes gathered in each importance source run. Besides the predictors' importance, it contains maximal, mean and minimal importance of shadow attributes for each run. Rejected attributes get `-Inf` importance. This output is set to NULL if we specify `holdHistory=FALSE` in the Boruta call. \n\n*Caution*: Running the code below will take several minutes.\n\n\nThis is a fairly time-consuming computation. Boruta determines the *important* attributes from *unimportant* and *tentative* features. Here the importance is measured by the [Out-of-bag (OOB) error](https://en.wikipedia.org/wiki/Out-of-bag_error). The OOB estimates the prediction error of machine learning methods (e.g., random forests and boosted decision trees) that utilize bootstrap aggregation to sub-sample training data. **OOB** represents the mean prediction error on each training sample $x_i$, using only the trees that did not include $x_i$ in their bootstrap samples. Out-of-bag estimates provide *internal* assessment of the learning accuracy and avoid the need for an independent *external* validation dataset.\n\nThe importance scores for all features at every iteration are stored in the data frame `als$ImpHistory`. Let's plot a graph depicting the essential features. \n\n*Note*: Again, running this code will take several minutes to complete.\n\n\nWe can see that plotting the graph is easy but extracting matched feature names may require more work. Another basic plot may be rendered using `plot(als, xlab=\"\", xaxt=\"n\")`, where `xaxt=\"n\"` suppresses labeling the x-axis. To reconstruct the correct x-axis labels, we need to create a list by using the `apply()` function. Each element in the list contains all the important scores for a single feature in the original dataset. Then, we can exclude all rejected features and sort these non-rejected features according to their median importance and printed them on the x-axis by using `axis()`. \n\nWe have already seen similar groups of boxplots back in [Chapter 2](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Visualization.html). In this graph, variables with *green* boxes are more important than the ones represented with *red* boxes, and we can see the range of importance scores within a single variable in the graph.\n\nIt may be desirable to get rid of tentative features. Notice that this function should be used only when strict decision is highly desired, because this test is much weaker than Boruta and can lower the confidence of the final result.\n\n\nThis shows the final features selection result.\n\n*Step 4 - evaluating model performance*\n\n#### Comparing with RFE\n\nLet's compare the `Boruta` results against a classical variable selection method - *recursive feature elimination (RFE)*. First, we need to load two packages: `caret` and `randomForest`. Then, similar to [Chapter 9](https://socr.umich.edu/DSPA2/DSPA2_notes/09_ModelEvalImprovement.html) we must specify a resampling method. Here we use *10-fold CV* to do the resampling. \n\n\nNow, all preparations are complete and we are ready to do the RFE variable selection.\n\n\nThis calculation may take a long time to complete. The RFE invocation is different from `Boruta`. Here we have to specify the feature data frame and the class labels separately. Also, the `sizes=` option allows us to specify the number of features we want to include in the model. Let's try `sizes=c(10, 20, 30, 40)` to compare the model performance for alternative numbers of features.\n\nTo visualize the results, we can plot the 5 different feature size combinations listed in the summary. The one with 30 features has the lowest RMSE measure. This result is similar to the `Boruta` output, which selected around 30 features.\n\n\nUsing the functions `predictors()` and `getSelectedAttributes()`, we can compare the final results of the two alternative feature selection methods.\n\n\nThe results are almost identical: \n\n\nThere are 26 common variables chosen by the two techniques, which suggests that both the `Boruta` and RFE methods are robust. Also, notice that the `Boruta` method can give similar results without utilizing the *size* option. If we want to consider 10 or more different sizes, the procedure will be quite time consuming. Thus, `Boruta` method is effective when dealing with complex real world problems.\n\n#### Comparing with stepwise feature selection\n\nNext, we can contrast the `Boruta` feature selection results against another classical variable selection method - *stepwise model selection*. Let's start with fitting a bidirectional stepwise linear model-based feature selection.\n\n\nWe can report the stepwise \"Confirmed\" (important) features:\n\n\nThe feature selection results of `Boruta` and `step` are similar. \n\n\nThere are about $10$ common variables chosen by the Boruta and Stepwise feature selection methods.\n\nThere is another more elaborate stepwise feature selection technique that is implemented in the function `MASS::stepAIC()` that is useful for a wider range of object classes.\n\n\n## Regularized Linear Modeling and Controlled Variable Selection\n\nMany biomedical and biosocial studies involve large amounts of complex data, including cases where the number of features ($k$) is large and may exceed the number of cases ($n$). In such situations, parameter estimates are difficult to compute or may be unreliable as the [system is underdetermined](https://en.wikipedia.org/wiki/Underdetermined_system). Regularization provides one approach to improve model reliability, prediction accuracy, and result interpretability. It is based on augmenting the fidelity term of the objective function used in the model fitting process with a regularization term that provides restrictions on the parameter space.\n\nClassical techniques for choosing *important* covariates to include in a model of complex multivariate data rely on various types of stepwise variable selection processes. These tend to improve prediction accuracy in certain situations, e.g., when a small number of features are strongly predictive, or heavily associated, with the clinical outcome or the specific biosocial trait. However, the prediction error may be large when the model relies purely on a fidelity term. Including an additional regularization term in the optimization of the cost function improves the prediction accuracy. For example, below we show that by shrinking large regression coefficients, ridge regularization reduces overfitting and improves prediction error. Similarly, the least absolute shrinkage and selection operator (LASSO) employs regularization to perform simultaneous parameter estimation and variable selection. LASSO enhances the prediction accuracy and provides a natural interpretation of the resulting model. *Regularization* refers to forcing certain characteristics on the model, or the corresponding scientific inference. Examples include discouraging complex models or extreme explanations, even if they fit the data, enforcing model generalizability to prospective data, or restricting model overfitting of accidental samples.\n\nIn this section, we extend the mathematical foundation we presented in [Chapter 3](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html) and (1) discuss computational protocols for handling complex high-dimensional data, (2) illustrate model estimation by controlling the false-positive rate of selection of salient features, and (3) derive effective forecasting models.\n\n### General Questions\n\nApplications of regularized linear modeling techniques will help us address problems like:\n\n - How to deal with extremely high-dimensional data (hundreds or thousands of features)?\n - Why mix fidelity (model fit) and regularization (model interpretability) terms in objective function optimization?\n - How to reduce the false-positive rate, increase scientific validation, and improve result reproducibility (e.g., Knockoff filtering)?\n\n### Model Regularization\n\nIn data-driven sciences, *regularization* is the process of introducing constraints, adding information to, or smoothing a model aiming to generate a realistic solution to an ill-posed (or under-determined) problem, to prevent overfitting, or to improve the model interpretability.\n\nRegularization of objective functions is a commonly used strategy to solve ill-posed [optimization problems (Chapter 13)](https://socr.umich.edu/DSPA2/DSPA2_notes/13_FunctionOptimization.html). This involves introducing another regularization term penalizing the model for not complying with the additional constraints or increasing the magnitude of the cost function to enforce convergence of the model to an \"optimal\" or a \"unique\" solution. The example below illustrates a schematic of regularization. \n\nSuppose we fit several different (polynomial) models that have near perfect model-fidelity, i.e., all models go very close to the set of anchor points we specified. In that sense, all models represent near-perfect solutions to this unconstrained, not-regularized, optimization problem. They fit the data well. Now, we can introduce an additional constraint that we want a simple model, e.g., smooth, differentiable, integrable. We are looking for an easy to interpret model. This can be accomplished by adding a *regularization term* to the objective function that requires in addition to passing through (or near-by) the anchor points, the model to be \"simple\". Which of the different models appear simpler in the example below? The *fidelity* of the model is captured by how closely it fits the set of anchor points (see RMSE error). The model *regularizer* enforces simple model representation, i.e., lower polynomial order. \n\nThe following example demonstrates the heuristics of fitting a regularized model where the objective function is a mixture of a fidelity term (polynomial fit to data) and a penalty term (enforcing conditions restricting the model flexibility to specific points).\n\n\n\n### Matrix notation  \n\nWe should review the basics of matrix notation, linear algebra, and matrix computing we covered in [Chapter 3](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html). At the core of matrix manipulations are scalars, vectors and matrices.\n\n - ${y}_i$: output or response variable, $i = 1, ..., n$ (cases, subjects, units, etc.)\n - $x_{ij}$: input, predictor, or feature variable,  $1\\leq j \\leq k,\\ 1\\leq i \\leq n.$\n\n$${y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\ ,$$\n\nand \n\n$$\\quad {X} = \n  \\begin{pmatrix} \n    x_{1,1} & x_{1,2} & \\cdots & x_{1,k} \\\\ \n    x_{2,1} & x_{2,2} & \\cdots & x_{2,k} \\\\ \n    \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n    x_{n,1} & x_{n,2} & \\cdots & x_{n,k} \n  \\end{pmatrix}_{cases\\times features}.$$\n  \n### Regularized Linear Modeling\n\nIn the [special case where we assume that the covariates are orthonormal and the number of cases exceeds the number of features ($k<n$)](https://doi.org/10.1098%2Frsta.2009.0159), we may have a *design matrix*, $X$, corresponding to an identity cross product matrix $X^T X = I$.\n\n - The ordinary least squares (`OLS`) estimates minimize the following objective function: \n$$\\min_{ \\beta \\in \\mathbb{R}^k } \\left\\{ \\frac{1}{N} \\left\\| y - X \\beta \\right\\|_2^2\\right\\}.$$ \n\nIn general, when $k<n$, $X^T X$ is a non-singular square matrix, which is invertible, and the OLS estimates are expressed analytically by \n\n$$\\hat{\\beta}^{OLS} = (X^T X)^{-1} X^T y.$$\nOtherwise, when $k>n$, the cross product matrix $X^T X)$ is singular, i.e., not invertible, however a related $k\\times k$ matrix $X^TX+\\lambda I$, where $\\lambda$ is a regularization constant, is invertible and leads to a *regularized* linear model solution.\n\n - `LASSO` estimates minimize a modified cost function \n $$\\min_{ \\beta \\in \\mathbb{R}^{k+1}, \\lambda \\in \\mathbb{R}^{+} } \\left\\{ \\frac{1}{N} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\| \\beta \\|_1 \\right\\}.$$ \n\nThese LASSO estimates may be expressed via a soft-thresholding function of the OLS estimates: \n$$\\hat{\\beta}_j = S_{N \\lambda}( \\hat{\\beta}^\\text{OLS}_j ) = \\hat{\\beta}^\\text{OLS}_j \\max \\left( 0, 1 - \\frac{ N \\lambda }{ |\\hat{\\beta}^{OLS}_j| } \\right), $$\n\nwhere $S_{N \\lambda}$ is a soft thresholding operator translating values *towards* zero. This is different from the hard thresholding operator, which *sets* smaller values to zero and leaves larger ones unchanged.\n\n - `Ridge` regression minimizes a similar objective function (using a different norm):\n\n$$\\min_{ \\beta \\in \\mathbb{R}^{k+1}, \\lambda \\in \\mathbb{R}^{+} } \\left\\{ \\frac{1}{N} \\| y - X \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2 \\right\\}.$$\n\nThis yields the ridge estimates $\\hat{\\beta}_j = (1 + N \\lambda )^{-1} \\hat{\\beta}^{OLS}_j$. Thus, ridge regression shrinks all coefficients by a uniform factor, $(1 + N \\lambda)^{-1}$, and does not set any coefficients to zero.\n\n - `Best subset` selection regression, also known as orthogonal matching pursuit (OMP), minimizes the same cost function with respect to the zero-norm:\n\n$$\\min_{ \\beta \\in \\mathbb{R}^{k+1}, \\lambda \\in \\mathbb{R}^{+} } \\left\\{ \\frac{1}{N} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\| \\beta \\|_0 \\right\\}, $$\n\nwhere $\\|.\\|_0$ is the \"$\\ell^0$ norm\", defined for $z\\in R^d$ as  $\\| z \\|_o = m$, where exactly $m$ components of $z$ are nonzero. In this case, a closed form of the parameter estimates is\n\n$$\\hat{\\beta}_j = H_{ \\sqrt{ N \\lambda } } \\left( \\hat{\\beta}^{OLS}_j \\right) = \\hat{\\beta}^{OLS}_j I \\left( \\left| \\hat{\\beta}^{OLS}_j \\right| \\geq \\sqrt{ N \\lambda } \\right), $$\n\nwhere $H_\\alpha$ is a `hard-thresholding` function and $I$ is an indicator function (it is 1 if its argument is true, and 0 otherwise).\n\nThe LASSO estimates may share similar features selection/estimates with both `Ridge` and `Best (OMP)`. This is because they both shrink the magnitude of all the coefficients, like ridge regression, but also set some of them to zero, as in the best subset selection case.  `Ridge` regression scales all of the coefficients by a constant factor, whereas LASSO translates the coefficients towards zero by a constant value and then sets the small values to zero.\n\n#### Ridge Regression\n\n[Ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) relies on $L^2$ regularization to improve the model prediction accuracy. It improves prediction error by shrinking large regression coefficients and reducing overfitting. By itself, ridge regularization does not perform variable selection and does not really help with model interpretation.\n\nLet's show an example using the MLB dataset [01a_data.txt](https://umich.instructure.com/courses/38100/files/folder/data), which includes, player's Name, Team, Position, Height, Weight, and Age. We may fit in any regularized linear mode, e.g., $Weight \\sim Age + Height$.\n\n\nIn the plots above, different colors represent the vector of features, and the corresponding coefficients, displayed as a function of the regularization parameter, $\\lambda$. The top horizontal axis indicates the number of nonzero coefficients at the current value of $\\lambda$. For LASSO regularization, this top-axis corresponds to the effective degrees of freedom (df) for the model. \n\nNotice the usefulness of Ridge regularization for model estimation in highly ill-conditioned problems ($n\\ll k$) where slight feature perturbations may cause disproportionate alterations of the corresponding weight calculations. When $\\lambda$ is very large, the regularization effect dominates the optimization of the objective function and the coefficients tend to zero. At the other extreme, as $\\lambda\\longrightarrow 0$, the resulting model solution tends towards the ordinary least squares (OLS) and the coefficients exhibit large oscillations. In practice, we often may need to tune $\\lambda$ to balance this tradeoff.\n\nAlso note that in the `cv.glmnet` call, the extreme values of the parameter $\\alpha = 0$ (ridge) and $\\alpha = 1$ (LASSO) correspond to different types of regularization, and intermediate values of $0<\\alpha<1$ corresponds to *elastic net* blended regularization.\n\n#### Least Absolute Shrinkage and Selection Operator (LASSO) Regression\n\nEstimating the linear regression coefficients in a linear regression model using LASSO involves minimizing an objective function that includes an $L^1$ regularization term which tends to shrink the number of features. A descriptive representation of the fidelity (left) and regularization (right) terms of the objective function are shown below:\n\n$$\\underbrace{\\sum_{i=1}^n \\left [ y_i - \\beta_0 - \\sum_{j=1}^k \\beta_j x_{ij} \\right ]^2}_{\\text{fidelity term}} + \\underbrace{\\lambda\\sum_{j=1}^{k}|\\beta_j|}_{\\text{regularization term}}.$$\n\nLASSO jointly achieves model quality, reliability and variable selection by penalizing the sum of the absolute values of the regression coefficients. This forces the shrinkage of certain coefficients effectively acting as a variable selection process. This is similar to ridge regression's penalty on the sum of the squares of the regression coefficients, although ridge regression only shrinks the magnitude of the coefficients without truncating them to $0$.\n\nLet's show how to select the regularization weight parameter $\\lambda$ using `training` data and report the error using `testing` data.\n\n\nLet's retrieve the estimates of the model coefficients.\n\n\nPerhaps obtain a classical OLS linear model, as well.\n\n\nThe OLS linear (unregularized) model has slightly larger coefficients and greater MSE than LASSO, which attests to the shrinkage of LASSO.\n\n\nCompare the results of the three alternative models (LM, LASSO and Ridge) for these data and contrast the derived RMS results.\n\n\nAs both the *inputs* (features or predictors) and the *output* (response) are observed for the testing data, we can build a learner examining the relationship between the two types of features (controlled covariates and observable responses). Most often, we are interested in forecasting or predicting responses based on prospective (new, testing, or validation) data.\n\n### Predictor Standardization\n \nPrior to fitting regularized linear modeling and estimating the effects, covariates may be standardized. Scaling the features ensures the measuring units of the features do not bias the distance measures or norm estimates. Standardization can be accomplished by using the classic [\"z-score\" formula](https://wiki.socr.umich.edu/index.php/AP_Statistics_Curriculum_2007_Normal_Prob#General_Normal_Distribution]). This puts each predictor on the same scale (unitless quantities) - the mean is 0 and the variance is 1. We use $\\hat{\\beta_0} = \\bar{y}$, for the mean intercept parameter, and estimate the coefficients of the remaining predictors. To facilitate interpretation of the results, after the model is estimated, in the context of the specific case-study, we can transform the results back to the original scale/units.\n\n### Estimation Goals\n\nThe basic problem is this: given a set of predictors ${X}$, find a function, $f({X})$, to model or predict the outcome $Y$.\n\nLet's denote the objective (loss or cost) function by $L(y, f({X}))$. It determines adequacy of the fit and allows us to estimate the squared error loss:\n$$L(y, f({X})) = (y - f({X}))^2 . $$\n\nWe are looking to find $f$ that minimizes the expected loss:\n$$  E[(Y - f({X}))^2] \\Rightarrow f = E[Y | {X} = {x}].$$\n\n### Linear Regression\n\nFor a linear model:\n$$Y_i = \\beta_0 + x_{i,1}\\beta_1 + x_{i,2}\\beta_2 + \\dots + x_{i,k}\\beta_k + \\epsilon, $$\nLet's assume that:\n\n* The model shorthand matrix notation is: $Y = X\\beta + \\epsilon.$\n* And the expectation of the observed outcome given the data, $E[Y | {X} = {x}]$, is a linear function, which in certain situations can be expressed as:\n$$\\arg\\min_{{\\beta}} \\sum_{i=1}^n \\left (y_i - \\sum_{j=1}^{k} x_{ij} \\beta_j \\right )^2 = \\arg\\min_{{\\beta}} \\sum_{i=1}^{n} (y_i - x_{i}^T \\beta)^2.$$\n\nMultiplying  both hand-sides on the left by $X^T=X'$, which is the transpose of the design matrix $X$ (recall that matrix multiplication is not always commutative), yields:\n$$X^T Y = X^T (X\\beta) = (X^TX)\\beta.$$\n\nTo solve for the effect-size coefficients, $\\beta$, we can multiply both sides of the equation by the inverse of its (right hand side) multiplier:\n$$(X^TX)^{-1} (X^T Y) = (X^TX)^{-1} (X^TX)\\beta = \\beta.$$\n\nThe *ordinary least squares (OLS)* estimate of ${\\beta}$ is given by:\n$$\\hat{{\\beta}} = \n  \\arg\\min_{{\\beta}} \\sum_{i=1}^n (y_i - \\sum_{j=1}^{k} x_{ij} \\beta_j)^2 = \\arg\\min_{{\\beta}} \\| {y} - {X}{\\beta} \\|^2_2 \\Rightarrow$$\n  $$\\hat{{\\beta}}^{OLS} = ({X}'{X})^{-1} {X}'{y} \\Rightarrow \\hat{f}({x}_i) = {x}_i'\\hat{{\\beta}}.$$\n\n### Drawbacks of Linear Regression\n\nDespite its wide use and elegant theory, linear regression has some shortcomings.\n\n - Prediction accuracy - Often can be improved upon;\n - Model interpretability - Linear model does not automatically do variable selection.\n\n#### Assessing Prediction Accuracy\n\nGiven a new input, ${x}_0$, how do we assess our prediction $\\hat{f}({x}_0)$?\n\nThe *Expected Prediction Error (EPE)* is:\n$$\\begin{aligned} EPE({x}_0) &= E[(Y_0 - \\hat{f}({x}_0))^2] \\\\\n                   &= \\text{Var}(\\epsilon) + \\text{Var}(\\hat{f}({x}_0)) + \\text{Bias}(\\hat{f}({x}_0))^2 \\\\\n                   &= \\text{Var}(\\epsilon) + MSE(\\hat{f}({x}_0)) \\end{aligned} .$$\n\nwhere\n\n - $\\text{Var}(\\epsilon)$: irreducible error variance\n - $\\text{Var}(\\hat{f}({x}_0))$: sample-to-sample variability of $\\hat{f}({x}_0)$ , and\n - $\\text{Bias}(\\hat{f}({x}_0))$: average difference of  $\\hat{f}({x}_0)$ & $f({x}_0)$.\n    \n#### Estimating the Prediction Error \n\nOne common approach to estimating prediction error include:\n\n - Randomly splitting the data into \"training\" and \"testing\" sets, where the testing data has $m$ observations that will be used to independently validate the model quality. We estimate/calculate $\\hat{f}$ using training data;\n - Estimating prediction error using the *testing set MSE*\n$$ \\hat{MSE}(\\hat{f}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{f}(x_i))^2.$$\n\nIdeally, we want our model/predictions to perform well with new or prospective data.\n\n#### Improving the Prediction Accuracy\n\nIf $f(x) \\approx \\text{linear}$, $\\hat{f}$ will have low bias but possibly high variance, e.g., in high-dimensional setting due to correlated predictors, when $k\\ \\text{features} \\ll n\\ \\text{cases}$, or under-determination, when $k > n$.  The goal is to minimize total error by trading off bias (centrality) and precision (variability).\n\n$$MSE(\\hat{f}(x)) = \\text{Var}(\\hat{f}(x)) +\\text{Bias}(\\hat{f}(x))^2.$$\nWe can sacrifice bias to reduce variance, which may lead to decrease in $MSE$. So, regularization allows us to tune this tradeoff.\n\nWe aim to predict the outcome variable, $Y_{n\\times1}$, in terms of other features $X_{n,k}$. Assume a first-order relationship relating $Y$ and $X$ is of the form $Y=f(X)+\\epsilon$, where the error term is $\\epsilon \\sim N(0,\\sigma)$. An estimate model $\\hat{f}(X)$ can be computed in many different ways (e.g., using least squares calculations for linear regressions, Newton-Raphson, steepest descent, stochastic gradient descent, or other methods). Then, we can decompose the expected squared prediction error at $x$ as:\n\n$$E(x)=E[(Y-\\hat{f}(x))^2] = \\underbrace{\\left ( E[\\hat{f}(x)]-f(x) \\right )^2}_{Bias^2} + \n\\underbrace{E\\left [\\left (\\hat{f}(x)-E[\\hat{f}(x)] \\right )^2 \\right]}_{\\text{precision (variance)}} + \\underbrace{\\sigma^2}_{\\text{irreducible error (noise)}}.$$\n\nWhen the true $Y$ vs. $X$ relation is not known, infinite data may be necessary to calibrate the model $\\hat{f}$ and it may be impractical to jointly reduce both the model *bias* and *variance*. In general, minimizing the *bias* at the same time as minimizing the *variance* may not be possible.\n\nThe figure below illustrates diagrammatically the dichotomy between *bias* and *precision* (variance), additional information is available in the [SOCR SMHS EBook](https://wiki.socr.umich.edu/index.php/SMHS_BiasPrecision).\n\n![](https://wiki.socr.umich.edu/images/d/dd/SMHS_BIAS_Precision_Fig_1_cg_07282014.png)\n\n### Variable Selection\n\nOftentimes, we are only interested in using a subset of the original features as model predictors. Thus, we need to identify the most relevant predictors, which usually capture the big picture of the process. This helps us avoid overly complex models that may be difficult to interpret. Typically, when considering several models that achieve similar results, it's natural to select the simplest of them.\n\nLinear regression does not directly determine the importance of features to predict a specific outcome. The problem of selecting critical predictors is therefore very important.\n\nAutomatic feature subset selection methods should directly determine an optimal subset of variables. Forward or backward stepwise variable selection and forward stagewise are examples of classical methods for choosing the best subset by assessing various metrics like [$MSE$, $C_p$, AIC, or BIC](https://doi.org/10.1080/03610918.2012.737491).\n\n### Regularization Framework\n\nAs before, we start with a given ${X}$ and look for a (linear) function, $f({X})=\\sum_{j=1}^{p} {x_{j} \\beta_j}$, to model or predict $y$ subject to certain objective cost function, e.g., squared error loss. Adding a second term to the cost function minimization process yields (model parameter) estimates expressed as:\n\n$$\\hat{{\\beta}}(\\lambda) = \\arg\\min_{\\beta} \n  \\left\\{\\sum_{i=1}^n (y_i - \\sum_{j=1}^{k} {x_{ij} \\beta_j})^2 \n  + \\lambda J({\\beta})\\right\\}$$\n\nIn the above expression, $\\lambda \\ge 0$ is the regularization (tuning or penalty) parameter, $J({\\beta})$ is a `user-defined penalty function` - typically, the intercept is not penalized.\n \n#### Role of the *Penalty Term*\n\nConsider $\\arg\\min J({\\beta}) = \\sum_{j=1}^k \\beta_j^2 =\\| {\\beta} \\|^2_2$ (Ridge Regression, *RR*).\n\nThen, the formulation of the regularization framework is:\n$$\\hat{{\\beta}}(\\lambda)^{RR} = \\arg\\min_{{\\beta}} \n  \\left\\{\\sum_{i=1}^n \\left (y_i - \\sum_{j=1}^{k} x_{ij} \\beta_j\\right )^2 + \n  \\lambda \\sum_{j=1}^k \\beta_j^2 \\right\\}.$$\n\nOr, alternatively:\n\n$$\\hat{{\\beta}}(t)^{RR} = \\arg\\min_{{\\beta}} \n  \\sum_{i=1}^n \\left (y_i - \\sum_{j=1}^{k} x_{ij} \\beta_j\\right )^2, $$ \nsubject to\n$$\\sum_{j=1}^k \\beta_j^2 \\le t .$$\n\n#### Role of the *Regularization Parameter*\n\nThe regularization parameter $\\lambda\\geq 0$ directly controls the bias-variance trade-off:\n\n - $\\lambda = 0$ corresponds to OLS, and\n - $\\lambda \\rightarrow \\infty$ puts more weight on the penalty function and results in more shrinkage of the coefficients, i.e., we introduce bias at the sake of reducing the variance.\n\nThe choice of $\\lambda$ is crucial and will be discussed below as each $\\lambda$ results in a different solution $\\hat{{\\beta}}(\\lambda)$.  \n\n#### LASSO\n\nThe LASSO (Least Absolute Shrinkage and Selection Operator) regularization relies on: \n$$\\arg\\min J({\\beta}) = \\sum_{j=1}^k |\\beta_j| =  \\| {\\beta} \\|_1,$$\nwhich leads to the following objective function:\n$$\\hat{{\\beta}}(\\lambda)^{L} = \\arg\\min_{\\beta} \n  \\left\\{\\sum_{i=1}^n \\left (y_i - \\sum_{j=1}^{k} x_{ij} \\beta_j\\right )^2 + \n  \\lambda \\sum_{j=1}^k |\\beta_j| \\right\\}.$$\n\nIn practice, subtle changes in the penalty terms frequently lead to big differences in the results. Not only does the regularization term shrink coefficients towards zero, but it sets some of them to be exactly zero. Thus, it performs continuous variable selection, hence the name, Least Absolute Shrinkage and Selection Operator (LASSO).\n\nFor further details, see the [Tibshirani's LASSO website](https://statweb.stanford.edu/~tibs/lasso.html).\n\n### General Regularization Framework\n\nThe general regularization framework involves optimization of a more general objective function:\n\n$$\\min_{f \\in {\\mathcal{H}}} \\sum_{i=1}^n \\left\\{L(y_i, f(x_i)) + \\lambda J(f)\\right\\}, $$\n\nwhere $\\mathcal{H}$ is a space of possible functions, $L$ is the *fidelity term*, e.g., squared error, absolute error, zero-one, negative log-likelihood (GLM), hinge loss (support vector machines), and $J$ is the *regularizer*, e.g., [ridge regression, LASSO, adaptive LASSO, group LASSO, fused LASSO, thresholded LASSO, generalized LASSO, constrained LASSO, elastic-net, Dantzig selector, SCAD, MCP, smoothing splines](https://www.stat.cmu.edu/~ryantibs/papers/genlasso.pdf), etc. \n\nThis represents a very general and flexible framework that allows us to incorporate prior knowledge (sparsity, structure, etc.) into the model estimation.\n\n### Likelihood Ratio Test (LRT), False Discovery Rate (FDR), and Logistic Transform\n\nThese concepts will be important in theoretical model development as well as in the applications we show below.\n\n#### Likelihood Ratio Test (LRT)\n\nThe Likelihood Ratio Test (LRT) compares the data fit of two models. For instance, removing predictor variables from a model may reduce the model quality (i.e., a model will have a lower log likelihood). To statistically assess whether the observed difference in model fit is significant, the LRT compares the difference of the log likelihoods of the two models. When this difference is statistically significant, the full model (the one with more variables) represents a better fit to the data, compared to the reduced model. LRT is computed using the log likelihoods ($ll$) of the two models:\n\n$$LRT = -2 \\ln\\left (\\frac{L(m_1)}{L(m_2)}\\right )  = 2(ll(m_2)-ll(m_1)), $$\nwhere:\n\n - $m_1$ and $m_2$ are the reduced and the full models, respectively,\n - $L(m_1)$ and $L(m_2)$ denote the likelihoods of the 2 models, and\n - $ll(m_1)$ and $ll(m_2)$ represent the *log likelihood* (natural log of the model likelihood function).\n\nAs $n\\longrightarrow \\infty$, the distribution of the LRT is asymptotically chi-squared with degrees of freedom equal to the number of parameters that are reduced (i.e., the number of variables removed from the model). In our case, $LRT \\sim \\chi_{df=2}^2$, as we have an intercept and one predictor (SE), and the null model is empty (no parameters).\n\n#### False Discovery Rate (FDR)\n\nThe FDR rate measures the performance of a test:\n\n$$\\underbrace{FDR}_{\\text{False Discovery Rate}} =\\underbrace{E}_{\\text{expectation}} \\underbrace{\\left( \\frac{\\# False Positives}{\\text{total number of selected features}}\\right )}_{\\text{False Discovery Proportion}}.$$\n\nThe Benjamini-Hochberg (BH) FDR procedure involves ordering the p-values, specifying a target FDR, calculating and applying the threshold. Below we show how this is accomplished in R.\n\n\nStarting with the smallest p-value and moving up, we find that the largest $k$ for which the corresponding p-value is less than its threshold, $\\alpha^*$, which yields an index $\\hat{k}=4$.\n\nNext, the algorithm rejects the null hypotheses for the tests that correspond to p-values with indices $k\\leq \\hat{k}=4$, i.e., we determine that $p_{(1)}, p_{(2)}, p_{(3)}, p_{(4)}$ survive FDR correction for multiple testing.\n\n*Note*: Since we controlled FDR at $\\alpha^*=0.05$, we expect that on average only 5% of the tests that we rejected are spurious. In other words, of the FDR-corrected p-values, only about $\\alpha^*=0.05$ are expected to represent false-positives, e.g., features chosen to be salient, that are in fact not really important.\n\nAs a comparison, the *Bonferroni corrected* $\\alpha$-value for these data is $\\frac{0.05}{14} = 0.0036$. Note that Bonferroni coincides with the 1-st threshold value corresponding to the smallest p-value. If we had used this correction for multiple testing, then we would have concluded that *none* of our $14$ results were significant!\n\n#### Graphical Interpretation of the Benjamini-Hochberg (BH) Method\n\nThere's an intuitive graphical interpretation of the BH calculations. \n\n - Sort the $p$-values from largest to smallest.\n - Plot the ordered p-values $p_{(k)}$ on the y-axis versus their indices on the x-axis.\n - Superimpose on this plot a line that passes through the origin and has slope $\\alpha^*$.\n\nAny p-value that falls on or below this line corresponds to a significant result.\n\n\n#### FDR adjusting the $p$-values\n\n`R` can automatically perform the Benjamini-Hochberg procedure. The adjusted p-values are obtained by\n\n\nThe adjusted p-values indicate the corresponding null hypothesis we need to reject to preserve the initial $\\alpha^*$ false-positive rate. We can also compute the adjusted p-values as follows:\n\n\nNote that the manually computed (`adj.p`) and the automatically computed (`pvals.adjusted`) adjusted-p-values are the same.\n\n### Logistic Transformation\n\nFor **binary outcome variables**, or **ordinal categorical variables**, we may need to employ the `logistic curve` to transform the polytomous outcomes into real values.\n\nThe Logistic curve is $y=f(x)=  \\frac{1}{1+e^{-x}}$, \nwhere y and x represent probability and quantitative-predictor values, respectively. A slightly more general form is: $y=f(x)=  \\frac{K}{1+e^{-x}}$, where the covariate $x \\in (-\\infty, \\infty)$ and the response $y \\in [0, K]$. For example, \n\n\nThe point of this logistic transformation is that:\n$$y=  \\frac{1}{1+e^{-x}} \\Longleftrightarrow x=\\ln\\frac{y}{1-y},$$\nwhich represents the `log-odds` (when $y$ is the probability of an event of interest)!!!\n\nWe use the logistic regression equation model to estimate the probability of specific outcomes:\n\n(Estimate of)$P(Y=1| x_1, x_2, \\cdots, x_l)=  \\frac{1}{1+e^{-(a_o+\\sum_{k=1}^l{a_k x_k })}}$, \nwhere the coefficients $a_o$ (intercept) and effects $a_k, k = 1, 2, \\cdots, l$, are estimated using GLM according to a maximum likelihood approach. Using this model allows us to estimate the probability of the dependent (clinical outcome) variable $Y=1$ (CO), i.e., surviving surgery, given the observed values of the predictors $X_k, k = 1, 2, \\cdots, l$.\n\n#### Example: Heart Transplant Surgery\n\nLet's look at an example of estimating the **probability of surviving a heart transplant based on surgeon's experience**. Suppose a group of 20 patients undergo heart transplantation with different surgeons having experience in the range {0(least), 2, ..., 10(most)}, representing 100's of operating/surgery hours. How does the surgeon's experience affect the probability of the patient surviving?\n\nThe data below shows the outcome of the surgery (1=survival) or (0=death) according to the surgeons' experience in 100's of hours of practice.\n\n\n\nGraph of a logistic regression curve showing probability of surviving the surgery versus surgeon's experience.\n\nThe graph shows the probability of the clinical outcome, survival, (Y-axis) versus the surgeon's experience (X-axis), with the logistic regression curve fitted to the data.\n\n\nThe output indicates that a surgeon's experience (SE) is significantly associated with the probability of surviving the surgery (0.0157, Wald test). The output also provides the coefficients for:\n\n* Intercept = -4.1030 and  SE = 0.7583.\n\nThese coefficients can then be used in the logistic regression equation model to estimate the probability of surviving the heart surgery:\n\nProbability of surviving heart surgery $CO =1/(1+exp(-(-4.1030+0.7583\\times SE)))$\n\nFor example, for a patient who is operated by a surgeon with 200 hours of operating experience (SE=2), we plug in the value 2 in the equation to get an estimated probability of survival, $p=0.07$:\n\n\n[1] 0.07001884\n\nSimilarly, a patient undergoing heart surgery with a doctor that has 400 operating hours experience (SE=4), the estimated probability of survival is p=0.26:\n\n\n[1] 0.2554411\n\nThe table below shows the probability of surviving surgery for several values of surgeons' experience.\n\nThe output from the logistic regression analysis yields an SE effect of $\\beta=0.0157$, which is based on the Wald z-score. In addition to the Wald method, we can calculate the p-value for logistic regression using the Likelihood Ratio Test (LRT), which for these data yields $0.0006476922$.\n\n\nThe *logit* of a number $0\\leq p\\leq 1$ is given by the formula: $logit(p)=log\\frac{p}{1-p}$, and represents the log-odds ratio (of survival in this case).\n\n\nSo, why `exponentiating the coefficients`? Because, \n\n$$logit(p)=\\log\\frac{p}{1-p} \\longrightarrow e^{logit(p)} =e^{\\log\\frac{p}{1-p}}\\longrightarrow RHS=\\frac{p}{1-p}, \\ \n\\text{(odds-ratio, OR)}.$$\n\n\n  + exp(coef(mylogit))\n  \n  +  coef(mylogit)    \t# raw logit model coefficients \n\n\nWe can compute the LRT and report its p-value by using the `with()` function:\n\n`with(mylogit, df.null - df.residual)`\n\n\nLRT p-value < 0.001 tells us that our model as a whole fits significantly better than an empty model. The deviance residual `mylogit$deviance` is `-2*log likelihood`, and we can report the model's log likelihood by:\n\n\n\n### Implementation of Regularization\n\nBefore we dive into the theoretical formulation of model regularization, let's start with a specific application that will ground the subsequent analytics.\n\n#### Example: Neuroimaging-genetics study of Parkinson's Disease Dataset\n\nMore information about this specific study and the included derived [neuroimaging biomarkers is available here](https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata). A link to the data and a brief summary of the features are included below:\n\n - [05_PPMI_top_UPDRS_Integrated_LongFormat1.csv](https://umich.instructure.com/files/330397/download?download_frd=1)\n - Data elements include: FID_IID, L_insular_cortex_ComputeArea, L_insular_cortex_Volume, R_insular_cortex_ComputeArea, R_insular_cortex_Volume, L_cingulate_gyrus_ComputeArea, L_cingulate_gyrus_Volume, R_cingulate_gyrus_ComputeArea, R_cingulate_gyrus_Volume, L_caudate_ComputeArea, L_caudate_Volume, R_caudate_ComputeArea, R_caudate_Volume, L_putamen_ComputeArea, L_putamen_Volume, R_putamen_ComputeArea, R_putamen_Volume, Sex, Weight, ResearchGroup, Age, chr12_rs34637584_GT, chr17_rs11868035_GT, chr17_rs11012_GT, chr17_rs393152_GT, chr17_rs12185268_GT, chr17_rs199533_GT, UPDRS_part_I, UPDRS_part_II, UPDRS_part_III, time_visit\n\nNote that the dataset includes missing values and repeated measures.\n\nThe *goal* of this demonstration is to use `OLS`, `ridge regression`, and `LASSO` to **find the best predictive model for the clinical outcomes** -- UPDRS score (vector) and Research Group (factor variable), in terms of demographic, genetics, and neuroimaging biomarkers.\n\nWe can utilize the `glmnet` package in R for most calculations.\n\n\nReaders are encouraged to compare and contrast the resulting *ridge* and *LASSO* models.\n\n### Computational Complexity\n\nRecall that the regularized regression estimates depend on the regularization parameter $\\lambda$. Fortunately, efficient algorithms for choosing optimal $\\lambda$ parameters do exist. Examples of solution path algorithms include:\n\n - [LARS Algorithm for the LASSO](https://projecteuclid.org/euclid.aos/1083178935) (Efron et al., 2004)\n - [Piecewise linearity](https://www.jstor.org/stable/25463590?seq=1#page_scan_tab_contents) (Rosset & Zhu, 2007)\n - [Generic path algorithm](https://dx.doi.org/10.1080/01621459.2013.864166) (Zhou & Wu, 2013)\n - [Pathwise coordinate descent](https://projecteuclid.org/euclid.aoas/1196438020) (Friedman et al., 2007)\n - [Alternating Direction Method of Multipliers (ADMM)](https://doi.org/10.1561/2200000016) (Boyd et al. 2011)\n\nWe will show how to visualize the relations between the regularization parameter ($\\ln(\\lambda)$) and the number and magnitude of the corresponding coefficients for each specific regularized regression method.\n\n### Ridge and LASSO Algorithms\n\nBelow we will demonstrate *hands-on implementations of the Ridge and LASSO regularization algorithms.* These *manual Ridge/LASSO functions*, `ridge()` and\n`lasso()` draw direct parallels between the mathematical foundations of\nregularized linear modeling, the symbolic matrix computational algorithms, \nand the corresponding empirical validations. In practical linear modeling\napplications, to avoid complexities, improve performance, and ensure\nreproducibility, it's always best to utilize the highly-optimized, \nextensively validated, and generic `R` function `glmnet()`. Again, \nin this demonstration we use the `UPDRS` (Parkinson's disease) dataset\nto predict the latent outcome UPDRS\n\n$$y =UPDRS\\_part\\_I + UPDRS\\_part\\_II + UPDRS\\_part\\_III$$\nusing the other covariates.\n\nFor each regularization scheme (ridge or LASSO) the algorithms will include the following four steps:\n\n 1. Define the regularized loss-function,\n 2. Optimize the objective (loss function), see [DSPA Chapter 13 (Optimization) for details](https://socr.umich.edu/DSPA2/DSPA2_notes/13_FunctionOptimization.html),\n 3. Derive the effect-size estimates $\\beta$'s using matrix algebra, see [DSPA Chapter 3 (Linear Algebra and Matrix Computing) for details](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html),\n 4. Compare and contrast the alternative linear model coefficient estimates.\n\n#### Manual Ridge Linear Modeling\n\n\n#### Manual LASSO Linear Modeling\n\n[This blog post outlines the details of the derivation of coordinate descent for LASSO regularized linear modeling](https://xavierbourretsicotte.github.io/lasso_derivation.html).\n\n##### LASSO algorithmic Solution\n\nThe cyclic coordinate descent LASSO optimization leading to the effect-size estimates\nrequires iteratively going over all features, one at a time, and minimizing the loss\nfunction with respect to each effect-size, $\\beta_i$:\n\n - Start with an initial guess $\\beta^{(0)}=(\\beta^{(0)}_1,\\cdots,\\beta^{(0)}_n)$.\n - At $k+1$ iteration, define $\\beta^{(k+1)}$ in terms of $\\beta^{(k)}$ by iteratively solving the single variable optimization problem $\\beta_i^{(k+1)} = \\arg\\min_{\\omega} f(\\beta_1^{(k+1)}, \\cdots, \\beta_{i-1}^{(k+1)}, \\underbrace{\\omega}_{optimiz.}, \\beta_{i+1}^{(k)}, \\cdots, \\beta_n^{(k)})$, corresponding to $\\beta_i:=\\beta_i - \\alpha \\frac{\\partial f}{\\partial \\beta_i}(\\beta)$.\n - Repeat for each effect-size $\\beta_i,\\ \\forall 1\\leq i\\leq n$.\n\nAs a single variable optimization problem, the LASSO cost function optimization \nhas a closed form solution in the special case of coordinate descent. \nFor normalized data, the closed form solution is defined in terms of the \n*soft threshold* function\n\n$$\\hat{\\beta}_j^{LASSO}=\\mathcal{S}(\\hat{\\beta}_j^{LS}, \\lambda)\\equiv \\begin{aligned}\n     \\begin{cases}\n       \\hat{\\beta}_j^{LS} + \\lambda , & \\text{for} \\ \\hat{\\beta}_j^{LS} < - \\lambda \\\\\n       0, & \\text{for} \\ - \\lambda \\leq \\hat{\\beta}_j^{LS} \\leq \\lambda \\\\\n       \\hat{\\beta}_j^{LS} - \\lambda , & \\text{for} \\ \\hat{\\beta}_j^{LS} > \\lambda \n    \\end{cases}\n\\end{aligned}\\ .$$\n\nIterative coordinate descent updating involves repeating this step either\nuntil a convergence tolerance is achieved or the max number of iterations is exceeded.\n\n$$\\hat{\\beta}_j^{LASSO}=\\mathcal{S}(\\hat{\\beta}_j^{LS} := \\sum_{i=1}^m { \\left ( x_j^{(i)}  \\left (y^{(i)}  - \n\\sum_{k \\neq j}^n \\hat{\\beta}_j^{LASSO} x_k^{(i)} \\right ) \\right )} = \\\\\n\\sum_{i=1}^m x_j^{(i)}  \\left ( y^{(i)}  - \\hat y^{(i)}_{pred} + \\hat{\\beta}_j^{LASSO} x_j^{(i)} \\right )\\ , \\\\\n\\hat{\\beta}_j^{LASSO} :=\\mathcal{S}(\\hat{\\beta}_j^{LS} , \\lambda)\\ .$$\n\nNotice that any constant (intercept) term $\\beta_o$ is not regularized, i.e., \n$\\hat{\\beta}_o^{LASSO}=\\hat{\\beta}_o^{LS}$.\n\nThe example below shows a rudimentary `lasso()` function definition, which has\ncertain level of flexibility in particular to accommodate kernel-based linear regularization\nmodeling.\n\n\n\n### LASSO and Ridge Solution Paths\n\nThe plot for the *LASSO* results can be obtained as followis.\n\n\nSimilarly, the plot for the *Ridge* regularization can be obtained by:\n\n\n### Regression Solution Paths - Ridge vs. LASSO\n\nLet's try to compare the paths of the *LASSO* and *Ridge* regression solutions. Below, you will see that the curves of LASSO are steeper and non-differentiable at some points, which is the result of using the $L_1$ norm. On the other hand, the Ridge path is smoother and asymptotically tends to $0$ as $\\lambda$ increases.\n\nLet's start by examining the joint objective function (including LASSO and Ridge terms):\n\n$$\\min_\\beta \\left (\\sum_i (y_i-x_i\\beta)^2+\\frac{1-\\alpha}{2}||\\beta||_2^2+\\alpha||\\beta||_1\n\\right ),$$\n\nwhere $||\\beta||_1 = \\sum_{j=1}^{p}|\\beta_j|$ and $||\\beta||_2 = \\sqrt{\\sum_{j=1}^{p}||\\beta_j||^2}$ are the norms of $\\boldsymbol\\beta$ corresponding to the $L_1$ and $L_2$ distance measures, respectively. The parameters $\\alpha=0$ and $\\alpha=1$ correspond to *Ridge* and *LASSO* regularization. The following two natural questions raise:\n\n - What if $0 <\\alpha<1$?\n - How does the regularization penalty term affect the optimal solution?\n\nIn [Chapter 3](https://socr.umich.edu/DSPA2/DSPA2_notes/03_LinearAlgebraMatrixComputingRegression.html), we explored the minimal SSE (Sum of Square Error) for the OLS (without penalty) where the feasible parameter ($\\beta$) spans the entire real solution space. In penalized optimization problems, the best solution may actually be unachievable. Therefore, we look for solutions that are \"closest\", within the feasible region, to the enigmatic best solution.\n\nThe effect of the penalty term on the objective function is separate from the *fidelity term* (OLS solution). Thus, the effect of $0\\leq \\alpha \\leq 1$ is limited to the **size and shape of the penalty region**. Let's try to visualize the feasible region as:\n\n - centrosymmetric topology, when $\\alpha=0$, and\n - super diamond topology, when $\\alpha=1$.\n\nBelow is a hands-on demonstration of that process using the following simple quadratic equation solver.\n\n\nTo make this realistic, we will use the [MLB dataset](https://umich.instructure.com/files/330381/download?download_frd=1) to first fit an OLS model. The dataset contains $1,034$ records of *heights and weights* for some current and recent Major League Baseball (MLB) Players. \n\n - *Height*: Player height in inch,\n - *Weight*: Player weight in pounds,\n - *Age*: Player age at time of record.\n\nThen, we can obtain the SSE for any $||\\boldsymbol\\beta||$:\n\n$$SSE = ||Y-\\hat Y||^2 = (Y-\\hat Y)^{T}(Y-\\hat Y)=Y^TY - 2\\beta^TX^TY + \\beta^TX^TX\\beta.$$\n\nNext, we will compute the contours for SSE in several situations.\n\n\n\n\n\n\n\n\n\n\nThen, let's add the six feasible regions corresponding to $\\alpha=0$ (Ridge), $\\alpha=\\frac{1}{9}$, $\\alpha=\\frac{1}{5}$, $\\alpha=\\frac{1}{2}$, $\\alpha=\\frac{3}{4}$ and $\\alpha=1$ (LASSO).\n\nThis figure provides some intuition into the continuum from Ridge to LASSO regularization. The feasible regions are drawn as ellipse contours of the SSE in *red*. Curves around the corresponding feasible regions represent the *boundary of the constraint function* $\\frac{1-\\alpha}{2}||\\beta||_2^2+\\alpha||\\beta||_1\\leq t$.\n\nIn this example, $\\beta_2$ shrinks to $0$ for $\\alpha=\\frac{1}{5}$, $\\alpha=\\frac{1}{2}$, $\\alpha=\\frac{3}{4}$ and $\\alpha=1$.\n\nWe observe that it is almost impossible for the contours of Ridge regression to touch the circle at any of the coordinate axes. This is also true in higher dimensions ($nD$), where the $L_1$ and $L_2$ metrics are unchanged and the 2D ellipse representations of the feasibility regions become hyper-ellipsoidal shapes.\n\nGenerally, as $\\alpha$ goes from $0$ to $1$. The coefficients of more features tend to shrink towards $0$. This specific property makes LASSO useful for variable selection.\n\nBy Lagrangian duality, any solution of $\\min_\\beta {||Y-X\\beta||^2_2} +\\lambda ||\\beta||_2$ and $\\min_\\beta {||Y-X\\beta||_1} +\\lambda ||\\beta||_1$ must also represent a solution to the corresponding Ridge ($\\hat{\\beta}^{RR}$) or LASSO ($\\hat{\\beta}^{L}$) optimization problems:\n\n$$\\min_\\beta {||Y-X\\beta||^2_2},\\ \\ \\text{subject to}\\ \\ ||\\beta||_2 \\leq||\\hat{\\beta}^{RR}||_2, $$\n$$\\min_\\beta {||Y-X\\beta||_1},\\ \\ \\text{subject to}\\ \\ ||\\beta||_1 \\leq ||\\hat{\\beta}^{L}||_1, $$\n\nSuppose we actually know the values of $||\\hat{\\beta}^{RR}||_2$ and $||\\hat{\\beta}^{L}||_1$, then we can pictorially represent the optimization problem and illustrate the complementary model-fitting, variable selection and shrinkage of the Ridge and LASSO regularization.\n\nThe topologies of the solution (*domain*) regions are different for Ridge and LASSO. Ridge regularization corresponds with ball topology and LASSO with diamond topology. This is because the solution regions are defined by $||\\hat{\\beta}||_2\\leq ||\\hat{\\beta}^{RR}||_2$ and $||\\hat{\\beta}||_1\\leq ||\\hat{\\beta}^{L}||_1$, respectively.\n\nOn the other hand, the topology of the *fidelity term* $||Y-X\\beta||^2_2$ is ellipsoidal, centered at the OLS estimate,  $\\hat{\\beta}^{OLS}$. To solve the optimization problem, \nwe look for the tightest contour around $\\hat{\\beta}^{OLS}$ that hits the solution domain (ball for Ridge or diamond for LASSO). This intersection point would represent the solution estimate $\\hat{\\beta}$.  As the LASSO domain space ($l_1$ unit ball) has these corners, the solution estimate $\\hat{\\beta}$ is likely to be at the corners. Hence LASSO solutions tend to include many zeroes, whereas Ridge regression solutions (constraint set is a round ball) may not.\n\nLet's compare the feasibility regions corresponding to *Ridge* (top, $p1$) and *LASSO* (bottom, $p6$) regularization.\n\n\n\nThen, we can plot the *progression* from Ridge to LASSO.\n(This composite *plot is intense* and may take several minutes to render!)\n\n\n### Choice of the Regularization Parameter\n\nEfficiently obtaining the entire solution path is nice, but we still have to choose a specific $\\lambda$ regularization parameter. This is critical as $\\lambda$ `controls the bias-variance tradeoff`.\n\nTraditional model selection methods rely on various metrics like [Mallows' $C_p$](https://en.wikipedia.org/wiki/Mallows%27s_Cp), [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion), [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion), and adjusted $R^2$.\n\nInternal statistical validation (Cross validation) is a popular modern alternative, which offers some of these benefits:\n\n - Choice is based on predictive performance,\n - Makes fewer model assumptions,\n - More widely applicable.\n\n### Cross Validation Motivation\n\nWe discussed statistical internal *cross validation* (CV) in [Chapter 9](https://socr.umich.edu/DSPA2/DSPA2_notes/09_ModelEvalImprovement.html). \nWhen assessing model performance using a regularized approach, we would like a separate validation set for choosing the parameter $\\lambda$ controlling the weight of the regularizer. Reusing training sets may encourage overfitting and using testing data to pick $\\lambda$ may underestimate the true error rate. Often, when we do not have enough data for a separate validation set, cross validation provides an alternative strategy.\n\n### $n$-Fold Cross Validation\n\nWe have already seen examples of using cross validation, e.g., [Chapter 9](https://socr.umich.edu/DSPA2/DSPA2_notes/09_ModelEvalImprovement.html) provides more details about this internal statistical assessment strategy. \n\nWe can use either automated or manual cross-validation. In either case, the protocol involves the following iterative steps:\n\n 1. Randomly split the training data into $n$ parts (\"folds\").\n 2. Fit a model using data in $n-1$ folds for multiple $\\lambda\\text{s}$.\n 3. Calculate some prediction quality metrics (e.g., MSE, accuracy) on the last remaining fold, see [Chapter 9](https://socr.umich.edu/DSPA2/DSPA2_notes/09_ModelEvalImprovement.html).\n 4. Repeat the process and average the prediction metrics across iterations.\n\nCommon choices of $n$ are 5, 10, and $N$ ($n=N$, the sample size, corresponds to `leave-one-out` CV). The *one standard error* rule suggests choosing a $\\lambda$ value corresponding to a model with the smallest number of parameters, which has MSE within one standard error of the minimum MSE.\n\n### LASSO 10-Fold Cross Validation\n\nNow, let's apply an internal statistical cross-validation to assess the quality of the LASSO and Ridge models, based on our Parkinson's disease case-study. Recall our split of the PD data into training (yTrain, XTrain) and testing (yTest, XTest) sets.\n\n\n\nNote that the `predict()` method applied to `cv.gmlnet` or `glmnet` forecasting models is effectively a function wrapper to `predict.gmlnet()`. According to what you would like to get as a **prediction output**, you can use `type=\"...\"` to specify one of the following types of prediction outputs:\n\n - `type=\"link\"`, reports the linear predictors for \"binomial\", \"multinomial\", \"poisson\" or \"cox\" models; for \"gaussian\" models it gives the fitted values. \n - `type=\"response\"`, reports the fitted probabilities for \"binomial\" or \"multinomial\", fitted mean for \"poisson\" and the fitted relative-risk for \"cox\"; for \"gaussian\" type \"response\" is equivalent to type \"link\". \n - `type=\"coefficients\"`, reports the coefficients at the requested values for `s`. Note that for \"binomial\" models, results are returned only for the class corresponding to the second level of the factor response. \n - `type=\"class\"`, applies only to \"binomial\" or \"multinomial\" models, and produces the class label corresponding to the maximum probability. \n - `type=\"nonzero\"`, returns a list of the indices of the nonzero coefficients for each value of `s`.\n\n### Stepwise OLS (ordinary least squares)\n\nFor a fair comparison, let's also obtain an OLS stepwise model selection, which we presented earlier.\n\n\nWe use `direction=both` for both *forward* and *backward* selection and choose the optimal one. `k=2` specifies AIC and BIC criteria, and you can choose $k\\sim \\log(n)$.\n\nThen, we use the `ols_step` model to predict the outcome $Y$ for some new test data.\n\n\nAlternatively, we can predict the outcomes directly using the `predict()` function, and the results should be identical:\n\n\n### Final Models\n\nLet's identify the most important (predictive) features, which can then be interpreted in the context of the specific data.\n\n\nThis plot shows a rank-ordered list of the key predictors of the clinical outcome variable (total UPDRS, `y <- data1$UPDRS_part_I + data1$UPDRS_part_II + data1$UPDRS_part_III`).\n\n\n### Model Performance\n\nWe next quantify the performance of the models.\n\n\n### Compare the selected variables \n\n\nStepwise variable selection for OLS selects 12 variables, whereas LASSO selects 9 variables with the best $\\lambda$. There are 6 common variables common for both OLS and LASSO.\n\n### Summary\nTraditional linear models are useful but also have their shortcomings:\n\n - Prediction accuracy may be sub-optimal.\n - Model interpretability may be challenging (especially when a large number of features are used as regressors).\n - Stepwise model selection may improve the model performance and add some interpretations, but still may not be optimal.\n\nRegularization adds a penalty term to the estimation:\n\n -  Enables exploitation of the *bias-variance* tradeoff.\n -  Provides flexibility on specifying penalties to allow for continuous variable selection.\n -  Allows incorporation of prior knowledge.\n \n## Knockoff Filtering (FDR-Controlled Feature Selection)\n\n### Simulated Knockoff Example\n\nVariable selection that controls the false discovery rate (FDR) of *salient features* can be accomplished in different ways. The [knockoff filtering](https://web.stanford.edu/~candes/Knockoffs/) represents one strategy for controlled variable selection.\nTo show the usage of `knockoff.filter` we start with a synthetic dataset constructed so that the true coefficient vector $\\beta$ has only a few nonzero entries.\n\nThe essence of the knockoff filtering is based on the following three-step process:\n\n - Construct the decoy features (knockoff variables), one for each real observed feature. These act as controls for assessing the importance of the real variables.\n - For each feature, $X_j$, compute the knockoff statistic, $W_j$, which measures the importance of the variable, relative to its decoy counterpart, $\\tilde{X}_j$. This *importance* is measured by comparing the corresponding parameter estimates, $\\hat{\\beta}_{X_j}$ and $\\hat{\\beta}_{\\tilde{X}_j}$, obtained via regularized linear modeling (e.g., LASSO).\n - Determine the overall knockoff threshold. This is computed by rank-ordering the $W_j$ statistics (from large to small), walking down the list of $W_j$'s, selecting variables $X_j$ corresponding to positive $W_j$'s, and terminating this search the last time the ratio of negative to positive $W_j$'s is below the default FDR $q$ value, e.g., $q=0.10$.\n\nMathematically, we consider $X_j$ to be *unimportant* (i.e., peripheral or extraneous) if the conditional distribution of $Y$ given $X_1,\\cdots,X_p$ does not depend on $X_j$. Formally, $X_j$ is unimportant if it is conditionally independent of $Y$ given all other features, $X_{-j}$:\n\n$$Y \\perp X_j | X_{-j}.$$\nWe want to generate a Markov Blanket of $Y$, such that the smallest set of features $J$ satisfies this condition. Further, to make sure we do not make too many mistakes, we search for a set $\\hat{S}$ controlling the false discovery rate (FDR):\n\n$$FDR(\\hat{S}) = \\mathrm{E} \\left (\\frac{\\#j\\in \\hat{S}:\\ x_j\\ unimportant}{\\#j\\in \\hat{S}} \\right) \\leq q\\ (e.g.\\ 10\\%).$$\n\nLet's look at one simulation example.\n\n\nTo begin with, we will invoke the `knockoff.filter` using the default settings.\n\n\nThe false discovery proportion (fdp) is:\n\n\nThis yields an approximate FDR of $0.10$.\n\nThe default settings of the knockoff filter uses a test statistic based on LASSO -- `knockoff.stat.lasso_signed_max`, which computes the $W_j$ statistics that quantify the discrepancy between a real ($X_j$) and a decoy, knockoff ($\\tilde{X}_j$), feature coefficient estimates:\n\n$$W_j=\\max(X_j, \\tilde{X}_j) \\times sign(X_j - \\tilde{X}_j). $$\nEffectively, the $W_j$ statistics measures how much more important the variable $X_j$ is relative to its decoy counterpart $\\tilde{X}_j$. The strength of the importance of $X_j$ relative to $\\tilde{X}_j$ is measured by the magnitude of $W_j$.\n\nThe `knockoff` package includes several other test statistics, with appropriate names prefixed by *knockoff.stat*. For instance, we can use a statistic based on forward selection ($fs$) and a lower target FDR of $0.10$.\n\n\nOne can also define additional test statistics, complementing the ones included in the package already. For instance, if we want to implement the following test-statistics:\n\n$$W_j= || X^t . y|| - ||\\tilde{X^t} . y||.$$\n\nWe can code it as:\n\n\n### Knockoff invocation\n\nThe `knockoff.filter` function is a wrapper around several simpler functions that (1) construct knockoff variables (*knockoff.create*); (2) compute the test statistic $W$ (various functions with prefix *knockoff.stat*); and (3) compute the threshold for variable selection (*knockoff.threshold*).\n\nThe high-level function *knockoff.filter* will automatically `normalize the columns` of the input matrix (unless this behavior is explicitly disabled). However, all other functions in this *package assume that the columns of the input matrix have unitary Euclidean norm*.\n\n### PD Neuroimaging-genetics Case-Study\n\nLet's illustrate controlled variable selection via knockoff filtering using the real PD dataset.\n\nThe goal is to determine which imaging, genetics and phenotypic covariates are associated with the clinical diagnosis of PD. The [dataset is available at the DSPA case-study archive site](https://umich.instructure.com/files/330397/download?download_frd=1).\n\n*Preparing the data*\n\nThe data set consists of clinical, genetics, and demographic measurements. To evaluate our results, we will compare diagnostic predictions created by the model for the *UPDRS scores* and the *ResearchGroup* factor variable. \n\n*Fetching and cleaning the data*\n\nFirst, we download the data and read it into data frames.\n\n\n*Preparing the design matrix*\n\nWe now construct the design matrix $X$ and the response vector $Y$. The features (columns of $X$) represent covariates that will be used to explain the response $Y$.\n\n\n*Preparing the response vector*\n\nThe knockoff filter is designed to control the FDR under Gaussian noise. A quick inspection of the response vector shows that it is highly non-Gaussian.\n\n\nA `log-transform` may help to stabilize the clinical response measurements:\n\n\nFor **binary outcome variables**, or **ordinal categorical variables**, we can employ the `logistic curve` to transform the polytomous outcomes into real values.\n\nThe Logistic curve is $y=f(x)=  \\frac{1}{1+e^{-x}}$, \nwhere y and x represent probability and quantitative-predictor values, respectively. A slightly more general form is: $y=f(x)=  \\frac{K}{1+e^{-x}}$, where the covariate $x \\in (-\\infty, \\infty)$ and the response $y \\in [0, K]$.\n\n*Running the knockoff filter*\n\nWe now run the knockoff filter along with the Benjamini-Hochberg (BH) procedure for controlling the false-positive rate of feature selection. More [details about the Knock-off filtering methods are available here](https://www.stat.uchicago.edu/~rina/knockoff/knockoff_slides.pdf).\n\nBefore running either selection procedure, remove rows with missing values, reduce the design matrix by removing predictor columns that do not appear frequently (e.g., at least three times in the sample), and remove any columns that are duplicates.\n\n\nWe see that there are some features that are selected by both methods suggesting they may be indeed salient.\n\n<!---",
      "word_count": 9583
    },
    {
      "title": "length(y)",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "X = X[!missing, ]",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "X = X[, colSums(X) >= 3]",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "X = X[, colSums(abs(cor(X)-1) < 1e-4) == 1]",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "knockoff_selected = names(result$selected)",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "bhq_selected = names(which(p.values <= fdr * cutoff / k))",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "}",
      "content": "#",
      "word_count": 1
    },
    {
      "title": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "content": "## One more example.\n\nSee an [HIV Example](https://cran.r-project.org/web/packages/knockoff/vignettes/hiv.html), including knockoff filter, BH method, evaluation and visualization.\n\n### Bootstrap knockoff and LASSO\nFurther, we can apply Bootstrap to select variables. The major advantage of Bootstrap is to get a variable importance table and various statistics. Notice that when we use Bootstrap, we assume that the data follows the same structure.\n\nFirst, let's try to use Bootstrap LASSO. In each iteration, we randomly select half of the data as a sample and tune on these data to get the best $\\lambda$. Then, record those non-zero variables. Finally, we output the top 10 important variables and their frequency. \n\n\nSimilarly, we can compare these results to the performance of the *knockoff* model, where we need more iterations as we use a somewhat stricter `fdr` control.\n\n\nNow, we can calculate any statistics based on the sampling result.\n\nFinally, you can compare the sampling result of LASSO and knockoff, and compare the result on sampling and on whole data. Do these results make sense?\n--->\n\n## Practice Problems\n\nWe can practice variable selection with the [SOCR_Data_AD_BiomedBigMetadata](https://wiki.socr.umich.edu/index.php/SOCR_Data_AD_BiomedBigMetadata) on SOCR website. This is a smaller dataset that has 744 observations and 63 variables. Here we utilize `DXCURREN` or current diagnostics as the class variable.\n\nLet's import the dataset first.\n\n\nThe data summary shows that we have several factor variables. After converting their type to numeric we find some missing data. We can manage this issue by selecting only the complete observation of the original dataset or by using multivariate imputation, see [Chapter 2](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/02_ManagingData.html). \n\n\nFor simplicity, here we eliminated the missing data and are left with 408 complete observations. Now, we can apply the `Boruta` method for feature selection.\n\n\nYou might get a result that is a little bit different. We can plot the variable importance graph using some previous knowledge.\n\n\nThe final step is to get rid of the tentative features.\n\n\n\nThese techniques can be applied to many [other datasets available in the Case-Studies archive](https://umich.instructure.com/courses/38100/files/).\n\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 422
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    self_contained: yes\n---",
      "row_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": "Method  |  Process Type  |  Goals  |  Approach\n--------|----------------|---------|-----------------------------------------\nVariable selection | Discrete process | To select unique representative features from each group of *similar* features | To identify highly correlated variables and choose a representative feature by post processing the data\nDimension reduction | Continuous process | To denoise the data, enable simpler prediction, or group features so that low impact features have smaller weights | Find the *essential*, $k\\ll n$, components, factors, or clusters representing linear, or nonlinear, functions of the $n$ variables which maximize an objective function like the proportion of explained variance",
      "row_count": 4
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": "Surgeon's Experience (SE) | 1 | 1.5 | 2 | 2.5 | 3 | 3.5 | 3.5 | 4 | 4.5 | 5 | 5.5 | 6 | 6.5 | 7 | 8 | 8.5 | 9 | 9.5 | 10 | 10\n----------------------|---|-----|---|-----|---|-----|-----|---|-----|---|-----|---|-----|---|---|-----|---|-----|----|---\nClinical Outcome (CO) | 0 | 0   | 0 | 0   | 0 | 0   | 1   | 0 | 1   | 0 | 1   | 0 | 1   | 0 | 1 | 1   | 1 | 1   | 1  | 1",
      "row_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": "Surgeon's Experience (SE) | Probability of patient survival (Clinical Outcome)\n----------------------|---------------------------------------------------\n1                     | 0.034\n2                     | 0.07\n3                     | 0.14\n4                     | 0.26\n5                     | 0.423",
      "row_count": 7
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": ".  | Estimate  | Std. Error  | z value | $Pr(\\gt|z|)$ Wald \n---|-----------|-------------|---------|--------------\nSE | 0.7583    | 0.3139      | 2.416   | 0.0157 *",
      "row_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": "(Intercept) | SE \n------------|----\n0.01652254  | 2.13474149 == exp(0.7583456)",
      "row_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": "(Intercept) | \tSE \n------------|------\n-4.1030298 |\t0.7583456",
      "row_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "content": "  .         | OR          | 2.5%          | 97.5%      \n------------|-------------|---------------|---------\n(Intercept) | 0.01652254  | 0.0001825743  | 0.277290\nSE          | 2.13474149  | 1.3083794719  | 4.839986",
      "row_count": 4
    }
  ],
  "r_code": [
    {
      "section": "Variable Importance and Feature Selection",
      "code": "ALS.train <- read.csv(\"https://umich.instructure.com/files/1789624/download?download_frd=1\")\nsummary(ALS.train)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# install.packages(\"Boruta\")\nlibrary(Boruta)\nset.seed(123)\nals <- Boruta(ALSFRS_slope ~ . -ID, data=ALS.train, doTrace=0)\nprint(als)\nals$ImpHistory[1:6, 1:10]",
      "line_count": 6
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(plotly)\n# plot(als, xlab=\"\", xaxt=\"n\")\n# lz<-lapply(1:ncol(als$ImpHistory), function(i)\n# als$ImpHistory[is.finite(als$ImpHistory[, i]), i])\n# names(lz)<-colnames(als$ImpHistory)\n# lb<-sort(sapply(lz, median))\n# axis(side=1, las=2, labels=names(lb), at=1:ncol(als$ImpHistory), cex.axis=0.5, font = 4)\n\ndf_long <- tidyr::gather(as.data.frame(als$ImpHistory), feature, measurement)\n\nplot_ly(df_long, x=~feature, y = ~measurement, color = ~feature, type = \"box\") %>%\n    layout(title=\"Box-and-whisker Plots across all 102 Features (ALS Data)\",\n           xaxis = list(title=\"Features\", categoryorder = \"total descending\"), \n           yaxis = list(title=\"Importance\"), showlegend=F)",
      "line_count": 14
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "final.als <- TentativeRoughFix(als)\nprint(final.als)\nfinal.als$finalDecision\ngetConfirmedFormula(final.als)\n\n# report the Boruta \"Confirmed\" & \"Tentative\" features, removing the \"Rejected\" ones\nprint(final.als$finalDecision[final.als$finalDecision %in% c(\"Confirmed\", \"Tentative\")])\n# how many are actually \"confirmed\" as important/salient?\nimpBoruta <- final.als$finalDecision[final.als$finalDecision %in% c(\"Confirmed\")]; length(impBoruta)",
      "line_count": 9
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(caret)\nlibrary(randomForest)\nset.seed(123)\ncontrol <- rfeControl(functions = rfFuncs, method = \"cv\", number=10)",
      "line_count": 4
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "rf.train <- rfe(ALS.train[, -c(1, 7)], ALS.train[, 7], sizes=c(10, 20, 30, 40), rfeControl=control)\nrf.train",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "plot(rf.train, type=c(\"g\", \"o\"), cex=1, col=1:5)\n\n# df <- as.data.frame(cbind(variables=rf.train$variables$var[1:5], RMSE=rf.train$results$RMSE,\n#                           Rsquared=rf.train$results$Rsquared, MAE=rf.train$results$MAE,\n#                           RMSESD = rf.train$results$RMSESD,\n#                           RsquaredSD= rf.train$results$RsquaredSD, MAESD=rf.train$results$MAESD))\n# \n# data_long <- tidyr::gather(df, Metric, value, RMSE:MAESD, factor_key=TRUE)\n# \n# plot_ly(data_long, x=~variables, y=~value, color=~as.factor(Metric), type = \"scatter\", mode=\"lines\")",
      "line_count": 10
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "predRFE <- predictors(rf.train)\npredBoruta <- getSelectedAttributes(final.als, withTentative = F)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "intersect(predBoruta, predRFE)",
      "line_count": 1
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "data2 <- ALS.train[, -1]\n# Define a base model - intercept only\nbase.mod <- lm(ALSFRS_slope ~ 1 , data= data2)\n# Define the full model - including all predictors\nall.mod <- lm(ALSFRS_slope ~ . , data= data2)\n# ols_step <- lm(ALSFRS_slope ~ ., data=data2)\nols_step <- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = 'both', k=2, trace = F)\nsummary(ols_step); # ols_step",
      "line_count": 8
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# get the shortlisted variable\nstepwiseConfirmedVars <- names(unlist(ols_step[[1]]))\n# remove the intercept \nstepwiseConfirmedVars <- stepwiseConfirmedVars[!stepwiseConfirmedVars %in% \"(Intercept)\"]\nprint(stepwiseConfirmedVars)",
      "line_count": 5
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(mlbench)\nlibrary(caret)\n\n# estimate variable importance\npredStepwise <- varImp(ols_step, scale=FALSE)\n# summarize importance\nprint(predStepwise)\n# plot predStepwise\n# plot(predStepwise)\n\n# Boruta vs. Stepwise feature selection\nintersect(predBoruta, stepwiseConfirmedVars) ",
      "line_count": 12
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# define a function of interest e.g., Runge's function\nrunge <- function(x){\n  runge <- 1/(1+25*x^2)\n}\n\n# define the anchor (knot) points \nknots <- seq(-1, 1, 0.01)\n\n# library(rSymPy)\nlibrary(polynom)\nlibrary(tidyverse)\nlibrary(ModelMetrics)\n\n# function generator:\n# INPUT: (Number of Interpolation nodes - 1 == order) between -1 and 1\n# OUTPUT: A list containing the tuple (data_frame, f)\n#   where data_frame is the corresponding nodes\n#   where f is function object induced by the lagrange interpolation\n\nlag_poly <- function(order) {\n  X_nodes <- seq(-1, 1, 2/order)\n  Y_coor <- runge(X_nodes)\n  f <- as.function(poly.calc(X_nodes,Y_coor))\n  RMSE = rmse(f(knots), runge(knots))\n  print(paste(\"The \", order, \" order polynomial interpolation has this RMS Error:\", RMSE))\n  X <- data.frame(X_nodes)\n  Y <- data.frame(Y_coor)\n  lag_poly <- c(f, X, Y)\n}\n\n#Adding OTHER order functions here\nthird_order <- lag_poly(3)\nfourth_order <- lag_poly(4)\nsixth_order <- lag_poly(6)\neigth_order <- lag_poly(8)\n# twentyth_order <- lag_poly(20)\n\n# unlist other created polynomials\nX_dat <- c(\n  unlist(flatten(third_order[2])),\n  unlist(flatten(fourth_order[2])),\n  unlist(flatten(sixth_order[2])),\n  unlist(flatten(eigth_order[2])) # ,\n  #unlist(flatten(twentyth_order[2]))\n  )\nY_dat <-c(\n  unlist(flatten(third_order[3])),\n  unlist(flatten(fourth_order[3])),\n  unlist(flatten(sixth_order[3])),\n  unlist(flatten(eigth_order[3])) #,\n  # unlist(flatten(twentyth_order[3]))\n  )\nLabels <- c(\n  rep(\"third_order\",4),\n  rep(\"fourth_order\",5),\n  rep(\"sixth_order\",7),\n  rep(\"eigth_order\",9) # ,\n  # rep(\"twentyth_order\",21)\n  )\ndat <- data.frame(X=X_dat,Y=Y_dat,label=Labels) # print(second_order[[2]])\n\nord=8\nX_nodes <- seq(-1, 1, 2/ord)\nY_coor <- runge(X_nodes)\n# fit8 <- lm(Y_coor ~ poly(X_nodes, 8, raw=TRUE))\n\nlibrary(plotly)\nxSample <- seq(-1, 1, length.out = 1000)\nplot_ly(x=~xSample, y=~third_order[[1]](xSample), type=\"scatter\", mode=\"lines\", name=\"third_order\") %>%\n  add_trace(x=~xSample, y=~fourth_order[[1]](xSample), mode=\"lines\", name=\"fourth_order\") %>%\n  add_trace(x=~xSample, y=~sixth_order[[1]](xSample), mode=\"lines\", name=\"sixth_order\") %>%\n  add_trace(x=~xSample, y=~eigth_order[[1]](xSample), mode=\"lines\", name=\"eigth_order\") %>%\n  add_markers(x=~dat$X, y=~dat$Y, mode=\"markers\", name=\"Anchor Points\", marker=list(size=20)) %>%\n  layout(title=\"Objective Functions as a Mix of Fidelity and Regularization Terms\", \n         xaxis=list(title=\"Domain\"), yaxis=list(title=\"Model Range\"))\n\n# the color sequence will be reversed\n# ggplot(dat , aes(x=X, y=Y,color=label)) + labs(title= \"Perfect Fidelity Models\",\n#                       y=\"Y\", x = \"X\")+ scale_color_manual(values = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\",\"#CC79A7\"))+\n#   theme(plot.title = element_text(hjust = 0.5))+\n#   geom_point(size=3,shape=1, aes(colour = label)) + \n#   geom_function(fun = third_order[[1]], size=1, alpha=0.4,color=\"#CC79A7\")+\n#   stat_function(fun = fourth_order[[1]], size=1, alpha=0.4,color = \"#FC4E07\")+\n#   stat_function(fun = sixth_order[[1]], size=1, alpha=0.4,color =  \"#E7B800\")+\n#   stat_function(fun = eigth_order[[1]], size=1, alpha=0.4, color= \"#00AFBB\")+\n#   stat_function(fun = runge, size=1, alpha=0.4)",
      "line_count": 86
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# install.packages(\"doParallel\")\nlibrary(\"doParallel\")\nlibrary(plotly)\nlibrary(tidyr)\n\n# Data: https://umich.instructure.com/courses/38100/files/folder/data   (01a_data.txt)\ndata <- read.table('https://umich.instructure.com/files/330381/download?download_frd=1', as.is=T, header=T)    \nattach(data); str(data)\n\n# Training Data\n# Full Model: x <- model.matrix(Weight ~ ., data = data[1:900, ])\n# Reduced Model\nx <- model.matrix(Weight ~ Age + Height, data = data[1:900, ])\n# creates a design (or model) matrix, and adds 1 column for outcome according to the formula.\ny <- data[1:900, ]$Weight\n\n# Testing Data\nx.test <- model.matrix(Weight ~ Age + Height, data = data[901:1034, ])\ny.test <- data[901:1034, ]$Weight\n\n# install.packages(\"glmnet\")\nlibrary(\"glmnet\")\nlibrary(doParallel)\ncl <- makePSOCKcluster(6)\nregisterDoParallel(cl); getDoParWorkers()\n# getDoParName(); getDoParVersion()\ncv.ridge <-  cv.glmnet(x, y, type.measure=\"mse\", alpha=0, parallel=T)\n## alpha =1 for lasso only, alpha = 0 for ridge only, and 0<alpha<1 to blend ridge & lasso penalty !!!!\n\n# plot(cv.ridge)\n\nplotCV.glmnet <- function(cv.glmnet.object, name=\"\") {\n  df <- as.data.frame(cbind(x=log(cv.glmnet.object$lambda), y=cv.glmnet.object$cvm, \n                           errorBar=cv.glmnet.object$cvsd), nzero=cv.glmnet.object$nzero)\n\n  featureNum <- cv.glmnet.object$nzero\n  xFeature <- log(cv.glmnet.object$lambda)\n  yFeature <- max(cv.glmnet.object$cvm)+max(cv.glmnet.object$cvsd)\n  dataFeature <- data.frame(featureNum, xFeature, yFeature)\n\n  plot_ly(data = df) %>%\n    # add error bars for each CV-mean at log(lambda)\n    add_trace(x = ~x, y = ~y, type = 'scatter', mode = 'markers',\n        name = 'CV MSE', error_y = ~list(array = errorBar)) %>% \n    # add the lambda-min and lambda 1SD vertical dash lines\n    add_lines(data=df, x=c(log(cv.glmnet.object$lambda.min), log(cv.glmnet.object$lambda.min)), \n              y=c(min(cv.glmnet.object$cvm)-max(df$errorBar), max(cv.glmnet.object$cvm)+max(df$errorBar)),\n              showlegend=F, line=list(dash=\"dash\"), name=\"lambda.min\", mode = 'lines+markers') %>%\n    add_lines(data=df, x=c(log(cv.glmnet.object$lambda.1se), log(cv.glmnet.object$lambda.1se)), \n              y=c(min(cv.glmnet.object$cvm)-max(df$errorBar), max(cv.glmnet.object$cvm)+max(df$errorBar)), \n              showlegend=F, line=list(dash=\"dash\"), name=\"lambda.1se\") %>%\n    # Add Number of Features Annotations on Top\n    add_trace(dataFeature, x = ~xFeature, y = ~yFeature, type = 'scatter', name=\"Number of Features\",\n        mode = 'text', text = ~featureNum, textposition = 'middle right',\n        textfont = list(color = '#000000', size = 9)) %>%\n    # Add top x-axis (non-zero features)\n    # add_trace(data=df, x=~c(min(cv.glmnet.object$nzero),max(cv.glmnet.object$nzero)),\n    #           y=~c(max(y)+max(errorBar),max(y)+max(errorBar)), showlegend=F, \n    #           name = \"Non-Zero Features\", yaxis = \"ax\", mode = \"lines+markers\", type = \"scatter\") %>%\n    layout(title = paste0(\"Cross-Validation MSE (\", name, \")\"),\n\t\t\t\t\t\t\txaxis = list(title=paste0(\"log(\",TeX(\"\\\\lambda\"),\")\"),\tside=\"bottom\", showgrid=TRUE), # type=\"log\"\n\t\t\t\t\t\t\thovermode = \"x unified\", legend = list(orientation='h'),  # xaxis2 = ax,  \n\t\t\t\t\t\t\tyaxis = list(title = cv.glmnet.object$name,\tside=\"left\", showgrid = TRUE))\n}\n\nplotCV.glmnet(cv.ridge, \"Ridge\")\n\ncoef(cv.ridge)\nsqrt(cv.ridge$cvm[cv.ridge$lambda == cv.ridge$lambda.1se])\n\n#plot variable feature coefficients against the shrinkage parameter lambda.\nglmmod <-glmnet(x, y, alpha = 0)\nplot(glmmod, xvar=\"lambda\")\ngrid()\n\n# for plot_glmnet with ridge/lasso coefficient path labels\n# install.packages(\"plotmo\")\nlibrary(plotmo) \nplot_glmnet(glmmod, lwd=4)        #default colors\n# More elaborate plots can be generated using:\n# plot_glmnet(glmmod,label=2,lwd=4) #label the 2 biggest final coefficients\n# specify color of each line\n# g <- \"blue\" \n# plot_glmnet(glmmod, lwd=4, col=c(2,g))\n\n\n# report the model coefficient estimates\ncoef(glmmod)[, 1]\n\ncv.glmmod <- cv.glmnet(x, y, alpha=0)\n\nmod.ridge <-  cv.glmnet(x, y, alpha = 0, thresh = 1e-12, parallel = T)\nlambda.best <-  mod.ridge$lambda.min\nlambda.best\nridge.pred <-  predict(mod.ridge, newx = x.test, s = lambda.best)\nridge.RMS <- mean((y.test - ridge.pred)^2); ridge.RMS\nridge.test.r2 <-  1 - mean((y.test - ridge.pred)^2)/mean((y.test - mean(y.test))^2)\n\n#plot(cv.glmmod)\nplotCV.glmnet(cv.glmmod, \"Ridge\")\n\nbest_lambda <- cv.glmmod$lambda.min\nbest_lambda",
      "line_count": 103
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "mod.lasso <-  cv.glmnet(x, y, alpha = 1, thresh = 1e-12, parallel = T)\n## alpha =1 for lasso only, alpha = 0 for ridge only, and 0<alpha<1 for elastic net, a blend ridge & lasso penalty !!!!\nlambda.best <- mod.lasso$lambda.min\nlambda.best\nlasso.pred <- predict(mod.lasso, newx = x.test, s = lambda.best)\nLASSO.RMS <- mean((y.test - lasso.pred)^2); LASSO.RMS",
      "line_count": 6
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "mod.lasso <-  glmnet(x, y, alpha = 1)\npredict(mod.lasso, s = lambda.best, type = \"coefficients\")\nlasso.test.r2 <-  1 - mean((y.test - lasso.pred)^2)/mean((y.test - mean(y.test))^2)",
      "line_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "lm.fit <-  lm(Weight ~ Age + Height, data = data[1:900, ])\nsummary(lm.fit)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "lm.pred <-  predict(lm.fit, newx = x.test)\nLM.RMS <- mean((y - lm.pred)^2); LM.RMS\nlm.test.r2 <-  1 - mean((y - lm.pred)^2) / mean((y.test - mean(y.test))^2)\n  \n# barplot(c(lm.test.r2, lasso.test.r2, ridge.test.r2), col = \"red\", names.arg = c(\"OLS\", \"LASSO\", \"Ridge\"), main = \"Testing Data Derived R-squared\")\n\nplot_ly(x = c(\"OLS\", \"LASSO\", \"Ridge\"),  y = c(lm.test.r2, lasso.test.r2, ridge.test.r2),  \n        name = paste0(\"Model \", TeX(\"R^2\") ,\" Performance\"), type = \"bar\") %>%\n  layout(title=TeX(\"\\\\text{Model} \\\\ R^2\\\\ \\\\text{Performance}\")) %>%\n  config(mathjax = 'cdn')",
      "line_count": 10
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(knitr) #  kable function to convert tabular R-results into Rmd tables\n# create table as data frame\nRMS_Table = data.frame(LM=LM.RMS, LASSO=LASSO.RMS, Ridge=ridge.RMS)\n\n# convert to markdown\nkable(RMS_Table, format=\"pandoc\", caption=\"Test Dataset RMS Results\", align=c(\"c\", \"c\", \"c\"))\nstopCluster(cl)",
      "line_count": 7
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# List the p-values (these are typically computed by some statistical\n# analysis, later these will be ordered from smallest to largest)\npvals <- c(0.9, 0.35, 0.01, 0.013, 0.014, 0.19, 0.35, 0.5, 0.63, 0.67, 0.75, 0.81, 0.01, 0.051)\nlength(pvals)\n#enter the target FDR\nalpha.star <- 0.05\n\n# order the p-values small to large\npvals <- sort(pvals); pvals\n\n#calculate the threshold for each p-value\n# threshold[i] = alpha*(i/n), where i is the index of the ordered p-value\nthreshold<-alpha.star*(1:length(pvals))/length(pvals)\n\n# for each index, compare the p-value against its threshold and display the results\ncbind(pvals, threshold, pvals<=threshold)",
      "line_count": 16
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "#generate the \"relative-indices\" (i/n) that will be plotted on the x-axis\nx.values<-(1:length(pvals))/length(pvals)\n\n#select observations that are less than threshold\nfor.test <- cbind(1:length(pvals), pvals)\npass.test <- for.test[pvals <= 0.05*x.values, ]\npass.test\n#use largest k to color points that meet Benjamini-Hochberg FDR test\nlast<-ifelse(is.vector(pass.test), pass.test[1], pass.test[nrow(pass.test), 1])\n\n#widen right margin to make room for labels\npar(mar=c(4.1, 4.1, 1.1, 4.1))\n\n#plot the points (relative-index vs. probability-values)\n# we can also plot the y-axis on a log-scale to spread out the values\n# plot(x.values, pvals, xlab=expression(i/n), ylab=\"log(p-value)\", log = 'y')\n\n# plot(x.values, pvals, xlab=expression(i/n), ylab=\"p-value\")\n# #add FDR line\n# abline(a=0, b=0.05, col=2, lwd=2)\n# #add naive threshold line\n# abline(h=.05, col=4, lty=2)\n# #add Bonferroni-corrected threshold line\n# abline(h=.05/length(pvals), col=4, lty=2)\n# #label lines\n# mtext(c('naive', 'Bonferroni'), side=4, at=c(.05, .05/length(pvals)), las=1, line=0.2)\n# #use largest k to color points that meet Benjamini-Hochberg FDR test\n# points(x.values[1:last], pvals[1:last], pch=19, cex=1.5)\n\nplot_ly(x=~x.values, y=~pvals, type=\"scatter\", mode=\"markers\", marker=list(size=15), \n        name=\"observed p-values\", symbols='o') %>%\n  # add bounding horizontal lines\n  # add naive threshold line\n  add_lines(x=~c(0,1), y=~c(0.05, 0.05), mode=\"lines\", line=list(dash='dash'), name=\"p=0.05\") %>%\n  # add conservative Bonferroni line\n  add_lines(x=~c(0,1), y=~c(0.05/length(pvals), 0.05/length(pvals)), mode=\"lines\", \n            line=list(dash='dash'), name=\"Bonferroni (p=0.05/n)\") %>%\n  # add FDR line\n  add_lines(x=~c(0,1), y=~c(0, 0.05), mode=\"lines\", line=list(dash='dash'), name=\"FDR Line\") %>%\n  # highlight the largest k to color points meeting the Benjamini-Hochberg FDR test\n  add_trace(x=~x.values[1:last], y=~pvals[1:last], mode=\"markers\",symbols='0', name=\"FDR Test Points\") %>%\n  layout (title=\"Benjamini-Hochberg FDR Test\", legend = list(orientation='h'),\n          xaxis=list(title=expression(i/n)), yaxis=list(title=\"p-value\"))",
      "line_count": 43
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "pvals.adjusted <- p.adjust(pvals, \"BH\")\npvals.adjusted",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# manually calculate the thresholds for the ordered p-values list\ntest.p <- length(pvals)/(1:length(pvals))*pvals   # test.p\n\n# loop through each p-value and carry out the manual FDR adjustment for multiple testing\nadj.p <- numeric(14)\nfor(i in 1:14) {\n    adj.p[i]<-min(test.p[i:length(test.p)])\n    ifelse(adj.p[i]>1, 1, adj.p[i])\n}\nadj.p",
      "line_count": 10
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(\"ggplot2\")\nk=7\nx <- seq(-10, 10, 0.1)\n# plot(x, k/(1+exp(-x)), xlab=\"X-axis (Covariate)\", ylab=\"Outcome k/(1+exp(-x)), k=7\", type=\"l\")\n\nplot_ly(x=~x, y=~k/(1+exp(-x)), type=\"scatter\", mode=\"line\", name=\"Logistic model\") %>%\n  layout (title=\"Logistic Model Y=k/(1+exp(-x)), k=7\",\n          xaxis=list(title=\"x\"), yaxis=list(title=\"Y=k/(1+exp(-x))\"))",
      "line_count": 8
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "mydata <- read.csv(\"https://umich.instructure.com/files/405273/download?download_frd=1\")  # 01_HeartSurgerySurvivalData.csv\n# estimates a logistic regression model for the clinical outcome (CO), survival, using the glm \n# (generalized linear model) function. \n# convert Surgeon's Experience (SE) to a factor to indicate it should be treated as a categorical variable.\n# mydata$rank <- factor(mydata$SE)\n# mylogit <- glm(CO ~ SE, data = mydata, family = \"binomial\")\n# library(ggplot2)\n# ggplot(mydata, aes(x=SE, y=CO)) + geom_point() + \n#   \tstat_smooth(method=\"glm\", method.args=list(family = \"binomial\"), se=FALSE)\n\nmylogit <- glm(CO ~ SE, data=mydata, family = \"binomial\")\n\nplot_ly(data=mydata, x=~SE, y=~CO, type=\"scatter\", mode=\"markers\", name=\"Data\", marker=list(size=15)) %>%\n  add_trace(x=~SE, y=~mylogit$fitted.values, type=\"scatter\", mode=\"lines\", name=\"Logit Model\") %>%\n  layout (title=\"Logistic Model Clinical Outcome ~ Surgeon's Experience\",\n          xaxis=list(title=\"SE\"), yaxis=list(title=\"CO\"), hovermode = \"x unified\")",
      "line_count": 16
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "mylogit <- glm(CO ~ SE, data = mydata, family = \"binomial\")\nsummary(mylogit)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "SE=2\nCO =1/(1+exp(-(-4.1030+0.7583*SE)))\nCO",
      "line_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "SE=4; CO =1/(1+exp(-(-4.1030+0.7583*SE))); CO\nCO\n\nfor (SE in c(1:5)) {\n  CO <- 1/(1+exp(-(-4.1030+0.7583*SE))); \n  print(c(SE, CO))\n}",
      "line_count": 7
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "mylogit <- glm(CO ~ SE, data = mydata, family = \"binomial\")\nsummary(mylogit)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "confint(mylogit) ",
      "line_count": 1
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "exp(coef(mylogit)) \t  # exponentiated logit model coefficients",
      "line_count": 1
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "exp(cbind(OR = coef(mylogit), confint(mylogit)))",
      "line_count": 1
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))\n# mylogit$null.deviance - mylogit$deviance  # 11.63365\n# mylogit$df.null - mylogit$df.residual   # 1\n# [1] 0.0006476922\n\n# CONFIRM THE RESULT\n# qchisq(1-with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)), 1)\n# qchisq(1-0.0006476922, 1)",
      "line_count": 8
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "mylogit$deviance   # model residual deviance\n\n-2*logLik(mylogit) # -2 * model_ll",
      "line_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "#### Initial Stuff ####\n# clean up\nrm(list=ls())\n# load required packages\n# install.packages(\"arm\")\nlibrary(glmnet)\nlibrary(arm)\nlibrary(knitr) #  kable function to convert tabular R-results into Rmd tables\n# pick a random seed, but set.seed(seed) only affects the next block of code!\nseed = 1234\n\n#### Organize Data ####\n# load dataset\n# Data: https://umich.instructure.com/courses/38100/files/folder/data  \n# (05_PPMI_top_UPDRS_Integrated_LongFormat1.csv)\ndata1 <- read.table('https://umich.instructure.com/files/330397/download?download_frd=1', sep=\",\", header=T)    \n# we will deal with missing values using multiple imputation later. For now, let's just ignore incomplete cases\ndata1.completeRowIndexes <- complete.cases(data1); table(data1.completeRowIndexes)\nprop.table(table(data1.completeRowIndexes))\nattach(data1)\n# View(data1[data1.completeRowIndexes, ])\n\n# define response and predictors\ny <- data1$UPDRS_part_I + data1$UPDRS_part_II + data1$UPDRS_part_III\ntable(y)   # Show Clinically relevant classification\ny <- y[data1.completeRowIndexes]\n\n# X = scale(data1[,])  # Explicit Scaling is not needed, as glmnet auto standardizes predictors\n# X = as.matrix(data1[, c(\"R_caudate_Volume\", \"R_putamen_Volume\", \"Weight\", \"Age\", \"chr17_rs12185268_GT\")])  # X needs to be a matrix, not a data frame\ndrop_features <- c(\"FID_IID\", \"ResearchGroup\", \"UPDRS_part_I\", \"UPDRS_part_II\", \n                   \"UPDRS_part_III\", \"time_visit\")\nX <- data1[ , !(names(data1) %in% drop_features)]\nX = as.matrix(X)   # remove columns: index, ResearchGroup, and y=(PDRS_part_I + UPDRS_part_II + UPDRS_part_III)\nX <- X[data1.completeRowIndexes, ]\nsummary(X)\n\n# randomly split data into training (80%) and test (20%) sets\nset.seed(seed)\ntrain = sample(1 : nrow(X), round((4/5) * nrow(X)))\ntest = -train\n\n# subset training data\nyTrain = y[train]\nXTrain = X[train, ]\nXTrainOLS = cbind(rep(1, nrow(XTrain)), XTrain)\n  \n# subset test data\nyTest = y[test]\nXTest = X[test, ]\n\n#### Model Estimation & Selection ####\n# Estimate models\nfitOLS = lm(yTrain ~ XTrain)  # Ordinary Least Squares\n# glmnet automatically standardizes the predictors\nfitRidge = glmnet(XTrain, yTrain, alpha = 0)  # Ridge Regression\nfitLASSO = glmnet(XTrain, yTrain, alpha = 1)  # The LASSO",
      "line_count": 56
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# 1. Define RIDGE Loss\n# X: model Design matrix (observed covariates data) \n# y: target (observed outcomes of interest)\n# lambda: regularization parameter (default to 0.1)\n# beta:  effect-size estimates for all covariates in X (incl. intercept)\nridge <- function(beta, X, y, lambda = 0.1) {\n  crossprod(y - X %*% beta) + lambda * length(y) * crossprod(beta)\n}\n\nX = XTrain\ny = yTrain\n\n# 2. Optimize the Ridge objective (loss function), See DSPA Chap. 13 (Optimization)\nresult_ridgeOptimization <-\n  optim(rep(0, ncol(X)), ridge, X=X, y=y, lambda=0.1, method='BFGS')\n\n# 3. Analytic Ridge solution\nresult_ridgeAnalytic <-\n  solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y)\n\n# 4. Official `glmnet()` Ridge solution (alpha=0 corresponds to Ridge)\nlibrary(glmnet)\nglmnet_res <-\n  coef(glmnet(X, y, alpha=0,lambda=c(10, 1, 0.1), \n              thresh=1e-12, intercept=F), s=0.1)\n\n# 4. Compare and Contrast the 4 alternative Linear Model solutions\nlibrary(DT)\ndf <- data.frame(covariates=colnames(X),\n                 lm=coef(lm(y ~ . -1, data.frame(X))),\n                 ridgeOptim  = result_ridgeOptimization$par,\n                 ridgeAnalyt = result_ridgeAnalytic,\n                 glmnet = glmnet_res[-1, 1])\ndatatable(df[, -1], caption=\"Compare & Contrast Ridge and Alternative Linear Model Estimates\")\ndf_long <- df %>%\n  pivot_longer(!covariates, names_to = \"Method\", values_to = \"betaEstimates\")\nplot_ly(data=df_long, x=~covariates, y=~betaEstimates, \n        color=~Method, symbol=~Method, type=\"scatter\", mode=\"markers\") %>%\n  layout(title=\"UPDRS: Comparing Automated and Manual Ridge Linear Model Estimation\")",
      "line_count": 39
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# Use the same UPDRS Parkinson's disease dataset\nX = XTrain   \ny = yTrain\nX = apply(X, 2, scales::rescale, to = c(0, 1))  # normalize/scale the features!\n\nlambda = 0.1\n\ninverse <- function(X, eps = 1e-9) { # a generalization of solve() to avoid singularities\n  eig.X = eigen(X, symmetric = TRUE)\n  P = eig.X[[2]] \n  lambda = eig.X[[1]]\n  # to avoid singularities, identify the indices of all eigenvalues > epsilon\n  ind = lambda > eps\n  lambda[ind]  = 1/lambda[ind] \n  lambda[!ind] = 0\n  P %*% diag(lambda) %*% t(P)\n}\n\n# Default Linear kernel: k(x,y) = x' * y\nrk <- function(s, t) {\n  init_len = length(s)\n  rk = 0\n  for (i in 1:init_len) { rk = s[i]*t[i] + rk }\n  return(rk)\n} \n\n# Gram Function - kernelized cross-product\ngram <- function(X, rkfunc = rk) { # compute the `crossprod` using the specified RK\n  apply(X, 1, function(Row) \n    apply(X, 1, function(tRow) rkfunc(Row, tRow)) # specifies the Reproducing Kernel\n  )  \n}\n\n# When we kernalize the predicting covariate Design matrix X, \n# we need to kernalize the response outcome Y, as well\nkernelized_y <- function(X,y, rkfunc = rk) { # compute the `crossprod` using the specified RK\n  apply(X, 1, function(Row) \n     rkfunc(Row, y) # specifies the Reproducing Kernel\n  )  \n}\n\n# An alternative Laplace kernel, as an additional example\nrk_Laplace <- function(s, t, sigma=1) {\n  if (sigma <= 0) sigma=1  # avoid singularities\n  init_len = length(s)\n  rk_Lap = 0\n  for (i in 1:init_len) { rk_Lap = (s[i] - t[i])^2 + rk_Lap }\n  rk_Lap = exp(-(sqrt(rk_Lap))/sigma)\n  return(rk_Lap)\n} \n\n# Naive LASSO\nlasso <- function(X, y, lambda=0.1, tol=1e-6, iter=100,kernel=rk) { # tolerance & iter-max\n  GramMatrix = gram(t(X),kernel)                            # GramMatrix (nxn) xTX\n\n# n = length(y)                                   \nQ = cbind(1, GramMatrix)                        # Formulate the Design matrix Q\n\n\nM = crossprod(Q)                     # no need for singularity protection as 6*6\nM_inv = inverse(M)                              # (XTXXTX)^(-1)\n\n\nkernel_y  = kernelized_y(t(X),y,kernel)  # kernelized \\phi(y) linear case is XTy\n\ngamma_hat = crossprod(M_inv, crossprod(Q, kernel_y)) # Q and M_inv are both symmetric, this is the final (XTXXTX)^(-1)XTXXTy\n\nJ = ncol(X)\nfor (j in 1:J) { \n  # The explicit formula for LASSO loss\n  gamma_hat[j] = gamma_hat[j]*max(0,1-length(gamma_hat)*lambda/abs(gamma_hat[j]))\n}\n\nf_hat = Q %*% gamma_hat\n  \nA = Q %*% M_inv %*% t(Q)\ntr_A = sum(diag(A))                             # trace of hat matrix\n\nrss  = crossprod(kernel_y - f_hat)                     # residual sum of squares\ngcv  = J*rss / (J - tr_A)^2                     # compute GCV score\n return(list(f_hat = f_hat, beta_hat = \n                gamma_hat, gcv = gcv,rss=rss))\n}\n\n# Test\nlas = lasso(X,y)$beta_hat[-1]\n\n# 3. Official `glmnet()` LASSO solution (alpha=1 corresponds to LASSO)\nglmnet_res <-\n  coef(glmnet(X, y, alpha  = 1, lambda = lambda, \n              thresh=1e-12, intercept=FALSE), s=lambda)\n\n# 5. Compare and Contrast the 4 alternative Linear Model solutions\n# df <- data.frame(covariates=colnames(X), lm=coef(lm(y ~ . -1, data.frame(X))),\n#                  lasso_soft=result_soft, lasso_hard=result_hard,\n#                  glmnet=glmnet_res[-1, 1])\ndf <- data.frame(covariates=colnames(X), lm=coef(lm(y ~ . -1, data.frame(X))),\n                 lasso=las, glmnet=glmnet_res[-1, 1])\n\ndatatable(df[, -1], caption=\"Compare & Contrast LASSO and Alternative Linear Model Estimates\")\ndf_long <- df %>%\n  pivot_longer(!covariates, names_to = \"Method\", values_to = \"betaEstimates\")\nplot_ly(data=df_long, x=~covariates, y=~betaEstimates, \n        color=~Method, symbol=~Method, type=\"scatter\", mode=\"markers\") %>%\n  layout(title=\"UPDRS: Comparing Automated and Manual LASSO Linear Model Estimation\")",
      "line_count": 105
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# # Define a soft-thresholding utility function S(a=beta, b=lambda)\n# soft_thresh <- function(a, b) {\n#   out = rep(0, length(a))\n#   # out[a >  b] = a[a > b] - b\n#   # out[a < -b] = a[a < -b] + b\n#   for (i in 1:length(a)) {\n#     if (a[i] < -b) out[i] = a[i] + b\n#     else if (a[i] > b) out[i] = a[i] - b\n#   }\n#   return(out)\n# }\n# \n# soft_thresh1 <- function(a, b) {\n#   out = 0\n#   if (a < -b) out = a + b\n#   else if (a > b) out = a - b\n#   return(out)\n# }\n# \n# hard_thresh <- function(a, l) {\n#   out = a\n#   out[a < l & a > -l] = 0\n#   return(out)\n# }\n# \n# # 1. Define LASSO Loss - Coordinate gradient descent for lasso regression using normalized data\n# # X: model Design matrix (observed covariates data) \n# # y: target (observed outcomes of interest)\n# # lambda: regularization parameter (default to 0.1)\n# # soft: soft or hard thresholding of LS betas\n# # tol: convergence error tolerance level\n# # iter: maximum number of iterations (to prevent infinite loops)\n# # verbose: Boolean indicator to print out every 10-th iteration index (tracking)\n# lasso <- function(X, y, lambda=0.1,soft=TRUE,tol=1e-6,iter=100,verbose=TRUE) {\n#   # X = scale(X)/norm(X, type=\"O\")\n#   X = scale(X)\n#   # X = X/(sqrt(sum(t(X) %*% X)))\n#   \n#   beta = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y)\n#   tol_curr = 1\n#   J = ncol(X)\n#   a = rep(0, J)\n#   c_ = rep(0, J)\n#   i = 1\n#   \n#   # while (tol < tol_curr && i < iter) {\n#   #   beta_old = beta\n#   #   a = colSums(t(X) %*% X)       # tcrossproduct( , )\n#   #   l = length(y)*lambda          # for consistency with glmnet approach\n#   #   pred = X %*% beta             # predicted\n#   #   c_ = sapply(1:J, function(j)  sum( t(X[,j]) %*% (y - pred + X[,-j] %*% beta_old[-j])))\n#   #   if (soft) {                   # soft threshold\n#   #     for (j in 1:J) {\n#   #       # beta[j] = soft_thresh(c_[j]/a[j], l/a[j])\n#   #       beta[j] = soft_thresh(c_[j], l)\n#   #     }\n#   #   } else {                      # hard threshold\n#   #       beta = beta_old\n#   #       beta[c_< l & c_ > -l] = 0\n#   #   }\n#   # \n#   #   tol_curr = crossprod(beta - beta_old)\n#   #   i = i + 1\n#   #   if (verbose && i%%10 == 0) message(i)  # report verbose iteration\n#   # }\n#   \n#   # beta = rep(1, J)  # initialize the (Jeffrey's uniform prior on the effects, beta)\n#   beta_old = beta\n#   #Looping until max number of iterations\n#   while (tol < tol_curr && i < iter) {\n#     a = colSums(t(X) %*% X)       # tcrossproduct( , ) Normalization factor\n#     # Inner Looping through each coordinate index\n#     for (j in 1:J) {\n#         # Vectorized implementation\n#         X_j = X[,j]\n#         # beta_old[j] = beta[j]\n#         #   a = colSums(t(X) %*% X)       # tcrossproduct( , )\n#         l= lambda                   # length(y)*lambda for consistency with glmnet approach\n#         y_pred = X %*% beta            # predicted\n#         beta_old[j] = sum(t(X_j) %*% (y - y_pred  + beta[j]*X_j)) # /J\n# \n#         if (soft) beta[j] =  soft_thresh(beta_old[j], l)\n#         else beta[j] = hard_thresh(beta_old[j], l)  # hard threshold case\n#     }\n#     tol_curr = crossprod(beta - beta_old)\n#     i = i + 1\n#     if (verbose && i%%10 == 0) message(i)  # report verbose iteration\n#   }\n#   return(beta)\n# }\n# \n# # 2. Optimize the LASSO objective (loss function), See DSPA Chap. 13 (Optimization)\n# result_soft = lasso(X, y, lambda=lambda, tol=1e-12, soft=TRUE)\n# result_hard = lasso(X, y, lambda=lambda, tol=1e-12, soft=FALSE)",
      "line_count": 94
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(RColorBrewer)\n### Plot Solution Path ###\n# LASSO\n# plot(fitLASSO, xvar=\"lambda\", label=\"TRUE\")\n# # add label to upper x-axis\n# mtext(\"LASSO regularizer: Number of Nonzero (Active) Coefficients\", side=3, line=2.5)\n\nplot.glmnet <- function(glmnet.object, name=\"\") {\n  df <- as.data.frame(t(as.matrix(glmnet.object$beta)))\n  df$loglambda <- log(glmnet.object$lambda)\n  df <- as.data.frame(df)\n  data_long <- gather(df, Variable, coefficient, 1:(dim(df)[2]-1), factor_key=TRUE)\n  \n  plot_ly(data = data_long) %>%\n    # add error bars for each CV-mean at log(lambda)\n    add_trace(x = ~loglambda, y = ~coefficient, color=~Variable, \n              colors=colorRampPalette(brewer.pal(10,\"Spectral\"))(dim(df)[2]),  # \"Dark2\", \n              type = 'scatter', mode = 'lines',\n              name = ~Variable) %>%\n    layout(title = paste0(name, \" Model Coefficient Values\"),\n\t\t\t\t\t\t\txaxis = list(title = paste0(\"log(\",TeX(\"\\\\lambda\"),\")\"),\tside=\"bottom\",  showgrid = TRUE),\n\t\t\t\t\t\t  hovermode = \"x unified\", legend = list(orientation='h'),\n\t\t\t\t\t\t\tyaxis = list(title = ' Model Coefficient Values',\tside=\"left\", showgrid = TRUE))\n}\n\nplot.glmnet(fitLASSO, name=\"LASSO\")",
      "line_count": 26
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "### Plot Solution Path ###\n# Ridge\n# plot(fitRidge, xvar=\"lambda\", label=\"TRUE\")\n# # add label to upper x-axis\n# mtext(\"Ridge regularizer: Number of Nonzero (Active) Coefficients\", side=3, line=2.5)\n\nplot.glmnet(fitRidge, name=\"Ridge\")",
      "line_count": 7
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(needs)\n\n# Constructing Quadratic Formula\nquadraticEquSolver <- function(a,b,c){\n  if(delta(a,b,c) > 0){ # first case D>0\n    x_1 = (-b+sqrt(delta(a,b,c)))/(2*a)\n    x_2 = (-b-sqrt(delta(a,b,c)))/(2*a)\n    result = c(x_1,x_2)\n    # print(result)\n  }\n  else if(delta(a,b,c) == 0){ # second case D=0\n    result = -b/(2*a)\n    # print(result)\n  }\n  else {\"There are no real roots.\"} # third case D<0\n}\n# Constructing delta\ndelta<-function(a,b,c){\n  b^2-4*a*c\n}",
      "line_count": 20
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(\"ggplot2\")\n# load data\nmlb <- read.table('https://umich.instructure.com/files/330381/download?download_frd=1', \n                  as.is=T, header=T)\nstr(mlb)\nfit <- lm(Height ~ Weight + Age -1, data = as.data.frame(scale(mlb[,4:6])))\npoints = data.frame(x=c(0,fit$coefficients[1]),y=c(0,fit$coefficients[2]),z=c(\"(0,0)\",\"OLS Coef\"))\n\nY=scale(mlb$Height)\nX = scale(mlb[,c(5,6)])\nbeta1=seq(-0.556, 1.556, length.out = 100)\nbeta2=seq(-0.661, 0.3386, length.out = 100)\ndf <- expand.grid(beta1 = beta1, beta2 = beta2)\nb = as.matrix(df)\ndf$sse <- rep(t(Y)%*%Y,100*100) - 2*b%*%t(X)%*%Y + diag(b%*%t(X)%*%X%*%t(b))",
      "line_count": 15
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "base <- ggplot(df) + \n  stat_contour(aes(beta1, beta2, z = sse),breaks = round(quantile(df$sse, seq(0, 0.2, 0.03)), 0), \n               size = 0.5,color=\"darkorchid2\",alpha=0.8)+ \n  scale_x_continuous(limits = c(-0.4,1))+\n  scale_y_continuous(limits = c(-0.55,0.4))+\n  coord_fixed(ratio=1)+\n  geom_point(data = points,aes(x,y))+\n  geom_text(data = points,aes(x,y,label=z),vjust = 2,size=3.5)+\n  geom_segment(aes(x = -0.4, y = 0, xend = 1, yend = 0),colour = \"grey46\",\n               arrow = arrow(length=unit(0.30,\"cm\")),size=0.5,alpha=0.8)+\n  geom_segment(aes(x = 0, y = -0.55, xend = 0, yend = 0.4),colour = \"grey46\",\n               arrow = arrow(length=unit(0.30,\"cm\")),size=0.5,alpha=0.8)",
      "line_count": 12
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "plot_alpha = function(alpha=0,restrict=0.2,beta1_range=0.2,annot=c(0.15,-0.25,0.205,-0.05)){\n  a=alpha; t=restrict; k=beta1_range; pos=data.frame(V1=annot[1:4])\n  text=paste(\"(\",as.character(annot[3]),\",\",as.character(annot[4]),\")\",sep = \"\")\n  K = seq(0,k,length.out = 50)\n  y = unlist(lapply((1-a)*K^2/2+a*K-t, quadraticEquSolver,\n                    a=(1-a)/2,b=a))[seq(1,99,by=2)]\n  fills = data.frame(x=c(rev(-K),K), y1=c(rev(y),y), y2=c(-rev(y),-y))\n  p<-base+geom_line(data=fills,aes(x = x,y = y1),colour = \"salmon1\",alpha=0.6,size=0.7)+\n    geom_line(data=fills,aes(x = x,y = y2),colour = \"salmon1\",alpha=0.6,size=0.7)+\n    geom_polygon(data = fills, aes(x, y1),fill = \"red\", alpha = 0.2)+\n    geom_polygon(data = fills, aes(x, y2), fill = \"red\", alpha = 0.2)+\n    geom_segment(data=pos,aes(x = V1[1] , y = V1[2], xend = V1[3], yend = V1[4]),\n                 arrow = arrow(length=unit(0.30,\"cm\")),alpha=0.8,colour = \"magenta\")+\n    ggplot2::annotate(\"text\", x = pos$V1[1]-0.01, y = pos$V1[2]-0.11,\n                      label = paste(text,\"\\n\",\"Point of Contact \\n i.e., Coef of\", \"alpha=\",fractions(a)),size=3)+\n    xlab(expression(beta[1]))+\n    ylab(expression(beta[2]))+\n    ggtitle(paste(\"alpha =\",as.character(fractions(a))))+\n    theme(legend.position=\"none\")\n}",
      "line_count": 20
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# $\\alpha=0$ - Ridge\np1 <- plot_alpha(alpha=0,restrict=(0.21^2)/2,beta1_range=0.21,annot=c(0.15,-0.25,0.205,-0.05))\np1 <- p1 + ggtitle(expression(paste(alpha, \"=0 (Ridge)\")))\n# $\\alpha=1/9$\np2 <- plot_alpha(alpha=1/9,restrict=0.046,beta1_range=0.22,annot =c(0.15,-0.25,0.212,-0.02))\np2 <- p2 + ggtitle(expression(paste(alpha, \"=1/9\")))\n# $\\alpha=1/5$\np3 <- plot_alpha(alpha=1/5,restrict=0.063,beta1_range=0.22,annot=c(0.13,-0.25,0.22,0))\np3 <- p3 + ggtitle(expression(paste(alpha, \"=1/5\")))\n# $\\alpha=1/2$\np4 <- plot_alpha(alpha=1/2,restrict=0.123,beta1_range=0.22,annot=c(0.12,-0.25,0.22,0))\np4 <- p4 + ggtitle(expression(paste(alpha, \"=1/2\")))\n# $\\alpha=3/4$\np5 <- plot_alpha(alpha=3/4,restrict=0.17,beta1_range=0.22,annot=c(0.12,-0.25,0.22,0))\np5 <- p5 + ggtitle(expression(paste(alpha, \"=3/4\")))",
      "line_count": 15
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# $\\alpha=1$ - LASSO\nt=0.22\nK = seq(0,t,length.out = 50)\nfills = data.frame(x=c(-rev(K),K),y1=c(rev(t-K),c(t-K)),y2=c(-rev(t-K),-c(t-K)))\np6 <- base + \n  geom_segment(aes(x = 0, y = t, xend = t, yend = 0),colour = \"salmon1\",alpha=0.1,size=0.2)+\n  geom_segment(aes(x = 0, y = t, xend = -t, yend = 0),colour = \"salmon1\",alpha=0.1,size=0.2)+\n  geom_segment(aes(x = 0, y = -t, xend = t, yend = 0),colour = \"salmon1\",alpha=0.1,size=0.2)+\n  geom_segment(aes(x = 0, y = -t, xend = -t, yend = 0),colour = \"salmon1\",alpha=0.1,size=0.2)+\n  geom_polygon(data = fills, aes(x, y1),fill = \"red\", alpha = 0.2)+\n  geom_polygon(data = fills, aes(x, y2), fill = \"red\", alpha = 0.2)+\n  geom_segment(aes(x = 0.12 , y = -0.25, xend = 0.22, yend = 0),colour = \"magenta\",\n               arrow = arrow(length=unit(0.30,\"cm\")),alpha=0.8)+\n  ggplot2::annotate(\"text\", x = 0.11, y = -0.36,\n                    label = \"(0.22,0)\\n Point of Contact \\n i.e Coef of LASSO\",size=3)+\n  xlab( expression(beta[1]))+\n  ylab( expression(beta[2]))+\n  theme(legend.position=\"none\")+  \n  ggtitle(expression(paste(alpha, \"=1 (LASSO)\")))",
      "line_count": 19
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "plot(p1)",
      "line_count": 1
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "plot(p6)",
      "line_count": 1
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(\"gridExtra\")\ngrid.arrange(p1,p2,p3,p4,p5,p6,nrow=3)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "plotCV.glmnet <- function(cv.glmnet.object, name=\"\") {\n  df <- as.data.frame(cbind(x=log(cv.glmnet.object$lambda), y=cv.glmnet.object$cvm, \n                           errorBar=cv.glmnet.object$cvsd), nzero=cv.glmnet.object$nzero)\n\n  featureNum <- cv.glmnet.object$nzero\n  xFeature <- log(cv.glmnet.object$lambda)\n  yFeature <- max(cv.glmnet.object$cvm)+max(cv.glmnet.object$cvsd)\n  dataFeature <- data.frame(featureNum, xFeature, yFeature)\n\n  plot_ly(data = df) %>%\n    # add error bars for each CV-mean at log(lambda)\n    add_trace(x = ~x, y = ~y, type = 'scatter', mode = 'markers',\n        name = 'CV MSE', error_y = ~list(array = errorBar)) %>% \n    # add the lambda-min and lambda 1SD vertical dash lines\n    add_lines(data=df, x=c(log(cv.glmnet.object$lambda.min), log(cv.glmnet.object$lambda.min)), \n              y=c(min(cv.glmnet.object$cvm)-max(df$errorBar), max(cv.glmnet.object$cvm)+max(df$errorBar)),\n              showlegend=F, line=list(dash=\"dash\"), name=\"lambda.min\", mode = 'lines+markers') %>%\n    add_lines(data=df, x=c(log(cv.glmnet.object$lambda.1se), log(cv.glmnet.object$lambda.1se)), \n              y=c(min(cv.glmnet.object$cvm)-max(df$errorBar), max(cv.glmnet.object$cvm)+max(df$errorBar)), \n              showlegend=F, line=list(dash=\"dash\"), name=\"lambda.1se\") %>%\n    # Add Number of Features Annotations on Top\n    add_trace(dataFeature, x = ~xFeature, y = ~yFeature, type = 'scatter', name=\"Number of Features\",\n        mode = 'text', text = ~featureNum, textposition = 'middle right',\n        textfont = list(color = '#000000', size = 9)) %>%\n    # Add top x-axis (non-zero features)\n    # add_trace(data=df, x=~c(min(cv.glmnet.object$nzero),max(cv.glmnet.object$nzero)),\n    #           y=~c(max(y)+max(errorBar),max(y)+max(errorBar)), showlegend=F, \n    #           name = \"Non-Zero Features\", yaxis = \"ax\", mode = \"lines+markers\", type = \"scatter\") %>%\n    layout(title = paste0(\"Cross-Validation MSE (\", name, \")\"),\n\t\t\t\t\t\t\txaxis = list(title=paste0(\"log(\",TeX(\"\\\\lambda\"),\")\"),\tside=\"bottom\", showgrid=TRUE), # type=\"log\"\n\t\t\t\t\t\t\thovermode = \"x unified\", legend = list(orientation='h'),  # xaxis2 = ax,  \n\t\t\t\t\t\t\tyaxis = list(title = cv.glmnet.object$name,\tside=\"left\", showgrid = TRUE))\n}\n\n#### 10-fold cross validation ####\n# LASSO\nlibrary(\"glmnet\")\nlibrary(doParallel)\ncl <- makePSOCKcluster(6)\nregisterDoParallel(cl)\nset.seed(seed)  # set seed \n# (10-fold) cross validation for the LASSO\ncvLASSO = cv.glmnet(XTrain, yTrain, alpha = 1, parallel=TRUE)\n\n# plot(cvLASSO)\n# mtext(\"CV LASSO: Number of Nonzero (Active) Coefficients\", side=3, line=2.5)\n\nplotCV.glmnet(cvLASSO, \"LASSO\")\n\n# Report MSE LASSO\npredLASSO <-  predict(cvLASSO, s = cvLASSO$lambda.1se, newx = XTest)\ntestMSE_LASSO <- mean((predLASSO - yTest)^2); testMSE_LASSO",
      "line_count": 52
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "plotCV.glmnet <- function(cv.glmnet.object, name=\"\") {\n  df <- as.data.frame(cbind(x=log(cv.glmnet.object$lambda), y=cv.glmnet.object$cvm, \n                           errorBar=cv.glmnet.object$cvsd), nzero=cv.glmnet.object$nzero)\n\n  featureNum <- cv.glmnet.object$nzero\n  xFeature <- log(cv.glmnet.object$lambda)\n  yFeature <- max(cv.glmnet.object$cvm)+max(cv.glmnet.object$cvsd)\n  dataFeature <- data.frame(featureNum, xFeature, yFeature)\n\n  plot_ly(data = df) %>%\n    # add error bars for each CV-mean at log(lambda)\n    add_trace(x = ~x, y = ~y, type = 'scatter', mode = 'markers',\n        name = 'CV MSE', error_y = ~list(array = errorBar)) %>% \n    # add the lambda-min and lambda 1SD vertical dash lines\n    add_lines(data=df, x=c(log(cv.glmnet.object$lambda.min), log(cv.glmnet.object$lambda.min)), \n              y=c(min(cv.glmnet.object$cvm)-max(df$errorBar), max(cv.glmnet.object$cvm)+max(df$errorBar)),\n              showlegend=F, line=list(dash=\"dash\"), name=\"lambda.min\", mode = 'lines+markers') %>%\n    add_lines(data=df, x=c(log(cv.glmnet.object$lambda.1se), log(cv.glmnet.object$lambda.1se)), \n              y=c(min(cv.glmnet.object$cvm)-max(df$errorBar), max(cv.glmnet.object$cvm)+max(df$errorBar)), \n              showlegend=F, line=list(dash=\"dash\"), name=\"lambda.1se\") %>%\n    # Add Number of Features Annotations on Top\n    add_trace(dataFeature, x = ~xFeature, y = ~yFeature, type = 'scatter', name=\"Number of Features\",\n        mode = 'text', text = ~featureNum, textposition = 'middle right',\n        textfont = list(color = '#000000', size = 9)) %>%\n    # Add top x-axis (non-zero features)\n    # add_trace(data=df, x=~c(min(cv.glmnet.object$nzero),max(cv.glmnet.object$nzero)),\n    #           y=~c(max(y)+max(errorBar),max(y)+max(errorBar)), showlegend=F, \n    #           name = \"Non-Zero Features\", yaxis = \"ax\", mode = \"lines+markers\", type = \"scatter\") %>%\n    layout(title = paste0(\"Cross-Validation MSE (\", name, \")\"),\n\t\t\t\t\t\t\txaxis = list(title=paste0(\"log(\",TeX(\"\\\\lambda\"),\")\"),\tside=\"bottom\", showgrid=TRUE), # type=\"log\"\n\t\t\t\t\t\t\thovermode = \"x unified\", legend = list(orientation='h'),  # xaxis2 = ax,  \n\t\t\t\t\t\t\tyaxis = list(title = cv.glmnet.object$name,\tside=\"left\", showgrid = TRUE))\n}\n\n#### 10-fold cross validation ####\n# Ridge Regression\nset.seed(seed)  # set seed \n# (10-fold) cross validation for Ridge Regression\ncvRidge = cv.glmnet(XTrain, yTrain, alpha = 0, parallel=TRUE)\n\n# plot(cvRidge)\n# mtext(\"CV Ridge: Number of Nonzero (Active) Coefficients\", side=3, line=2.5)\n\nplotCV.glmnet(cvRidge, \"Ridge\")\n\n# Report MSE Ridge\npredRidge <-  predict(cvRidge, s = cvRidge$lambda.1se, newx = XTest)\ntestMSE_Ridge <- mean((predRidge - yTest)^2); testMSE_Ridge\nstopCluster(cl)",
      "line_count": 49
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "dt = as.data.frame(cbind(yTrain,XTrain))\nols_step <- lm(yTrain ~., data = dt)\nols_step <- step(ols_step, direction = 'both', k=2, trace = F)\nsummary(ols_step)",
      "line_count": 4
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "betaHatOLS_step = ols_step$coefficients\nvar_step <- colnames(ols_step$model)[-1]\nXTestOLS_step = cbind(rep(1, nrow(XTest)), XTest[,var_step])\npredOLS_step = XTestOLS_step%*%betaHatOLS_step \ntestMSEOLS_step = mean((predOLS_step - yTest)^2)\n# Report MSE OLS Stepwise feature selection\ntestMSEOLS_step",
      "line_count": 7
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "pred2 <- predict(ols_step,as.data.frame(XTest))\nany(pred2 == predOLS_step)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# Determine final models\n\n# Extract Coefficients\n# OLS coefficient estimates\nbetaHatOLS = fitOLS$coefficients\n# LASSO coefficient estimates \nbetaHatLASSO = as.double(coef(fitLASSO, s = cvLASSO$lambda.1se))  # s is lambda\n# Ridge  coefficient estimates \nbetaHatRidge = as.double(coef(fitRidge, s = cvRidge$lambda.1se))\n\n# Test Set MSE\n# calculate predicted values\n\nXTestOLS = cbind(rep(1, nrow(XTest)), XTest) # add intercept to test data\npredOLS = XTestOLS%*%betaHatOLS \npredLASSO = predict(fitLASSO, s = cvLASSO$lambda.1se, newx = XTest)\npredRidge = predict(fitRidge, s = cvRidge$lambda.1se, newx = XTest)\n\n# calculate test set MSE\ntestMSEOLS = mean((predOLS - yTest)^2)\ntestMSELASSO = mean((predLASSO - yTest)^2)\ntestMSERidge = mean((predRidge - yTest)^2)",
      "line_count": 22
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# Plot Regression Coefficients\n# create variable names for plotting \nlibrary(\"arm\")\npar(mar=c(2, 13, 1, 1))   # extra large left margin\nvarNames <- colnames(data1[ , !(names(data1) %in% drop_features)]); varNames; length(varNames)\n  \n# # Graph 27 regression coefficients (exclude intercept [1], betaHat indices 2:27)\n# coefplot(betaHatOLS[2:27], sd = rep(0, 26), pch=0, cex.pts = 3, main = \"Regression Coefficient Estimates\", varnames = varNames)\n# coefplot(betaHatLASSO[2:27], sd = rep(0, 26), pch=1, add = TRUE, col.pts = \"red\", cex.pts = 3)\n# coefplot(betaHatRidge[2:27], sd = rep(0, 26), pch=2, add = TRUE, col.pts = \"blue\", cex.pts = 3)\n# legend(\"bottomright\", c(\"OLS\", \"LASSO\", \"Ridge\"), col = c(\"black\", \"red\", \"blue\"), pch = c(0, 1 , 2), bty = \"o\", cex = 2)\n\ndf <- as.data.frame(cbind(Feature=attributes(betaHatOLS)$names[2:26], OLS=betaHatOLS[2:26],\n                          LASSO=betaHatLASSO[2:26], Ridge=betaHatRidge[2:26]))\n\ndata_long <- gather(df, Method, value, OLS:Ridge, factor_key=TRUE)\ndata_long$value <- as.numeric(data_long$value)\n\n# Note that Plotly will automatically order your axes by the order that is present in the data\n# When using character vectors - order is alphabetic; in case of factors the order is by levels. \n# To override this behavior, specify categoryorder and categoryarray for the appropriate axis in the layout\nformY <- list(categoryorder = \"array\", categoryarray = df$Feature)\n\nplot_ly(data_long, x=~value, y=~Feature, type=\"scatter\", mode=\"markers\", \n        marker=list(size=20), color=~Method, symbol=~Method, symbols=c('circle-open','x-open','hexagon-open')) %>%\n  layout(yaxis = formY)\n\n# par()",
      "line_count": 28
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# Test Set MSE Table \n# create table as data frame\nMSETable = data.frame(OLS=testMSEOLS, OLS_step=testMSEOLS_step, LASSO=testMSELASSO, Ridge=testMSERidge)\n\n# convert to markdown\nkable(MSETable, format=\"pandoc\", caption=\"Test Set MSE\", align=c(\"c\", \"c\", \"c\", \"c\"))",
      "line_count": 6
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "var_step = names(ols_step$coefficients)[-1]\nvar_lasso = colnames(XTrain)[which(coef(fitLASSO, s = cvLASSO$lambda.min)!=0)-1]\nintersect(var_step,var_lasso)\ncoef(fitLASSO, s = cvLASSO$lambda.min)",
      "line_count": 4
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# Problem parameters\nn = 1000          # number of observations\np = 300           # number of variables\nk = 30            # number of variables with nonzero coefficients\namplitude = 3.5   # signal amplitude (for noise level = 1)\n\n# Problem data\nX = matrix(rnorm(n*p), nrow=n, ncol=p)\nnonzero = sample(p, k)\nbeta = amplitude * (1:p %in% nonzero)\ny.sample <- function() X %*% beta + rnorm(n)",
      "line_count": 11
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# install.packages(\"knockoff\")\nlibrary(knockoff)\ny = y.sample()\nresult = knockoff.filter(X, y)\nprint(result)",
      "line_count": 5
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "fdp <- function(selected) sum(beta[selected] == 0) / max(1, length(selected))\nfdp(result$selected)",
      "line_count": 2
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "result = knockoff.filter(X, y, fdr = 0.10, statistic = stat.glmnet_coefdiff)         # Old: statistic=knockoff.stat.fs)\n#knockoff::stat.forward_selection\t\tImportance statistics based on forward selection\n#knockoff::stat.glmnet_coefdiff\t\tImportance statistics based on a GLM with cross-validation\n#knockoff::stat.glmnet_lambdadiff\t\tImportance statistics based on a GLM\n#knockoff::stat.glmnet_lambdasmax\t\tGLM statistics for knockoff\n#knockoff::stat.lasso_coefdiff\t\tImportance statistics based the lasso with cross-validation\n#knockoff::stat.lasso_coefdiff_bin\t\tImportance statistics based on regularized logistic regression with cross-validation\n#knockoff::stat.lasso_lambdadiff\t\tImportance statistics based on the lasso\n#knockoff::stat.lasso_lambdadiff_bin\t\tImportance statistics based on regularized logistic regression\n#knockoff::stat.lasso_lambdasmax\t\tPenalized linear regression statistics for knockoff\n#knockoff::stat.lasso_lambdasmax_bin\t\tPenalized logistic regression statistics for knockoff\n#knockoff::stat.random_forest\t\tImportance statistics based on random forests\n# knockoff::stat.sqrt_lasso\t\tImportance statistics based on the square-root lasso\n#knockoff::stat.stability_selection\t\tImportance statistics based on stability selection\n#knockoff::verify_stat_depends\t\tVerify dependencies for chosen statistics)\nfdp(result$selected)",
      "line_count": 16
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "new_knockoff_stat <- function(X, X_ko, y) {\n  abs(t(X) %*% y) - abs(t(X_ko) %*% y)\n}\nresult = knockoff.filter(X, y, statistic = new_knockoff_stat)\n# print indices of selected features\nprint(sprintf(\"Number of KO-selected features: %d\", length(result$selected)))\ncat(\"Indices of KO-selected features: \", result$selected)\nfdp(result$selected)",
      "line_count": 8
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "data1 <- read.table('https://umich.instructure.com/files/330397/download?download_frd=1', sep=\",\", header=T)    \n# we will deal with missing values using multiple imputation later. For now, let's just ignore incomplete cases\ndata1.completeRowIndexes <- complete.cases(data1) # table(data1.completeRowIndexes)\nprop.table(table(data1.completeRowIndexes))\n\n# attach(data1)\n# View(data1[data1.completeRowIndexes, ])\ndata2 <- data1[data1.completeRowIndexes, ]\nDx_label  <- data2$ResearchGroup; table(Dx_label)",
      "line_count": 9
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# Construct preliminary design matrix.\n# define response and predictors\nY <- data1$UPDRS_part_I + data1$UPDRS_part_II + data1$UPDRS_part_III\ntable(Y)   # Show Clinically relevant classification\nY <- Y[data1.completeRowIndexes]\n\n# X = scale(ncaaData[, -20])  # Explicit Scaling is not needed, as glmnet auto standardizes predictors\n# X = as.matrix(data1[, c(\"R_caudate_Volume\", \"R_putamen_Volume\", \"Weight\", \"Age\", \"chr17_rs12185268_GT\")])  # X needs to be a matrix, not a data frame\ndrop_features <- c(\"FID_IID\", \"ResearchGroup\", \"UPDRS_part_I\", \"UPDRS_part_II\", \"UPDRS_part_III\")\nX <- data1[ , !(names(data1) %in% drop_features)]\nX = as.matrix(X)   # remove columns: index, ResearchGroup, and y=(PDRS_part_I + UPDRS_part_II + UPDRS_part_III)\nX <- X[data1.completeRowIndexes, ]; dim(X)\nsummary(X)\nmode(X) <- 'numeric'\n\nDx_label <- Dx_label[data1.completeRowIndexes]; length(Dx_label)",
      "line_count": 16
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "h <- hist(Y, breaks='FD', plot = F)\nplot_ly(x = h$mids, y = h$density, type = \"bar\") %>%\n   layout(bargap=0.1, title=\"Histogram of Computed Variable Y = (UPDRS) part_I + part_II + part_III\")",
      "line_count": 3
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "# hist(log(Y), breaks='FD')\nh <- hist(log(Y), breaks='FD', plot = F)\nplot_ly(x = h$mids, y = h$density, type = \"bar\") %>%\n   layout(bargap=0.1, title=\"Histogram of log(Y)\")",
      "line_count": 4
    },
    {
      "section": "Variable Importance and Feature Selection",
      "code": "library(knockoff)\n\nY <- data1$UPDRS_part_I + data1$UPDRS_part_II + data1$UPDRS_part_III\ntable(Y)   # Show Clinically relevant classification\nY <- as.matrix(Y[data1.completeRowIndexes]); colnames(Y) <- \"y\"\nmode(Y)\n\n# X = scale(ncaaData[,-20])  # Explicit Scaling is not needed, as glmnet auto standardizes predictors\n# X = as.matrix(data1[,c(\"R_caudate_Volume\", \"R_putamen_Volume\",\"Weight\", \"Age\", \"chr17_rs12185268_GT\")])  # X needs to be a matrix, not a data frame\ndrop_features <- c(\"FID_IID\", \"ResearchGroup\", \"UPDRS_part_I\", \"UPDRS_part_II\", \"UPDRS_part_III\")\nX <- data1[ , !(names(data1) %in% drop_features)]\nX = as.matrix(X)   # remove columns: index, ResearchGroup, and y=(PDRS_part_I + UPDRS_part_II + UPDRS_part_III)\nX <- X[data1.completeRowIndexes,]; dim(X); mode(X)\n\nView(cbind(X,Y))\n\n# Direct call to knockoff filtering looks like this:\nfdr <- 0.4\nset.seed(1234)\nresult = knockoff.filter(X, Y, fdr=fdr, knockoffs=create.second_order); print(result$selected) # Old: knockoffs='equicorrelated')\n# knockoff::create.fixed\t\tFixed-X knockoffs\n#knockoff::create.gaussian\t\tModel-X Gaussian knockoffs\n#knockoff::create.second_order\t\tSecond-order Gaussian knockoffs\n#knockoff::create.solve_asdp\t\tRelaxed optimization for fixed-X and Gaussian knockoffs\n#knockoff::create.solve_equi\t\tOptimization for equi-correlated fixed-X and Gaussian knockoffs\n#knockoff::create.solve_sdp\t\tOptimization for fixed-X and Gaussian knockoffs\n#knockoff::create_equicorrelated\t\tCreate equicorrelated fixed-X knockoffs.\n#knockoff::create_sdp\t\tCreate SDP fixed-X knockoffs.\n#knockoff::create.vectorize_matrix\t\tVectorize a matrix into the SCS format\n\nnames(result$selected)\nknockoff_selected <- names(result$selected)\n  \n# Run BH (Benjamini-Hochberg)\nk = ncol(X)\nlm.fit = lm(Y ~ X - 1) # no intercept\np.values = coef(summary(lm.fit))[,4]\ncutoff = max(c(0, which(sort(p.values) <= fdr * (1:k) / k)))\nBH_selected = names(which(p.values <= fdr * cutoff / k))\n\nknockoff_selected; BH_selected\n\nlist(Knockoff = knockoff_selected, BHq = BH_selected)\n\n# Alternatively, for more flexible Knockoff invocation\nset.seed(1234)\nknockoffs = function(X) create.gaussian(X, 0, Sigma=diag(dim(X)[2])) # identify var-covar matrix Sigma of rank equal to the number of features\nstats = function(X, Xk, y) stat.glmnet_coefdiff(X, Xk, y, nfolds=10) # The Output X_k is an n-by-p matrix of knockoff features\nresult = knockoff.filter(X, Y, fdr=fdr, knockoffs=knockoffs, statistic=stats); print(result$selected)\n\n# Housekeeping: remove the \"X\" prefixes in the BH_selected list of features\nfor(i in 1:length(BH_selected)){\n  BH_selected[i] <- substring(BH_selected[i], 2)\n}\n\nintersect(BH_selected,knockoff_selected)",
      "line_count": 56
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "set.seed(15)\nmat = matrix(0,ncol = 27)\nN = 400\nsca.x = as.matrix(X)\nfor (i in 1:N){\n  #if (i%%50==0){\n  #print(i)\n  #}\n  tp = sample(1:nrow(X),0.5*nrow(X))\n  x.tp = sca.x[tp,]\n  y.tp = Y[tp]\n  cvtp = cv.glmnet(x.tp, y.tp, alpha = 1, parallel = T)\n  tt <- as.matrix(coef(cvtp, s = \"lambda.min\"))\n  mat[which(tt!=0)]=mat[which(tt!=0)]+1\n}\n\ndf <- as.data.frame(t(mat))\ndf$id <- 1:27\nvarname = colnames(X)[1:26]\ndf$names <- c(\"intercept\",varname)\nddf <- df[order(df$V1,decreasing = T),]\nddf$fre = ddf$V1/N\nddf[2:11,c(3,4)] # 1 for intercept",
      "line_count": 23
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "set.seed(15)\nN=1000\nnum = matrix(0,ncol = 27)\nfor (i in 1:N){\n  #if (i%%50==0){\n  # print(i)\n  #}\n  tp = sample(1:nrow(X),0.5*nrow(X))\n  x.tp = sca.x[tp,]\n  y.tp = Y[tp]\n  tt <- knockoff.filter(X = x.tp,fdr=0.2,y = y.tp) # OLD: randomize = FALSE)\n  tt <- as.matrix(tt$selected)\n  num[tt]=num[tt]+1\n}\ndf2 <- as.data.frame(t(num))\ndf2$id <- 1:27\ndf2$names <- c(\"intercept\",varname)\nddf2 <- df2[order(df2$V1,decreasing = T),]\nddf2$fre = ddf2$V1/N\nddf2[1:10,c(3,4)]",
      "line_count": 20
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "library(rvest)\nwiki_url <- read_html(\"https://wiki.socr.umich.edu/index.php/SOCR_Data_AD_BiomedBigMetadata\")\nhtml_nodes(wiki_url, \"#content\")\nalzh <- html_table(html_nodes(wiki_url, \"table\")[[1]])\nsummary(alzh)",
      "line_count": 5
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "chrtofactor<-c(3, 5, 8, 10, 21:22, 51:54)\nalzh[alzh==\".\"] <- NA  # replace all missing \".\" values with \"NA\"\nalzh[chrtofactor]<-data.frame(apply(alzh[chrtofactor], 2, as.numeric))\nalzh<-alzh[complete.cases(alzh), ]",
      "line_count": 4
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "set.seed(123)\ntrain<-Boruta(DXCURREN~.-SID, data=alzh, doTrace=0)\nprint(train)",
      "line_count": 3
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "plot(train, xlab = \"\", xaxt=\"n\")\nlz<-lapply(1:ncol(train$ImpHistory), function(i)\ntrain$ImpHistory[is.finite(train$ImpHistory[, i]), i])\nnames(lz)<-colnames(train$ImpHistory)\nlb<-sort(sapply(lz, median))\naxis(side=1, las=2, labels=names(lb), at=1:ncol(train$ImpHistory), cex.axis=0.7)",
      "line_count": 6
    },
    {
      "section": "results = lapply(Y, function(y) knockoff_and_BH(X, y, fdr))",
      "code": "final.train<-TentativeRoughFix(train)\nprint(final.train)\ngetSelectedAttributes(final.train, withTentative = F)",
      "line_count": 3
    }
  ]
}