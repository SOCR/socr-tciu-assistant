{
  "metadata": {
    "created_at": "2024-11-30T13:46:16.585844",
    "total_sections": 4,
    "total_code_chunks": 69,
    "total_tables": 3,
    "r_libraries": [
      "MuMIn",
      "TTR",
      "arm",
      "caret",
      "dplyr",
      "forecast",
      "gee",
      "geepack",
      "ggplot2",
      "gtrendsR",
      "imager",
      "keras",
      "lavaan",
      "lme4",
      "plotly",
      "plyr",
      "prophet",
      "recipes",
      "reshape2",
      "rnn",
      "semPlot",
      "sjPlot",
      "stringr",
      "tibble",
      "tibbletime",
      "tidyverse",
      "zoo"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Big Longitudinal Data Analysis</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 3\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\n    code_folding: show",
      "word_count": 54
    },
    {
      "title": "Big Longitudinal Data Analysis",
      "content": "The time-varying (longitudinal) characteristics of large information flows represent a special case of the complexity, dynamic and multi-scale nature of big biomedical data that we discussed in the [DSPA Motivation section](https://socr.umich.edu/DSPA2/DSPA2_notes/01_Introduction.html). Previously, in [Chapter 2](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Visualization_Part1.html), we saw space-time (4D) functional magnetic resonance imaging (fMRI) data, and in [Chapter 10](https://socr.umich.edu/DSPA2/DSPA2_notes/10_SpecializedML_FormatsOptimization.html) we discussed streaming data, which also has a natural temporal dimension.\n\nIn this Chapter, we will expand our predictive data analytic strategies specifically for analyzing longitudinal data. We will interrogate datasets that track homologous information, across subjects, units or locations, over a period of time. In the first part of the Chapter, we will present classical approaches including time series analysis, forecasting using autoregressive integrated moving average (ARIMA) models, structural equation models (SEM), and longitudinal data analysis via linear mixed models. In the second part, we will discuss recent AI methods for time-series analysis and forecasting including Recurrent Neural Networks (RNN) and long short-term memory (LSTM) networks.\n\n## Classical Time-Series Analytic Approaches\n\n### Information theoretic model evaluation criteria\n\nThe [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and the [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion) are metrics that quantify the fit of different (typically) regression models. The AIC is calculated by the formula $AIC = 2k – 2\\ln(L)$, where $k$ is the number of parameters estimated in the model, and $\\ln(L)$ is the log-likelihood of the model. In essence, the AIC quantifies how likely the model is, given the observed data (in terms of the likelihood value). Suppose we have estimated several different regression models, the optimal model corresponds to the smallest AIC value, i.e., the model with the lowest AIC yields the best fit (highest likelihood, relative to the number of model parameters). The sign of the AIC does not matter, the lower the AIC value, the better the model fit.\n\nSimilarly, the BIC is calculated by the formula $BIC = k\\ln(n) – 2\\ln(L)\\equiv \\ln\\left( \\frac{n^k}{L^2}\\right )$, where $n$ is the number of data points, $k$ is the number of parameters estimated in the model, and $\\ln(L)$ is the log-likelihood of the model. Lower BIC values correspond to higher likelihoods ($2\\ln(L)$) and lower penalty terms ($k \\ln(n)$), i.e., better models.\n\n\n### Information theoretic model evaluation criteria\n\nThe *Akaike information criterion (AIC)* and the *Bayesian information criterion (BIC)* are metrics that quantify the fit of different (typically) regression models.\n\nThe *AIC* is calculated by the formula $AIC = 2k – 2\\ln(L)$, where $k$ is the number of parameters estimated in the model, and $\\ln(L)$ is the log-likelihood of the model. In essence, the AIC quantifies how likely the model is, given the observed data (in terms of the likelihood value). Suppose we have estimated several different regression models, the optimal model corresponds to the smallest AIC value, i.e., the model with the lowest AIC yields the best fit (highest likelihood, relative to the number of model parameters). The sign of the AIC does not matter, the lower the AIC value, the better the model fit.\n\nSimilarly, the *BIC* is calculated by the formula $BIC = k\\times  \\ln(n) – 2\\ln(L)$, where $n$ is the number of data points, $k$ is the number of parameters estimated in the model, and $\\ln(L)$ is the log-likelihood of the model. Lower *BIC* values correspond to higher likelihoods ($2\\ln(L)$) and lower penalty terms ($k\\times  \\ln(n)$); i.e., better models.\n\n### Time series analysis\n\nTime series analysis relies on models like [ARIMA (Autoregressive integrated moving average)](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) that utilize past longitudinal information to predict near future outcomes. Time series data tend to track univariate, sometimes multivariate, processes over a continuous time interval. The stock market, e.g., daily closing value of the [Dow Jones Industrial Average index](https://quotes.wsj.com/index/DJIA/historical-prices), or [electroencephalography (EEG) data](https://headit.org/) provide examples of such longitudinal datasets (time series).\n\nThe basic concepts in time series analysis include:\n\n - The characteristics of *(second-order) stationary time series* (e.g., the first two moments are stable over time) do not depend on the time at which the series process is observed.\n - *Differencing* -- a transformation applied to time-series data to make it stationary. Differences between consecutive time-observations may be computed by $\\displaystyle y_{t}'=y_{t}-y_{t-1}$. Differencing removes the level changes in the time series, eliminates trend, reduces seasonality, and stabilizes the mean of the time series. Differencing the time series repeatedly may yield a stationary time series. For example, a second order differencing: $$\\displaystyle {\\begin{aligned}y_{t}^{''}&=y_{t}'-y_{t-1}'\\\\&=(y_{t}-y_{t-1})-(y_{t-1}-y_{t-2})\\\\&=y_{t}-2y_{t-1}+y_{t-2}\\end{aligned}}.$$\n - *Seasonal differencing* is computed as a difference between one observation and its corresponding observation in the previous epoch, or season (e.g., annually, there are $m=4$ seasons in a cycle), like in this example:\n$$\\displaystyle y_{t}'''=y_{t}-y_{t-m}\\quad {\\text{where }}m={\\text{number of seasons}}.$$\nThe differenced data may then be used to estimate an ARMA model.\n\nWe will use the [Beijing air quality PM2.5 dataset](https://umich.instructure.com/files/1823138/download?download_frd=1) as an example to demonstrate the analysis process. This dataset measures air pollutants - [PM2.5 particles in micrograms per cubic meter](data.worldbank.org/indicator/EN.ATM.PM25.MC.M3) for 8 years (2008-2016). It measures the *hourly average of the number of particles that are of size 2.5 microns (PM2.5)* once per hour in Beijing, China. \n\nLet's first import the dataset into `R`.\n\n\nThe `Value` column records PM2.5 AQI (Air Quality Index) for 8 years. We observe that there are some missing data in the `Value` column. By looking at the `QC.Name` column, we have about 6.5% (~4,408) of the observations with missing values. One way of solving the missingness is to replace them by the corresponding variable mean. Of course, in practical applications, some of the statistical imputation methods we discussed earlier should be used.\n\n\nHere we first reassign the missing values into `NA` labels. Then we replace all `NA` labels with the *mean* computed using all non-missing observations. Note that the `floor()` function casts the arithmetic averages as integer numbers, which is needed as AQI values are expected to be whole numbers. \n\nNow, let's observe the trend of hourly average PM2.5 across one day. You can see a significant pattern: The PM2.5 level peaks in the afternoons and is the lowest in the early mornings and exhibits approximate periodic boundary conditions (these patterns oscillate daily). \n\n\nAre there any daily or monthly trends? We can start the data interrogation by building an ARIMA model and examining detailed patterns in the data.\n\n*Step 1 - Plot time series*\n\nTo begin with, we can visualize the overall trend by plotting PM2.5 values against time. This can be achieved using the `plyr` package.\n\n\nThe dataset is recorded hourly and the eight-year time interval includes about $69,335$ hours of records. Therefore, we start at the first hour and end with $69,335^{th}$ hour. Each hour has a univariate [PM2.5 AQI value measurement](https://airnow.gov/index.cfm?action=resources.conc_aqi_calc), so `frequency=1`.\n\nFrom this time series plot, we observe that the data has some extreme peaks and many AQI (air quality index) values are around 200 (which is considered hazardous).\n\nThe original plot seems to have no trend at all. Remember we have our measurements in hours. Will there be any difference if we use monthly average instead of hourly reported values? In this case, we can use Simple Moving Average (SMA) technique to smooth the original graph.\n\nTo do this, we need to install the `TTR` R-package.\n\n\nWe chose the lag smoothing parameter to be $24\\times 30=720$ and we can see some patterns in the plot. It seems that for the first 4 years (or approximately $35,040$ hours) the AQI fluctuates less than the last 5 years. Let's see what happens if we use *exponentially-weighted moving average* (EMA), instead of *arithmetic mean* (SMA).\n\n\nThe pattern seems less obvious in this graph. Here we used an exponential smoothing ratio of $2/(n+1)$.\n\n*Step 2 - Find proper parameter values for ARIMA model*\n\nARIMA models have 2 components -- autoregressive part (AR) and moving average part (MA). An $ARMA(p, d, q)$ model is a model with *p* terms in AR and *q* terms in MA, and *d* representing the order difference. Where differencing is used to make the original dataset approximately stationary. If we denote by $L$ the *lag operator*, by $\\phi_i$ the parameters of the autoregressive part of the model, by $\\theta_i$ the parameters of the moving average part, and by $\\epsilon_t$ error terms, then the $ARMA(p, d, q)$ has the following analytical form:\n\n$$\\left (1-\\sum_{i=1}^p\\phi_i L^i\\right )(1-L)^d X_t = \\left (1+\\sum_{i=1}^q\\theta_i L^i\\right )\\epsilon_t .$$\n\n*Check the differencing parameter*\n\nFirst, let's try to determine the parameter $d$. To make the data stationary on the mean (remove any trend), we can use first differencing or second order differencing. Mathematically, first differencing is taking the difference between two adjacent data points:\n\n$$y_t'=y_t-y_{t-1}.$$\nWhile second order differencing is differencing the data twice:\n\n$$y_t^*=y_t'-y_{t-1}'=y_t-2y_{t-1}+y_{t-2}.$$\nLet's see which differencing method is proper for the Beijing PM2.5 dataset. Function `diff()` in R base can be used to calculate differencing. We can plot the differences by `plot.ts()` or using `plot_ly()`.\n\n\nNeither of them appears quite stationary. In this case, we can consider using some smoothing techniques on the data like we just did above (`bj.month<-SMA(ts, n=720)`). Let's see if smoothing by exponentially-weighted mean (EMA) can help make the data approximately stationary.\n\n\nBoth of these EMA-filtered graphs have tempered variance and appear pretty stationary with respect to the first two moments, mean and variance.\n\n*Identifying the AR and MA parameters*\n\nTo decide the auto-regressive (AR) and moving average (MA) parameters in the model we need to create **autocorrelation factor (ACF)** and **partial autocorrelation factor (PACF) plots**. PACF may suggest a value for the AR-term parameter $p$, and ACF may help us determine the MA-term parameter $q$. We plot the ACF and PACF, we use approximately stationary time series `bj.diff` objects. The dash lines on the ACF/PACF plots indicate the bounds on the correlation values beyond which the auto-correlations are considered statistically significantly different from zero. Correlation values outside the confidence lines could suggest non-purely-random/correlated parts in the longitudinal signal.\n\n\n - Pure AR model, ($p=0$), will have a cut off at lag $q$ in the PACF. \n - Pure MA model, ($q=0$), will have a cut off at lag $p$ in the ACF. \n - ARIMA(p, q) will (eventually) have a decay in both.\n\nAll ACF spikes appear outside of the insignificant zone in the *ACF plot*, whereas only a few are significant in the *PACF plot.* In this case, the best ARIMA model is likely to have both AR and MA parts. \n\nWe can examine for seasonal effects in the data using `stats::stl()`, a flexible function for decomposing and forecasting the series, which uses averaging to calculate the seasonal component of the series and then subtracts the seasonality. Decomposing the series and removing the seasonality can be done by subtracting the seasonal component from the original series using `forecast::seasadj()`. The frequency parameter in the `ts()` object specifies the periodicity of the data or the number of observations per period, e.g., 30 (for monthly smoothed daily data).\n\n\nThe augmented Dickey-Fuller (ADF) test, `tseries::adf.test` can be used to examine the time series stationarity. The *null hypothesis is that the series is non-stationary*. The ADF test quantifies if the change in the series can be explained by a lagged value and a linear trend. Non-stationary series can be *corrected* by differencing to remove trends or cycles. \n\n\nWe see that we can reject the null and therefore, there is no statistically significant non-stationarity in the `bj.diff` time series.\n\n*Step 3 - Build an ARIMA model*\n\nAs we have some evidence suggesting *d=1*, the `auto.arima()` function in the `forecast` package can help us to find the optimal estimates for the remaining pair parameters of the ARIMA model, *p* and *q*.  \n\n\nFinally, the optimal model determined by the stepwise selection is `ARIMA(1, 1, 4)`.\n\nWe can also use external information to fit ARIMA models. For example, if we want to add the month information, in case we suspect a seasonal change in PM2.5 AQI, we can fit an $ARIMA(2, 1, 0)$ model.\n\n\nWe want the model AIC and BIC to be as small as possible. This model is actually worse than the last model without `Month` predictor in terms of AIC and BIC. Also, the coefficient is very small and not significant (according to the t-test). Thus, we can remove the `Month` term.\n\nWe can examine further the ACF and the PACF plots and the residuals to determine the model quality.  When the model order parameters and structure are correctly specified, we expect no significant autocorrelations present in the model residual plots. \n\n\nThere is a clear pattern present in ACF/PACF plots suggesting that the model residuals repeat with an approximate lag of 12 or 24 months. We may try a modified model with different parameters, e.g., $p = 24$ or $q = 24$. We can define a new `displayForecastErrors()` function to show a histogram of the forecasted errors.\n\n\n*Step 4 - Forecasting with ARIMA model*\n\nNow, we can use our models to make predictions for future PM2.5 AQI. We will use the function `forecast()` to make predictions. In this function, we have to specify the number of periods we want to forecast. Note that the data has been smoothed. Let's make predictions for the next month or July 2016. Then there are $24\\times30=720$ hours, so we specify a horizon `h=720`. \n\n\nWhen plotting the forecasted values with the original smoothed data, we include only the last 3 months in the original smoothed data to see the predicted values clearer. The shaded regions indicate ranges of expected errors. The darker (inner) region represents by *80% confidence range* and the lighter (outer) region bounds by the *95% interval*. Obviously near-term forecasts have tighter ranges of expected errors, compared to longer-term forecasts where the variability naturally expands. \n\n#### Autoregressive Integrated Moving Average *Extended/Exogenous* (ARIMAX) Model\n\nThe [classical ARIMA model](https://books.google.com/books?hl=en&lr=&id=rNt5CgAAQBAJ) makes prediction of a univariate outcome based only on the past values of the specific forecast variable. It assumes that future values of the outcome linearly depend on an additive representation of the effects of its previous values and some stochastic components. \n\nThe [extended, or exogenous feature, (vector-based) time-series model (ARIMAX)](https://doi.org/10.1109/PESGM.2014.6939802) allows forecasting of the univariate outcome based on multiple independent (predictor) variables. Similar to going from simple linear regression to multivariate linear models, *ARIMAX* generalizes the *ARIMA* model accounting for autocorrelation present in residuals of the linear regression to improve the accuracy of a time-series forecast. ARIMAX models, also called dynamic regression models, represent a combination of ARIMA, regression, and single-input-single-output transfer function models.\n\nWhen there are no covariates, traditional time-series $y_1, y_2, \\dots,y_n$ can be modeled by ARMA$(p,q)$ model:\n\n$$y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} - \\left ( \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q}\\right ) + \\epsilon_t,$$\n\nwhere $\\epsilon_t \\sim N(0,1)$ is a white Gaussian noise.\n\nThe extended dynamic time-series model, ARIMAX, adds the covariates to the right hand side:\n\n$$ y_t = \\left ( \\displaystyle\\sum_{k=1}^K{\\beta_k x_{k,t}}\\right ) + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} - \\left ( \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q z_{t-q}\\right ) + \\epsilon_t, $$\nwhere $x_{k,t}$ is the value of the $k^{th}$ covariate at time $t$ and $\\beta_k$ is its regression coefficient. In the ARIMAX models, the interpretation of the covariate coefficient, $\\beta_k$, is somewhat different from the linear modeling case; $\\beta_k$ does not really represent the effect of increasing $x_{k,t}$ by 1 on $y_t$. The coefficients $\\beta_k$ may be interpreted as conditional effects of the predictor $x_k$ on the prior values of the response variable, $y$, as the right hand side of the equation includes lagged values of the response variable.\n\nUsing the short-hand back-shift operator representation, the ARMAX model is:\n\n$$\\phi(B)y_t = \\left ( \\displaystyle\\sum_{k=1}^K{\\beta_k x_{k,t}}\\right ) + \\theta(B)\\epsilon_t.$$ \nAlternatively,\n$$y_t = \\displaystyle\\sum_{k=1}^K{\\left (\\frac{\\beta_k}{\\phi(B)}x_{k,t}\\right )} + \\frac{\\theta(B)}{\\phi(B)}\\epsilon_t,$$\nwhere $\\phi(B)=1-\\left ( \\phi_1 B + \\phi_2 B +\\cdots + \\phi_p B^p\\right )$ and $\\theta(B)= 1- \\left (\\theta_1 B + \\theta_2 B + \\cdots + \\theta_qB^q\\right)$.\n\nAs the auto-regressive coefficients get mixed up with both the covariate and the error terms. To clarify these effects, we can look at the regression models with ARMA errors:\n\n$$y_t = \\displaystyle\\sum_{k=1}^K{\\left (\\frac{\\beta_k}{\\phi(B)}x_{k,t}\\right )} + \\eta_t,$$\n$$\\eta_t = \\left (\\phi_1 \\eta_{t-1} + \\phi_2 \\eta_{t-2} +\\cdots + \\phi_p \\eta_{t-p}\\right ) - \\left (\\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} +\\dots + \\theta_q z_{t-q} + \\epsilon_t \\right ).$$\n\nIn this formulation, the regression coefficients have a more natural interpretation. Both formulations lead to the same model-based forecasting, but the second one helps with explication and interpretation of the effects.</p>\n\nThe back-shift model formulation is\n$$y_t = \\displaystyle\\sum_{k=1}^K{\\left (\\frac{\\beta_k}{\\phi(B)}x_{k,t}\\right )} + \\frac{\\theta(B)}{\\phi(B)}\\epsilon_t.$$\n\nTransfer function models generalize both of these models.\n\n$$ y_t = \\displaystyle\\sum_{k=1}^K{\\left (\\frac{\\beta_k(B)}{v(B)} x_{k,t}\\right )}  + \\frac{\\theta(B)}{\\phi(B)}\\epsilon_t. $$\n\nThis representation models the lagged effects of covariates (by the $\\beta_k(B)$ back-shift operator) and allows for decaying effects of covariates (by the $v(B)$ back-shift operator).\n\nThe Box-Jenkins method for selecting the orders of a transfer function model is often challenging in practice. An alternative order-selection protocol is presented in the [Hyndman's 1998 forecasting textbook](https://robjhyndman.com/forecasting/).\n\nTo estimate the ARIMA errors when dealing with non-stationary data, we can replace $\\phi(B)$ with $\\nabla^d\\phi(B)$, where $\\nabla=(1-B)$ is the differencing operator. This accomplishes both differencing in $y_t$ and $x_t$ prior to fitting ARIMA models with errors. For model consistency and to avoid spurious regression effects, we do need to *difference* all variables before we estimate models with non-stationary errors.\n\nThere are alternative R functions to implement ARIMAX models, e.g., `forecast::arima()`, `forecast::Arima()`, and `forecast::auto.arima()` fit extended ARIMA models with multiple covariates and error terms. Remember that there are differences in the signs of the moving average coefficients between the alternative model parameterization formulations. Also, the `TSA::arimax()` function fits the *transfer function model*, not the ARIMAX model.\n\n#### Simulated ARIMAX Example\n\nLet's look at one ARIMAX simulated example representing a 4D synthetic dataset covering $t=700$ days of records (from \"2016-05-01 00:00:00 EDT\" to \"2018-03-31 00:00:00 EDT\"), i.e., 100 weeks of observations, $Y$=time-series values (volume of customers) and $X$=(weekday, Holiday, day) exogenous variables.\n\n\nThere are also [Vector Autoregressive Moving-Average Time Series (VARMA)](https://faculty.chicagobooth.edu/ruey.tsay/teaching/mtsbk/) models that can be used to fit multivariate linear time series for stationary processes.\n\n#### Google Trends Analytics\n\nWe can use dynamic Google Trends data to retrieve information, identify temporal patterns and motifs due to various human experiences, conduct time-series analyses, and forecast future trends. If we retrieve data for multiple terms (keywords or phrases) over different periods of time and across geographic locations the problem can certainly get complex. We will utilize the R packages `gtrendsR` and `prophet`.\n\n\nLet's search *Google Trends* for `Big Data`, `data science`, `predictive analytics`, and `Artificial Intelligence` in the USA over the past ten years. Note that a time span can be specified via:\n\n - \"now 1-H\": Last hour\n - \"now 4-H\": Last four hours\n - \"now 1-d\": Last day\n - \"now 7-d\": Last seven days\n - \"today 1-m\": Past 30 days\n - \"today 3-m\": Past 90 days\n - \"today 12-m\": Past 12 months\n - \"today+5-y\": Last five years (default)\n - \"all\": Since the beginning of Google Trends (2004)\n - \"Y-m-d Y-m-d\": Time span between two dates (e.g., \"2013-06-22 2023-06-21\")\n\n\nAnd similarly, we can contrast the *Google Trends* for `Big Data`, `data science` and `predictive analytics` global (worldwide) searches over the past ten years.\n\n\nWe can also examine global geographic trends in searches for \"data science\", e.g., US, China, UK.\n\n\nWhat interesting patterns, trends and behavior is evident in the plot above?\n\nNext, we can use the `prophet` package to automatically identify patterns and provide forecasts of future searches. The `prophet()` function call requires the input data-frame to have columns \"ds\" and \"y\" representing the time-series dates and values, respectively. *Prophet* implements an additive time series forecasting model with non-linear trends fit with annual, weekly, and daily seasonality, plus holiday effects.\n\n\nTo generate forward looking predictions, we will use the method `predict()`.\n\n\nFinally, we can extract daily, monthly, quarterly, and yearly patterns.\n\n\nTry to fit an ARIMA time-series model to the US data.\n\n\nYou can try to apply some of the time-series methods to the Google-Trends \"hits\" for some specific queries you may be interested in.\n\n\n### Structural Equation Modeling (SEM)-latent variables\n\nThe time series analysis provides an effective strategy to interrogate longitudinal univariate data. What happens if we have multiple, potentially associated, measurements recorded at each time point?\n\nSEM is a general multivariate statistical analysis technique that can be used for causal modeling/inference, path analysis, confirmatory factor analysis (CFA), covariance structure modeling, and correlation structure modeling. This method allows *separation of observed and latent variables*. Other standard statistical procedures may be viewed as special cases of SEM, where statistical significance may be less important, and covariances are the core of structural equation models.\n\n*Latent variables* are features that are not directly observed but may be inferred from the actually observed variables. In other words, a combination or transformation of observed variables can create latent features, which may help us reduce the dimensionality of data. Also, SEM can address multicollinearity issues when we fit models because we can combine some high collinearity variables to create a single (latent) variable, which can then be included into the model.\n\n#### Foundations of SEM \n\nSEMs consist of two complementary components - a *path model*, quantifying specific cause-and-effect relationships between observed variables, and a *measurement model*, quantifying latent linkages between unobservable components and observed variables. The LISREL (LInear Structural RELations) framework represents a unifying mathematical strategy to specify these linkages, see [Grace]( https://books.google.com/books?isbn=1139457845).\n\nThe most general kind of SEM is a structural regression path model with latent variables, which accounts for measurement errors of observed variables. *Model identification* determines whether the model allows for unique parameter estimates and may be based on model degrees of freedom ($df_M \\geq 0$) or a known scale for every latent feature. If $\\nu$ represents the number of observed variables, then the total degrees of freedom for a SEM, $\\frac{\\nu(1+\\nu)}{2}$, corresponds to the number of variances and unique covariances in a variance-covariance matrix for all the features, and the model degrees of freedom, $df_M = \\frac{\\nu(1+\\nu)}{2} - l$, where $l$ is the number of estimated parameters.\n\nExamples include:\n\n - *just-identified model* ($df_M=0$) with unique parameter estimates, \n - *over-identified model* ($df_M>0$) desirable for model testing and assessment,\n - *under-identified model* ($df_M<0$) is not guaranteed unique solutions for all parameters. In practice, such models occur when the effective degrees of freedom are reduced due to two or more highly-correlated features, which presents problems with parameter estimation. In these situations, we can exclude or combine some of the features boosting the degrees of freedom.\n\nThe latent variables' *scale* property reflects their unobservable, not measurable, characteristics. The latent scale, or unit, may be inferred from one of its observed constituent variables, e.g., by imposing a unit loading identification constraint fixing at $1.0$ the factor loading of one observed variable.\n\nAn SEM model with appropriate *scale* and degrees of freedom conditions may be identifiable subject to [Bollen's two-step identification rule]( https://books.google.com/books?isbn=111861903X). When both the CFA path components of the SEM model are identifiable, then the whole SR model is identified, and model fitting can be initiated.\n\n - For the confirmatory factor analysis (CFA) part of the SEM, identification requires (1) a minimum of two observed variables for each latent feature, (2) independence between measurement errors and the latent variables, and (3) independence between measurement errors. \n - For the path component of the SEM, ignoring any observed variables used to measure latent variables, model identification requires: (1) errors associated with endogenous latent variables to be uncorrelated, and (2) all causal effects to be unidirectional. \n\nThe LISREL representation can be summarized by the following matrix equations:\n\n$$\\text{measurement model component} \n\\begin{cases} \n    x=\\Lambda_x\\xi +\\delta, \\\\ \n    y=\\Lambda_y\\eta +\\epsilon\n\\end{cases}.$$\n\nAnd\n\n$$\\text{path model component:  } \n    \\eta = B\\eta +\\Gamma\\xi + \\zeta,$$\n\nwhere $x_{p\\times1}$ is a vector of observed *exogenous variables* representing a linear function of $\\xi_{j\\times 1}$ vector of *exogenous latent variables*, $\\delta_{p\\times 1}$ is a *vector of measurement error*, $\\Lambda_x$ is a $p\\times j$ matrix of factor loadings relating $x$ to $\\xi$, $y_{q\\times1}$ is a vector of observed *endogenous variables*, $\\eta_{k\\times 1}$ is a vector of *endogenous latent variables*, $\\epsilon_{q\\times 1}$ is a vector of *measurement error for the endogenous variables*, and $\\Lambda_y$ is a $q\\times k$ matrix of factor loadings relating $y$ to $\\eta$. Let's also denote the two variance-covariance matrices, $\\Theta_{\\delta}(p\\times p)$ and $\\Theta_{\\epsilon}(q\\times q)$ representing the variance-covariance matrices among the measurement errors $\\delta$ and $\\epsilon$, respectively. The third equation describing the LISREL path model component as relationships among latent variables includes $B_{k\\times k}$ a matrix of path coefficients describing the *relationships among endogenous latent variables*, $\\Gamma_{k\\times j}$ as a matrix of path coefficients representing the *linear effects of exogenous variables on endogenous variables*, $\\zeta_{k\\times 1}$ as a vector of *errors of endogenous variables*, and the corresponding two variance-covariance matrices $\\Phi_{j\\times j}$ of the *latent exogenous variables*, and $\\Psi_{k\\times k}$ of the *errors of endogenous variables*.\n\nThe basic statistic for a typical SEM implementation is based on covariance structure modeling and model fitting relies on optimizing an objective function, $\\min f(\\Sigma, S)$, representing the difference between the model-implied variance-covariance matrix, $\\Sigma$,  predicted from the causal and non-causal associations specified in the model, and the corresponding observed variance-covariance matrix $S$, which is estimated from observed data. The objective function, $f(\\Sigma, S)$ can be estimated as follows, see [Shipley](https://books.google.com/books?isbn=1107442591). \n\n* In general, causation implies correlation, suggesting that if there is a causal relationship between two variables, there must also be a systematic relationship between them. Specifying a set of theoretical causal paths, we can reconstruct the model-implied variance-covariance matrix, $\\Sigma$, from total effects and unanalyzed associations. The LISREL strategy specifies the following mathematical representation:\n\n$$\\Sigma = \\begin{vmatrix} \n\\Lambda_yA(\\Gamma\\Phi\\Gamma'+\\Psi)A'\\Lambda'_y+\\Theta_{\\epsilon} & \\Lambda_yA\\Gamma\\Phi\\Lambda'_x \\\\ \n\\Lambda_x\\Phi\\Gamma'A'\\Lambda'_y & \\Lambda_x\\Phi\\Lambda'_x+\\Theta_{\\delta}\n\\end{vmatrix},$$\n\nwhere $A=(I-B)^{-1}$. This representation of $\\Sigma$ does not involve the observed and latent exogenous and endogenous variables, $x,y,\\xi,\\eta$. Maximum likelihood estimation (MLE) may be used to obtain the $\\Sigma$ parameters via iterative searches for a set of optimal parameters minimizing the element-wise deviations between $\\Sigma$ and $S$. \n\nThe process of optimizing the objective function $f(\\Sigma, S)$  can be achieved by computing the log likelihood ratio, i.e., comparing the likelihood of a given fitted model to the likelihood of a perfectly fit model. MLE estimation requires multivariate normal distribution for the endogenous variables and Wishart distribution for the observed variance-covariance matrix, $S$. \n\nUsing MLE estimation simplifies the objective function to:\n\n$$f(\\Sigma, S) = \\ln|\\Sigma| +tr(S\\times \\Sigma^{-1}) -\\ln|S| -tr(SS^{-1}),$$\n\nwhere $tr()$ is the trace of a matrix. The optimization of \n$f(\\Sigma, S)$ also requires independent and identically distributed observations, and positive definite matrices. The iterative MLE optimization generates estimated variance-covariance matrices and path coefficients for the specified model. More details on model assessment (using Root Mean Square Error of Approximation (RMSEA) and Goodness of Fit Index) and the process of defining a priori SEM hypotheses are available in [Lam & Maguire](https://dx.doi.org/10.1155/2012/263953).\n\n#### SEM components\n\nThe `R` *Lavaan* package uses the following SEM syntax to represent relationships between variables. We can follow the following table to specify `Lavaan` models:\n\nFor example in R we can write the following model\n`model<-`\n`'`\n`# regressions`\n\n$$y1 + y2 \\sim f1 + f2 + x1 + x2$$\n$$f1 \\sim f2 + f3$$\n$$f2 \\sim f3 + x1 + x2$$\n`# latent variable definitions`\n\n$$f1 =\\sim y1 + y2 + y3$$\n$$f2 =\\sim y4 + y5 + y6$$\n$$f3 =\\sim y7 + y8 + y9 + y10$$\n`# variances and covariances`\n$$y1 \\sim\\sim y1$$\n$$y1 \\sim\\sim y2$$\n$$f1 \\sim\\sim f2$$\n`# intercepts`\n$$y1 \\sim 1$$\n$$f1 \\sim 1$$\n`'`\n\nNote that the two \"$'$\" symbols (in the beginning and ending of a model description) are very important in the `R`-syntax.\n\n#### Case study - Parkinson's Disease (PD)\n\nLet's use the PPMI dataset in our class file as an example to illustrate SEM model fitting.\n\n*Step 1 - collecting data*\n\nThe [Parkinson's Disease Data](https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata) represents a realistic simulation case-study to examine associations between clinical, demographic, imaging and genetics variables for Parkinson's disease. This is an example of Big Data for investigating important neurodegenerative disorders. \n\n*Step 2 - exploring and preparing the data*\n\nNow, we can import the dataset into R and recode the `ResearchGroup` variable into a binary variable.\n\n\nThis large dataset has 1,746 observations and 31 variables with missing data in some of them. A lot of the variables are highly correlated. You can inspect high correlation using *heat maps*, which reorders these covariates according to correlations to illustrate clusters of high-correlations.\n\n\nAnd there are some specific correlations.\n\n\nOne way to solve this is to create some latent variables. We can consider the following model.\n\n\nHere we try to create three latent variables-`Imaging`, `DemoGeno`, and `UPDRS`. Let's fit a SEM model using `cfa()`(confirmatory factor analysis) function. Before fitting the data, we need to scale our dataset. However, we don't need to scale our binary response variable. We can use the following chunk of code to do the job.\n\n\n*Step 3 - fitting a model on the data*\n\nNow, we can start to build the model. The `cfa()` function we will use shortly belongs to the `lavaan` R-package.\n\n\nHere we can see some warning messages. Both our covariance and error term matrices are not positive definite. Non-positive definite matrices can cause the estimates of our model to be biased. There are many factors that can lead to this problem. In this case, we might create some latent variables that are not a good fit for our data. Let's try to delete the `DemoGeno` latent variable. We can add `Weight`, `Sex`, and `Age` directly to the regression model.\n\n\nWhen fitting `model2`, the warning messages are gone. We can see that falsely adding a latent variable can cause those matrices to be not positive definite. Currently, the `lavaan` functions `sem()` and `cfa()` are the same. \n\n\n#### Outputs of Lavaan SEM\n\nIn the output of our model, we have information about how to create these two latent variables (`Imaging`, `UPDRS`) and the estimated regression model. Specifically, it gives the following information.\n\n(1) First six lines are called the header contains the following information:\n\n - lavaan version number\n - lavaan converge info (normal or not), and # iterations needed\n - the number of observations that were effectively used in the analysis\n - the number of missing patterns identified, if any\n - the estimator that was used to obtain the parameter values (here: ML)\n - the model test statistic, the degrees of freedom, and a corresponding p-value\n\n(2)\tNext, we have the Model test baseline model and the value for the SRMR\n\n(3)\tThe last section contains the parameter estimates, standard errors (if the information matrix is expected or observed, and if the standard errors are standard, robust, or based on the bootstrap).  Then, it tabulates all free (and fixed) parameters that were included in the model.  Typically, first the latent variables are shown, followed by covariances and (residual) variances.  The first column (Estimate) contains the (estimated or fixed) parameter value for each model parameter;  the second column (Std.err) contains  the  standard  error  for  each  estimated  parameter;  the  third  column  (Z-value)  contains  the  Wald statistic (which is simply obtained by dividing the parameter value by its standard error), and the last column contains the p-value for testing the null hypothesis that the parameter equals zero in the population.\n\n### Longitudinal data analysis-Linear Mixed model\n\nAs mentioned earlier, longitudinal studies take measurements for the same individual repeatedly through a period of time. Under this setting, we can measure the change after a specific treatment. However, the measurements for the same individual may be correlated with each other. Thus, we need special models that deal with this type of internal inter-dependencies.\n\nIf we use the latent variable UPDRS (created in the output of the SEM model) rather than the research group as our response, we can conduct a longitudinal analysis.\n\n#### Mean trend\n\nAccording to the output of model `fit`, our latent variable `UPDRS` is a combination of three observed variables-`UPDRS_part_I`, `UPDRS_part_II`, and `UPDRS_part_III`. We can visualize how average `UPDRS` differ in different research groups over time; this graph is not shown as it's not very informative, there is no clear pattern emerging. \n\n\nThe above script stored `UPDRS` and `Imaging` variables into `mydata`. We can use `ggplot2` or `plotly` for data visualization, e.g., plot `UPDRS` or `Imaging` across time-visits, blocking for gender. Let's see if group-level graphs may provide more intuition.\n\nWe will use the `aggregate()` function to get the mean, minimum and maximum of `UPDRS` for each time point. Then, we will use separate colors for the two research groups and examine their mean trends.\n\n\nDespite slight overlaps in some lines, the resulting graph illustrates better the mean differences between the two cohorts. The control group (1) appears to have relative lower means and tighter ranges compared to the PD patient group (0). However, we need further data interrogation to determine if this visual (EDA) evidence translates into statistically significant group differences.\n\nGenerally speaking we can always use the *General Linear Modeling (GLM)* framework. However, GLM may ignore the individual differences. So, we can try to fit a *Linear Mixed Model (LMM)* to incorporate different intercepts for each individual participant. Consider the following GLM:\n\n$$UPDRS_{ij}\\sim \\beta_0+\\beta_1*Imaging_{ij}+\\beta_2*ResearchGroup_i+\\beta_3*{time\\_visit}_j+\\beta_4*ResearchGroup_i*time\\_visit_j+\\beta_5*Age_i+\\beta_6*Sex_i+\\beta_7*Weight_i+\\epsilon_{ij}$$\n\nIf we fit a different intercept for each individual (indicated by FID_IID) we obtain the following LMM model:\n\n$$UPDRS_{ij}\\sim \\beta_0+\\beta_1*Imaging+\\beta_2*ResearchGroup+\\beta_3*time\\_visit_j+\\beta_4*ResearchGroup_i*time\\_visit_j+\\beta_5*Age_i+\\beta_6*Sex_i+\\beta_7*Weight_i+b_i+\\epsilon_{ij}$$\n\nThe LMM actually has two levels:\n\n*Stage 1*\n\n$$Y_{i}=Z_i\\beta_i+\\epsilon_i,$$\nwhere both $Z_i$ and $\\beta_i$ are matrices.\n\n*Stage 2*\n\nThe second level allows fitting random effects in the model.\n$$\\beta_i=A_i*\\beta+b_i.$$\nSo, the full model in matrix form would be:\n$$Y_i=X_i*\\beta+Z_i*b_i+\\epsilon_i.$$\n\nIn this case study, we only consider random intercept and avoid including random slopes, however the model can indeed be extended. In other words, $Z_i=1$ in our model. Let's compare the two models (GLM and LMM). One R package implementing LMM is `lme4`.\n\n\nNote that we use the notation `ResearchGroup*time_visit` that is same as `ResearchGroup+time_visit+ResearchGroup*time_visit` where R will include both terms and their interaction into the model. According to the model outputs, the LMM model has a relatively smaller AIC. In terms of AIC, LMM may represent a better model fit than GLM.\n\n#### Modeling the correlation\n\nIn the reported summary of the LMM model, we can see a section called `Correlation of Fixed Effects`. The original model made no assumption about the correlation (unstructured correlation). In R, we usually have the following 4 types of correlation models.\n\n - *Independence*: no correlation.\n \n$$\\left(\\begin{array}{ccc} \n1 & 0 &0\\\\\n0 & 1 &0\\\\\n0&0&1\n\\end{array}\\right) . $$\n\n - *Exchangeable*: correlations are constant across measurements.\n\n$$\\left(\\begin{array}{ccc} \n1 & \\rho &\\rho\\\\\n\\rho & 1 &\\rho\\\\\n\\rho&\\rho&1\n\\end{array}\\right). $$\n\n - *Autoregressive order 1(AR(1))*: correlations are stronger for closer measurements and weaker for more distanced measurements.\n\n$$\\left(\\begin{array}{ccc} \n1 & \\rho &\\rho^2\\\\\n\\rho & 1 &\\rho\\\\\n\\rho^2&\\rho&1\n\\end{array}\\right). $$\n\n - *Unstructured*: correlation is different for each occasion.\n\n$$\\left(\\begin{array}{ccc} \n1 & \\rho_{1, 2} &\\rho_{1, 3}\\\\\n\\rho_{1, 2} & 1 &\\rho_{2, 3}\\\\\n\\rho_{1, 3}&\\rho_{2, 3}&1\n\\end{array}\\right). $$\n\nIn the LMM model, the output also seems unstructured. So, we needn't worry about changing the correlation structure. However, if the output under unstructured correlation assumption looks like an Exchangeable or AR(1) structure, we may consider changing the LMM correlation structure accordingly.\n\n### Generalized estimating equations (GEE)\n\nMuch like the generalized linear mixed models (GLMM), generalized estimating equations (GEE) may be utilized for longitudinal data analysis. If the response is a binary variable like `ResearchGroup`, we need to use the *General Linear Mixed Model (GLMM)* instead of *LMM.* Although GEE represents the marginal model of GLMM, GLMM and GEE are actually different.\n\nIn situations where the responses are discrete, there may not be a uniform or systematic strategy for dealing with the joint multivariate distribution of $Y_{i} = \\{(Y_{i,1}, Y_{i,2}, \\cdots, Y_{i,n})\\}^T$. That's where the GEE method comes into play as it's based on the concept of estimating equations. It provides a general approach for analyzing discrete and continuous responses with marginal models. \n\n*GEE is applicable when*:\n\n(1)\t$\\beta$, a generalized linear model regression parameter, characterizes systematic variation across covariate levels, \n(2)\tthe data represents repeated measurements, clustered data, multivariate response, and \n(3)\tthe correlation structure is a nuisance feature of the data. \n\n*Notation* \n\n - Response variables: $\\{Y_{i, 1}, Y_{i, 2}, ..., Y_{i, n_t} \\}$, where $i \\in [1, N]$ is the index for clusters or subjects, and $j\\in [1, n_t ]$  is the index of the measurement within cluster/subject. \n - Covariate vector: $\\{X_{i, 1}, X_{i, 2} , ..., X_{i, n_t} \\}$. \n\nThe primary focus of GEE is the estimation of the **mean model**: \n\n$E(Y_{i, j}  |X_{i, j} )=\\mu_{i, j}$, where $$g(\\mu_{i, j} )=\\beta_0  +\\beta_1  X_{i, j}  (1)+\\beta_2  X_{i, j}  (2)+\\beta_3  X_{i, j}  (3)+...+\\beta_p  X_{i, j}  (p)=X_{i, j} \\times \\beta.$$\nThis mean model can be any generalized linear model. For example: \n$P(Y_{i, j} =1|X_{i, j} )=\\pi_{i, j}$ (marginal probability, as we don't condition on any other variables):\n\n$$g(\\mu_{i, j})=\\ln \\left (\\frac{\\pi_{i, j}}{1- \\pi_{i, j} }\\right ) = X_{i, j} \\times \\beta.$$ \n\nSince the data could be clustered (e.g., within subject, or within unit), we need to choose a correlation model. Let\n$$V_{i, j} = var(Y_{i, j}|X_i),$$\n$$A_i=diag(V_{i, j}),$$\nthe paired correlations are denoted by:\n$$\\rho_{i, {j, k}}=corr(Y_{i, j}, Y_{i, k}|X_i),$$\n\nthe correlation matrix: \n$$R_i=(\\rho_{i, {j, k}}), \\ \\text{for all}\\ j\\ \\text{and} \\ k,$$\n\nand the paired predictor-response covariances are: \n$$V_i=cov(Y_i |X_i)=A_i^{1/2} R_i A_i^{1/2}.$$\n\nAssuming different correlation structures in the data leads to alternative models, see examples above. \n\n*GEE Summary* \n\n - GEE is a semi-parametric technique because: \n    + The specification of a mean model, $\\mu_{i, j}(\\beta)$, and a correlation model, $R_i(\\alpha)$, does not identify a complete probability model for $Y_i$\n    + The model $\\{\\mu_{i, j}(\\beta), R_i(\\alpha)\\}$ is semi-parametric since it only specifies the first two multivariate moments (mean and covariance) of $Y_i$. Higher order moments are not specified. \n - Without an explicit likelihood function, to estimate the parameter vector $\\beta$, (and perhaps the covariance parameter matrix $R_i(\\alpha)$) and perform valid statistical inference that takes the dependence into consideration, we need to construct an unbiased estimating function: \n   + $D_i (\\beta) = \\frac{ \\partial \\mu_i}{\\partial \\beta}$, the partial derivative, w.r.t. $\\beta$, of the mean-model for subject *i*.\n   + $D_i (j, k) = \\frac{ \\partial \\mu_{i, j}}{\\partial \\beta_k}$, the partial derivative, w.r.t. the *k*th regression coefficient ($\\beta k$), of the mean-model for subject *i* and measurement (e.g., time-point) *j*. \n\nEstimating (cost) function: \n\n$$U(\\beta)=\\sum_{i=1}^N D_i^T (\\beta) V_i^{-1}  (\\beta, \\alpha)\\{Y_i-\\mu_i (\\beta)\\}.$$ \n\nSolving the Estimating Equations leads to parameter estimating solutions: \n\n$$0=U(\\hat{\\beta})=\\sum_{i=1}^N\\underbrace{D_i^T(\\hat{\\beta})}_{\\text{scale}}\\underbrace{(V_i^{-1} \\hat{\\beta}, \\alpha)}_{\\text{variance weight}}\\underbrace{ \\{ Y_i-\\mu_i (\\hat{\\beta}) \\} }_{\\text{model mean}}.$$\n**Scale:** a change of scale term transforming the scale of the mean, $\\mu_i$, to the scale of the regression coefficients (covariates) .\n\n**Variance weight:** the inverse of the variance-covariance matrix is used to weight in the data for subject *i*, i.e., giving more weight to differences between observed and expected values for subjects that contribute more information.\n\n**Model Mean:** specifies the mean model, $\\mu_i(\\beta)$, compared to the observed data, $Y_i$. This fidelity term minimizes the difference between actually-observed and mean-expected (within the *i*th cluster/subject).\n\nSee also [SMHS EBook](https://wiki.socr.umich.edu/index.php/SMHS_GEE).\n\n#### GEE vs. GLMM\n\nThere is a difference in the interpretation of the model coefficients between GEE and GLMM. The fundamental difference between GEE and GLMM is in the target of inference: population-average vs. subject-specific. For instance, consider an example where the observations are dichotomous outcomes (Y), e.g., single Bernoulli trials or death/survival of a clinical procedure, that are grouped/clustered into hospitals and units within hospitals, with N additional demographic, phenotypic, imaging and genetics predictors. To model the failure rate between genders (males vs. females) in a hospital, where all patients are spread among different hospital units (or clinical teams), let Y represent the binary response (death or survival).\n\nIn GLMM, the model will be pretty similar to the LMM model.\n\n$$\\log\\left (\\frac{P(Y_{ij}=1)}{P(Y_{ij}=0)} \\middle|X_{ij}, b_i \\right )=\\beta_0+\\beta_1x_{ij}+b_i+\\epsilon_{ij}.$$\nThe only difference between GLMM and LMM in this situation is that GLMM used a *logit link* for the binary response.\n\nWith GEE, we don't have random intercept or slope terms.\n$$\\log\\left (\\frac{P(Y_{ij}=1)}{P(Y_{ij}=0)} \\middle| X_{ij}, b_i\\right )=\\beta_0+\\beta_1x_{ij}+\\epsilon_{ij}.$$\nIn the marginal model (GEE), we are ignoring differences among hospital-units and just aim to obtain population (hospital-wise) rates of failure (patient death) and its association with patient gender. The GEE model fit estimates the odds ratio representing the population-averaged (hospital-wide) odds of failure associated with patient gender.\n\nThus, parameter estimates ($\\hat{\\beta}$) from GEE and GLMM models may differ because they estimate different things. \n\n\n### PD/PPMI Case-Study: SEM, GLMM, and GEE modeling\n\nLet's use the [PD/PPMI dataset (05_PPMI_top_UPDRS_Integrated_LongFormat1.csv)](https://umich.instructure.com/files/330397/download?download_frd=1) to show longitudinal SEM, GEE, and GLMM data modeling.\n\n#### Exploratory data analytics\n\n\nNext we can display the histogram of patients' number of visits and plot the UPDRS_Part_I values across time for patients with different genotypes (*chr17_rs11868035_GT*).\n\n\n#### SEM\n\nLet's define a SEM model (`model1`) with three latent variables (*Imaging*, *DemoGeno*, and *UPDRS*), a single regression relation, and use the `lavaan::sem()` method to estimate the model. \n\n\n#### GLMM\n\n\n#### GEE\n\nWe can use several alternative R packages to fit and interpret GEE models, e.g., `gee` and `geepack`. Below we will demonstrate GEE modeling of the PD data using `gee`.\n\n\nThe results show that the estimated `UPDRS`-assessment effect of the clinical diagnosis (`ResearchGroup`) taken from the GEE model exchangeable structure (`summary(gee.fit)$working.correlation`) is $-0.458$. We can use the robust standard errors (`Robust S.E.`) to compute the associated 95% confidence interval, $[-0.65;\\ -0.33972648]$. Finally, remember that in the binomial/logistic outcome modeling, these effect-size estimates are on a log-odds scale. Interpretation of the results is simpler if we exponentiate the values to get the effects in terms of (simple raw) odds. This\ngives a UPDRS effect of $0.632$ and a corresponding 95% confidence interval of $[0.548;\\ 0.729]$. This CI clearly excludes the origin and suggests a strong association between `UPDRS_part_I` (non-motor experiences of daily living) and `ResearchGroup` (clinical diagnosis).\n\nThe three models are not directly comparable because they are so intrinsically different. The table below reports the AIC, but we can also compute other model-quality metrics, for SEM, GLMM, and GEE. \n\n\n\nTry to apply some of these longitudinal data analytics on:\n\n - The fMRI data we discussed in [Chapter 2 (Visualization)](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Visualization_Part1.html), and \n - The [Diet-Exer-Pulse Data](https://umich.instructure.com/files/6009692/download?download_frd=1) fitting models for  *Time* as a linear predictor of *Pulse*, accounting for the study design (diet by exercise).\n\n\n## Network-based Approaches\n\nThis section expands our model-based predictive data analytic strategies for analyzing longitudinal data to include model-free techniques. In the first part, we discussed datasets that track the same type of information, for the same subjects, units or locations, over a period of time. Specifically, we presented classical approaches such as time series analysis, forecasting using autoregressive integrated moving average (ARIMA) models, structural equation models (SEM), and longitudinal data analysis via linear mixed models.Next, we will present neural-network methods for time series analysis, including recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) Networks.\n\n\n### Background \n\nThe time-varying (longitudinal) characteristics of large information flows represent a special case of the complexity, dynamic and multi-scale nature of big biomedical data that we discussed in the [DSPA Introduction (Chapter 1)](https://socr.umich.edu/DSPA2/DSPA2_notes/01_Introduction.html). In [Chapter 2](https://socr.umich.edu/DSPA2/DSPA2_notes/02_Visualization.html), we saw space-time (4D) functional magnetic resonance imaging (fMRI) data, and in [Chapter 10](https://socr.umich.edu/DSPA2/DSPA2_notes/10_SpecializedML_FormatsOptimization.html) we discussed streaming data, which also has a natural temporal dimension.\n\n\n### Recurrent Neural Networks (RNN) \n\nEarlier, in [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html), we briefly outlined the ideas behind neural networks. Later, in [Chapter 14](https://socr.umich.edu/DSPA2/DSPA2_notes/14_DeepLearning.html), we will expand this to deep neural networks.\n\nLet's now focus on one specific type of neural networks - recursive NN. RNN are a special kind of neural networks that can be applied to predicting the behavior of longitudinal sequence-data. An example is forecasting the trends of a periodic sequence. In general, RNN may be memory intensive as they try to keep all past events in memory. Long short-term memory (LSTM) blocks represent a basic building unit for the RNN layers.\n\n![The anatomy of an RNN](https://wiki.socr.umich.edu/images/d/d9/DSPA_Chap18_RNN_Schematic.png).\n\nLSTM tends to yield better results and utilizes less computer resources, e.g., memory. A LSTM block consists of a cell, an input gate, an output gate and a forget gate. This allows each cell in the NN to \"remember\" values over specific time intervals, which explains the *short-memory* in LSTM. The 3 cell gates mimic the action of a classical artificial neuronal cell part of a multi-layer feed-forward network. The gates rely on an activation function that computes a weighted action-potential sum.\n\nRecurrent neural networks address the need to harvest prior knowledge into \"learning\" new patterns, which has traditionally been a challenge in machine learning. RNN's utilize loops to allow knowledge persistence. Following the [Gers-Schmidhuber-Cummins model](https://doi.org/10.1162/089976600300015015), the expressions below illustrate the equations describing the LSTM block forward pass with a forget gate.\n\n$$\\begin{align}\nf_t &= \\sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\\\\ni_t &= \\sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\\\\no_t &= \\sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\\\\nc_t &= f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\\\\nh_t &= o_t \\circ \\sigma_h(c_t)\n\\end{align}.$$\n\nIn this model:\n\n - $c_0 = 0$ and $h_0 = 0$ are initial values,\n - the math operator $\\circ$ denotes the matrix-based Hadamard product,\n - subscripts $_t$ denotes time (as an iteration step),\n - superscripts $^d$ and $^h$ refer to the number of input features and number of hidden units,\n - $x_t \\in \\mathbb{R}^{d}$ is the input vector to the LSTM block\n - $f_t \\in \\mathbb{R}^{h}$ is the activation vector for the *forget gate*\n - $i_t \\in \\mathbb{R}^{h}$ is the activation vector for the *input gate*\n - $o_t \\in \\mathbb{R}^{h}$ is the activation vector for the *output gate*\n - $h_t \\in \\mathbb{R}^{h}$ is the output vector of the LSTM block\n - $c_t \\in \\mathbb{R}^{h}$ is the cell state vector\n - $W \\in \\mathbb{R}^{h \\times d}$, $U \\in \\mathbb{R}^{h \\times h}$ and $b \\in \\mathbb{R}^{h}$ represent the weight matrices and bias vector parameters that will be learned during the training phase.\n\n![The anatomy of an LSTM RNN](https://wiki.socr.umich.edu/images/6/68/DSPA_Chap18_RNN_LSTM_Schematic.png).\n\nCell states in LSTM networks, represented as horizontal lines running at the bottom of the LSTM node diagram, aggregates feedback from the entire chain via linear operations. LSTM nodes can add or remove information from the cell state according to the specifications of the gates regulating the structure of the information flow. Gates provide control for the information stream according to sigmoid neural net layers ($\\sigma$) and a pointwise multiplication operation ($\\times$).\n\nSigmoid layers ($\\sigma$) output numbers in $[0,1]$ indicating the throughout proportion each component allows through the gate. At the extremes, output values of zero or one indicate \"gate is shut\" and \"let everything through\", respectively. The three LSTM gates directly control the cell state at the given time.\n\nVarious activation functions may be employed, e.g., *sigmoid*, *hyperbolic tangent*, rectified linear unit (*ReLU*), etc. During the network training phase, the LSTM's total error is minimized on the training data via iterative gradient descent. Either using back-propagation through time, or utilizing the derivative of the error with respect to time, we can derive/change the weights at each iteration (epoch). Note that then the spectral radius of $W<1$, $\\lim_{n\\rightarrow \\infty} (W^n) = 0$. Thus, when gradient descent optimization is used, the error gradients may quickly become trivial $(0)$ with time. However, with LSTM blocks, the error values are back-propagated from the output, and the total error is kept in the LSTM memory. This continuous error feedback is transferred to each of the gates, which helps in the learning of the gate threshold level. Thus, back-propagation effectively allows the LSTM unit to remember values over long periods of time.\n\nThere are also alternative strategies to train the LSTM network, e.g., combining artificial evolution for the *weights to the hidden units* and pseudo-inverse, or support vector machines, for *weights to the output units*. Reinforced LSTM nets may also be trained by policy gradient methods, evolution strategies, or genetic algorithms. Finally, stacks of LSTM networks may be trained by [connectionist temporal classification (CTC)](https://en.wikipedia.org/wiki/Connectionist_temporal_classification). This involves finding RNN weight matrices that using the input sequences maximize the probability of the outcome labels in the corresponding training datasets, which yields joint alignment and recognition.\n\n### Tensor Format Representation\n\nThe ability of LSTM RNN to recognize patterns and maintain the state over the length of the time series are useful for prediction tasks. Time series typically include correlation between consecutive versions of lagged segments of the time-series. The LSTM recurrent architecture models the persistence of the states by communicating updates between weight estimates across each epoch iteration. RNNs are enhanced by the LSTM cell architecture and facilitate long term persistence in addition to short term memory.\n\nIn RNN models, the predictors ($X$) are represented as tensors, specifically, 3D-array with dimensions $samples\\times timesteps\\times features$. \n\n - Dimension 1: represents the number of samples,\n - Dimension 2: is the number of time steps (lags), \n - Dimension 3: is the number of predictors (1 if univariate or $n$ if multivariate predictors).\n\nThe outcome ($Y$) is also a tensor, however it is univariate and is represented by a 2D-array of dimensions: $samples\\times timesteps$, where the two dimensions are (1) the *number of samples* (D1), and (2) the *number of time steps in a lag* (D2). Thus, the product $D1\\times D2$ is the total sample-size. The proportion of the training set length to the testing set length must be an integer.\n\nFor example, suppose we have a total of $1,000$ observations that are stacked in sets of 200 time-points, per lag, 5 time steps (5 lags), and two features (bivariate predictor-vector). Then, the RNN/LSTM input format will be a 3-tensor of dimensions (200, 5, 2).\n\nThe *batch size* is the number of training examples in one forward/backward pass of a RNN before a weight update. The choice of batch size requires that these proportions are integral:  $\\frac{training\\ length}{batch\\ size}$, $\\frac{testing\\ length}{batch\\ size}$. A *time step* is the number of lags included in the training and testing datasets. The *epochs* are the total number of forward-backward pass iterations. Typically, a larger number of epochs improves model performance, unless overfitting occurs at which time the validation accuracy or loss may revert.\n\nFor example, if we have daily data over 10 years, we can choose a prediction of window *3,650* days (365 days $\\times$ 10 years). Suppose we use the auto-correlation function (ACF) and determine that the best auto-correlation is seasonal, i.e., 91 (quarterly). We need to make sure that the autocorrelation period is evenly divisible by the forecasting range. If necessary, we may increase the forecasting period. We can select a batch size of 30 time points (days), which should evenly divide the number of testing and training observations. We may also select time steps = 1, to indicate we are only using one lag, and set epochs = 300, which we can adjust to balance the tradeoff between bias and precision.\n\nLet's look at a couple of RNN examples.\n\n### Simulated RNN case-study\n\nIn this example, we will try to predict a trigonometric function ($X$) from a noisy wave function ($Y$). The RNN model is expected to accurately estimate the phase shift of the oscillatory wave functions as well as generate an output ($X$) that represents a denoised version of the input ($Y$). The synthetic training dataset represents 25 oscillations each of which containing 50 point samples. \n\nFor simplicity, all data are renormalized to the unit interval $[0, 1]$. In general, all neural networks work best when the data are pre-normalized to ensure convergence and biased results. We can use any number of hidden layers, number of neurons, and epochs (learning iterations).\n\n\nNotice the learning process expressed indirectly as progressive improvement of the RNN prediction (blue curve) over the time span. Three specific (longitudinally-expressed) characteristics of the forecasting are clearly shown:\n\n - The improved amplitude, signal intensity magnitude,\n - Reduced level of noise, and\n - Reduction of the phase offset (initial phase was $\\frac{\\pi}{8}$).\n\n\n### Climate Data Study\n\nLet's use the [2009-2017 Climate Data from the Max Planck Institute in Jena, Germany](https://umich.instructure.com/courses/38100/files/folder/Case_Studies/24_ClimateData_2009_2017_Jena_Germany_MaxPlanck) to demonstrate time-series analysis via RNN. You can find more [details about this study here](https://umich.instructure.com/files/8014607/download?download_frd=1). In a nutshell, this sequence data represents a time-series recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany. The data includes date-time and 14 climate measurements/features (e.g., atmospheric pressure, temperature and humidity), which are recorded every 10 minutes over 9 years. \n\nWe will start by ingesting the large climate data (~50MB).\n\n\nPrior to modeling the data using RNN, we can plot some of the data.\n\n\nNow we can proceed with the RNN time-series modeling and prediction.\n\n\nThese results can be significantly improved, by extending the scope of the learning; here we only used $10,000$ points (about 10 weeks of data) to learn. In addition, only 10 epochs were used to quickly complete the learning, prediction and plotting operations. \n\nTry to run the experiment with a larger sample-size, modify the `rnn::trainr()` method parameters, and try to plot observed vs. predicted outcomes (temperature in Celsius). What conclusions can be drawn from this RNN forecasting?\n\n#### Examine the ACF\n\nThe Autocorrelation Function (ACF) determines the self-correlation within the time series by identifying similar repeats or lagged versions of itself. The `stats::acf()` function computes the ACF values for all lags and plots the results. We can also obtain the raw ACF values for the time series using a new function `my_acf()`.\n\n\nThe ACF is useful to identify that we have autocorrelation exceeding $0.5$ beyond the lag 52,560 (corresponding to annual measures). We can theoretically use this high autocorrelation lag to develop an LSTM model.\n\nWe can employ **backtesting** as time-series modeling cross-validation (CV). [Cross validation (Chapter 9)](https://socr.umich.edu/DSPA2/DSPA2_notes/09_ModelEvalImprovement.html) allows assessment of the model performance and evaluation of the statistical reliability using data sub-sampling. We aim to quantify the expected accuracy level and error range, we can iteratively split the complete dataset into training data and a complementary validation set. As time-series have intrinsic auto-correlation, they are distinct from cross-sectional data, so modified cross validation strategies are needed. In particular, the special time dependency on previous time samples has to be accounted for when developing a time-series CV sampling strategy. The simplest way to design a time-series CV approach is to use an offset window, e.g., one lag wide, to select sequential sub-samples. This type of strategy is called *backtesting*. This strategy suggests that the time-series CV splits the longitudinal data into multiple contiguous sequences offset by lag-windows, which facilitates testing and validation of past, current, and prospective (in time) observations.\n\nWe can employ the `rsample` package to perform time-series sampling, cross-validation, and backtesting. In our *Climate Data*, one sampling strategy may use 2 years of data (initial $105,120 = 24\\times 6\\times 365\\times 2$ observations, each 10-minutes apart) as a training set. The sequence of time-points, covering the 3-rd year, will include $105,121:157,681$) and will serve as a validation/testing dataset. [More information is available here](https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html).\n\n\n### Keras-based Multi-covariate LSTM Time-series Analysis and Forecasting\n\nThe LSTM RNN model may be appropriate if there is evidence of periodicity in the data with autocorrelation that can be estimated using the autocorrelation function (ACF). LSTM uses the autocorrelation estimate to make forward series predictions. For instance, to generate a 1-year *batch forecasting*, we can create a single pass prediction (batch mode) across the entire forecast time domain. This is different from the more traditional time-point based prediction, which can also be used to iteratively estimate a sequence of predictions on prospective time points (intervals). Of course, batch prediction requires that the autocorrelation lag is bigger than the one year (365 days, or $52,560$ ten-minute time increments that the *Climate Data* is actually acquired at).\n\n#### Using Keras to Model Stateful LSTM Time-series\n\nRecurrent Neural Networks are typically affected by *gradient vanishing*, which in ML refers to the  reduction of the gradient during the process of interactively training the artificial RNN using backpropagation and gradient-based learning. At each learning epoch, the updates of the neural networks' weights are proportional to the magnitude of the *gradient of the error function*, relative to the current weights. The ranges of the activation function values, e.g., hyperbolic tangent, include zero, e.g., $(-1,1)$ or $[0,1)$, and the use of chain rule in the backpropagation process may lead to multiplication of $n$ by small, or trivial, numbers. Thus, the gradient estimates of the \"front\" layers in an $n$-layer RNN yield to exponential decrease in the gradient (error signal) w.r.t. $n$, which in turn, leads to very slow training. \n\nStateful LSTM networks allow fine control over resetting the internal state of the LSTM network, which avoids the gradient vanishing by replacing update multiplication by *addition* when computing the candidate weights at each iteration. This additive update of the state of every cell in the network prevents the rapid decay of the gradient. A nice [visualization of the core LSTM RNN nets is available here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\n*Statefulness* is a property that determines if the network cell states are reset at each recursive iteration. *Stateless* models reset all cell states at each sequence, whereas *stateful* models propagate the cell states to the next batch; the state of the sample, $X_i$, located at index $i$, will be used in the computation of the sample $X_{i+bs}$ in the next batch, where $b$s is the batch size, i.e., no shuffling is applied. In practice, the `stateful` argument is a Boolean. The default is `stateful=False`, all cell states are reset at the next batch. In this situation, `keras` shuffles (i.e., permutes) the samples in the input matrix $X$ and the dependencies between $X_i$ and $X_{i+1}$ are lost.\n\nHowever, when `stateful=True`, the last state for each sample at index $i$ in a batch is used as the initial state of the following batch for the same sample index $i$.\n\nBuilding the input matrix $X$ is important. It's *shape* depends on `nb_samples`, `timesteps`, `input_dim`, and `batch_size`, which must divide `nb_samples`. A LSTM model with ratio `nb_samples/batch_size` will receive this many blocks of samples, compute each output (number of timesteps for each sample), average the gradients, and propagate it to update the weight parameters vector.\n\n\nHere is an example of building a single `keras` stateful LSTM model using a single sample. It's simpler to merge the training and testing datasets into a single long-format dataset, including a separate column specifying the data type - *training* or *testing*. We need to re-specify the *tbl_time* object during the `bind_rows()` step.\n\n\nThe LSTM model assumptions include standardized (centered and scaled) input. \n\n\nSave the standardizing transformation, so that later we would be able to transform back the results into the domain of the original data.\n\n\n#### Definitions\n\nTo build an LSTM plan we should clarify the basic terms.\n\n - **Tensor Format**: The predictors ($X$) are 3D arrays of dimensions $[D_1=samples, D_2=timesteps, D_3=features]$, where $D_1$ is the length of values, $D_2$ is the number of time steps (lags), and $D_3$ is the number of predictors (1 if univariate, or $n$ if multivariate)\n - **Outcomes/Targets**: ($y$) is a 2D Array of dimensions: $[D_1=samples, D_2=timesteps]$.\n - **Training/Testing**: The training and testing length must be evenly divisible (e.g., $\\frac{training\\ length}{testing\\ length}$ must be an integer).\n - **Batch Size**: Represents the number of training examples in one forward/backward pass of the RNN prior to a weight update. The batch size must be evenly divisible into both the training and the testing lengths.\n - **Time Steps**: The time step is the number of lags included in the training/testing set.\n - **Epochs**: The epochs represent the total number of forward/backward pass iterations. Higher number of epochs tends to improve model performance, unless overfitting occurs, however, it's more computationally intense.\n\n#### Keras modeling of time-series data\n\nLet's try to fit a stateful RNN model for a time-series problem on a subset of the *Climate Data*, covering 2 years of data. For training, we will use the initial $105,120 = 24\\times 6\\times 365\\times 2$ observations, each 10-minutes apart, the 3-rd year of observations including $105,121:157,681$ will serve as a validation/testing dataset. [More information is available here](https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html). \n\n\n#### Keras modeling of image classification data (CIFAR10)\n\nThe [CIFAR-10 dataset](https://en.wikipedia.org/wiki/CIFAR-10) includes common images along with human derived class-labels. Specifically, CIFAR-10 contains 60,000 color images of size $32\\times 32$, each labeled in 10 different classes: *airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks*. There are 6,000 images of each class. Each class label is represented as a nominal factor (from 0 to 9), representing the ground truth (human labeling of each image). These labels are included in the file *batches.meta.mat*. \n\nLet's try to build a `keras` deep learning model that can predict the image class labels.\n\n\nAlthough the prediction accuracy may only be around $\\sim 0.4$, this is pretty good as the expected random classifier accuracy is just $0.1$ (1 out of 10 classes).\n\nAdditional examples illustrating the practical use of `keras` are included in [Chapter 14 (Deep Learning)](https://socr.umich.edu/DSPA2/DSPA2_notes/14_DeepLearning.html).\n\n\n\n\n<!----------------------FIX THESE 2 EXAMPLES------------------------------------",
      "word_count": 10041
    },
    {
      "title": "Example 1: Temperature Time-series LSTM Modeling",
      "content": "Suppose we choose a prediction of window 12 months (1 year) and a batch size of 4 units (evenly dividing the number of testing and training observations), time steps = 1 (specifying one lag), and epochs = 100.\n\n\n\nWe build the LSTM model, which is a linear stack of layers representing a sequential model, using the `keras_model_sequential()` function. Let's start with two LSTM layers each with 10 units. *LSTM layer 1* takes the required input shape, $[time\\ steps, number\\ of\\ features]$ and returns a 3D shape, stateful LSTM sequence (*return_sequences = TRUE* and *stateful = TRUE*). \n\n*LSTM layer 2* takes the layer 1 output and returns a 2D shape (*return_sequences = FALSE*). Adding a `layer_dense(units = 1)` provides a standard ending to the `keras` sequential model. At the end, we can use `compile()` with `loss = \"mae\"` and `optimizer = \"adam\"`.\n\n\nNext we can begin the stateful LSTM model fitting, manually using a `for` loop (where we reset the states). To preserve sequences, we specify `shuffle = FALSE` which allows us direct control over the states reset after each epoch, `reset_states()`.\n\n\nOnce the LSTM model is estimated, we can use it to predict outcomes for the testing dataset, `x_test_arr`, using the `predict()` function. \n\nOnce we get the predictions, these need to be inversely transformed into the original measuring units using the saved mean and SD of the standardizing transformation.\n\nFinally, we combine the predictions with the original data into a single column vector, using `reduce()` and the `time_bind_rows()` function.\n\n\nThe final step involves evaluating the stateful LSTM model.\n\nThe `caret` package and `yardstick::rmse()` function may be appropriate to quantify the performance. As the data is in  long format we can write a wrapper function `calc_rmse()` that transforms the data into the `yardstick::rmse()` format.\n\n\nTo visualize the performance, we can use a plotting function, `plot_prediction()`.\n\n\nLet's test the plotting function.",
      "word_count": 309
    },
    {
      "title": "Example 2: Text Generation",
      "content": "## Implementing character-level LSTM text generation\n\n\nLet's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the English language.\n\n## Preparing the data\n\nLet's start by downloading the corpus and converting it to lowercase:\n\n\n\nExtract partially overlapping sequences of length `maxlen`,  generate dummy indicator variables (one-hot encoding), and pack them in a 3D array `x` of shape `(sequences, maxlen, unique_characters)`. The array `y` contains the corresponding targets - the one-hot-encoded characters that come after each extracted sequence.\n\n\n\n## Building the LSTM network\n\nThis network consists of a single LSTM layer, a dense classifier, and softmax over all possible characters. Recurrent neural networks represent just one way to generate sequence data.\n\n\n\nThe targets are dummy indicator variables (i.e., one-hot encoded features) and we can use `categorical_crossentropy` as the loss to train the model.\n\n\n## Training the language model and sampling from it\n\n\nGiven a trained model and a seeding text, we can generate new synth-text by:\n\n* Draw from the model a probability distribution over the next character given the available text\n* Re-weight the distribution to a certain \"temperature\"\n* Sample the next character at random according to the re-weighted distribution\n* Append the new character at the end of the synth-text.\n\nThe `sample_next_char()` function re-weights the original probability distribution from the current model and draws a new character index.\n\n\nAt each learning epoch, train the model and generate synth-text with a range of different temperatures.\n\n\n\n\n\nReport results at  `epoch 50`. Is there evidence of model convergence?\n\n\nReport results for `temperature=0.2`: ...\n\nReport results for `temperature=0.5`: ....\n\n\nReport the result for `temperature=1.0`: ...\n\nLower temperature results may yield extremely repetitive and predictable text \nwith realistic English-language structure. \n\nHigher temperatures may yield more elaborate synthetic text, however, new plausible words/phrases/tokens may sometimes be invented and break the local sentence structure or include semi-random strings of characters. \n\nOne has to experiment with multiple sampling strategies to identify optimal balance between learned structure and randomness.\n\nTraining larger models will take longer and demand more data, but also generate more coherent and realistic text.\n\n\n## Summary\n\n* LSTM modeling allows generating pseudo-clinical notes by training an LSTM model to predict the subsequent terms/tokens given some text.\n* These clinical text/notes generators represent \"language modeling\".\n* Sampling additional words/phrases/tokens requires a balance between model proposals and stochastic randomness. T his balance is controlled by the *softmax temperature* parameter.\n---------------------------------------------------------------------->\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 577
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    self_contained: yes\n---",
      "row_count": 2
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "content": "Formula Type               | Operator | Explanation\n---------------------------|----------|----------------\nLatent variable definition | =~       | Is measured by\nRegression                 | ~        | Is regressed on\n(Residual) (co)variance    | ~~       | Is correlated with\nIntercept                  | ~1       | Intercept",
      "row_count": 6
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "content": ".       |   SEM   |     GLMM    |   GEE    \n--------|---------|-------------|----------\nAIC     |   4871  |     96.8    |   773.9  ",
      "row_count": 3
    }
  ],
  "r_code": [
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "beijing.pm25<-read.csv(\"https://umich.instructure.com/files/1823138/download?download_frd=1\")\nsummary(beijing.pm25)",
      "line_count": 2
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "beijing.pm25[beijing.pm25$Value==-999, 9]<-NA\nbeijing.pm25[is.na(beijing.pm25$Value), 9]<-floor(mean(beijing.pm25$Value, na.rm = T))",
      "line_count": 2
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "library(plotly)\nlibrary(ggplot2)\nid = 1:nrow(beijing.pm25)\nmat = matrix(0,nrow=24,ncol=3)\nstat = function(x){\n  c(mean(beijing.pm25[iid,\"Value\"]),quantile(beijing.pm25[iid,\"Value\"],c(0.2,0.8)))\n}\nfor (i in 1:24){\n  iid = which(id%%24==i-1)\n  mat[i,] = stat(iid)\n}\n\nmat <- as.data.frame(mat)\ncolnames(mat) <- c(\"mean\",\"20%\",\"80%\")\n# mat$time = c(1:24)\nmat$time <- readr::parse_datetime(beijing.pm25$Date..LST., \"%m/%d/%Y %h:%M\")[1:24]\n\nrequire(reshape2)\ndt <- melt(mat,id=\"time\")\n# colnames(dt)\n# ggplot(data = dt,mapping = aes(x=time,y=value,color = variable))+geom_line()+\n#   scale_x_continuous(breaks = 0:23)+ggtitle(\"Beijing hour average PM2.5 from 2008-2016\")\n\nplot_ly(data = dt, x=~time, y=~value, color = ~variable, type=\"scatter\", mode=\"markers+lines\") %>%\n  layout(title=\"Beijing daily 24-hour average PM2.5 (2008-2016)\")",
      "line_count": 25
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "library(plyr)\nts <- ts(beijing.pm25$Value, start=1, end=69335, frequency=1)\n# ts.plot(ts)\n\ndateTime <- readr::parse_datetime(beijing.pm25$Date..LST., \"%m/%d/%Y %h:%M\")\ntsValue <- ifelse(beijing.pm25$Value>=0, beijing.pm25$Value, -100)\ndf <- as.data.frame(cbind(dateTime=dateTime, tsValue=tsValue))\n\nplot_ly(x=~dateTime, y=~tsValue, type=\"scatter\", mode=\"lines\") %>%\n  layout(\n    title = \"Time Series - Beijing daily 24-hour average PM2.5 (2008-2016)\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"PM2.5 Measurement\"))\n    # '2008-04-08 15:00:00 UTC', '2016-04-30 23:00:00 UTC')",
      "line_count": 13
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"TTR\")\nlibrary(TTR)\nbj.month <- SMA(tsValue, n=720)\n\n# plot.ts(bj.month, main=\"Monthly PM2.5 Level SMA\", ylab=\"PM2.5 AQI\")\n\nplot_ly(x=~dateTime, y=~bj.month, type=\"scatter\", mode=\"lines\") %>%\n  layout(\n    title = \"Beijing Simple Moving Average Monthly PM2.5 Levels\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Monthly PM2.5 SMA\"))",
      "line_count": 10
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "bj.month <- EMA(tsValue, n=1, ratio = 2/(720+1))\n# plot.ts(bj.month, main=\"Monthly PM2.5 Level EMA\", ylab=\"PM2.5 AQI\")\n\nplot_ly(x=~dateTime, y=~tsValue, type=\"scatter\", mode=\"lines\", opacity=0.2, name=\"Observed\") %>%\n  add_trace(x=~dateTime, y=~SMA(tsValue, n=720), type=\"scatter\", mode=\"lines\", opacity=1.0, name=\"SMA\") %>%\n  add_trace(x=~dateTime, y=~bj.month, type=\"scatter\", mode=\"lines\", name=\"EMA\") %>%  # line = list(dash = 'dot')\n  layout(\n    title = \"Beijing PM2.5 Levels with EMA and SMA Smoothing\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"PM2.5 Values\"))",
      "line_count": 9
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "#par(mfrow= c(2, 1))\nbj.diff2<-diff(tsValue, differences=2)\n#plot.ts(bj.diff2, main=\"2nd differencing\")\nbj.diff<-diff(tsValue, differences=1)\n# plot.ts(bj.diff, main=\"1st differencing\")\n\nplot_ly(x=~dateTime[2:length(dateTime)], y=~bj.diff, type=\"scatter\", mode=\"lines\", name=\"1st diff\", opacity=0.5) %>%\n  add_trace(x=~dateTime[3:length(dateTime)], y=~bj.diff2, type=\"scatter\", mode=\"lines\", \n            name=\"2nd diff\", opacity=0.5)  %>%\n  layout(title = \"Beijing PM2.5 Levels - First & Second Differencing\", legend = list(orientation='h'),\n    xaxis = list(title = \"date\"), yaxis=list(title=\"PM2.5\"))\n",
      "line_count": 12
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "bj.diff  <- diff(bj.month, differences=1)\nbj.diff2 <- diff(bj.month, differences=2)\n\n# par(mfrow=c(2, 1))\n# plot.ts(bj.diff2, main=\"2nd differencing\")\n# plot.ts(bj.diff, main=\"1st differencing\")\n\nplot_ly(x=~dateTime[2:length(dateTime)], y=~bj.diff, type=\"scatter\", mode=\"lines\", name=\"1st diff\", opacity=0.5) %>%\n  add_trace(x=~dateTime[3:length(dateTime)], y=~bj.diff2, type=\"scatter\", mode=\"lines\", \n            name=\"2nd diff\", opacity=0.5)  %>%\n  layout(title = \"Beijing EMA PM2.5 Levels - First & Second Differencing\", legend = list(orientation='h'),\n    xaxis = list(title = \"date\"), yaxis=list(title=\"EMA PM2.5\"))",
      "line_count": 12
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# par(mfrow=c(1, 2))\nacf1 <- acf(bj.diff, lag.max = 20, main=\"ACF\", plot=F)\n# 95% CI: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/acf.R\nci <- qnorm((1 + 0.95)/2)/sqrt(length(bj.diff))\n\nplot_ly(x = ~acf1$lag[,1,1], y = ~acf1$acf[,1,1], type = \"bar\", name=\"ACF\") %>%\n  # add CI lines\n  add_lines(x=~c(acf1$lag[1,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(ci, ci), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(acf1$lag[1,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(-ci, -ci), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(bargap=0.8, title=\"ACF Plot Beijing EMA PM2.5 Levels - First Differencing\",\n         xaxis=list(title=\"lag\"), yaxis=list(title=\"ACF\"))\n\npacf1 <- pacf(ts(bj.diff), lag.max = 20, main=\"PACF\", plot=F)\nplot_ly(x = ~pacf1$lag[,1,1], y = ~pacf1$acf[,1,1], type = \"bar\", name=\"PACF\") %>%\n   # add CI lines\n  add_lines(x=~c(pacf1$lag[1,1,1], acf1$lag[length(pacf1$lag),1,1]), y=~c(ci, ci), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(acf1$lag[1,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(-ci, -ci), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(bargap=0.8, title=\"PACF Plot Beijing EMA PM2.5 Levels - First Differencing\",\n         xaxis=list(title=\"lag\"), yaxis=list(title=\"Partial ACF\"))",
      "line_count": 23
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "count_ma = ts(bj.month, frequency=30*24)\ndecomp = stl(count_ma, s.window=\"periodic\")\ndeseasonal_count <- forecast::seasadj(decomp)\n# plot(decomp)\n\nplot_ly(x=~dateTime, y=~tsValue, type=\"scatter\", mode=\"lines\") %>%\n  layout(\n    title = \"Time Series - Beijing daily 24-hour average PM2.5 (2008-2016)\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"PM2.5 Measurement\"))\n\nfigTS_Data    <- plot_ly(x=~dateTime, y=~tsValue, type=\"scatter\", mode=\"lines\", name=\"TS Data\")\nfigSeasonal   <- plot_ly(x=~dateTime, y=~decomp$time.series[,1], type='scatter', mode='lines', name=\"seasonal\")\nfigTrend      <- plot_ly(x=~dateTime, y=~decomp$time.series[,2], type='scatter', mode='lines', name=\"trend\")\nfigRemainder  <- plot_ly(x=~dateTime, y=~decomp$time.series[,3], type='scatter', mode='lines', name=\"remainder\")\nfig <- subplot(figTS_Data, figSeasonal, figTrend, figRemainder, nrows = 4) %>% \n  layout(title = list(text = \"Decomposition of Beijing PM2.5 Time-Series Data (2008-2016)\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))\nfig",
      "line_count": 18
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "tseries::adf.test(count_ma, alternative = \"stationary\")\ntseries::adf.test(bj.diff, alternative = \"stationary\")",
      "line_count": 2
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"forecast\")\nlibrary(forecast)\nfit<-auto.arima(bj.month, approx=F, trace = F)\nfit\n\nacf1 <- Acf(residuals(fit), plot = F)\n# 95% CI: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/acf.R\nci <- qnorm((1 + 0.95)/2)/sqrt(length(bj.diff))\npacf1 <- Pacf(residuals(fit), plot = F)\n\n\nplot_ly(x = ~acf1$lag[-1,1,1], y = ~acf1$acf[-1,1,1], type = \"bar\", name=\"ACF\") %>%  # ACF\n  # PACF\n  add_trace(x = ~pacf1$lag[-1,1,1], y = ~pacf1$acf[-1,1,1], type = \"bar\", name=\"PACF\") %>%\n  # add CI lines\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(ci, ci), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(-ci, -ci), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(bargap=0.8, title=\"ACF & PACF Plot - ARIMA Model Residuals\",\n         xaxis=list(title=\"lag\"), yaxis=list(title=\"ACF/PACF (Residuals)\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 22
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "fit1<-auto.arima(bj.month, xreg=beijing.pm25$Month, approx=F, trace = F)\nfit1\nfit3<-arima(bj.month, order = c(2, 1, 0))\nfit3",
      "line_count": 4
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# resFitModel <- tsdisplay(residuals(fit), lag.max=45, main='(1,1,4) Model Residuals')\n\nplot_ly(x=~dateTime, y=~residuals(fit), type=\"scatter\", mode=\"lines\") %>%\n  # add CI lines\n  add_lines(x=~c(dateTime[1], dateTime[length(dateTime)]), y=~c(1.96, 1.96), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(dateTime[1], dateTime[length(dateTime)]), y=~c(-1.96, -1.96), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(title = \"Beijing PM2.5: ARIMA(2, 1, 0) Model Residuals\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Residuals\"), hovermode = \"x unified\")\n\nacf1 <- Acf(residuals(fit), plot = F)\n# 95% CI: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/acf.R\nci <- qnorm((1 + 0.95)/2)/sqrt(length(bj.diff))\npacf1 <- Pacf(residuals(fit), plot = F)\n\n\nplot_ly(x = ~acf1$lag[-1,1,1], y = ~acf1$acf[-1,1,1], type = \"bar\", name=\"ACF\") %>%  # ACF\n  # PACF\n  add_trace(x = ~pacf1$lag[-1,1,1], y = ~pacf1$acf[-1,1,1], type = \"bar\", name=\"PACF\") %>%\n  # add CI lines\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(ci, ci), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(-ci, -ci), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(bargap=0.8, title=\"ACF & PACF Plot - ARIMA Model Residuals\",\n         xaxis=list(title=\"lag\"), yaxis=list(title=\"ACF/PACF (Residuals)\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 28
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "fit24 <- arima(deseasonal_count, order=c(1,1,24)); fit24\n\n# tsdisplay(residuals(fit24), lag.max=36, main='Seasonal Model Residuals')\n\nplot_ly(x=~dateTime, y=~residuals(fit24), type=\"scatter\", mode=\"lines\") %>%\n  # add CI lines\n  add_lines(x=~c(dateTime[1], dateTime[length(dateTime)]), y=~c(1.96, 1.96), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(dateTime[1], dateTime[length(dateTime)]), y=~c(-1.96, -1.96), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(title = \"Beijing PM2.5: ARIMA(1,1,24) Model Residuals\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Residuals\"), hovermode = \"x unified\")\n\nacf1 <- Acf(residuals(fit24), plot = F)\n# 95% CI: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/acf.R\nci <- qnorm((1 + 0.95)/2)/sqrt(length(bj.diff))\npacf1 <- Pacf(residuals(fit24), plot = F)\n\n\nplot_ly(x = ~acf1$lag[-1,1,1], y = ~acf1$acf[-1,1,1], type = \"bar\", name=\"ACF\") %>%  # ACF\n  # PACF\n  add_trace(x = ~pacf1$lag[-1,1,1], y = ~pacf1$acf[-1,1,1], type = \"bar\", name=\"PACF\") %>%\n  # add CI lines\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(ci, ci), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(-ci, -ci), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(bargap=0.8, title=\"ACF & PACF Plot - ARIMA(1,1,24) Model Residuals\",\n         xaxis=list(title=\"lag\"), yaxis=list(title=\"ACF/PACF (Residuals)\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))\n\ndisplayForecastErrors <- function(forecastErrors)\n  {\n     # Generate a histogram of the Forecast Errors\n     binsize <- IQR(forecastErrors)/4\n     sd   <- sd(forecastErrors)\n     min  <- min(forecastErrors) - sd\n     max  <- max(forecastErrors) + sd\n     \n     # Generate 5K normal(0,sd) RVs \n     norm <- rnorm(5000, mean=0, sd=sd)\n     min2 <- min(norm)\n     max2 <- max(norm)\n     if (min2 < min) { min <- min2 }\n     if (max2 > max) { max <- max2 }\n     \n     # Plot red histogram of the forecast errors\n     bins <- seq(min, max, binsize)\n     # hist(forecastErrors, col=\"red\", freq=FALSE, breaks=bins)\n     # \n     # myHist <- hist(norm, plot=FALSE, breaks=bins)\n     # \n     # # Overlay the Blue normal curve on top of forecastErrors histogram\n     # points(myHist$mids, myHist$density, type=\"l\", col=\"blue\", lwd=2)\n     \n     histForecast <- hist(forecastErrors, breaks=bins, plot = F)\n     histNormal <- hist(norm, plot=FALSE, breaks=bins)\n     plot_ly(x = histForecast$mids, y = histForecast$density, type = \"bar\", name=\"Forecast-Errors Histogram\") %>%\n        add_lines(x=histNormal$mids/2, y=2*dnorm(histNormal$mids, 0, sd), type=\"scatter\", mode=\"lines\", \n                  name=\"(Theoretical) Normal Density\") %>%\n        layout(bargap=0.1, title=\"Histogram ARIMA(1,1,24) Model Residuals\",\n               legend = list(orientation = 'h'))\n}\n\ndisplayForecastErrors(residuals(fit24))",
      "line_count": 65
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "ts.forecasts<-forecast(fit, h=720)\n\n# par(mfrow=c(1, 1))\n# plot(ts.forecasts, include = 2880)\n\n# extend time for 30 days (hourly predictions), k=720 hours\nforecastDates <- seq(as.POSIXct(\"2016-05-01 00:00:00 UTC\"), as.POSIXct(\"2016-05-30 23:00:00 UTC\"), by = \"hour\")\n\nplot_ly(x=~dateTime, y=~ts.forecasts$model$fitted, type=\"scatter\", mode=\"lines\", name=\"Observed\") %>%\n  # add CI lines\n  add_lines(x=~forecastDates, y=~ts.forecasts$mean, mode=\"lines\", name=\"Mean Forecast\", line=list(size=5)) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$lower[,1], mode=\"lines\", name=\"Lower 80%\", line=list(dash='dash')) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$lower[,2], mode=\"lines\", name=\"Lower 95%\", line=list(dash='dot')) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$upper[,1], mode=\"lines\", name=\"Upper 80%\", line=list(dash='dash')) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$upper[,2], mode=\"lines\", name=\"Upper 95%\", line=list(dash='dot')) %>%\n  layout(title = paste0(\"Beijing PM2.5: Forecast using \", ts.forecasts$method, \" Model\"),\n    xaxis = list(title = \"date\"), yaxis=list(title=\"PM2.5\"), hovermode = \"x unified\", legend = list(orientation='h'))\n",
      "line_count": 18
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "library(forecast)\n\n# create the 4D synthetic data, t=700 days, 100 weeks, \n# Y=eventArrivals (customers), X=(weekday, NewYearsHoliday, day)\nweeks <- 100\ndays <- 7*weeks\n\n# define holiday effect (e.g., more customers go to stores to shop)\nholidayShopping <- rep(0, days)\ny <- rep(0, days)\nfor (i in 1:days) {\n  # Binary feature: Holiday\n  if (i %% 28 == 0) holidayShopping[i] = 1\n  else holidayShopping[i] = 0\n  # Outcome (Y)\n  if (i %% 56 == 0) y[i] = rpois(n=1, lambda=1500)\n  else y[i] = rpois(n=1, lambda=1000)\n  if (i>2) y[i] = (y[i-2] + y[i-1])/2 + rnorm(n=1, mean=0, sd=20)\n  if (i %% 56 == 0) y[i] = rpois(n=1, lambda=1050)\n}\n\n# plot(y)\n# extend time for \"days\", k=700 days\nforecastDates <- seq(as.POSIXct(\"2016-05-01 00:00:00 UTC\"), length.out = days, by = \"day\")\n\nplot_ly(x=~forecastDates, y=~y, type=\"scatter\", mode=\"lines\", name=\"Y=eventArrivals\", line=list(size=5)) %>%\n  layout(title = paste0(\"Simulated TS Data\"),\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Y\"), hovermode = \"x unified\")\n\nsynthData <- data.frame(eventArrivals = y, # expected number of arrivals=1000\n                         weekday = rep(1:7, weeks),     # weekday indicator variable\n                         Holiday = holidayShopping, # Binary feature: Holiday\n                         day=1:days)  # day indicator\n\n# Create matrix of numeric predictors\ncovariatesX <- cbind(weekday = model.matrix(~as.factor(synthData$weekday)), \n                     Day = synthData$day,\n                     Holiday=synthData$Holiday)\n# Generate prospective covariates for external validation/prediction\npred_length <- 56\nnewCovX <- cbind(weekday = model.matrix(~as.factor(synthData$weekday[1:pred_length])), \n                     Day = synthData$day[1:pred_length],\n                     Holiday=synthData$Holiday[1:pred_length])\n# Remove the intercept\nsynthX_Data <- covariatesX[,-1]\n\n# Rename columns\ncolnames(synthX_Data) <- c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Day\", \"Holiday\")\ncolnames(newCovX) <- c(\"Intercept\", colnames(synthX_Data)); colnames(newCovX[,-1])\n\n# Response (Y) to be modeled as a time-series, weekly oscillations (7-days)\nobservedArrivals <- ts(synthData$eventArrivals, frequency=7) \n\n# Estimate the ARIMAX model, remove day-index (7)\nfitArimaX <- auto.arima(observedArrivals, xreg=synthX_Data[,-7]); fitArimaX\n\n# Predict prospective arrivals (e.g., volumes of customers), remove Intercept (1) and day-index (8)\npredArrivals <- predict(fitArimaX, n.ahead = pred_length, newxreg = newCovX[,-c(1,8)])\n\n# plot(predArrivals$pred, main=\"Forward time-series predictions (fitArimaX)\")\n  \nforecastDates <- seq(as.POSIXct(\"2018-04-01 EDT\"), length.out = pred_length, by = \"day\")\n# forecastDates <- seq(as.POSIXct(\"2016-05-01 00:00:00 UTC\"), length.out = pred_length, by = \"day\")\ndf <- as.data.frame(cbind(x=forecastDates, y=predArrivals$pred, errorBar=predArrivals$pred))\nplot_ly(data = df) %>%\n    # add error bars for each CV-mean at log(lambda)\n    add_trace(x = ~forecastDates, y = ~y, type = 'scatter', mode = 'markers+lines',\n        name = 'CV MSE', error_y = ~list(array = errorBar/300, color=\"green\", opacity=0.5)) %>% \n    layout(title = paste0(\"ARIMA Forecasting using Simulated Data (\", \n                          forecastDates[1], \" - \", forecastDates[pred_length], \")\"),\n\t\t\t\t\t\t\thovermode = \"x unified\",  # xaxis2 = ax, \n\t\t\t\t\t\t\tyaxis = list(title = 'Y',\tside=\"left\", showgrid = TRUE))\n\n# plot(forecast(fitArimaX, xreg = newCovX[,-1]))\n  \ntrainingDates <- seq(as.POSIXct(\"2016-05-01 00:00:00 UTC\"), length.out = days, by = \"day\")\n# extend time for 56 days of forward prediction\nforecastDates <- seq(as.POSIXct(\"2018-04-01 EDT\"), length.out = pred_length, by = \"day\")\n\nts.forecasts <- forecast(fitArimaX, xreg = newCovX[,-c(1,8)])\n\nplot_ly(x=~trainingDates, y=~ts.forecasts$model$fitted, type=\"scatter\", mode=\"lines\", name=\"Observed\") %>%\n  # add CI lines\n  add_lines(x=~forecastDates, y=~ts.forecasts$mean, mode=\"lines\", name=\"Mean Forecast\", line=list(size=5)) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$lower[,1], mode=\"lines\", name=\"Lower 80%\", line=list(dash='dash')) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$lower[,2], mode=\"lines\", name=\"Lower 95%\", line=list(dash='dot')) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$upper[,1], mode=\"lines\", name=\"Upper 80%\", line=list(dash='dash')) %>%\n  add_lines(x=~forecastDates, y=~ts.forecasts$upper[,2], mode=\"lines\", name=\"Upper 95%\", line=list(dash='dot')) %>%\n  layout(title = paste0(\"Simulated Data: Forecast using \", ts.forecasts$method, \" Model\"),\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Customers Volume\"), hovermode = \"x unified\", \n    legend = list(orientation='h'))",
      "line_count": 91
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"prophet\")\n# install.packages(\"devtools\")\n# install.packages(\"gtrendsR\")\nlibrary(gtrendsR)\nlibrary(ggplot2)\nlibrary(prophet)",
      "line_count": 6
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "DSPA_trends_US <- gtrends(c(\"Big Data\", \"data science\", \"predictive analytics\", \n                            \"Artificial Intelligence\"), \n                          geo = c(\"US\"), gprop = \"web\", \n                          time = \"2013-06-22 2023-06-21\")[[1]]\n\n# ggplot(data = DSPA_trends_US, aes(x = date, y = hits, group = keyword)) +\n#   geom_line(size = 1.5, alpha = 0.8, aes(color = keyword)) +\n#   geom_point(size = 1.6) +\n#   ylim(0, NA) +\n#   theme(legend.title=element_blank(), axis.title.x = element_blank()) +\n#   theme(legend.position=\"bottom\") +\n#   ylab(\"Phrase Search Freq\") + \n#   ggtitle(\"USA: Google Trends (Big Data, Data Science, Predictive Analytics): 2010-2023\")\n\nplot_ly(data = DSPA_trends_US, x=~date, y=~hits, type=\"scatter\", mode=\"lines\", color=~keyword, name=~keyword) %>%\n  layout(title = \"USA: Google Trends (Big Data, Data Science, Predictive Analytics): 2013-2023\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Phrase Search Frequency\"), hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 17
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# DSPA_trends_World <- gtrends(c(\"Big Data\", \"data science\", \"predictive analytics\",\n#                                \"Artificial Intelligence\"), geo=c(\"all\"),\n#                           gprop = \"web\", time = \"2010-01-01 2023-06-21\")[[1]]\n\nSys.sleep(10)  # pause 10-seconds to allow gtrends API to relax\n\nDSPA_trends_World <- gtrends(c(\"Big Data\", \"data science\", \"predictive analytics\", \n                               \"Artificial Intelligence\"),  geo = c(\"\"), \n                            gprop = \"web\", time = \"2013-06-22 2023-06-21\")[[1]]\n\n# ggplot(data = DSPA_trends_World, aes(x = date, y = hits, group = keyword)) +\n#   geom_line(size = 1.5, alpha = 0.8, aes(color = keyword)) +\n#   geom_point(size = 1.6) +\n#   ylim(0, NA) +\n#   theme(legend.title=element_blank(), axis.title.x = element_blank()) +\n#   theme(legend.position=\"bottom\") +\n#   ylab(\"Phrase Search Freq\") + \n#   ggtitle(\"World: Google Trends (Big Data, Data Science, Predictive Analytics): 2010-2023\")\n\nplot_ly(data = DSPA_trends_World, x=~date, y=~hits, type=\"scatter\", mode=\"lines\", \n        color=~keyword, name=~keyword) %>%\n  layout(title = \"World: Google Trends (Big Data, Data Science, Predictive Analytics): 2010-2023\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Phrase Search Frequency\"), \n    hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 24
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# DSPA_trends_Global <- gtrends(c(\"data science\"), \n#                           geo = c(\"US\",\"CN\",\"GB\"), gprop = \"web\", \n#                           time = \"2010-01-01 2023-06-27\")[[1]]\nSys.sleep(10)  # pause 10-seconds to allow gtrends API to relax\n\nDSPA_trends_Global <- gtrends(c(\"data science\"),  geo = c(\"US\",\"CN\",\"GB\"), gprop = \"web\", \n                              time = \"2013-06-22 2023-06-21\")[[1]]\nDSPA_trends_Global$hits[is.na(DSPA_trends_Global$hits)] <- \"0\"\n# DSPA_trends_Global$hits\n\n# ggplot(data = DSPA_trends_Global, aes(x = date, \n#                      y = as.numeric(hits), group = geo)) +\n#   geom_line(size = 1.5, alpha = 0.8, aes(color = geo)) +\n#   geom_point(size = 1.6) +\n#   ylim(0, NA) +\n#   theme(legend.title=element_blank(), axis.title.x = element_blank()) +\n#   theme(legend.position=\"bottom\") +\n#   ylab(\"Data Science Search Freq'\") + \n#   ggtitle(\"Global 'Data Science' Google Trends (US, CN, GB): 2010-2020\")\n\nplot_ly(data = DSPA_trends_Global, x=~date, y=~hits, type=\"scatter\", mode=\"lines\", color=~geo, name=~geo) %>%\n  layout(title = \"Global 'Data Science' Google Trends (US, CN, GB): 2010-2023\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"Phrase Search Frequency\"), hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 23
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "US_ts <- DSPA_trends_US[which(DSPA_trends_US$keyword == \"data science\"), \n                        names(DSPA_trends_US) %in% c(\"date\",\"hits\")]\n# US_ts <- dplyr::select(filter(DSPA_trends_US, keyword=='data science'), c(\"date\",\"hits\"))\n\nUS_ts$hits <- as.numeric(US_ts$hits)\n\ncolnames(US_ts) <-c(\"ds\",\"y\")\nUS_ts_prophet <- prophet(US_ts, weekly.seasonality=TRUE)",
      "line_count": 8
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "future_df <- make_future_dataframe(US_ts_prophet, periods = 730)\nUS_ts_prophet_pred <- predict(US_ts_prophet, future_df)\n\n# plot(US_ts_prophet, US_ts_prophet_pred)\n\nplot_ly(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$yhat, type=\"scatter\", mode=\"markers+lines\", name=\"Prophet Model\") %>%\n  add_trace(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$yhat_lower, type=\"scatter\", mode=\"lines\", name=\"Lower Limit\") %>%\n  add_trace(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$yhat_upper, type=\"scatter\", mode=\"lines\", name=\"Upper Limit\") %>%\n  layout(title = \"Prophet Additive TS Model\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"TS Value (data science)\"), hovermode = \"x unified\", legend = list(orientation='h'))\n",
      "line_count": 11
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "prophet_plot_components(US_ts_prophet, US_ts_prophet_pred)\n\nplot_ly(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$trend, type=\"scatter\", mode=\"markers+lines\", name=\"Prophet Model\") %>%\n  add_trace(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$trend_lower, type=\"scatter\", mode=\"lines\", name=\"Lower Limit\") %>%\n  add_trace(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$trend_upper, type=\"scatter\", mode=\"lines\", name=\"Upper Limit\") %>%\n  layout(title = \"Prophet Model Trend\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"TS Value (data science)\"), hovermode = \"x unified\", legend = list(orientation='h'))\n\nplot_ly(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$yearly, type=\"scatter\", mode=\"markers+lines\", name=\"Prophet Model\") %>%\n  # add_trace(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$yearly_lower, type=\"scatter\", mode=\"lines\", name=\"Lower Limit\") %>%\n  # add_trace(x=~US_ts_prophet_pred$ds, y=~US_ts_prophet_pred$yearly_upper, type=\"scatter\", mode=\"lines\", name=\"Upper Limit\") %>%\n  layout(title = \"Detrended Prophet Model\",\n    xaxis = list(title = \"date\"), yaxis=list(title=\"TS Value (data science)\"), hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 13
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "library(forecast)\nfit_US_ts <- auto.arima(US_ts$y, approx=F, trace = F)\nfit_US_ts\n\n# Acf(residuals(fit_US_ts))\n\nacf1 <- Acf(residuals(fit_US_ts), plot = F)\n# 95% CI: https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/acf.R\nci <- qnorm((1 + 0.95)/2)/sqrt(length(US_ts$y))\npacf1 <- Pacf(residuals(fit_US_ts), plot = F)\n\n\nplot_ly(x = ~acf1$lag[-1,1,1], y = ~acf1$acf[-1,1,1], type = \"bar\", name=\"ACF\") %>%  # ACF\n  # PACF\n  add_trace(x = ~pacf1$lag[-1,1,1], y = ~pacf1$acf[-1,1,1], type = \"bar\", name=\"PACF\") %>%\n  # add CI lines\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(ci, ci), \n            mode=\"lines\", name=\"Upper CI\", line=list(dash='dash')) %>%\n  add_lines(x=~c(acf1$lag[2,1,1], acf1$lag[length(acf1$lag),1,1]), y=~c(-ci, -ci), \n            mode=\"lines\", name=\"Lower CI\", line=list(dash='dash')) %>%\n  layout(bargap=0.8, xaxis=list(title=\"lag\"), yaxis=list(title=\"ACF/PACF (Residuals)\"),\n         title=paste0(\"ACF & PACF Plots using \", fit_US_ts$method, \" Model Residuals\"),\n         hovermode = \"x unified\", legend = list(orientation='h'))\n\ndisplayForecastErrors <- function(forecastErrors)\n  {\n     # Generate a histogram of the Forecast Errors\n     binsize <- IQR(forecastErrors)/4\n     sd   <- sd(forecastErrors)\n     min  <- min(forecastErrors) - sd\n     max  <- max(forecastErrors) + sd\n     \n     # Generate 5K normal(0,sd) RVs \n     norm <- rnorm(5000, mean=0, sd=sd)\n     min2 <- min(norm)\n     max2 <- max(norm)\n     if (min2 < min) { min <- min2 }\n     if (max2 > max) { max <- max2 }\n     \n     # Plot red histogram of the forecast errors\n     bins <- seq(min, max, length.out = 20)  # binsize)\n     # hist(forecastErrors, col=\"red\", freq=FALSE, breaks=bins)\n     # \n     # myHist <- hist(norm, plot=FALSE, breaks=bins)\n     # \n     # # Overlay the Blue normal curve on top of forecastErrors histogram\n     # points(myHist$mids, myHist$density, type=\"l\", col=\"blue\", lwd=2)\n     \n     histForecast <- hist(forecastErrors, breaks=bins, plot = F)\n     histNormal <- hist(norm, plot=FALSE, breaks=bins)\n     plot_ly(x = histForecast$mids, y = histForecast$density, type = \"bar\", name=\"Forecast-Errors Histogram\") %>%\n        add_lines(x=histNormal$mids, y=dnorm(histNormal$mids, 0, sd), type=\"scatter\", mode=\"lines\", \n                  name=\"(Theoretical) Normal Density\") %>%\n        layout(bargap=0.1, legend = list(orientation = 'h'),\n               title=paste0(\"US TS Forecast using \", fit_US_ts$method, \" Model Residuals\"))\n}\n\n# tsdisplay(residuals(fit_US_ts), lag.max=45, main='(0,1,0) Model Residuals')\n\ndisplayForecastErrors(residuals(fit_US_ts))\n\nfit_US_ts_forecasts <- forecast(fit_US_ts, h=10, level=20)\n# plot(fit_US_ts_forecasts, include = 100)\n\nforecastDates <- seq(as.POSIXct(\"2023-06-28 GMT\"), length.out=10, by = \"month\")\n\nplot_ly(x=~US_ts$ds, y=~fit_US_ts_forecasts$fitted, type=\"scatter\", mode=\"lines\", name=\"Observed\") %>%\n  # add CI lines\n  add_lines(x=~forecastDates, y=~fit_US_ts_forecasts$mean, mode=\"lines\", name=\"Mean Forecast\", line=list(size=5)) %>%\n  add_lines(x=~forecastDates, y=~fit_US_ts_forecasts$lower[,1], mode=\"lines\", name=\"Lower 80%\", line=list(dash='dash')) %>%\n  add_lines(x=~forecastDates, y=~fit_US_ts_forecasts$lower[,1], mode=\"lines\", name=\"Lower 95%\", line=list(dash='dot')) %>%\n  add_lines(x=~forecastDates, y=~fit_US_ts_forecasts$upper[,1], mode=\"lines\", name=\"Upper 80%\", line=list(dash='dash')) %>%\n  add_lines(x=~forecastDates, y=~fit_US_ts_forecasts$upper[,1], mode=\"lines\", name=\"Upper 95%\", line=list(dash='dot')) %>%\n  layout(title = paste0(\"US TS Forecast using \", fit_US_ts_forecasts$method, \" Model\"),\n    xaxis = list(title = \"date\"), yaxis=list(title=\"(Data Science) Search Frequency\"), hovermode = \"x unified\", legend = list(orientation='h'))",
      "line_count": 75
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "par(mfrow=c(1, 1))\nPPMI <- read.csv(\"https://umich.instructure.com/files/330397/download?download_frd=1\")\nsummary(PPMI)\nPPMI$ResearchGroup <- ifelse(PPMI$ResearchGroup==\"Control\", \"1\", \"0\")\n# PPMI <- PPMI[ , !(names(PPMI) %in% c(\"FID_IID\", \"time_visit\"))] # remove Subject ID/time",
      "line_count": 5
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "pp_heat <- PPMI[complete.cases(PPMI), -c(1, 20, 31)]\ncorr_mat = cor(pp_heat)\n# Remove upper triangle\ncorr_mat_lower = corr_mat\ncorr_mat_lower[upper.tri(corr_mat_lower)] = NA\n# Melt correlation matrix and make sure order of factor variables is correct\ncorr_mat_melted = melt(corr_mat_lower)\ncolnames(corr_mat_melted) <- c(\"Var1\", \"Var2\", \"value\")\ncorr_mat_melted$Var1 = factor(corr_mat_melted$Var1, levels=colnames(corr_mat))\ncorr_mat_melted$Var2 = factor(corr_mat_melted$Var2, levels=colnames(corr_mat))\n\n# Plot\n# corr_plot = ggplot(corr_mat_melted, aes(x=Var1, y=Var2, fill=value)) +\n#   geom_tile(color='white') +\n#   scale_fill_distiller(limits=c(-1, 1), palette='RdBu', na.value='white',\n#                        name='Correlation') +\n#   ggtitle('Correlations') +\n#   coord_fixed(ratio=1) +\n#   theme_minimal() +\n#   scale_y_discrete(position=\"right\") +\n#   theme(axis.text.x=element_text(angle=90, vjust=1, hjust=1),\n#         axis.title.x=element_blank(),\n#         axis.title.y=element_blank(),\n#         panel.grid.major=element_blank(),\n#         legend.position=c(0.1,0.9),\n#         legend.justification=c(0,1))\n# corr_plot\n\nplot_ly(x =~colnames(corr_mat_lower), y = ~rownames(corr_mat_lower), z = ~corr_mat_lower, type = \"heatmap\")",
      "line_count": 29
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "cor(PPMI$L_insular_cortex_ComputeArea, PPMI$L_insular_cortex_Volume)\ncor(PPMI$UPDRS_part_I, PPMI$UPDRS_part_II, use = \"complete.obs\")\ncor(PPMI$UPDRS_part_II, PPMI$UPDRS_part_III, use = \"complete.obs\")",
      "line_count": 3
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "model1<-\n  ' \n    Imaging =~ L_cingulate_gyrus_ComputeArea  + L_cingulate_gyrus_Volume+R_cingulate_gyrus_ComputeArea+R_cingulate_gyrus_Volume+R_insular_cortex_ComputeArea+R_insular_cortex_Volume\n    UPDRS=~UPDRS_part_I+UPDRS_part_II+UPDRS_part_III\n    DemoGeno =~ Weight+Sex+Age\n\n    ResearchGroup ~ Imaging + DemoGeno + UPDRS\n  '",
      "line_count": 8
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "mydata<-scale(PPMI[, -c(1,20,31)]) # avoid scaling ID, Dx, Time\nmydata<-data.frame(PPMI$FID_IID, mydata, cbind(PPMI$time_visit, PPMI$ResearchGroup))\ncolnames(mydata)[1]<-\"FID_IID\"\ncolnames(mydata)[30]<-\"time_visit\"\ncolnames(mydata)[31]<-\"ResearchGroup\"",
      "line_count": 5
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"lavaan\")\n# install.packages(\"pbivnorm\")\nlibrary(lavaan)\n# lavaan requires all variables to be ordered\n# mydata[,] <- lapply(mydata[,], ordered)\nmydata$ResearchGroup <- as.ordered(mydata$ResearchGroup)\nmydata$time_visit <- as.ordered(mydata$time_visit)\n\n# See Lavaan's protocol for handling categorical engo/exogen covariates: \n# https://lavaan.ugent.be/tutorial/cat.html\n#fit <- lavaan::cfa(model1, data=mydata, missing = 'WLSMV')\nfit <- lavaan::cfa(model1, data=mydata, estimator = \"PML\")",
      "line_count": 12
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "model2 <-\n    '      # (1) Measurement Model\n      Imaging =~  L_cingulate_gyrus_ComputeArea +\n        L_cingulate_gyrus_Volume+R_cingulate_gyrus_ComputeArea+R_cingulate_gyrus_Volume+\n        R_insular_cortex_ComputeArea+R_insular_cortex_Volume\n      UPDRS =~  UPDRS_part_I  +UPDRS_part_II + UPDRS_part_III\n          # (2) Regressions\n      ResearchGroup ~ Imaging + UPDRS +Age+Sex+Weight\n    '",
      "line_count": 9
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "#fit2<-cfa(model2, data=mydata, missing = 'FIML')\nfit2 <- lavaan::cfa(model2, data=mydata, optim.gradient = \"numerical\")\nsummary(fit2, fit.measures=TRUE)",
      "line_count": 3
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "mydata$UPDRS <- mydata$UPDRS_part_I+1.890*mydata$UPDRS_part_II+2.345*mydata$UPDRS_part_III\nmydata$Imaging <- mydata$L_cingulate_gyrus_ComputeArea  +0.994*mydata$L_cingulate_gyrus_Volume+0.961*mydata$R_cingulate_gyrus_ComputeArea+0.955*mydata$R_cingulate_gyrus_Volume+0.930*mydata$R_insular_cortex_ComputeArea+0.920*mydata$R_insular_cortex_Volume",
      "line_count": 2
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "require(ggplot2)\n# p<-ggplot(data=mydata, aes(x=time_visit, y=UPDRS, group=FID_IID))\n# # dev.off()\n# p+geom_point()+geom_line()\n\nsexCast <- as.factor(ifelse(mydata$Sex>0, \"Male\", \"Female\"))\n# plot_ly(data=mydata, x=~time_visit, y=~UPDRS, group=~FID_IID)\nmydata$time_visit <- factor(mydata$time_visit, levels = c(\"0\", \"3\", \"6\", \"9\", \"12\", \"18\", \"24\", \"30\", \"36\", \"42\", \"48\", \"54\"))\nplot_ly(data=mydata, x=~time_visit, y = ~UPDRS, color = ~sexCast, type = \"box\") %>% \n  layout(title=\"Parkinson's Disease UPDRS across Time\", boxmode = \"group\")\n\nplot_ly(data=mydata, x=~time_visit, y = ~Imaging, color = ~sexCast, type = \"box\") %>% \n  layout(title=\"Parkinson's Disease Imaging Score across Time\", boxmode = \"group\")",
      "line_count": 13
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# ppmi.mean<-aggregate(UPDRS~time_visit+ResearchGroup, FUN = mean, data=mydata[, c(30, 31, 32)])\n# ppmi.min<-aggregate(UPDRS~time_visit+ResearchGroup, FUN = min, data=mydata[, c(30, 31, 32)])\n# ppmi.max<-aggregate(UPDRS~time_visit+ResearchGroup, FUN = max, data=mydata[, c(30, 31, 32)])\n# ppmi.boundary<-merge(ppmi.min, ppmi.max, by=c(\"time_visit\", \"ResearchGroup\"))\n# ppmi.all<-merge(ppmi.mean, ppmi.boundary, by=c(\"time_visit\", \"ResearchGroup\"))\n# pd <- position_dodge(0.1) \n# p1<-ggplot(data=ppmi.all, aes(x=time_visit, y=UPDRS, group=ResearchGroup, color=ResearchGroup))\n# p1+geom_errorbar(aes(ymin=UPDRS.x, ymax=UPDRS.y, width=0.1))+geom_point()+geom_line()\n\n# calculate all column means and SDs based on the time_visit column\nmeans <- aggregate( . ~time_visit, data=mydata, mean)\nerrorBars <- aggregate( . ~time_visit, data=mydata, sd)\n\n# plot_ly() %>%\n#     # add error bars for each CV-mean at log(lambda)\n#     add_trace(x = ~means$time_visit, y = ~means$UPDRS, type = 'scatter', mode = 'markers', name=~ResearchGroup,\n#         error_y = ~list(array = ~errorBars$UPDRS, color=\"green\", opacity=0.5)) %>% \n#     layout(title = \"ARIMA Forecasting using Simulated Data\",\n# \t\t\t\t\t\t\thovermode = \"x unified\",  # xaxis2 = ax, \n# \t\t\t\t\t\t\tyaxis = list(title = 'Y',\tside=\"left\", showgrid = TRUE))\n\nplot_ly(x = ~means$time_visit, y = ~means$UPDRS, type = 'scatter', mode = 'markers', name = 'UPDRS',\n        marker=list(size=20), error_y = ~list(array = errorBars$UPDRS, color = 'black')) %>%\n  layout(xaxis=list(title=\"time_visit\"), yaxis=list(title=\"UPDRS\"))\n\nplot_ly(x = ~means$time_visit, y = ~means$Imaging, type = 'scatter', mode = 'markers', name = 'Imaging',\n        marker=list(size=20), error_y = ~list(array = errorBars$Imaging, color = 'gray')) %>%\n  layout(xaxis=list(title=\"time_visit\"), yaxis=list(title=\"Imaging\"))",
      "line_count": 28
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "#install.packages(\"lme4\")\n#install.packages(\"arm\")\nlibrary(lme4)\nlibrary(arm)\n#GLM\nmodel.glm<-glm(UPDRS~Imaging+ResearchGroup*time_visit+Age+Sex+Weight, data=mydata)\nsummary(model.glm)\n#LMM\nmodel.lmm<-lmer(UPDRS~Imaging+ResearchGroup*time_visit+Age+Sex+Weight+(1|FID_IID), data=mydata)\nsummary(model.lmm)\n# display(model.lmm)\n\n#install.packages('sjPlot')\nlibrary('sjPlot')\nplot_model(model.lmm, vline = \"black\", sort.est = TRUE,\n                   transform = NULL, show.values = TRUE,\n                   value.offset = 0.5, dot.size = 2.5, value.size = 2.5)",
      "line_count": 17
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"lavaan\")\nlibrary(lavaan)\n#load data   05_PPMI_top_UPDRS_Integrated_LongFormat1.csv ( dim(myData) 1764   31 )\n# setwd(\"/dir/\")\ndataPD <-\n  read.csv(\"https://umich.instructure.com/files/330397/download?download_frd=1\",\n           header = TRUE)\n\n# dichotomize the \"ResearchGroup\" variable\ndataPD$ResearchGroup <-\n  ifelse(dataPD$ResearchGroup == \"Control\", 1, 0)\nhead(dataPD)",
      "line_count": 12
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# hist(dataPD$time_visit)\nlibrary(plotly)\nplot_ly(x = ~dataPD$time_visit, type = \"histogram\", nbinsx = 10) %>% \n  layout(title=\"PD - PPMI Data\", \n         bargap=0.1, xaxis=list(title=\"Number of Visits\"), yaxis=list(title=\"Frequency\"))\n\n# factorize the categorical features\ndataPD_new <- dataPD\ndataPD_new$ResearchGroup <- factor(dataPD_new$ResearchGroup)\ndataPD_new$Sex <- factor(dataPD_new$Sex)\ndataPD_new$chr12_rs34637584_GT <- factor(dataPD_new$chr12_rs34637584_GT)\ndataPD_new$chr17_rs11868035_GT <- factor(dataPD_new$chr17_rs11868035_GT)\n\n# by chr17_rs11868035_GT genotype and ResearchGroup\n# ggplot(dataPD_new, aes(x=time_visit, y=UPDRS_part_I, group=FID_IID)) +\n#   ggtitle(\"PD/PPMI Data chr17_rs11868035_GT genotype and ResearchGroup\") + \n#   geom_line(aes(color=ResearchGroup)) +\n#   geom_point(aes(color=ResearchGroup)) + facet_wrap(~chr17_rs11868035_GT)\n# \n# plot_ly(data=dataPD_new, x=~time_visit, y=~UPDRS_part_I, color=~FID_IID, name=~FID_IID, type=\"scatter\", mode=\"markers+lines\")\n# UPDRS by chr17_rs11868035_GT Genotype\ndataPD_new %>%\n    group_by(chr17_rs11868035_GT) %>%\n    do(p=plot_ly(., x=~time_visit, y=~UPDRS_part_I, color=~FID_IID, name=~paste0(\"pat=\",FID_IID), type=\"scatter\", mode=\"markers+lines\") %>% \n           layout(annotations = list(text = ~paste0(\"GT=\",chr17_rs11868035_GT),\n                                     xref = \"x\", yref = \"y\", x = 20, y = 10)) %>% \n         hide_colorbar()) %>% subplot(nrows = 2, shareX = TRUE, shareY = TRUE) %>%\n  layout(title=\"Patients UPDRS_part_I by chr17_rs11868035_GT\")\n\n# The previous graph can also be done using \"ggplot\"\n# by chr17_rs11868035_GT genotype and subjectID\n# ggplot(dataPD_new, aes(x=time_visit, y=UPDRS_part_I, group=FID_IID)) +\n#   ggtitle(\"PD/PPMI Data chr17_rs11868035_GT genotype and SubjectID\") + \n#   geom_line(aes(color=FID_IID)) +\n#   geom_point(aes(color=FID_IID)) + \n#   facet_wrap(~chr17_rs11868035_GT)\n# \n# # by ResearchGroup and chr17_rs11868035_GT genotype\n# ggplot(dataPD_new, aes(x=time_visit, y=UPDRS_part_I, group=FID_IID)) +\n#   ggtitle(\"PD/PPMI Data by Genotype (chr17_rs11868035_GT) & ResearchGroup\") + \n#   geom_line(aes(color=chr17_rs11868035_GT))+\n#   geom_point(aes(color=chr17_rs11868035_GT))+\n#   facet_wrap(~ResearchGroup)\n# \n# # by ResearchGroup + chr17_rs11868035_GT genotype subgroups\n# ggplot(dataPD_new, aes(x=time_visit, y=UPDRS_part_I, group=FID_IID)) +\n#   ggtitle(\"PD/PPMI Data chr17_rs11868035_GT genotype and SubjectID\") + \n#   geom_line(aes(color=FID_IID))+\n#   geom_point(aes(color=FID_IID))+\n#   facet_wrap(~ResearchGroup + chr17_rs11868035_GT)",
      "line_count": 50
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# time_visit is incongruent between cases, we make it ordinal category\n# convert the long-to-wide data format to unwind the time\nlibrary(reshape2)\n\n# check this R-blog post for some common data aggregation, melting, casting and other operations ...\n# https://www.r-statistics.com/2012/01/aggregation-and-restructuring-data-from-r-in-action/\n\nsubsetPD <-\n  dataPD[, c(\n    \"FID_IID\",\n    \"L_insular_cortex_ComputeArea\",\n    \"R_insular_cortex_ComputeArea\",\n    \"L_cingulate_gyrus_ComputeArea\",\n    \"R_cingulate_gyrus_ComputeArea\",\n    \"L_putamen_Volume\" ,\n    \"R_putamen_Volume\",\n    \"Sex\"  ,\n    \"Weight\" ,\n    \"ResearchGroup\" ,\n    \"Age\" ,\n    \"chr12_rs34637584_GT\" ,\n    \"chr17_rs11868035_GT\",\n    \"UPDRS_part_I\",\n    \"UPDRS_part_II\" ,\n    \"UPDRS_part_III\",\n    \"time_visit\"\n  )]\n\nsubsetPD$time_visit <-\n  c(\"T1\", \"T2\", \"T3\", \"T4\")   # convert all times to 1:4\n\n# First Wide to long for UPDRS variable.names\nsubsetPD_long <-\n  melt(\n    subsetPD,\n    id.vars = c(\n      \"FID_IID\",\n      \"L_insular_cortex_ComputeArea\",\n      \"R_insular_cortex_ComputeArea\",\n      \"L_cingulate_gyrus_ComputeArea\",\n      \"R_cingulate_gyrus_ComputeArea\",\n      \"L_putamen_Volume\" ,\n      \"R_putamen_Volume\",\n      \"Sex\"  ,\n      \"Weight\" ,\n      \"ResearchGroup\" ,\n      \"Age\" ,\n      \"chr12_rs34637584_GT\" ,\n      \"chr17_rs11868035_GT\",\n      \"time_visit\"\n    ),\n    variable.name = \"UPDRS\",\n    value.name = \"UPDRS_value\"\n  )\n\n# Need to concatenate 2 columns \"UPDRS\" & \"time_visit\"\nsubsetPD_long$UPDRS_Time <-\n  do.call(paste, c(subsetPD_long[c(\"UPDRS\", \"time_visit\")], sep = \"_\"))\n# remove the old \"UPDRS\" & \"time_visit\" columns\nsubsetPD_long1 <-\n  subsetPD_long[,!(names(subsetPD_long) %in% c(\"UPDRS\", \"time_visit\"))]\n# View(subsetPD_long1)\n\n# Convert Long to Wide format\nlibrary(reshape2)\n# From the source:\n# \"FID_IID\", \"L_insular_cortex_ComputeArea\",  \"R_insular_cortex_ComputeArea\", \"L_cingulate_gyrus_ComputeArea\", \"R_cingulate_gyrus_ComputeArea\", \"L_putamen_Volume\" ,  \"R_putamen_Volume\",  \"Sex\"  , \"Weight\" ,   \"ResearchGroup\" , \"Age\" , \"chr12_rs34637584_GT\" , \"chr17_rs11868035_GT\", are columns we want to keep the same\n# \"time_visit\" is the column that contains the names of the new columns to put things in\n# \"UPDRS_part_I\", \"UPDRS_part_II\" , \"UPDRS_part_III\" hold the measurements\nsubsetPD_wide <-\n  dcast(\n    subsetPD_long,\n    FID_IID + L_insular_cortex_ComputeArea + R_insular_cortex_ComputeArea +\n      L_cingulate_gyrus_ComputeArea + R_cingulate_gyrus_ComputeArea + L_putamen_Volume +\n      R_putamen_Volume + Sex + Weight + ResearchGroup + Age + chr12_rs34637584_GT +\n      chr17_rs11868035_GT ~ UPDRS_Time,\n    value.var = \"UPDRS_value\",\n    fun.aggregate = mean\n  )\n\n# Variables:\n# \"FID_IID\", \"L_insular_cortex_ComputeArea\" ,\"R_insular_cortex_ComputeArea\",  \"L_cingulate_gyrus_ComputeArea\",\"R_cingulate_gyrus_ComputeArea\",\"L_putamen_Volume\", \"R_putamen_Volume\", \"Sex\",\"Weight\", \"ResearchGroup\" ,\"Age\" , \"chr12_rs34637584_GT\", \"chr17_rs11868035_GT\",\"UPDRS_part_I_T1\",\"UPDRS_part_I_T2\"               \"UPDRS_part_I_T3\", \"UPDRS_part_I_T4\", \"UPDRS_part_II_T1\",\"UPDRS_part_II_T2\",\"UPDRS_part_II_T3\", \"UPDRS_part_II_T4\",\"UPDRS_part_III_T1\",\"UPDRS_part_III_T2\", \"UPDRS_part_III_T3\", \"UPDRS_part_III_T4\"\n\n# SEM modeling\nmodel1 <- '\n# latent variable definitions - defining how the latent variables are \"manifested by\" a set of observed\n# (or manifest) variables, aka \"indicators\"\n# (1) Measurement Model\nImaging =~ L_cingulate_gyrus_ComputeArea + R_cingulate_gyrus_ComputeArea + L_putamen_Volume + R_putamen_Volume\nDemoGeno =~ Weight+Sex+Age\nUPDRS =~ UPDRS_part_I_T1\n# UPDRS =~ UPDRS_part_II_T1 + UPDRS_part_II_T2 + UPDRS_part_II_T3 + UPDRS_part_II_T4\n\n# (2) Regressions\nResearchGroup ~ Imaging + DemoGeno + UPDRS\n'\n\n# confirmatory factor analysis (CFA)\n# The baseline is a null model constraining the observed variables to covary with no other variables.\n# That is, the covariances are fixed to 0 and only individual variances are estimated. This is represents\n# a \"reasonable worst-possible fitting model\", against which the new fitted model is compared\n# to calculate appropriate model-quality indices (e.g., CFA).\n\nsummary(subsetPD_wide)\n# selective scale features (e.g., avoid scaling Subject ID's)\ndataPD2 <-\n  as.data.frame(cbind(scale(subsetPD_wide[,!(\n    names(subsetPD_wide) %in% c(\n      \"FID_IID\",\n      \"Sex\",\n      \"ResearchGroup\",\n      \"chr12_rs34637584_GT\",\n      \"chr17_rs11868035_GT\"\n    )\n  )]), subsetPD_wide[, (\n    names(subsetPD_wide) %in% c(\n      \"FID_IID\",\n      \"Sex\",\n      \"ResearchGroup\",\n      \"chr12_rs34637584_GT\",\n      \"chr17_rs11868035_GT\"\n    )\n  )]))\n\nsummary(dataPD2)\n\n# fitSEM3 <- cfa(model3, data= dataPD2, missing='FIML') \t\t# deal with missing values (missing='FIML')\n# summary(fitSEM3, fit.measures=TRUE)\n\nfitSEM1 <- sem(model1, data = dataPD2, estimator = \"MLM\")\n# Check the SEM model covariances\nfitSEM1@SampleStats@cov\nsummary(fitSEM1)\n# report the standardized coefficients of the model\n# standardizedSolution(fitSEM1)\n# variation explained by model components (The R-square value for all endogenous variables)\ninspect(fitSEM1, \"r2\")  # lavInspect()\n\n# install.packages(\"semPlot\")\nlibrary(semPlot)\nsemPlot::semPaths(\n  fitSEM1,\n  intercept = FALSE,\n  whatLabel = \"est\",\n  residuals = FALSE,\n  exoCov = FALSE,\n  edge.label.cex = 1.0,\n  label.cex = 1.5,\n  sizeMan = 8\n)",
      "line_count": 150
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# scale some features\ndataPD_new$L_cingulate_gyrus_ComputeArea <- scale(dataPD_new$L_cingulate_gyrus_ComputeArea)\ndataPD_new$L_cingulate_gyrus_Volume <- scale(dataPD_new$L_cingulate_gyrus_Volume)\ndataPD_new$R_cingulate_gyrus_ComputeArea <- scale(dataPD_new$R_cingulate_gyrus_ComputeArea)\ndataPD_new$R_cingulate_gyrus_Volume<-scale(dataPD_new$R_cingulate_gyrus_Volume)\ndataPD_new$R_insular_cortex_ComputeArea<-scale(dataPD_new$R_insular_cortex_ComputeArea)\ndataPD_new$R_insular_cortex_Volume<-scale(dataPD_new$R_insular_cortex_Volume)\n\n# define the outcome UPDRS and imaging latent feature\ndataPD_new$UPDRS<-dataPD_new$UPDRS_part_I+1.890*dataPD_new$UPDRS_part_II+2.345*dataPD_new$UPDRS_part_III\ndataPD_new$Imaging<-dataPD_new$L_cingulate_gyrus_ComputeArea  +0.994*dataPD_new$L_cingulate_gyrus_Volume+0.961*dataPD_new$R_cingulate_gyrus_ComputeArea+0.955*dataPD_new$R_cingulate_gyrus_Volume+0.930*dataPD_new$R_insular_cortex_ComputeArea+0.920*dataPD_new$R_insular_cortex_Volume\n\nmodel.glmm<-glmer(ResearchGroup~UPDRS+Imaging+Age+Sex+Weight+(1|FID_IID), data=dataPD_new, family=\"binomial\")\narm::display(model.glmm)\ndataPD_new$predictedGLMM <- predict(model.glmm, newdata=dataPD_new[1:1764, !(names(dataPD_new) %in% c(\"ResearchGroup\", \"UPDRS_part_II_T4\"))], allow.new.levels=T, type=\"response\")  # lme4::predict.merMod()\n# factorize the predictions\ndataPD_new$predictedGLMM <- factor(ifelse(dataPD_new$predictedGLMM<0.5, \"0\", \"1\"))\n\n# compare overall the GLMM-predicted values against observed ResearchGroup labels values \n# install.packages(\"caret\")\n# library(\"caret\")\ncaret::confusionMatrix(dataPD_new$predictedGLMM, dataPD_new$ResearchGroup)\n\ntable(dataPD_new$predictedGLMM, dataPD_new$ResearchGroup)\n\n# compare predicted and observed by Sex\n# ggplot(dataPD_new, aes(predictedGLMM, fill=ResearchGroup)) + \n#    geom_bar() + facet_grid(. ~ Sex) \n# \n# dataPD_new%>%\n#     group_by(Sex) %>%\n#     do(p=plot_ly(., x=~ResearchGroup, y=~predictedGLMM, type = 'bar', name = ~Sex) %>% \n#            layout(annotations = list(text = ~Sex)) %>% hide_colorbar()) %>%\n#     subplot(nrows = 1, shareX = TRUE, shareY = TRUE)\n# \n# plot_ly(data = dataPD_new, x = ~ResearchGroup, y = ~momint, type=\"scatter\", mode=\"markers\",\n#         color = ~clusters, marker = list(size = 30), name=clusterNames) %>%\n#   hide_colorbar()\n# \n# \n# # compare predicted and observed by Genotype (chr12_rs34637584_GT)\n# ggplot(dataPD_new, aes(predictedGLMM, fill=ResearchGroup)) + \n#    geom_bar() + facet_grid(. ~ chr12_rs34637584_GT)",
      "line_count": 43
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"geepack\")\n# library(\"geepack\")\n#gee.fit <- geeglm(ResearchGroup~Imaging+Age+Sex+Weight+UPDRS_part_I, data=dataPD_new[complete.cases(dataPD_new), ], family = \"binomial\", id = FID_IID, corstr = \"exchangeable\", scale.fix = TRUE)\n\nlibrary(gee)\n# Full GEE model\ngee.fit <- gee(ResearchGroup~Imaging+Age+Sex+Weight+UPDRS_part_I, data=dataPD_new, family = \"binomial\", id = FID_IID, corstr = \"exchangeable\", scale.fix = TRUE)\n# reduced model (-UPDRS_part_I)\ngee.fit2 <- gee(ResearchGroup~Imaging+Age+Sex+Weight, data=dataPD_new, family = \"binomial\", id = FID_IID, corstr = \"exchangeable\", scale.fix = TRUE)\nsummary(gee.fit)\n\n# anova(gee.fit, gee.fit2) # compare two GEE models\n# To test whether a categorical predictor with more than two levels should be retained in a GEE model we need to test the entire set of dummy variables simultaneously as a single construct. \n# The geepack package provides a method for the anova function for a multivariate Wald test\n# When the anova function is applied to a single `geeglm` object it returns sequential Wald tests for individual predictors with the tests carried out in the order the predictors are listed in the model formula.\n\n# Individual Wald test and confidence intervals for each covariate\npredictors <- coef(summary(gee.fit))\ngee.fit.CI <- with(as.data.frame(predictors), cbind(lwr=Estimate-1.96*predictors[, 4], est=Estimate, upr=Estimate+1.96*predictors[, 4])) # predictors[, 4] == \"Robust S.E.\"\nrownames(gee.fit.CI) <- rownames(predictors)\ngee.fit.CI\n\n# exponentiate the interpret the results as \"odds\", instead of log-odds\nUPSRS_est <- coef(gee.fit)[\"UPDRS_part_I\"]\nUPDRS_se <- summary(gee.fit)$coefficients[\"UPDRS_part_I\", \"Robust S.E.\"]\n\nexp(UPSRS_est + c(-1, 1) * UPDRS_se * qnorm(0.975))",
      "line_count": 27
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "AIC(fitSEM1)\nAIC(model.glmm)\n# install.packages(\"MuMIn\")\nlibrary(\"MuMIn\")\n# to rank several models based on QIC:\n# model.sel(gee.fit, gee.fit2, rank = QIC)\nQIC(gee.fit) ",
      "line_count": 7
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"rnn\")\nlibrary(\"rnn\")\n\nset.seed(1234)\n\n# Set oscillatory wave frequency, period, and phase\nf <- 5\nw <- 2*pi*f\nphase <- -pi/8 # offset the real outcome signal\n# Note: period of sin(t*w+phase) is 1/10, thus, lag=50\n\n# Create input and output sequences, which are different in terms of \n# phase (offset of -pi/3), quadratic amplitude tau=tau(t), and noise N(0,0.3)\nn=1000\nseq_step= 1.0/n\nt <- seq(seq_step, 1, by=seq_step)\ntau <- (-4) * (t) * (t-1); length(tau) # plot(t, tau)\n# x <- sin(t*w+phase) + rnorm(1000, 0, 0.3)  # input\nx <- tau * (sin(t*w+phase) + rnorm(1000, 0, 0.3))  # input\ny <- sin(t*w)                             # output\n\n# Samples of repeated oscillations 20 time series (rows), each with 50 time-points (columns)\n# Putting data in the appropriate RNN format\n# Predictors (X) and Output (Y) are arrays, \n# dim 1: samples (must be equal to dim 1 of X), \n# dim 2: time (must be equal to dim 2 of X), \n# dim 3: variables (could be 1 or more, if a matrix, will be coerced to array)\n\n# For example, suppose we have a total 1,000 observations that are stacked in samples of 200 time-points (per lag), 5 time steps (5 lags), and 1 feature (univariate predictor-vector). Then, the RNN/LSTM input format will be a 3-tensor of dimensions (100, 10, 1).\n\nX <- matrix(x, nrow = 200, ncol = 5)\nY <- matrix(y, nrow = 200, ncol = 5)\ndim(X); dim(Y)\n\n# Plot noisy waves\n# plot(as.vector(X), col='blue', type='l', ylab = \"X,Y\", main = \"Noisy input (X), periodic output (Y)\", lwd=2)\n# lines(as.vector(Y), col = \"red\", lty=2, lwd=2)\n# legend(\"topright\", c(\"X\", \"Y\"), col = c(\"blue\",\"red\"), lty = c(1, 2), lwd = c(2,2))\nlibrary(plotly)\nplot_ly(x=~c(1:length(as.vector(X))), y=~as.vector(X), type=\"scatter\", mode=\"lines\", name=\"Noisy Signal\") %>%\n  add_trace(x=~c(1:length(as.vector(X))), y=~as.vector(Y), name=\"Periodic Model (y=sin(t*w))\") %>%\n  layout(title=\"Noisy input and periodic model\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"value\"), legend = list(orientation='h'))\n\n# The RNN/LSTM algorithm requires the input data to be normalized, centered and scaled\n# mean_x <- mean(X); mean_y <- mean(Y); sd_x <- sd(X); sd_y <- sd(Y) \n# X = (X-mean_x)/sd_x; Y = (Y-mean_y)/sd_y \n\n# RNN requires standardization in the interval 0 - 1\nmin_x <- min(X); min_y <- min(Y); max_x <- max(X); max_y <- max(Y)\n\n# Generic transformation:\n# forward (scale), for raw data, and reverse (unscale), for predicted data\nmy.scale <- function (x, forward=TRUE, input=TRUE) {\n  if (input && forward) {           # X=Input==predictors & Forward scaling\n    x <- (x - min_x) / (max_x - min_x)\n  } else if (input && !forward) {   # X=Input==predictors & Reverse scaling\n    x <- x * (max_x - min_x) + min_x\n  } else if (!input && forward) {   # X=Output==response & Forward scaling\n    x <- (x - min_y) / (max_y - min_y)\n  } else if (!input && !forward) {  # X=Output==response & Reverse scaling\n    x <- x * (max_y - min_y) + min_y\n  }\n  return (x)\n}\n\n# Save these transform parameters; center/scale, so we can invert the transforms\n# after we get the predicted values.\nX <- my.scale(X, forward=TRUE, input=TRUE)\nY <- my.scale(Y, forward=TRUE, input=FALSE)\n# Check (forward o reversed)(X) ==X\n# plot(X, my.scale(X, forward=FALSE, input=TRUE))\n# plot(Y, my.scale(Y, forward=FALSE, input=FALSE)) \n\n# random 80-20% training-testing split; 20-5 row (series) split, using all 40 features\nset.seed(1234)\ntrain_index <- sample(seq_len(nrow(X)), size = 0.8*nrow(X))\n\n# Train the RNN model using only the training data\nset.seed(1234)\nmodel_rnn <- rnn::trainr(Y = Y[train_index,],\n                X = X[train_index,],\n                learningrate = 0.06,\n                hidden_dim = 200,\n                learningrate_decay =0.99,\n                numepochs = 100,\n                network_type = \"rnn\")\n\n\n# Predicted RNN values\npred_Y <- predictr(model_rnn, X)\n#hist(pred_Y)\nplot_ly(x=~pred_Y[,1], type=\"histogram\", name=\"Lag 1\") %>%\n  add_histogram(x=~pred_Y[,2], name=\"Lag 2\") %>%\n  add_histogram(x=~pred_Y[,3], name=\"Lag 3\") %>%\n  add_histogram(x=~pred_Y[,4], name=\"Lag 4\") %>%\n  add_histogram(x=~pred_Y[,5], name=\"Lag 5\") %>%\n  layout(title=\"Y Histograms\",\n         xaxis=list(title=\"value\"), yaxis=list(title=\"frequency\"), legend = list(orientation='h'))\n\n# pred_Y_unscale <- my.scale(pred_Y, forward=FALSE, input=FALSE); hist(pred_Y_unscale)\n \n# Plot predicted vs actual time-series using the complete data (training AND testing data)\n# plot(as.vector(Y), col = 'red', type = 'l', main = \"All Data: Actual vs predicted\", ylab = \"Y, pred_Y\", lwd=1)\n# lines(as.vector(pred_Y), type = 'l', lty=2, col = 'blue', lwd=2)\n# legend(\"topright\", c(\"Predicted\", \"Real\"), col = c(\"blue\",\"red\"), lty = c(1,1), lwd = c(2,2))\n\nplot_ly(x=~c(1:length(as.vector(Y))), y=~as.vector(X), type=\"scatter\", mode=\"lines\", name=\"Noisy Signal\") %>%\n  add_trace(x=~c(1:length(as.vector(pred_Y))), y=~as.vector(pred_Y), name=\"Model Prediction\") %>%\n  layout(title=\"(Training range) raw data vs. RNN model prediction\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"value\"), legend = list(orientation='h'))\n\n# Plot predicted vs actual timeseries using only the testing data\n# plot(as.vector(Y[-train_index, ]), col = 'red', type='l', main = \"Testing Data: Actual vs predicted\", ylab = \"Y, pred_Y\", lwd=2)\n# lines(as.vector(pred_Y[-train_index, ]), type = 'l', lty=1, col = 'blue', lwd=2)\n# legend(\"topright\", c(\"Predicted\", \"Real\"), col = c(\"blue\",\"red\"), lty = c(1,1), lwd = c(2,1))\n\nplot_ly(x=~c(1:length(as.vector(Y[-train_index, ]))), y=~as.vector(X[-train_index, ]), \n        type=\"scatter\", mode=\"lines\", name=\"Noisy Signal\") %>%\n  add_trace(x=~c(1:length(as.vector(pred_Y[-train_index, ]))), y=~as.vector(pred_Y[-train_index, ]), \n            name=\"Model Prediction\") %>%\n  layout(title=\"(Testing range) raw data vs. RNN model prediction\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"value\"), legend = list(orientation='h'))",
      "line_count": 123
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# Download the climate data\t\nclim_data_url <- \"https://umich.instructure.com/files/8014703/download?download_frd=1\"\t\nclim_data_zip_file <- tempfile(); download.file(clim_data_url, clim_data_zip_file, mode=\"wb\")\t\nclimate_data <- read.csv(unzip(clim_data_zip_file))\ndim(climate_data); \thead(climate_data)\nunlink(clim_data_zip_file)\t",
      "line_count": 6
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# Date only: climate_time <- as.Date(climate_data$Date_Time, tryFormats = \"%d/%m/%Y %H:%M\")\nclimate_time <- as.POSIXct(climate_data$Date_Time, format = \"%d/%m/%Y %H:%M\")\nhead(climate_time)\n\n# install.packages(\"zoo\")\nlibrary(\"zoo\")\n\n# define each time series separately\nanyDuplicated(climate_time)   # there are some 9517 duplicated date-time points\n\n# extract unique elements\n# (climate_time1 <- climate_time[!duplicated(climate_time)])\nclimate_data_ts_temp <- zoo(climate_data$T_degC[!duplicated(climate_time)],\n                            climate_time[!duplicated(climate_time)])\nclimate_data_ts_pressure <- zoo(climate_data$p_mbar[!duplicated(climate_time)],\n                                climate_time[!duplicated(climate_time)])\nclimate_data_ts_humid <- zoo(climate_data$sh_g_kg[!duplicated(climate_time)],\n                             climate_time[!duplicated(climate_time)])\n\n# define aggregate TS object including all individual time-series for each feature\nclimate_data_ts_aggregate = cbind(climate_data_ts_temp, \n                                  climate_data_ts_pressure, climate_data_ts_humid)\n\n#plot(climate_data_ts_aggregate, main=\"Climate TS Data (Temperature, Pressure, Humidity)\", \n#     col=c(\"red\", \"green\", \"blue\"), lty=1, lwd=2, plot.type=\"single\") \n# plot(climate_data_ts_aggregate, main=\"Climate TS Data (Temperature, Pressure, Humidity)\", \n#      col=c(\"red\", \"green\", \"blue\"), lty=1, lwd=2) \n# legend(\"center\", legend=c(\"Temperature\", \"Pressure\", \"Humidity\"), \n#       col=c(\"red\", \"green\", \"blue\"), lty=1, lwd=2, cex=0.6, x.intersp=0.5)\n\nx_len <- dim(climate_data_ts_aggregate)[1]\npl1 <- plot_ly(x=~climate_time[!duplicated(climate_time)], y=~climate_data_ts_aggregate[, 1], \n        type=\"scatter\", mode=\"lines\", name=colnames(climate_data_ts_aggregate)[1]) %>% \n  layout(hovermode = \"x unified\")\npl2 <- plot_ly(x=~climate_time[!duplicated(climate_time)], y=~climate_data_ts_aggregate[, 2], \n        type=\"scatter\", mode=\"lines\", name=colnames(climate_data_ts_aggregate)[2])  %>% \n  layout(hovermode = \"x unified\")\npl3 <- plot_ly(x=~climate_time[!duplicated(climate_time)], y=~climate_data_ts_aggregate[, 3], \n        type=\"scatter\", mode=\"lines\", name=colnames(climate_data_ts_aggregate)[3])  %>% \n  layout(hovermode = \"x unified\")\nsubplot(pl1, pl2, pl3, nrows=3,  shareX = TRUE, titleX = TRUE) %>% \n  layout(title=\"(Climate Data) Temperature, Pressure & Humidity\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"value\"), legend = list(orientation='h'))",
      "line_count": 43
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "#colnames(climate_data)\n# [1] \"Date_Time\"     \"p_mbar\"        \"T_degC\"        \"Tpot_K\"        \"Tdew_degC\"  \"rh_percent\" \n# [7] \"VPmax_mbar\"    \"VPact_mbar\"    \"VPdef_mbar\"    \"sh_g_kg\"       \"H2OC_mmol_mol\" \"rho_g_m3\"\n# [13] \"wv_m_s\"        \"max_wv_m_s\"    \"wd_deg\"       \n\n# Create random data frame and date column, then bind into a DF\n\n# Recall: Bivariate Input=(X,Y); Univariate Outcome (Z)\nX <- climate_data_ts_pressure\nY <- climate_data_ts_humid\nZ <- climate_data_ts_temp\n\n# X <- smooth.spline(X, spar=0.3)$y   # rh_percent (Humidity)\n# Y <- smooth.spline(Y, spar=0.3)$y   # p_mbar (pressure)\n# Z <- smooth.spline(Z, spar=0.3)$y   # T_degC (temperature)\n\n# The RNN/LSTM algorithm requires the input data to be normalized, centered and scaled\n# mean_x <- mean(X); mean_y <- mean(Y); sd_x <- sd(X); sd_y <- sd(Y) \n# X = (X-mean_x)/sd_x; Y = (Y-mean_y)/sd_y \n\n# RNN requires standardization in the interval 0 - 1\nmin_x <- min(X); min_y <- min(Y); min_z <- min(Z)\nmax_x <- max(X); max_y <- max(Y); max_z <- max(Z)\n\n# Generic transformation:\n# forward (scale), for raw data, and reverse (unscale), for predicted data\nmy.scale <- function (x, forward=TRUE, input=\"X\") {\n  if (input==\"X\" && forward) {           # X=Input==predictors & Forward scaling\n    x <- (x - min_x) / (max_x - min_x)\n  } else if (input==\"X\" && !forward) {   # X=Input==predictors & Reverse scaling\n    x <- x * (max_x - min_x) + min_x\n  } else if (input==\"Y\" && forward) {    # Y=Input==predictors & Forward scaling\n    x <- (x - min_y) / (max_y - min_y)\n  } else if (input==\"Y\" && !forward) {   # Y=Input==predictors & Reverse scaling\n    x <- x * (max_y - min_y) + min_y\n  } else if (input==\"Z\" && forward) {    # Z=Output==predictors & Forward scaling\n    x <- (x - min_z) / (max_z - min_z)\n  } else if (input==\"Z\" && !forward) {   # Z=Output==predictors & Reverse scaling\n    x <- x * (max_z - min_z) + min_z\n  } \n  return (x)\n}\n\n# Save these transform parameters; center/scale, so we can invert the transforms\n# after we get the predicted values.\nX <- my.scale(X, forward=TRUE, input=\"X\")\nY <- my.scale(Y, forward=TRUE, input=\"Y\")\nZ <- my.scale(Z, forward=TRUE, input=\"Z\")\n# Check (forward o reversed)(Y) == Y\n# plot(Y, my.scale(Y, forward=FALSE, input=\"Y\"))\n\n# length(Z) = 472731 ~ 9(yrs) * 6(min/hr) *24 (hrs/day) *365(day/yr)\n\n# For example, suppose we have a total of 472731 observations that are stacked in annual samples of 52560 time-points (per lag=year), 9 time steps (9 lags), and 2 features (bivariate predictor-vector, (X,Y)). Then, the RNN/LSTM input format will be a 3-tensor of dimensions (52560, 9, 2).\n\nX <- matrix(X, nrow = 52560, ncol = 9)\nY <- matrix(Y, nrow = 52560, ncol = 9)\nZ <- matrix(Z, nrow = 52560, ncol = 9)\nX_tensor <- array(0, dim=c(52560,9,2))\nX_tensor[,,1] <- X\nX_tensor[,,2] <- Y\ndim(X_tensor); dim(Z)\n\n# Plot the time courses of the Input (predictor) tensor and the Output (Z) as time-series\nX_ts <- zoo(as.vector(X_tensor[ , , 1]), climate_time[!duplicated(climate_time)])\nY_ts <- zoo(as.vector(X_tensor[ , , 2]), climate_time[!duplicated(climate_time)])\nZ_ts <- zoo(as.vector(Z[ , ]), climate_time[!duplicated(climate_time)])\n\n# plot(X_ts, col='green', type='l', ylab = \"X-tensor, Z\", \n#         main = \"Renormalized Input (X-tensor), Output (Z)\", lwd=2)    # ,log = \"y\")\n# lines(Y_ts, col = \"blue\", lty=1, lwd=2)\n# lines(Z_ts, col = \"red\", lty=1, lwd=2)\n# legend(\"topright\", c(\"X-tensor(Pressure)\", \"X-tensor(Humid)\", \"Z(Temp)\"), col = c(\"green\", \"blue\",\"red\"), \n#        lty = c(1, 1, 1), lwd = c(2,2,2), cex=0.6, x.intersp=0.5)\n\nx_len <- dim(X_ts)[1]\n# Vertically stacked subplot\n# pl1 <- plot_ly(x=~climate_time[!duplicated(climate_time)], y=~X_ts,\n#         type=\"scatter\", mode=\"lines\", name=\"X-tensor(Pressure)\") %>% \n#   layout(hovermode = \"x unified\")\n# pl2 <- plot_ly(x=~climate_time[!duplicated(climate_time)], y=~Y_ts, \n#         type=\"scatter\", mode=\"lines\", name=\"X-tensor(Humidity)\")  %>% \n#   layout(hovermode = \"x unified\")\n# pl3 <- plot_ly(x=~climate_time[!duplicated(climate_time)], y=~Z_ts, \n#         type=\"scatter\", mode=\"lines\", name=\"Z(Temp)\")  %>% \n#   layout(hovermode = \"x unified\")\n# subplot(pl1, pl2, pl3, nrows=3,  shareX = TRUE, titleX = TRUE) %>% \n#   layout(title=\"(Climate Data) Renormalized Input (X-tensor), Output (Z)\",\n#          xaxis=list(title=\"time\"), yaxis=list(title=\"X-tensor, Z\"), legend = list(orientation='h'))\n\nplot_ly(x=~climate_time[!duplicated(climate_time)], y=~X_ts,\n        type=\"scatter\", mode=\"lines\", name=\"X-tensor(Pressure)\") %>% \n  add_trace(x=~climate_time[!duplicated(climate_time)], y=~Y_ts, \n        type=\"scatter\", mode=\"lines\", name=\"X-tensor(Humidity)\")  %>% \n  add_trace(x=~climate_time[!duplicated(climate_time)], y=~Z_ts, \n        type=\"scatter\", mode=\"lines\", name=\"Z(Temp)\")  %>% \n  layout(title=\"(Climate Data) Renormalized Input (X-tensor), Output (Z)\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"X-tensor, Z\"), legend = list(orientation='h'))\n\n# training-testing split; 52560(annual) and 9 (years) of data.\n# Train on first 8 years of data and predict the final, 9th year of data\n# train_index <- sample(seq_len(nrow(X)), size = 0.8*nrow(X))\n\n# Train the RNN model using only the training data\n# Running the full model is extremely computationally expensive:\n#     model_rnn <- rnn::trainr(Y=Z[train_index,], X=X_tensor[train_index, , ], ....\n# We run a reduced model as a demo, only learning on 1:10000, 1 (yr1) time points\nset.seed(1234)\nmodel_rnn <- rnn::trainr(Y=Z[1:10000, ], X=X_tensor[1:10000, , ],\n                learningrate = 0.06,\n                hidden_dim = 32,\n                learningrate_decay =0.99,\n                numepochs = 3,\n                network_type = \"rnn\")\n\n# Predicted RNN values\npred_Z <- predictr(model_rnn, X_tensor[10001:20000, , ])\n# hist(pred_Z)\npl <- plot_ly()\nfor (i in 1:dim(pred_Z)[2]) {\n  pl <- pl %>% add_trace(x=~pred_Z[ , i], type=\"histogram\", name=paste0(\"Lag \", i))\n}\npl <- pl %>%\n  layout(title=\"Z Histograms\",\n         xaxis=list(title=\"value\"), yaxis=list(title=\"frequency\"), legend = list(orientation='h'))\npl\n\n# pred_Y_unscale <- my.scale(pred_Y, forward=FALSE, input=FALSE); hist(pred_Y_unscale)\n \n# Plot (transformed/normalized) predicted vs actual time series using the small-sample data\n# plot(as.vector(Z[10001:20000, ]), col = 'red', type = 'l', main = \"Normalized Small Data: Actual vs predicted\", ylab = \"Z, pred_Z\", lwd=1)\n# lines(as.vector(pred_Z), type = 'l', lty=1, col = 'blue', lwd=2)\n# legend(\"topright\", c(\"Predicted\", \"Real\"), col = c(\"blue\",\"red\"), lty = c(1,1), lwd = c(2,2), cex=0.6, x.intersp=0.5)\n\nx_sample <- climate_time[!duplicated(climate_time)]\nx1_sample <- x_sample[10001:20000]\n   \nplot_ly(x=~x1_sample, y=~as.vector(Z[10001:20000, 1]),\n        type=\"scatter\", mode=\"lines\", name=\"Raw Data\") %>% \n  add_trace(x=~x1_sample, y=~as.vector(pred_Z[ , 1]), \n        type=\"scatter\", mode=\"lines\", name=\"Model Predicted\")  %>% \n  layout(title=\"Normalized Small Data: Actual vs Predicted\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"X-tensor, Z\"), legend = list(orientation='h'))\n\n# # Plot (Native C temperature space) predicted vs actual time series\n# plot(my.scale(as.vector(Z[10001:20000, ]), forward=FALSE, input=\"Z\"), \n#      col = 'red', type = 'l', main = \"Small Data: Actual vs predicted (orig-temp-domain)\", \n#      ylab = \"Celsius Temperature (Observed_Z vs. Predicted_Z\", lwd=1)\n# lines(my.scale(as.vector(pred_Z), forward=FALSE, input=\"Z\"),\n#       type = 'l', lty=1, col = 'blue', lwd=2)\n# legend(\"topright\", c(\"Predicted\", \"Real\"), col = c(\"blue\",\"red\"), lty = c(1,1), lwd = c(2,2), cex=0.6, x.intersp=0.5)\n# \n# plot_ly(x=~x1_sample, y=~as.vector(Z[10001:20000, 1]),\n#         type=\"scatter\", mode=\"lines\", name=\"Raw Data\") %>% \n#   add_trace(x=~x1_sample, y=~as.vector(pred_Z[ , 1]), \n#         type=\"scatter\", mode=\"lines\", name=\"Model Predicted\")  %>% \n#   layout(title=\"Normalized Small Data: Actual vs Predicted\",\n#          xaxis=list(title=\"time\"), yaxis=list(title=\"X-tensor, Z\"), legend = list(orientation='h'))\n# \n# # Add time on X-axis\n# obs_Temp_Z_ts <- zoo(my.scale(as.vector(Z[10001:20000, ]), forward=FALSE, input=\"Z\"),\n#                             climate_time[10001:20000])\n# pred_Temp_Z_ts <- zoo(my.scale(as.vector(pred_Z[1:10000]), \n#                      forward=FALSE, input=\"Z\"), climate_time[10001:20000])\n# plot(obs_Temp_Z_ts, col = 'red', type = 'l',\n#      main = \"Small Data: Actual vs predicted\", \n#      ylab = \"Celsius Temperature (Observed_Z vs. Predicted_Z\", lwd=1)\n# lines(pred_Temp_Z_ts, type = 'l', lty=1, col = 'blue', lwd=2)\n# legend(\"topleft\", c(\"Predicted\", \"Real\"), col = c(\"blue\",\"red\"), lty = c(1,1), lwd = c(2,2), cex=1.0, x.intersp=0.4)",
      "line_count": 169
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "acf(climate_data_ts_temp, lag.max = 52560)\n# myACF <- acf(climate_data_ts_temp, lag.max = 52560)\n# myACF$acf\n# plot_ly(x=~c(1:length(myACF$acf[, 1, 1])), y=~myACF$acf[, 1, 1],\n#         type=\"scatter\", mode=\"lines\", name=\"Raw Data\", fill = 'tozeroy') %>% \n#   add_trace(x=~x1_sample, y=~as.vector(pred_Z[ , 1]), \n#         type=\"scatter\", mode=\"lines\", name=\"Model Predicted\")  %>% \n#   layout(title=\"Auto-correlation function plot\",\n#          xaxis=list(title=\"lag\"), yaxis=list(title=\"ACF\"), legend = list(orientation='h'))",
      "line_count": 9
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "library(ggplot2)\ndf_Temp_Time <- data.frame(Z=climate_data$T_degC[!duplicated(climate_time)], \n                           t=as.Date(climate_time[!duplicated(climate_time)]))\n# plot1 <- ggplot(df_Temp_Time, aes(x=t, y=Z)) +\n#     geom_line(color = 'darkblue', alpha = 0.5) +   # plot Temp in C\n#     # geom_smooth(method = \"loess\", span = 0.2, se = TRUE) +  # plot smoothed-Temp\n#     labs(title = \"2009-2017 (Full Data Set)\")\n# plot1\n\nplot_ly(data=df_Temp_Time, x=~t, y=~Z,\n        type=\"scatter\", mode=\"lines\", name=\"Raw Data\") %>%\n  layout(title=\"2009-2017 Temperature (Full Data Set)\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"T_degC\"), legend = list(orientation='h'))",
      "line_count": 13
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"keras\")\n# install.packages(\"devtools\")\n# devtools::install_github(\"rstudio/keras\")\nlibrary(keras)",
      "line_count": 4
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# install.packages(\"tidyverse\")\n# library(\"tidyverse\")\n\ndf_train <- as.numeric(df_Temp_Time[1:10000, 1])\ndf_test <- as.numeric(df_Temp_Time[10001:20000, 1])",
      "line_count": 5
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "df_train.std <- scale(df_train)\ndf_test.std <- scale(df_test)",
      "line_count": 2
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "mean.train <- mean(df_train); sd.train <- sd(df_train) \nmean.test <- mean(df_test); sd.test <- sd(df_test) \n\nc(\"TRAIN: center\" = mean.train, \"TRAIN: SD\" = sd.train)\nc(\"TEST: center\" = mean.test, \"TEST: SD\" = sd.test)",
      "line_count": 5
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# Install TensorFlow in R\n# https://tensorflow.rstudio.com/installation/\n# install.packages(\"tensorflow\")\n\n# May also need to install TensorFlow in Anaconda\n# See: https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\n# Run these in Anaconda terminal shell\n# conda create -n tf tensorflow\n# conda activate tf\n\n# for stateful LSTM rnn, tsteps can be set to 1\ntsteps <- 1\nbatch_size <- 25\nepochs <- 25\n# number of elements ahead that are used to make the prediction\nlahead <- 1\n\n# Prep the data\ndf_Temp_Humid_Time <- data.frame(C=climate_data$T_degC[!duplicated(climate_time)], \n                           H=climate_data$rh_percent[!duplicated(climate_time)],\n                           T=as.Date(climate_time[!duplicated(climate_time)]))\ndf_Temp_train <- as.numeric(df_Temp_Humid_Time[1:10000, 1])\ndf_Temp_test <- as.numeric(df_Temp_Humid_Time[10001:20000, 1])\ndf_Humid_train <- as.numeric(df_Temp_Humid_Time[1:10000, 2])\ndf_Humid_test <- as.numeric(df_Temp_Humid_Time[10001:20000, 2])\n\n# The LSTM model assumptions include standardized (centered and scaled) input. \ndf_Temp_train.std <- scale(df_Temp_train); df_Temp_test.std <- scale(df_Temp_test)\ndf_Humid_train.std <- scale(df_Humid_train); df_Humid_test.std <- scale(df_Humid_test)\n\nx <- as.POSIXct(climate_data$Date_Time, format = \"%d/%m/%Y %H:%M\")\nhead(x[1:6])\n\n# Reformat the data as a 3D tensor, see above\ndf_Temp_train.std.tensor <- array(data = df_Temp_train.std, dim = c(dim(df_Temp_train.std)[1], 1))\ndf_Humid_train.std.tensor <- array(data=df_Humid_train.std, dim = c(dim(df_Humid_train.std)[1], 1,1))\n\ny <- df_Temp_train.std.tensor; x <- df_Humid_train.std.tensor\ndim(x); dim(y); summary(x); summary(y)\n\n# model formulation\nmodel.2 <- keras_model_sequential()\nmodel.2 %>%\n  layer_lstm(units = 50, input_shape = c(tsteps, 1), batch_size = batch_size,\n             return_sequences = TRUE, stateful = TRUE) %>% \n  layer_lstm(units = 50, return_sequences = FALSE, stateful = TRUE) %>% \n  layer_dense(units = 1)\nmodel.2 %>% compile(loss = 'mse', optimizer = 'rmsprop')\n\n### Iterative model fitting on training data\nfor (i in 1:epochs) {\n  model.2 %>% fit(x, y, batch_size = batch_size,\n                epochs = 1, verbose = 1, shuffle = FALSE)\n            \n  model.2 %>% reset_states()\n}\n\n# Predict the (standardized) Temp (C) using (standardized) Humidity (Time=1:10K)\npredicted_output <- model.2 %>% predict(x, batch_size = batch_size)\n# plot(df_Temp_Time$t[1:10000], y, xlab = 'time', \n#      main=\"Time: [1:10K] Training model (Temp~Humidity): rNN vs. Observed\",  \n#      lty=1, ylab='(std) T_degC ~ rh_percent', type=\"l\", col=\"green\")\n# lines(df_Temp_Time$t[1:10000], predicted_output,  col=\"blue\", lwd=2)\n# legend(\"bottom\", legend=c(\"Observed Temp\", \"LSTM rNN Predicted Temp\"),\n#        col=c(\"green\", \"blue\"), lty=c(1,1), cex=0.8)\n# legend(\"top\", legend=sprintf(\"Temperature Corr(Obs,Pred)=%s\", \n#                              round(cor(y[,1], predicted_output[,1]), 2)), cex=0.8)\n\nx1 <- as.POSIXct(climate_data$Date_Time, format = \"%d/%m/%Y %H:%M\")\nplot_ly(x=~x1[1:10000], y=~y[ , 1], type=\"scatter\", mode=\"lines\", name=\"Observed Temp\") %>%\n  add_trace(x=~x1[1:10000], y=~predicted_output[ , 1], name=\"LSTM RNN Predicted Temp\") %>%\n  layout(title=\"2009-2017 Temperature (Full Data Set)\",\n         xaxis=list(title=\"time\"), yaxis=list(title=\"T_degC\"), legend = list(orientation='h'))\n\n### Prospective Forecasting/Prediction of Temp using Humidity (time: 10001 - 20000)\nx_test <- array(data=df_Humid_test.std, dim = c(dim(df_Humid_test.std)[1], 1,1))\ny_test <- array(data=df_Temp_test.std, dim = c(dim(df_Temp_test.std)[1], 1))\n\npredicted_output_test <- model.2 %>% predict(x_test, batch_size = batch_size)\n# plot(df_Temp_Time$t[10001:20000], y_test, xlab = 'time', \n#      main=\"Time: [10K:20K]) Forecasting (Temp~Humidity): rNN vs. Observed\",  \n#      lty=1, ylab='(std) T_degC ~ rh_percent', type=\"l\", col=\"green\")\n# lines(df_Temp_Time$t[10001:20000], predicted_output_test,  col=\"blue\", lwd=2)\n# legend(\"bottom\", legend=c(\"Observed Temp\", \"LSTM rNN Predicted Temp\"),\n#        col=c(\"green\", \"blue\"), lty=c(1,1), cex=0.8)\n# legend(\"top\", legend=sprintf(\"Temperature Corr(Obs,Pred)=%s\", \n#                              round(cor(y_test[,1], predicted_output_test[,1]), 2)), cex=0.8)\n\nplot_ly(x=~x1[10001:20000], y=~y_test[ , 1], type=\"scatter\", mode=\"lines\", name=\"Observed Temp\") %>%\n  add_trace(x=~x1[10001:20000], y=~predicted_output_test[ , 1], name=\"LSTM RNN Predicted Temp\") %>%\n  layout(title=paste0(\"Temperature Corr(Obs,Pred)=\", round(cor(y_test[,1], predicted_output_test[,1]), 2)),\n         xaxis=list(title=\"time\"), yaxis=list(title=\"T_degC\"), legend = list(orientation='h'))",
      "line_count": 92
    },
    {
      "section": "Big Longitudinal Data Analysis",
      "code": "# For using GPU TensorFlow, follow these instructions:\n# GPU install instructions for tf and you require the exact CUDA and cuDNN versions:\n# https://www.tensorflow.org/install/gpu\n# Check if you installed CUDA 9.0 and cuDNN > 7.2 (follow the install instructions here)\n# Reinstall keras with install_keras(tensorflow = \"gpu\")\n# You can list the devices with:\n#     library(keras)\n#     k = backend()\n#     sess = k$get_session()\n#     sess$list_devices()\n\nlibrary(keras)\n# 1. define model parameters\nbatch_size <- 50\nepochs <- 4   # increase the epochs to improve the results\ndata_augmentation <- TRUE\n\n# 2. prepare the data, also available on canvas\n# https://umich.instructure.com/courses/38100/files/folder/Case_Studies/29_CIFAR_10_Labeled_Images\n# run ?dataset_cifar10 for more info on the data that is provided with keras distribution\ncifar10 <- dataset_cifar10()\n\n# 3. scale RGB values in test and train inputs to [0; 1] range\nx_train <- cifar10$train$x/255\nx_test <- cifar10$test$x/255\ny_train <- to_categorical(cifar10$train$y, num_classes = 10)\ny_test <- to_categorical(cifar10$test$y, num_classes = 10)\nclass_labels <- c(\"airplanes\", \"cars\", \"birds\", \"cats\", \"deer\", \"dogs\", \n      \"frogs\", \"horses\", \"ships\", \"trucks\")\n\ny_class_label <- rep(\"\", dim(y_train)[1]); str(y_class_label)\n\nfor (i in 1:dim(y_train)[1]) {   # dim(y_train) is 50000(images) * 10 (class-label-indicators)\n  for (j in 1:dim(y_train)[2]) {\n    if (y_train[i,j] == 1) \n      y_class_label[i] <- class_labels[j]\n  }\n}\n# y_class_label[1001]\n\n# 4. Visualize some of the images\nlibrary(\"imager\")\ndim(x_train)\n# [1] 50000    32    32     3\n# first convert the CSV data (one row per image, 42,000 rows)\nN <- 4\narray_3D <- array(x_train[1001:(1000+N), , , 1], c(4, 32, 32, 3)) # array_3D[index, x, y, RGB]\nmat_2D <- t(matrix(array_3D[1, , , 1], nrow = 32, ncol = 32))\nplot(as.cimg(mat_2D))\n\npretitle <- function(index) {\n  sprintf(\"Image: %d, true label: %s\", index, y_class_label[index])\n}\nop <- par(mfrow = c(2,2), oma = c(5,4,0,0) + 0.1, mar = c(0,0,1,1) + 0.1)\nimg_3D <- as.cimg(array_3D[N , , , 1], 32, 32, 1)\nfor (k in 1:N) {\n  img_3D <- as.cimg(t(matrix(array_3D[k, , , 1], nrow = 32, ncol = 32)))\n  plot(img_3D, k, xlim = c(0,32), ylim = c(32,0), axes=F, ann=T, main=pretitle(1000+k))\n}\n\npretitle0 <- function(index) { sprintf(\"Image: %d,\\n true label:\\n %s\", index, y_class_label[index])  }\nplt_list <- list()\nN=2 \nfor (i in 1:N) {\n  for (j in 1:N) {\n    plt_list[[i+(j-1)*N]] <- \n      plot_ly(z=255*array_3D[i+ (j-1)*N,,,], type=\"image\", showscale=FALSE, \n              name=pretitle0(1000+i+(j-1)*N), hoverlabel=list(namelength=-1)) %>%     \n      layout(showlegend=FALSE,  # hovermode = \"y unified\",\n             xaxis=list(zeroline=F, showline=F, showticklabels=F, showgrid=F),\n             yaxis=list(zeroline=F,showline=F,showticklabels=F,showgrid=F)) #,\n             # yaxis = list(scaleratio = 1, scaleanchor = 'x'))\n  }\n}\n\n# plt_list[[2]]     \nplt_list %>%    \n  subplot(nrows = N, margin = 0.0001, which_layout=1) %>%  \n  layout(title=\"Sample of CIFAR-10 Images\")\n\n# 5. define a sequential LSTM rNN model \n#### Initialize sequential model\nmodel.3 <- keras_model_sequential()\n\nmodel.3 %>%\n  # First hidden 2D convolutional layer of 32x32 pixel 2D images\n  layer_conv_2d(\n    filter = 32, kernel_size = c(4,4), padding = \"same\", \n    input_shape = c(32, 32, 3)\n  ) %>%\n  layer_activation(\"relu\") %>%\n\n  # Second hidden layer\n  layer_conv_2d(filter = 32, kernel_size = c(4,4)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # add 2 additional hidden 2D convolutional layers\n  layer_conv_2d(filter = 32, kernel_size = c(4,4), padding = \"same\") %>%\n  layer_activation(\"relu\") %>%\n  layer_conv_2d(filter = 32, kernel_size = c(4,4)) %>%\n  layer_activation(\"relu\") %>%\n\n  # Use max pooling again\n  layer_max_pooling_2d(pool_size = c(2,2)) %>%\n  layer_dropout(0.25) %>%\n  \n  # Flatten max filtered output into feature vector \n  # and feed into dense layer\n  layer_flatten() %>%\n  layer_dense(512) %>%\n  layer_activation(\"relu\") %>%\n  layer_dropout(0.5) %>%\n\n  # Outputs from dense layer are projected onto 10-unit output layer\n  layer_dense(10) %>%\n  layer_activation(\"softmax\")\n\nopt <- optimizer_rmsprop(learning_rate = 0.0001)  #, decay = 1e-6)\n\n### Compile the model (i.e., specify loss function, optimizer, and metrics)\nmodel.3 %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = opt,\n  metrics = \"accuracy\"\n)\n\n# 6. Model training\nif(!data_augmentation){\n  history <- model.3 %>% fit(\n    x_train, y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data = list(x_test, y_test),\n    shuffle = TRUE\n  )\n\n} else {\n    datagen <- image_data_generator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    horizontal_flip = TRUE\n  )\n\n  datagen %>% fit_image_data_generator(x_train)\n\n  history <- model.3 %>% fit(\n    flow_images_from_data(x_train, y_train, datagen, batch_size = batch_size),\n    steps_per_epoch = as.integer(20000/batch_size),\n    # increase the iterations (steps_per_epoch) to improve the results, comp-complexity increases\n    # steps_per_epoch = as.integer(40000/batch_size),\n    epochs = epochs,\n    validation_data = list(x_test, y_test)\n  )\n}\n\n# 7. Validation: illustrate the relation between real and predicted class labels\n# Generate the 10 * 10 confusion matrix to \npred_prob <- predict(object = model.3, x = x_test)\n\ny_pred_class_label <- rep(\"\", dim(y_test)[1]); str(y_pred_class_label)\n\nfor (i in 1:dim(y_test)[1]) {   # dim(y_test) is 10000(images) * 10 (class-label-indicators)\n  for (j in 1:dim(y_test)[2]) {\n    if(j==1) j_max = 1\n    else if (pred_prob[i,j] > pred_prob[i, j_max]) j_max = j\n  }\n  y_pred_class_label[i] <- class_labels[j_max]\n}\n\ny_test_class_label <- rep(\"\", dim(y_test)[1]); str(y_test_class_label)\n\nfor (i in 1:dim(y_test)[1]) {   # dim(y_train) is 10000(images) * 10 (class-label-indicators)\n  for (j in 1:dim(y_test)[2]) {\n    if (y_test[i,j] == 1) \n      y_test_class_label[i] <- class_labels[j]\n  }\n}\n\nlength(y_test_class_label)==length(y_pred_class_label)\ntable(y_pred_class_label, y_test_class_label)\ncaret::confusionMatrix(as.factor(y_pred_class_label), as.factor(y_test_class_label))\n\n# Plot algorithm convergence history\n# plot(history, type=\"l\")\ntime <- 1:epochs\nhist_df <- data.frame(time=time, loss=history$metrics$loss, acc=history$metrics$accuracy,\n                      valid_loss=history$metrics$val_loss, valid_acc=history$metrics$val_accuracy)\nplot_ly(hist_df, x = ~time)  %>%\n  add_trace(y = ~loss, name = 'training loss', type=\"scatter\", mode = 'lines') %>%\n  add_trace(y = ~acc, name = 'training accuracy', type=\"scatter\", mode = 'lines+markers') %>%\n  add_trace(y = ~valid_loss, name = 'validation loss', type=\"scatter\",mode = 'lines+markers') %>%\n  add_trace(y = ~valid_acc, name = 'validation accuracy', type=\"scatter\", mode = 'lines+markers') %>% \n  layout(title=\"CIFAR-10 Classificaiton - NN Model Performance\",\n           legend = list(orientation = 'h'), yaxis=list(title=\"metric\"))\n\n# Plot heatmap of actual and predicted CIFAR-10 class labels\n# car_mat <- caret::confusionMatrix(as.factor(y_pred_class_label), as.factor(y_test_class_label))$table\nheat <- table(factor(y_pred_class_label), factor(y_test_class_label))\nplot_ly(x =~class_labels, y = ~class_labels, z = ~matrix(heat, 10,10), name=\"NN Model Performance\",\n        hovertemplate = paste('<i>Matching</i>: %{z:.0f}', \n                              '<br><b>True</b>: %{x}<br>', '<b>Pred</b>: %{y}'),\n        colors = 'Reds', type = \"heatmap\") %>% \n  layout(title=\"CIFAR-10 Predicated vs. True Image Class Labels\", \n         xaxis=list(title=\"Actual Image Class\"), yaxis=list(title=\"Predicted Image Class\"))",
      "line_count": 209
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "# Model inputs\nlag_setting  <- 12 # = nrow(df_test)\nbatch_size   <- 0\ntrain_length <- 10000\n\n# the number of predictions we'll make equals the length of the hidden state\ntsteps       <- 1\nepochs       <- 100\n\n# how many features = predictors we have\nn_features <- 1",
      "line_count": 11
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "library(\"dplyr\")\nlibrary(\"recipes\")\nlibrary(\"tibble\")\n#install.packages(\"tibbletime\")\nlibrary(\"tibbletime\")\n\ndf_train <- as.data.frame(cbind(df_Temp_Time$t[1:10000], df_train.std))\n# head(df_train)\ndf_test <- as.data.frame(cbind(df_Temp_Time$t[1:10000], df_test.std))\ncolnames(df_train) <- c(\"value\"); colnames(df_test) <- c(\"value\")\ncolnames(df_Temp_Time) <- c(\"Z\", \"index\")\n\ndf <- bind_rows(\n    df_train %>% add_column(key = \"training\"),\n    df_test %>% add_column(key = \"testing\")\n) # %>% \n    # as_tbl_time(index = as.Date(df_Temp_Time$index[1:10000]))\n# df\n\n\ndf1 <- as.data.frame(cbind(as.Date(df_Temp_Time$index[1:20000]),\n            as.double(df$value), df$key))\ncolnames(df1) <- c(\"index\", \"value\", \"class\")\nhead(df1)\ndates <- as.Date(df_Temp_Time$index[1:20000])\n# head(as_tbl_time(df, dates))\n\ndf2 <- tibble::tibble(\n  index  = as.POSIXct(dates),\n  value = as.numeric(as.character(df1$value)),\n  class = df1$class\n); head(df2)\ndf3 <- as_tbl_time(df2, index)\nhead(df3); tail(df3); str(df3$value); head(df3$value)\n\nrec_obj <- recipe(value ~ ., df3) %>%\n    step_sqrt(value) %>%\n    step_center(value) %>%\n    step_scale(value) %>%\n    prep()\n    \ntail(rec_obj$template)\n\n# define time, value and labels for 10,000 timepoints\nclimate_time_10000 <- climate_time[1:10000]\ntrainLables <- rep(\"training\", 10000); testLabels <- rep(\"testing\", 10000)\nTrainTestLabels <- rbind(trainLables, testLabels)\nTrainTestValues <- rbind(df_train.std, df_test.std)\ntibble(date = as.Date(as.vector(rbind(climate_time_10000, climate_time_10000))), \n       value = TrainTestValues[ , 1], key=as.vector(TrainTestLabels))\n\n# Training Set\nlag_train_tbl <- rec_obj$template %>%\n    mutate(value_lag = lag(value, n = lag_setting)) %>%\n    filter(!is.na(value_lag)) %>%\n    filter(class == \"training\") %>%\n    tail(train_length)\n\nif (is.na(lag_train_tbl$value)) lag_train_tbl$value <- 0\n\nx_train_vec <- lag_train_tbl$value_lag\nx_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))\n\ny_train_vec <- lag_train_tbl$value\ny_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))\n\n# Testing Set\nlag_test_tbl <- rec_obj$template %>%\n    mutate(value_lag = lag(value, n = lag_setting)) %>%\n    filter(!is.na(value_lag)) %>%\n    filter(class == \"testing\")\n\nx_test_vec <- lag_test_tbl$value_lag\nx_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))\n\ny_test_vec <- lag_test_tbl$value\ny_test_arr <- array(data = y_test_vec, dim = c(length(y_test_vec), 1))",
      "line_count": 77
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "model.1 <- keras_model_sequential()\n\nmodel.1 %>%\n    layer_lstm(units            = 50, \n               # batch_input_shape = c(batch_size, tsteps, n_features),\n               batch_input_shape=c(32, 1, 1),\n               return_sequences = TRUE, \n               stateful         = TRUE) %>% \n    layer_lstm(units            = 50, \n               return_sequences = FALSE, \n               stateful         = TRUE) %>% \n    layer_dense(units = 1)\n\nmodel.1 %>% \n    compile(loss = 'mae', optimizer = 'adam')\n\nmodel.1",
      "line_count": 17
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "for (i in 1:epochs) {\n    model.1 %>% fit(x          = x_train_arr[1:(166*32), ,], \n                    y          = y_train_arr[1:(166*32), ], \n                    batch_size = 32,\n                    epochs     = 1, \n                    verbose    = 1, \n                    shuffle    = FALSE)\n    \n    model.1 %>% reset_states()\n    cat(\"Epoch: \", i)\n}\n\n\n\n########################### RStudio Documentation\n# https://cran.rstudio.com/web/packages/keras/vignettes/sequential_model.html\n\n# define a model along with the compilation step (\nmodel.2 <- keras_model_sequential() \nmodel.2 %>% \n  layer_dense(units = 32, input_shape = c(100)) %>% \n  layer_activation('relu') %>% \n  layer_dense(units = 10) %>% \n  layer_activation('softmax')\n\n# the compile() function has appropriate arguments for a multi-class/binary classification problem\nmodel.2 %>% compile(\n  optimizer = 'rmsprop',\n  # loss = 'categorical_crossentropy',\n  # metrics = c('accuracy')\n  loss = loss_binary_crossentropy,\n  metrics = metric_binary_accuracy\n)\n\n#  Compilation for mean squared error regression problem\nmodel %>% compile(\n  optimizer = optimizer_rmsprop(lr = 0.002),\n  loss = 'mse'\n)\n\n# Train the Keras models on R matrices or higher dimensional arrays of input data and labels using \n# the fit() function, e.g., a single-input model with 2 classes (binary classification)\n\n# Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)\n\n# Train the model, iterating on the data in batches of 32 samples\nmodel.2 %>% fit(data, labels, epochs=10, batch_size=32)\n\n# constants\ndata_dim <- 16\ntimesteps <- 8\nnum_classes <- 10\n\n# define and compile model\n# expected input data shape: (batch_size, timesteps, data_dim)\ndata_dim <- 1\ntimesteps <- 2\nnum_classes <- 2\n\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_lstm(units = 32, return_sequences = TRUE, input_shape = c(timesteps, data_dim)) %>% \n  layer_lstm(units = 32, return_sequences = TRUE) %>% \n  layer_lstm(units = 32) %>% # return a single vector dimension 32\n  layer_dense(units = 10, activation = 'softmax') %>% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n  \n# training data\nx_train <- array(runif(1000 * timesteps * data_dim), dim = c(1000, timesteps, data_dim))\ny_train <- matrix(runif(1000 * num_classes), nrow = 1000, ncol = num_classes)\nx_train <- array(rec_obj$template$value[1:10000], dim = c(1000, 2, 1))\ny_train <- matrix(rec_obj$template$class[1:10000], nrow = 5000, ncol = 2)\n\n\n# testing validation data\nx_val <- array(runif(100 * timesteps * data_dim), dim = c(100, timesteps, data_dim))\ny_val <- matrix(runif(100 * num_classes), nrow = 100, ncol = num_classes)\n\nx_val <- array(rec_obj$template$value, dim = c(2000, 2, 1))\ny_val <- matrix(rec_obj$template$class, nrow = 10000, ncol = 2)\n\n# train\nmodel %>% fit( \n  x_train, y_train, batch_size = 64, epochs = 5, validation_data = list(x_val, y_val)\n)\n",
      "line_count": 92
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "# Make Predictions\npred_out <- model %>% \n    predict(x_test_arr, batch_size = batch_size) %>%\n    .[,1] \n\n# Retransform values\npred_tbl <- tibble(\n    index   = lag_test_tbl$index,\n    value   = (pred_out * scale_history + center_history)^2\n) \n\n# Combine actual data with predictions\ntbl_1 <- df_trn %>%\n    add_column(key = \"actual\")\n\ntbl_2 <- df_tst %>%\n    add_column(key = \"actual\")\n\ntbl_3 <- pred_tbl %>%\n    add_column(key = \"predict\")\n\n# Create time_bind_rows() to solve dplyr issue\ntime_bind_rows <- function(data_1, data_2, index) {\n    index_expr <- enquo(index)\n    bind_rows(data_1, data_2) %>%\n        as_tbl_time(index = !! index_expr)\n}\n\nret <- list(tbl_1, tbl_2, tbl_3) %>%\n    reduce(time_bind_rows, index = index) %>%\n    arrange(key, index) %>%\n    mutate(key = as_factor(key))\n\nret",
      "line_count": 34
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "calc_rmse <- function(prediction_tbl) {\n    \n    rmse_calculation <- function(data) {\n        data %>%\n            spread(key = key, value = value) %>%\n            select(-index) %>%\n            filter(!is.na(predict)) %>%\n            rename(\n                truth    = actual,\n                estimate = predict\n            ) %>%\n            rmse(truth, estimate)\n    }\n    \n    safe_rmse <- possibly(rmse_calculation, otherwise = NA)\n    \n    safe_rmse(prediction_tbl)\n        \n}\n\n# assess the RMSE on the model\ncalc_rmse(ret)",
      "line_count": 22
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "# Setup single plot function\nplot_prediction <- function(data, id, alpha = 1, size = 2, base_size = 14) {\n    \n    rmse_val <- calc_rmse(data)\n    \n    g <- data %>%\n        ggplot(aes(index, value, color = key)) +\n        geom_point(alpha = alpha, size = size) + \n        theme_tq(base_size = base_size) +\n        scale_color_tq() +\n        theme(legend.position = \"none\") +\n        labs(\n            title = glue(\"{id}, RMSE: {round(rmse_val, digits = 1)}\"),\n            x = \"\", y = \"\"\n        )\n    \n    return(g)\n}",
      "line_count": 18
    },
    {
      "section": "Example 1: Temperature Time-series LSTM Modeling",
      "code": "ret %>% \n    plot_prediction(id = split_id, alpha = 0.65) +\n    theme(legend.position = \"bottom\")",
      "line_count": 3
    },
    {
      "section": "Example 2: Text Generation",
      "code": "library(keras)\nlibrary(stringr)\n\ndata.csv <- read.csv(\"https://umich.instructure.com/files/12554194/download?download_frd=1\", header=T)\ntext <- tolower(data.csv$text)\ncat(\"Corpus length:\", nchar(text), \"\\n\")",
      "line_count": 6
    },
    {
      "section": "Example 2: Text Generation",
      "code": "maxlen <- 60  # Length of extracted character sequences\nstep <- 3  # We sample a new sequence every `step` characters\n  \ntext_indexes <- seq(1, nchar(text) - maxlen, by = step)\n# This holds our extracted sequences\nsentences <- str_sub(text, text_indexes, text_indexes + maxlen - 1)\n# This holds the targets (the follow-up characters)\nnext_chars <- str_sub(text, text_indexes + maxlen, text_indexes + maxlen)\ncat(\"Number of sequences: \", length(sentences), \"\\n\")\n# List of unique characters in the corpus\nchars <- unique(sort(strsplit(text, \"\")[[1]]))\ncat(\"Unique characters:\", length(chars), \"\\n\")\n# Dictionary mapping unique characters to their index in `chars`\nchar_indices <- 1:length(chars) \nnames(char_indices) <- chars\n# Next, one-hot encode the characters into binary arrays.\ncat(\"Vectorization...\\n\") \nx <- array(0L, dim = c(length(sentences), maxlen, length(chars)))\ny <- array(0L, dim = c(length(sentences), length(chars)))\nfor (i in 1:length(sentences)) {\n  sentence <- strsplit(sentences[[i]], \"\")[[1]]\n  for (t in 1:length(sentence)) {\n    char <- sentence[[t]]\n    x[i, t, char_indices[[char]]] <- 1\n  }\n  next_char <- next_chars[[i]]\n  y[i, char_indices[[next_char]]] <- 1\n}",
      "line_count": 28
    },
    {
      "section": "Example 2: Text Generation",
      "code": "model <- keras_model_sequential() %>% \n  layer_lstm(units = 128, input_shape = c(maxlen, length(chars))) %>% \n  layer_dense(units = length(chars), activation = \"softmax\")",
      "line_count": 3
    },
    {
      "section": "Example 2: Text Generation",
      "code": "optimizer <- optimizer_rmsprop(lr = 0.01)\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = optimizer\n)   ",
      "line_count": 5
    },
    {
      "section": "Example 2: Text Generation",
      "code": "sample_next_char <- function(preds, temperature = 1.0) {\n  preds <- as.numeric(preds)\n  preds <- log(preds) / temperature\n  exp_preds <- exp(preds)\n  preds <- exp_preds / sum(exp_preds)\n  which.max(t(rmultinom(1, 1, preds)))\n}",
      "line_count": 7
    },
    {
      "section": "Example 2: Text Generation",
      "code": "for (epoch in 1:50) {\n  \n  cat(\"Epoch\", epoch, \"\\n\")\n  \n  # Fit the LSTM model for 1 epoch on the available training data\n  model %>% fit(x, y, batch_size = 64, epochs = 1) \n  \n  # Select a random seed\n  start_index <- sample(1:(nchar(text) - maxlen - 1), 1)  \n  seed_text <- str_sub(text, start_index, start_index + maxlen - 1)\n  \n  cat(\"--- Synth-Text Generation with seed:\", seed_text, \"\\n\\n\")\n  \n  for (temperature in c(0.2, 0.5, 1.0, 1.2)) {\n    \n    cat(\"----------- temperature:\", temperature, \"\\n\")\n    cat(seed_text, \"\\n\")\n    \n    generated_text <- seed_text\n    \n    # Generate text with 500 characters\n    for (i in 1:500) {\n      \n      sampled <- array(0, dim = c(1, maxlen, length(chars)))\n      generated_chars <- strsplit(generated_text, \"\")[[1]]\n      for (t in 1:length(generated_chars)) {\n        char <- generated_chars[[t]]\n        sampled[1, t, char_indices[[char]]] <- 1\n      }\n        \n      preds <- model %>% predict(sampled, verbose = 0)\n      next_index <- sample_next_char(preds[1,], temperature)\n      next_char <- chars[[next_index]]\n      \n      generated_text <- paste0(generated_text, next_char)\n      generated_text <- substring(generated_text, 2)\n      \n      cat(next_char)\n    }\n    cat(\"\\n\")\n  }\n}",
      "line_count": 42
    }
  ]
}