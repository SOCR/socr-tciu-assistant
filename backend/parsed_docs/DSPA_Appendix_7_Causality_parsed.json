{
  "metadata": {
    "created_at": "2024-11-30T13:46:17.507710",
    "total_sections": 7,
    "total_code_chunks": 10,
    "total_tables": 1,
    "r_libraries": [
      "RTransferEntropy",
      "corrplot",
      "future",
      "igraph",
      "knitr",
      "magrittr",
      "plotly",
      "xlsx"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Appendix 7: Causality</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: ../SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\n      smooth_scroll: true\nThis [DSPA Appendix](https://dspa2.predictive.space/) presents the foundations of causal inference in data science.",
      "word_count": 63
    },
    {
      "title": "Introduction",
      "content": "In previous [DPSA chapters](https://dspa2.predictive.space/), we introduced many alternative model-based and model-free methods for supervised and unsupervised regression, classification, clustering and forecasting. \n\nIn model-based supervised problems, linear regression and correlation analyses play important roles. However, establishing a correlation relation between a pair of features does not necessarily indicate that changes in one of the variables drives the behavior of the other, i.e., causation is not necessarily guaranteed by establishing correlation.\n\nLet's look at two simple examples.\n\n* *Example 1*: In children, shoe-size is heavily correlated with outcomes on math tests, however, an increase of neither variable causes the rise (or fall) of the other. Shoe-size and cognition move synchronously up and down without a direct causal effect of one on the other.\n\n* *Example 2*: Let's show a simulation of an explicit causal relation that is undetectable by correlation analysis. \n\n\nThe graph does not illustrate a clear $x_1$ to $x_2$ relationship - correlational or causal. At the same time, the apparent stochasticity in system hides the simple (quadratic) causal relationship defined by:\n\n$$x_1(n) = \\frac{1}{2}x_1(n-1) + \\epsilon_1; \\ x_1(1)=1, \\ 1\\leq n \\leq 1024,$$\n\n$$x_2(n) = \\frac{2}{5}x_1^2(n-1) + \\epsilon_2; \\ \\epsilon_1,\\epsilon_2 \\sim N(0,1).$$\n\nUsing any (multivariate) observational data, we will showcase how to use computational statistics, data science, information theory, and causal inference to uncover such hidden causal relationships.",
      "word_count": 218
    },
    {
      "title": "Causality",
      "content": "*Causality* was introduced by [Granger](https://doi.org/10.2307/1912791), Wiener, and [Pearl](https://doi.org/10.1016/0004-3702(88)90015-X). A process $X$ is (Granger-)causally related to another process $Y$, when predicting prospective $Y$ realizations solely based on past $Y$ records can be improved based on the past information from the first process $X$ along with past knowledge about $Y$.\n\n## Prediction-based causality \n\nAssume we have three time-varying processes indexed by time $t\\geq 0$: \n\n - $\\{Y_t\\}$, the outcome process we are studying or forecasting,\n - $\\{X_t\\}$, the (presumed) causal effect on the outcome $\\{Y_t\\}$, and\n - Some side-information $\\{Z_t\\}$. Clearly, $Z_t$ may represent a number of different tangentially-related features.\n\nThe complete information up to time $t$ for a random process will be denoted by $X^t=\\cup_{s<t}{ \\{X_s\\}}$. Also, for time $t+1$, let's denote by:\n\n - $E(W)$, the expectation of the random variable $W$,\n - $\\hat{Y}_{t+1}=f(X^t,Y^t,Z^t)$, the predicted outcome of the process $Y$,\n - the corresponding error of the prediction by\n\n$$e_{t+1}= \\underbrace{{Y}_{t+1}}_{actual}-\n\\underbrace{\\hat{Y}_{t+1}}_{predicted},$$\n \n - $g(e)$, the loss function, typically a norm like $||\\ ||_2$, $||\\ ||_1$, or $||\\ ||_p$,\n - *Expected prediction errors*:\n\n$$\\underbrace{R(Y^{t+1}| Y^t,Z^t)}_{given\\ only\\ Y^t}\n=E(g(e_{t+1}))=E(g({Y}_{t+1}-\\hat{Y}_{t+1}))=E(g({Y}_{t+1}-f(Y^t,Z^t))),$$\n$$\\underbrace{R(Y^{t+1}| X^t,Y^t,Z^t)}_{given\\ both\\ X^t\\ and\\ Y^t}\n=E(g(e_{t+1}))=E(g({Y}_{t+1}-\\hat{Y}_{t+1}))=E(g({Y}_{t+1}-f(X^t,Y^t,Z^t))).$$\n\nAs with other model-based techniques, e.g., linear models, the estimate $\\hat{Y}_{t+1}=f(X^t,Y^t,Z^t)$ is computed by estimating (fitting) the function $f$ that optimizes (minimizes) the expected loss, $R(Y^{t+1}| Y^t,Z^t)$ or $R(Y^{t+1}| X^t,Y^t,Z^t)$.\n\n**Granger-causality**: The process $X$ is *not Granger-causal* to the process $Y$, relative to the side information $Z$, if and only if:\n$$\\underbrace{R(Y^{t+1}| Y^t,Z^t)}_{independent\\ of\\ X^t} = R(Y^{t+1}| X^t,Y^t,Z^t).$$\n\nFor example, a linear *vector-autoregressive model* function $f$ with a number of lags $l$ may look like:\n\n$$\\hat{Y}(t)=\\underbrace{f(Y^t)}_{independent\\ of\\ X^t}=\\beta_o + \\sum_{s=1}^l{\\beta_s Y(t-s)+\\epsilon_t}, \\ \\ \\ \\ (1)$$\n$$\\tilde{Y}(t)=\\underbrace{f(X^t,Y^t)}_{dependent\\ on\\ X^t}=\\tilde{\\beta}_o + \\sum_{s=1}^l{\\tilde{\\beta}_s Y(t-s)+ \\sum_{s=1}^l{\\tilde\\gamma}_s X(t-s) + \\tilde{\\epsilon}_t}.\n \\ \\ \\ \\ (2)$$\n\nSimilarly, one can choose alternative models to estimate the expected prediction errors based on deep learning, neural networks, support vector machines, random forest, or any other supervised technique to fit the model.\n\nIn each situation, the variable $X$ does not Granger-cause $Y$ if and only if the expected prediction errors of $X$ in the restricted $\\hat{Y}(t)$ and unrestricted $\\tilde{Y}(t)$ models are equal.  In other words, the two models are statistically indistinguishable and any differences in prediction are only due to random factors (noise). \n\nOne parametric test for assessing a Granger-causality test is the one-way analysis of variance (one-way ANOVA) test to compare the residuals of the restricted and unrestricted models, i.e., compare the statistical differences of the residuals. When performing multiple such tests, e.g., in contrasting multiple $l$ lags, one needs to adjust the resulting p-values by correcting for multiple hypothesis tests, using false discovery rate (FDR), family-wise error rate (FWER), or Bonferroni correction.\n\n\n## Probabilistic-based causality \n\nWe can also formulate a more general probabilistic definition of causality using conditional probability, $p(. | .)$. This approach avoids the need to explicitly define a priori prediction function models in advance.\n\nRecall that $Y_{t+1}$ and $X^t$ are (statistically) independent given the past information $(X^t,Y^t)$ if and only if\n$$p(Y_{t+1}|X^t,Y^t, Z^t)=p(Y_{t+1}|Y^t, Z^t).$$\nIn other words, knowing or ignoring past information, $X^t$, about the process $X$ does not affect the probability distribution of the future outcome, $Y_{t+1}$.\n\n**Probability-based Granger causality**: The process $X$ does not Granger-cause the process $Y$, relative to some side information $Z$, if and only if $Y_{t+1} \\perp X^t\\ |\\ Y^t,Z^t$. That is, given the past information about $Y^t$ and $Z^t$, $Y_{t+1}$ is independent of (orthogonal to) $X^t$.\n\n\n\n## Causality and Transfer Entropy\n\nProbabilistic Granger causality does not require an explicit function model coupling the two processes $X$ and $Y$. However, it still requires a strategy to compute the conditional dependence of $Y_{t+1}$ on the other previously observed variables, $p(Y_{t+1}|X^t,Y^t, Z^t)$. \n\nNow, we will connect Granger causality with information theory.\n\n### Entropy\n\nReviewing some of the information theory details in the [DSPA Information Theory and Statistical Learning Appendix](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/DSPA_Appendix_02_InformationTheory_StatisticalLearning.html) may be useful before proceeding further in this section.\n\nFor a bivariate process $W=(X,Y)$, let's denote the pair of marginal and the joint distribution functions by $p_X(x)$, $p_Y(y)$, and $p_{X,Y}(x,y)$. In the discrete and continuous cases, the joint (Shannon/Differential) **entropy** between the univariate processes $X$ and $Y$ is defined by:\n\n$$\\underbrace{H(X,Y)}_{Shannon\\ entropy}=-\\sum_{x\\in X}{\\sum_{y\\in Y}{p_{X,Y}(x,y)\\log  ( p_{X,Y}(x,y)) }},$$\n\n$$\\underbrace{h(X,Y)}_{Differential\\ entropy}=-\\int_{x\\in \\Omega_X}{\\int_{y\\in \\Omega_Y}{p_{X,Y}(x,y)\\log ( p_{X,Y}(x,y)) }}dxdy,$$\n\nThe uncertainty of $Y$ given a specific realization $X$ is captured by the *conditional entropy* :\n\n$$H(Y|X) = H(X,Y) - H(X),$$\n\nwhere the entropy of the single variable $X$ is:\n\n$$H(X)=-\\sum_{x\\in X}{p_{X}(x)\\log  ( p_{X}(x)) }.$$\n\nThe Granger causality can be computed using *transfer entropy*, which detects directional and dynamical information without assuming any particular function modeling process interactions.\n\n### Transfer entropy\n\n*Transfer entropy* quantifies the amount of directed information transfer between the random processes $X$ and $Y$. As a non-parametric statistic, the transfer entropy measures the reduction of uncertainty in prospective values of $Y$ given the past values of $X$ and $Y$. \n\nThe *transfer entropy* is the following difference of conditional entropies:\n\n$$T_{X\\rightarrow Y\\ |\\ Z}=H\\left (\\underbrace{Y_{t+1}}_{future}\\ |\\ \\underbrace{Y^t, Z^t}_{past}\\right ) - H\\left (\\underbrace{Y_{t+1}}_{future}\\ |\\ \\underbrace{X^t, Y^t, Z^t}_{past}\\right ),$$\n$$T_{X\\rightarrow Y\\ |\\ Z}=\\left (H(Y^t, X^t) - H(Y_{t+1},Y^t, X^t)\\right )-\n\\left (H(Y_{t+1}) - H(Y_{t+1},Y^t)\\right ).$$\n\nwhere $H(X)$ is Shannon entropy of $X$. Transfer entropy can also be considered as conditional mutual information:\n\n$$T_{X\\rightarrow Y\\ |\\ Z}=I(Y_{t+1}\\ ;\\ X^t\\ |\\ Y^t,Z^t),$$\n\nwhere\n\n$$I(Y_{t+1}\\ ;\\ X^t\\ |\\ Y^t,Z^t)=$$\n$$\\sum_{(y^t,z^t)\\in \\Omega_Y^t\\times \\Omega_Z^t}{p_{(Y^t,Z^t)}(y^t,z^t)\\sum_{x^t\\in \\Omega_X^t}{\\sum_{y_{t+1}\\in \\Omega_Y^{t+1}}{\n\\left (p_{(Y_{t+1},X^t|Y^t,Z^t)}(y_{t+1},x^t|y^t,z^t)\n\\times \\log\\left ( \\frac{p_{(Y_{t+1},X^t| Y^t,Z^t)}(y_{t+1},x^t|y^t,z^t)}\n{p_{(Y_{t+1}| Y^t,Z^t)}(y_{t+1}|y^t,z^t)\\times\np_{(X^t| Y^t,Z^t)}(x^t|y^t,z^t)} \\right )\n\\right )\n}}}=$$\n\n$$\\sum_{(y^t,z^t)\\in \\Omega_Y^t\\times \\Omega_Z^t}{\\sum_{x^t\\in \\Omega_X^t}{\\sum_{y_{t+1}\\in \\Omega_Y^{t+1}}{\n\\left (p_{(Y_{t+1},X^t,Y^t,Z^t)}(y_{t+1},x^t,y^t,z^t)\n\\times \\log\\left ( \\frac{p_{(Y^t,Z^t)}(y^t,z^t)\\times p_{(Y_{t+1},X^t,Y^t,Z^t)}(y_{t+1},x^t,y^t,z^t)}\n{p_{(Y_{t+1}, Y^t,Z^t)}(y_{t+1},y^t,z^t)\\times\np_{(X^t,Y^t,Z^t)}(x^t,y^t,z^t)} \\right )\n\\right )\n}}}.$$\n\nIn terms of *transfer entropy*, $X$ does not cause $Y$, relative to side information $Z$, if and only\nif $T_{X\\rightarrow Y, Z^t}=0.$, i.e.,\n\n$$H(Y_{t+1}\\ |\\ Y^t, Z^t)=H(Y_{t+1}\\ |\\ X^t,Y^t, Z^t).$$\n\n### Net information flow\n\nAs the *transfer entropy* is not a symmetric function, $T_{X\\rightarrow Y\\ |\\ Z}\\not=T_{Y\\rightarrow X\\ |\\ Z}$, it allows us to infer directionality of the information flow, i.e., causality. More specifically, we can define the **net information flow measure**, $F_{X\\rightarrow Y\\ |\\ Z}$, as:\n\n$$F_{X\\rightarrow Y\\ |\\ Z}=T_{X\\rightarrow Y\\ |\\ Z}-T_{Y\\rightarrow X\\ |\\ Z}.$$\n\nThe symmetrized net information flow measure, $F_{X\\rightarrow Y\\ |\\ Z}$, quantifies the dominant direction of the information flow; positive values indicate a dominant information flow from $X\\longrightarrow Y$, rather than the opposite direction, and similarly, negative values indicate a reversed dominant information flow from $Y\\longrightarrow X$. Hence, it suggests which process yields more predictive information about the other.\n\nIn the special case of exploring vector auto-regressive processes, such as the ones we showed in equations (1) and (2) above, the transfer entropy reduces to Granger causality, see [this reference](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.103.238701). Using the classical $g=||.||_{2}$ norm as the loss function and based on the linear  *vector-autoregressive model* function $f$, see equations (1) and (2), Granger-causality of a bivariate process can be expressed as:\n\n$$GrangerCausality_{\\{X\\rightarrow Y\\}}=\\log \\left ( \\frac{Var(\\epsilon_t)}{Var(\\hat{\\epsilon}_t)}\\right ).$$\n\nThis explicit connection between the transfer entropy and the linear Granger-causality facilitates the estimation of the transfer entropy, $T_{X\\rightarrow Y\\ |\\ Z}$, and the net information flow metric, $F_{X\\rightarrow Y\\ |\\ Z}$.",
      "word_count": 1156
    },
    {
      "title": "Simulation Examples",
      "content": "Let's look at a pair of linear and non-linear synthetic data examples.\n\n## Synthetic linear relationships\n\n\nAccording to the definition above, \n$linearGC_{\\{X\\rightarrow Y\\}}=\\log \\left ( \\frac{Var(\\epsilon_t)}{Var(\\hat{\\epsilon}_t)}\\right )$, we define a function, `linearGC()`, to compute the linear Granger-causality for a bivariate process $(X,Y)$.\n\n\nNext we employ the function `RTransferEntropy::calc_te()` to estimate the pairwise information flow among the 6-variate synthetic data process $(X_1, \\cdots,X_6)$.\n\n\nFinally, compare and contrast the Granger-causality and Transfer Entropy for this 6-variate simulation. The resulting $6\\times 6$ cells presents the information flow from a (row) variable $Y$ to a (column) variable $X$. Note that the linear Granger-causality and the non-linear transfer entropy show similar results. Thus, both techniques capture analogous dependencies of the 6-variate process. This congruence is natural due to the simple linear associations between the variables in this simulation.\n\n\n\n## Non-linear synthetic relationships\n\nNext we will introduce non-linear (quadratic) relations in the 6-variate process we considered earlier.\n\n\nSimilarly to the linear case, we can compute and display the Granger-causality and the transfer entropy measures for this 6-variate nonlinear simulated process.",
      "word_count": 174
    },
    {
      "title": "Macroeconomic Market Forecasting Example",
      "content": "Let's use the complete monthly [SOCR Macroecon Market Data for the US (1979 - 2020)](https://umich.instructure.com/courses/38100/files/folder/Case_Studies/34_US_MacroEconMarketData_CompleteMonthly_1979_2020) to illustrate causality.\n\nWe will begin by loading and plotting the longitudinal dataset along with some [DJIA](https://www.spglobal.com/spdji/en/indices/equity/dow-jones-industrial-average) and [NASDAQ](https://www.nasdaq.com) (linear) models.\n\n\nThe DJIA and NASDAQ markets represent complex dynamic processes that are linked across many socioeconomic, cyclical, psychological, technological, and other factors. The multiple types and effects and their dynamic nature lead to highly complex interactions; some causal, some correlational, and some random. Clearly, it is vital to understand the mechanistic effects of different observable processes on macro- and micro-economic indices.\n\nLet's employ the *transfer entropy* to examine the dependency relations among various factors in this dataset (tensor of dimensions $499\\times 17$). Specifically, we can display the resulting $17\\times 17$ matrix of normalized transfer entropy values. This normalization can be accomplished in many different ways. We'll divide each cell value by the maximum value in the matrix to ensure all values are between 0 and 1. The graph shows the pairs of features that are highly interconnected in the period 1979-2020. These are the cell values with the highest information flow going from one economic index to another, along with the direction of the information flow; this may suggest causal mechanistic dependence.\n\n\nNote the different patterns in the paired causal matrix plots above (linear Granger causality vs. the non-linear transfer entropy).",
      "word_count": 226
    },
    {
      "title": "Discussion",
      "content": "Uncovering directional cause effects is generally difficult and requires a number of assumptions. Data science and statistical inference strategies can assist with discriminating simple linear associations (correlations) from mechanistic effects (causal relations). In this DSPA appendix, we explored some information-theoretic approaches to Granger-causality under the assumptions of the linear vector-autoregressive modeling framework. \n\nIn addition, causality may be estimated via the transfer entropy metric to estimate statistical causal-relations in non-linear multivariate processes. We used synthetic simulations of linear and non-linear multivariate processes to demonstrate that linear Granger-causality may fail to detect explicit non-linear (quadratic) causal patterns, which can be untangled by transfer entropy estimation. The demonstration of calculating paired causal effects in multivariate processes using US macroeconomic data showed how transfer entropy quantifies causal effects of DJIA and NASDAQ equity indices.",
      "word_count": 130
    },
    {
      "title": "References",
      "content": "- [DSPA](https://dspa.predictive.space/)\n - [Granger](https://doi.org/10.2307/1912791)\n - [Wiener](https://doi.org/10.7551/mitpress/2946.001.0001)\n - [Pearl](https://doi.org/10.1016/0004-3702(88)90015-X)\n - [DSPA Information Theory and Statistical Learning Appendix](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/DSPA_Appendix_02_InformationTheory_StatisticalLearning.html)\n - [Transfer entropy, Shannon information, and mutual information](https://www.frontiersin.org/articles/10.3389/fncom.2020.00045/full)\n - [Critiques of transfer and causal entropy - examples of causal entropy overestimating information flow or underestimating influence](https://arxiv.org/pdf/1512.06479.pdf).\n\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 137
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "    code_folding: show\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Main",
      "code": "library(knitr)\nlibrary(plotly)",
      "line_count": 2
    },
    {
      "section": "Introduction",
      "code": "n <- 1024\nt <- seq(from=1, to=n, by=1)\nx1 <- t; x2 <- t; \n\nfor (i in 2:n) {\n  x1[i] <- 0.5*x1[i-1] + rnorm(1, mean = 0, sd = 1)\n  x2[i] <- 0.4*x1[i-1]*x1[i-1] + rnorm(1, mean = 0, sd = 1)\n}  \n\nx1_ord <- seq(from=min(x1), to=max(x1), length.out=n)\nx2_ord <- x1_ord\nx2_ord[1] <- (2/5)*x1_ord[1]*x1_ord[1]\nfor (i in 2:n) {\n  x2_ord[i] <- (2/5)*x1_ord[i-1]*x1_ord[i-1]\n} \n\n# plot(x1,x2)\nfig <- plot_ly(x=~x1, y=~x2, type=\"scatter\",\n               name = 'data', mode='markers')\nfig <- fig %>% add_trace(\n  x=~x1_ord, y = ~x2_ord, name = 'model', \n  mode = 'lines', line = list(color = 'red', \n                        width = 2, dash = 'dash'))\nfig",
      "line_count": 24
    },
    {
      "section": "Simulation Examples",
      "code": "# initialization (Gaussian noise)\nn <- 10000\nx1 <- x2 <- x3 <- x4 <- x5 <- x6 <- rep(0, n + 1)\n\n# instantiation (linear relationships)\nfor (i in 2:(n + 1)) {\n  x1[i] <- 0.8 * sqrt(3)* x1[i-1] - 0.8*x1[i-1] + rnorm(1, mean=0, sd=1)\n  x2[i] <- 0.5*x1[i-1] + rnorm(1, mean=0, sd=1)\n  x3[i] <- -0.3*x1[i-1] + rnorm(1, mean=0, sd=1)\n  x4[i] <- -0.6*x1[i-1] + 0.3*sqrt(3)*x4[i-1] + 0.2*sqrt(3)*x5[i-1] + rnorm(1, mean=0, sd=1)\n  x5[i] <- -0.2*sqrt(3)*x4[i-1] + 0.2*sqrt(3)*x5[i-1] + rnorm(1, mean=0, sd=1)\n  x6[i] <- 0.3*sqrt(5)*x3[i-1] + 0.4*sqrt(3)*x4[i-1] + rnorm(1, mean=0, sd=1)\n}\n\nx1 <- x1[-1]; x2 <- x2[-1]; x3 <- x3[-1]\nx4 <- x4[-1]; x5 <- x5[-1]; x6 <- x6[-1]\nlin.system <- data.frame(x1, x2, x3, x4, x5, x6)\n\nlibrary(igraph)\ng<-graph(c(1,1, 1,2, 1,3, 1,4, 4,4, 5,4, 4,5, 5,5, 3,6, 4,6), n=6)\nplot(g, main=\"Simulated linear dependencies\")",
      "line_count": 21
    },
    {
      "section": "Simulation Examples",
      "code": "linearGC <- function(X, Y){\n  n<-length(X)\n  X.past <- X[1:(n-1)]\n  Y.past <- Y[1:(n-1)]\n  Y.future <- Y[2:n]\n\n  regression.uni <- lm(Y.future ~ Y.past)\n  regression.mult <- lm(Y.future ~ Y.past + X.past)\n  var.eps.uni <- (summary(regression.uni)$sigma)^2\n  var.eps.mult <- (summary(regression.mult)$sigma)^2\n  linGC <- log(var.eps.uni/var.eps.mult)\n  return(linGC)\n}",
      "line_count": 13
    },
    {
      "section": "Simulation Examples",
      "code": "# install.packages('RTransferEntropy')\nlibrary(future)\nlibrary(RTransferEntropy)\n\n## Enable multi-core parallel computing\nplan(multicore)\n\n## Estimate the Granger-causality and the transfer entropy, based on linearGC()\nn = seq(1:ncol(lin.system))\n# apply the GC and TE estimating functions to all possible pairs of columns in the DF\nff.GC.value <- function(a, b) linearGC(lin.system[,a], lin.system[,b])\nGC.matrix <- outer(n, n, Vectorize(ff.GC.value))\nff.TE.value <- function(a, b) calc_te(lin.system[,a], lin.system[,b])\nTE.matrix <- outer(n, n, Vectorize(ff.TE.value))\n\nstr(TE.matrix)\n\nrownames(TE.matrix) <-\n  colnames(TE.matrix)<-var.names<-c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\")\nrownames(GC.matrix) <- colnames(GC.matrix) <- var.names",
      "line_count": 20
    },
    {
      "section": "Simulation Examples",
      "code": "# 6*6 Granger-Causality and Transfer Entropy matrices for 6-variate simulation\nlibrary(corrplot)\ncorrplot(GC.matrix, method = \"circle\", title = \n           \"Granger Causality (linear model)\", \n         tl.cex = 0.9, tl.col = 'black', mar=c(1, 1, 1, 1))\n# note the boost *10 for TE.matrix to artificially enhance the plot (not required)\ncorrplot(10*TE.matrix, method = \"circle\", title = \n           \"Transfer Entropy (linear model)\", \n         tl.cex = 0.9, tl.col = 'black', mar=c(1, 1, 1, 1))",
      "line_count": 9
    },
    {
      "section": "Simulation Examples",
      "code": "# initialization (Gaussian noise)\nn <- 10000\nx1 <- x2 <- x3 <- x4 <- x5 <- x6 <- rep(0, n + 1)\n\n# instantiation (linear relationships)\nfor (i in 2:(n + 1)) {\n  x1[i] <- 0.8 * sqrt(3)* x1[i-1] -0.9*x1[i-1] + rnorm(1, mean=0, sd=1)\n  x2[i] <- 0.5*x1[i-1]^2 + rnorm(1, mean=0, sd=1)\n  x3[i] <- -0.5*x1[i-1] + rnorm(1, mean=0, sd=1)\n  x4[i] <- -0.5*x1[i-1]^2 + 0.3*sqrt(3)*x4[i-1] + 0.3*sqrt(2)*x5[i - 1] +\n    rnorm(1, mean=0, sd=1)\n  x5[i]<- -0.3*sqrt(3)*x4[i-1] + 0.3*sqrt(3)*x5[i-1] + rnorm(1, mean=0, sd=1)\n  x6[i]<- 0.3*sqrt(3)*x4[i-1]^2 + 0.25*sqrt(3)*x5[i-1]^2 + rnorm(1, mean=0, sd=1)\n}\n\nx1 <- x1[-1]; x2 <- x2[-1]; x3 <- x3[-1]\nx4 <- x4[-1]; x5 <- x5[-1]; x6 <- x6[-1]; \nnl.system <- data.frame(x1, x2, x3, x4, x5, x6)\n\n# library(igraph)\ng <- make_empty_graph(n = 6) %>%\n  # Linear relations in gray\n  add_edges(c(1,1, 1,3, 4,4,  5,4, 4,5, 5,5)) %>%\n  set_edge_attr(\"color\", value = \"gray\") %>%\n  # (Quadratic) non-linear relations in green\n  add_edges(c(1,2, 1,4, 4,6, 5,6), color = \"green\")\n# E(g)[[]]\nplot(g, main=paste(\"Simulated 6-variate process\", \"linear (gray) and Nonlinear (green) relations\", sep=\"\\n\"))",
      "line_count": 28
    },
    {
      "section": "Simulation Examples",
      "code": "#library(future)\n#library(RTransferEntropy)\n\n## Enable multi-core parallel computing\n#plan(multicore)\n\n## Estimate the Granger-causality and the transfer entropy, based on linearGC()\nn = seq(1:ncol(nl.system))\n# apply the GC and TE estimating functions to all possible pairs of columns in the DF\nnl.ff.GC.value <- function(a, b) linearGC(nl.system[,a], nl.system[,b])\nnl.GC.matrix <- outer(n, n, Vectorize(nl.ff.GC.value))\nnl.ff.TE.value <- function(a, b) calc_te(nl.system[,a], nl.system[,b])\nnl.TE.matrix <- outer(n, n, Vectorize(nl.ff.TE.value))\n\nrownames(TE.matrix) <-\n  colnames(TE.matrix)<-var.names<-c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\")\nrownames(GC.matrix) <- colnames(GC.matrix) <- var.names\n\ncorrplot(nl.GC.matrix, method = \"circle\", title = \"Granger (non-lin model)\",\n         tl.cex = 0.9, tl.col = 'black', mar=c(1, 1, 1, 1))\n# note the boost *5 for TE.matrix to artificially enhance the plot (not required)\ncorrplot(5*nl.TE.matrix, method = \"circle\", title = \"Transfer Entropy (non-lin model)\", \n         tl.cex = 0.9, tl.col = 'black', mar=c(1, 1, 1, 1))",
      "line_count": 23
    },
    {
      "section": "Macroeconomic Market Forecasting Example",
      "code": "# install.packages(magrittr)\nlibrary(magrittr)\n\n# load SOCR data\n# install.packages(\"xlsx\")\nlibrary(xlsx)\nUS_data <- read.csv(\"https://umich.instructure.com/files/20026184/download?download_frd=1\", header = TRUE) # Complete sheet=2\n\n# order dates chronologically; as dates is in string format, \nUS_data_ord <- \n  US_data[\n    order(as.Date(US_data$Date, format = \"%m/%d/%Y\")), ]\n\nUS_data_ord$Date <- as.Date(US_data$Date, format = \"%m/%d/%Y\")\nstr(US_data_ord) \n# dim(US_data_ord) # 499  16\nattach(US_data_ord)\n\n# MODEL the data\n# mod <- nls(US_data_ord$DJIA ~ exp(a + b * time_index), start = list(a = 0, b = 0))\n# exp_model_fit <- predict(mod, time_index)\n\n# Fit and plot linear models according to specified predictors and outcomes\nfitPlot_LM_Model <- function (Y, X) {   \n    # Y= outcome column name, e.g., Y=c(DJIA_3m_Shift, NASDAQ_3m_Shift)\n    # X=vector of predictor column names, e.g., \n    # X=c(US_Debt_B, US_GDP_B, Debt2GDP_Pct, US_CapacityUtilizationRate, Jobless_Claims,\n    #        Industrial_Production, Total_Reserves_B, Monetary_Base_B, CoincidentEcon_ActivityInd, UMCSENT)\n    # fitPlot_LM_Model(Y=c(DJIA_3m_Shift, NASDAQ_3m_Shift), X=c(US_Debt_B, US_GDP_B, Debt2GDP_Pct,\n    #  US_CapacityUtilizationRate, Jobless_Claims, Industrial_Production, Total_Reserves_B, Monetary_Base_B,\n    #  CoincidentEcon_ActivityInd, UMCSENT))\n    \n    nX <- length(X)\n    nY <- length(Y)\n    b <- array(dim=nX)  # DJIA model coefficients\n    c <- array(dim=nX)  # NASDAQ model coefficients\n    \n    if (nX < 1 || nY != 2) return (\"Error 1\")\n    if (nX == 1) {\n      formulaDJIA <- paste0(Y[1], \" ~ \", X[1], collapse = \" \")\n      formulaNASDAQ <- paste0(Y[2], \" ~ \", X[1], collapse = \" \")\n    }\n    if (nX > 1) {\n      formulaDJIA <- paste0(Y[1], \" ~ \", X[1], paste0(\" +\", X[-1], collapse = \" \"))\n      formulaNASDAQ <- paste0(Y[2], \" ~ \", X[1], paste0(\" +\", X[-1], collapse = \" \"))\n    }\n    \n    ### Y[1]: DJIA  \n    print(paste0(\"Fitting LM: \", formulaDJIA))\n    # DJIA_3m_Shift ~ US_Debt_B + US_GDP_B + Debt2GDP_Pct + US_CapacityUtilizationRate +\n    # Jobless_Claims + Industrial_Production + Total_Reserves_B+ Monetary_Base_B +\n    # CoincidentEcon_ActivityInd + UMCSENT,\n    modDJIA <- lm(formula = formulaDJIA, data = US_data_ord)\n    summary(modDJIA)\n    coef(modDJIA)\n    \n    # Y[2]: NASDAQ\n    print(paste0(\"Fitting LM: \", formulaNASDAQ))\n    modNASDAQ <- lm(formula = formulaNASDAQ, data = US_data_ord)\n      # lm(NASDAQ_3m_Shift ~ US_Debt_B + US_GDP_B + Debt2GDP_Pct + \n      #               US_CapacityUtilizationRate + Jobless_Claims + Industrial_Production +\n      #               Total_Reserves_B+ Monetary_Base_B + CoincidentEcon_ActivityInd + UMCSENT, \n      #            data = US_data_ord)\n    \n    summary(modNASDAQ)\n    coef(modNASDAQ)\n    \n    for (i in 1:(nX+1)) {\n      b[i] <- round(coef(modDJIA)[i], 2)\n      c[i] <- round(coef(modNASDAQ)[i], 2)\n    }\n    \n    # define the string for the DJIA LM\n    modDJIA_label   <- paste0(\"DJIA ~ \", b[1], paste0(\" +\", b[-1], \"*\", X, collapse = \" \"), \"\\n\\n\")\n    modNASDAQ_label <- paste0(\"NASDAQ ~ \", c[1], paste0(\" +\", c[-1], \"*\", X, collapse = \" \"))\n   \n    modDJIA_NASDAQ_label <- paste0(modDJIA_label, modNASDAQ_label)\n    modDJIA_NASDAQ_label\n    \n    library(plotly)\n    myPlot <- US_data_ord %>% \n      plot_ly(x = ~Date) %>% \n      add_markers(y = ~DJIA, name=\"DJIA Data\", \n                  marker = list(color = \"blue\",\n                                line = list(color = \"blue\", width = 1))) %>% \n      add_trace(x = ~Date, y = fitted(modDJIA), name=\"DJIA Model\", type = \"scatter\", # DJIA\n                mode='lines+markers', marker = list(color = \"orange\",\n                                line = list(color = \"orange\", width = 2))) %>%\n      add_markers(y = ~NASDAQ, name=\"NASDAQ Data\", marker = list(color = \"green\",\n                                line = list(color = \"green\", width = 1))) %>% \n      add_trace(x = ~Date, y = fitted(modNASDAQ), name=\"NASDAQ Model\", type = \"scatter\", # NASDAQ\n                mode='lines+markers', marker = list(color = \"red\",\n                                line = list(color = \"red\", width = 1))) %>%\n      add_trace(x = ~Date, y = ~Recession*30000, type = 'bar',                         # recessions\n                marker = list(color='gray', line = list(color='gray', width=4)),\n                opacity=0.2, name=\"Recessions\", text = \"Recessions\", hoverinfo = 'recessions') %>%\n      layout(title=modDJIA_NASDAQ_label, font=list(size=8))\n    \n    return (myPlot)\n}\n\n#### Run the Full model-fitting and plotting function\nmyPlot <- fitPlot_LM_Model(Y=c(\"DJIA_3m_Shift\", \"NASDAQ_3m_Shift\"), \n                 X=c(\"US_Debt_B\", \"US_GDP_B\", \"Debt2GDP_Pct\", \"US_CapacityUtilizationRate\", \"Jobless_Claims\",\n                     \"Industrial_Production\", \"Total_Reserves_B\", \"Monetary_Base_B\", \n                     \"CoincidentEcon_ActivityInd\", \"UMCSENT\"))\n\nmyPlot\n\n#### Run a Smaller model-fitting and plotting function\nmyPlot1 <- fitPlot_LM_Model(Y=c(\"DJIA_3m_Shift\", \"NASDAQ_3m_Shift\"), \n                 X=c(\"Debt2GDP_Pct\", \"Jobless_Claims\", \"Monetary_Base_B\", \"UMCSENT\"))\n\nmyPlot1",
      "line_count": 114
    },
    {
      "section": "Macroeconomic Market Forecasting Example",
      "code": "# initialization (Gaussian noise)\nlin.system <- US_data_ord[,-1]  # remove date as non-numeric\n\nlibrary(igraph)\n# g<-graph(c(1,13, 1,14, 2,13, 2,14, 3,13, 3,14, 4,13, 4,14,\n#            5,13, 5,14, 6,13, 6,14, 7,13, 7,14, 8,13, 8,14,\n#            9,13, 9,14, 10,13, 10,14, 11,13, 11,14, 12,13, 12,14,\n#            2,10, 5,4, 7,8, 9,4, 13,3, 11,10, 10,10, 7,7), n=17)\n# plot(g, main=\"Exemplary Causal Relations (suggestive)\")\n\ng <- make_empty_graph(n = 17) %>%\n  # Linear relations in gray\n  add_edges(c(1,13, 1,14, 2,13, 2,14, 3,13, 3,14, 4,13, 4,14,\n           5,13, 5,14, 6,13, 6,14, 7,13, 7,14, 8,13, 8,14,\n           9,13, 9,14, 10,13, 10,14, 11,13, 11,14, 12,13, 12,14,\n           2,10, 5,4, 7,8, 9,4, 13,3, 11,10, 10,10, 7,7)) %>%\n  set_edge_attr(\"color\", value = \"gray\") %>%\n  # (Quadratic) non-linear relations in green\n  add_edges(c(5,10, 5,6, 4,6, 5,6, 17,1, 16,5, 15,13), color = \"green\")\n# E(g)[[]]\n# V(g)$label <- colnames(US_data_ord); V(g)[[]]\n# node names\nV(g)$label <- colnames(US_data_ord); V(g)[[]]\nplot.igraph(g, main=paste(\"Exemplary (suggested) Economic Causal Relations\", \n                   \"linear (gray) and Nonlinear (green) relations\", sep=\"\\n\"),\n     layout=layout_with_kk, vertex.label=V(g)$names, edge.arrow.size=0.5,\n     vertex.label.color = \"black\", vertex.size=10)\n\n## Enable multi-core parallel computing\nplan(multicore)\n\n## Estimate the Granger-causality and the transfer entropy, based on linearGC()\nn = seq(1:ncol(lin.system))\n\n# apply the GC and TE estimating functions to all possible pairs of columns in the DF\nff.GC.value <- function(a, b) linearGC(lin.system[,a], lin.system[,b])\nGC.matrix <- outer(n, n, Vectorize(ff.GC.value))\nff.TE.value <- function(a, b) calc_te(lin.system[,a], lin.system[,b])\nTE.matrix <- outer(n, n, Vectorize(ff.TE.value))\n\nstr(TE.matrix)\n\nrownames(TE.matrix) <-\n  colnames(TE.matrix)<-var.names<-colnames(US_data_ord)[-1]\nrownames(GC.matrix) <- colnames(GC.matrix) <- var.names\n\n# Plot the pairwise causal relations \n# Mind artificial boosting (*2 and *10) to stress paired causal effects\ncorrplot(2*GC.matrix, method = \"circle\", title = \"Granger (MacroEcon lin model)\",\n         tl.cex = 0.9, tl.col = 'black', mar=c(1, 1, 1, 1))\n# note the boost *5 for TE.matrix to artificially enhance the plot (not required)\ncorrplot(10*TE.matrix, method = \"circle\", title = \"Transfer Entropy (MacroEcon lin model)\", \n         tl.cex = 0.9, tl.col = 'black', mar=c(1, 1, 1, 1))",
      "line_count": 53
    }
  ]
}