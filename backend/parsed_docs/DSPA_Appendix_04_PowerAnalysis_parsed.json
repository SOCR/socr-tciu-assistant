{
  "metadata": {
    "created_at": "2024-11-30T13:46:17.167391",
    "total_sections": 2,
    "total_code_chunks": 10,
    "total_tables": 2,
    "r_libraries": [
      "Hmisc",
      "WebPower",
      "coin",
      "doParallel",
      "foreach",
      "plotly",
      "pwr",
      "tidyverse"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Appendix: Power Analysis in Experimental Design</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: ../SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false\nIn this [DSPA Appendix section](https://dspa2.predictive.space) we will demonstrate the classical\napproaches for statistical power-analysis and balancing sample-size vs. statistical-power \nto detect effects of interest.",
      "word_count": 76
    },
    {
      "title": "Power Analysis in Experimental Design",
      "content": "## Background\n\n*Power analysis* represents a statistical approach to explicate the relations between a number of parameters that affect most experimental designs. It is well known that data are proxies of the natural phenomena, or processes, about which we try to make inference, and the *size of a sample* is associated with our ability to derive useful information about the underlying process or make predictions about its past, present or future states. In some situations, given the sample-size and a certain degree of confidence, we can compute the **power** to statistically detect an effect of interest. Similarly, we can determine the likelihood of detecting an effect of a certain size, subject to a predefined *level of confidence* and specific sample size constraints. This power, or probability, to detect the effect of interest may be *low*, *medium*, or *high*, which would help us determine the *potential* value of the experiment.\n\nIn most experimental designs, *power analyses* establish a relation between 5 quantities:\n\n - *Statistical test*, an explicit reference to the statistical inference that will be conducted on the data collected by the experiment\n - *Sample size*, there are pros and cons to running large, or small, experiments\n - *Effect size*, how strong is the expected effect that we are trying to uncover by the experiment\n - *Significance level*, false-positive rate $\\alpha=P(Type I error) =$ probability of finding an effect that is not there\n - *Power* = $\\beta=1 - P(Type II error) =sensitivity=$ probability of finding an effect that is there\n\nIn mathematical terms, having any 3 of the last 4 parameters *may* allow us to estimate the last one. Note that there is no general analytical expression that provides an exact closed-form expression (e.g., implicit or explicit function) encoding relation between all 5 terms.\n\n## R-based Power Analysis\n\nThe [R package pwr](https://cran.r-project.org/web/packages/pwr/index.html) provides the core functionality to conduct power analysis for some situations. It includes the following methods:\n\nAs each method explicitly specifies the statistical inference procedure, we need to only specify 3 of the remaining 4 quantities (effect size, sample size, significance level, and power) to calculate the last parameter.\nA common practice is to use the default significance level of $\\alpha=0.05$, and hence we are down to specifying 2 out of 3 remaining parameters. For instance, given an effect size (from prior research or an oracle) and a desired power, we can calculate an appropriate experimental design sample size.\n\nDetermining an effective and appropriate effect size is often a challenge that can be tackled either by running simulations, collecting data, or using Cohen's social-studies protocol, which provides an outline of categorizing the effect size as small, medium or large.\n\n## Cohen's Protocol for categorizing the effect size\n\nLet's look at some examples.\n\n - `pwr.t.test(n = n, d = d, sig.level = a, power = b, type = c(\"two.sample\", \"one.sample\", \"paired\"))`: In this method definition, $n$ is the sample size, $d$ is Cohen's effect size, the desired power is $b$, and  type indicates the specific parametric t-test we choose. \n - `pwr.t2n.test(n1 = n1, n2= n2, d = d, sig.level = a, power = b)`: This is a more general call for unequal sample-sizes $n1$ and $n2$, for independent t-tests. **Cohen's d** characterizes the effect size to three values, $0.2$, $0.5$, and $0.8$ representing *small*, *medium*, and *large* effect sizes, respectively.\n\n - `pwr.anova.test(k = k, n = n, f = f, sig.level = a, power = b)`: A one-way analysis of variance (ANOVA) test with $k$ number of groups, $n$ common sample size within each group, and effect size $f$. **Cohen's f** values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively.\n\n - `pwr.r.test(n = n, r = r, sig.level = a, power = b)`: Correlation coefficient analysis, where $n$ is the sample size and $r$ is the correlation, which uses the population correlation coefficient as a measure of the effect size. **Cohen's r** values of 0.1, 0.3, and 0.5 represent small, medium, and large effect sizes, respectively.\n\n - `pwr.f2.test(u = u, v = v, f2 = f2, sig.level = a, power = b)`: Multivariate linear Models, including multiple linear regression, with $u$, $v$, and $f2 representing the ANOVA numerator and denominator degrees of freedom, and the effect size measure. **Cohen's f2** values of 0.02, 0.15, and 0.35 approximately represent small, medium, and large effect sizes, respectively.\n\n - `pwr.chisq.test(w = w, N = N, df = df, sig.level = a, power = b)`: Chi-square Test with $w$ the effect size, $N$ the total sample size, and $df$ the degrees of freedom.  **Cohen's w** values of 0.1, 0.3, and 0.5 represent small, medium, and large effect sizes, respectively.\n\n## R power calculation examples\n\n### One-way ANOVA \n\nLet's try to run power analysis for a 1-way ANOVA comparing 5 groups. Specifically, we are interested in\nestimating the sample size needed in each group to secure a power $\\beta \\geq 0.80$, given a moderate effect size ($0.25$) and a significance level of 0.025.\n\n\nThis suggests that at least $47$ participants will be required ($n=46.12892$).\n\nWould that sample size estimate increase or decrease when we increase or decrease the effect-size? Inspect the following two examples.\n\n\nFor a 1-way ANOVA test, Cohen's effect size $f$ is categorized as 0.1 (small), 0.25 (medium), and 0.4 (large), but computed by:\n\n$$f=\\sqrt{\\frac{\\sum_{i=1}^k{p_i\\times(\\mu_i-\\mu)^2}}{\\sigma^2}},$$\nwhere $n$ is the total number of observations in all groups, $n_i$ is the number of observations in group $i$, $p_i=\\frac{n_i}{n}$, $\\mu_i$ and $\\mu$ are the group $i$ and overall means, and $\\sigma^2$ is the within-group variance. Similar analytical expressions exist for other statistical tests and there are corresponding sample-driven estimates of these effects that can be used for the practical calculations. \n\n###  Two-sample T-test\n\nLet's run power analysis for a two-sample, one-sided, T-test using a significance level of $\\alpha=0.001$, $n=30$ participants per group, and a large effect size of $0.8$.\n\n\nThis yields a power of $\\beta = 0.4526868$ to detect an effect.\n\n\n## Power and Sample Size Graphs\n\nThe `pwr` package also provides some functions to generate power and sample size plots. \n\n### Correlation Test\nFor instance, we can plot sample-size vs. effect-size curves for the power of detecting different levels of correlations, $0.1\\leq \\rho\\leq 0.8$, for a number of power values, $0.3\\leq\\beta\\leq 0.85$.\n\n\nIn this case, we can also plot the sample-size against power. This graph indicates the *optimal* sample-size selection that achieves the lower-bound of the power ($0.8$) for the minimal sample-size ($n=20$).\n\n\n### Cohen's $d$ measure of effect size \n\nThe Cohen's $d$ measure is used to quantify the *standardized difference between two means.* \nIt is often used in the context of *t-tests* and *ANOVA* to quantify the magnitude\nof a *treatment effect* or the difference between two groups. SYmbolically, Cohen's $d$ is\n\n$$d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p}, $$\n\nwhere $\\bar{X}_1$ and $\\bar{X}_2$ are the *means* of the two groups, and $s_p$\nis the *pooled standard deviation*\n\n$$s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}, $$\n\nwhere $n_1$ and $n_2$ are the *sample sizes* of the two groups, respectively, and \n$s_1$ and $s_2$ are the *standard deviations* of the groups, respectively.\n\nWhen the sample sizes of the two groups are equal, the pooled standard deviation \n$s_p$ simplifies to the average of the two standard deviations\n\n$$s_p = \\sqrt{\\frac{s_1^2 + s_2^2}{2}}$$\n\nand the Cohen's $d$ is $d = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2 + s_2^2}{2}}}$.\n\nInterpretation pf $d$ always needs to be contextualized, but here is a general guideline:\n\n- Small effect: $d = 0.2$\n- Medium effect: $d = 0.5$\n- Large effect: $d = 0.8$\n\n\n###  Two paired samples T-test\n\nLet's run power analysis for a two paired samples T-test using a significance level of $\\alpha=0.05$, $n$ participants per group, and $power=0.8$, under differetn *effect-sizes*.\n\n\nThis yields a power of $\\beta = 0.4526868$ to detect an effect.\n\n###  Power Analysis using hte Nonparametric Kruskal-Wallis Statistics\n\nIn the above mouse experimental example, we made Normal parametric assumptions. \nIn this example we will relax these and rely on the nonparametric Kruskal-Wallis test.\nThe Kruskal-Wallis test is a nonparametric method for testing whether samples \n(e.g., control vs. intervention) originate from the same distribution. \n\nAs there isn't a direct power calculation function for the Kruskal-Wallis test, \nwe will need to use a simulation approach for *estimating* the required sample size.\n\n\n*Notes*:\n\n - **Preprocessing**: First, load and clean the data.\n - Next, run the **Kruskal-Wallis Test** on the dataset.\n - The **Power Analysis** relies on a simulation approach to *estimate* the required sample size necessary to achieve the desired power for the Kruskal-Wallis test.\n - Rationally adjust the *effect size* (needs to be backed up by prior literature reports!) to ensure accurate power analysis is reported.\n\n\n### Multivariate Linear Regression (MLR)\n\nSimilarly, we can plot a sample-size vs. effect-size curve for a multivariate linear model of the efficacy of [Argus retinal prosthesis (treatment)](https://en.wikipedia.org/wiki/Argus_retinal_prosthesis) to enhance brain plasticity in the visual cortex. Suppose we are trying to determine the relation between the smallest possible sample size that can yield $\\beta$ sensitivity to detect an a change in the functional connectivity (fMRI data) and in [Argus II blind patients](https://en.wikipedia.org/wiki/Argus_retinal_prosthesis). These two articles provide some background information and support for the range of effect-sizes used in this example:\n\n - Samantha Cunningham; Yonggang Shi; James D. Weiland; Paulo Falabella; Lisa C. Olmos de Koo; David N. Zacks; Bosco Tjan (2015) [Investigate alteration in functional connectivity and cross-modal plasticity in Argus II patients](https://tvst.arvojournals.org/article.aspx?articleid=2474808).\n - Samantha Cunningham; James D. Weiland; Pinglei Bao; Gilberto Raul Lopez-Jaime; Bosco Tjan. (2015) [Correlation of vision loss with tactile-evoked V1 responses in retinitis pigmentosa](https://doi.org/10.1016/j.visres.2014.10.015).\n\nThe research goal is to model the outcome $Y$ in terms of eight (8) covariates $X_i$:\n\n - *Outcome*: $Y$ representing an event-related functional magnetic resonance imaging (fMRI) scan of blind using an Argus II vision-enhancing device,\n - *Covariates*: Eight $X=\\{X_i\\}_{i=1}^{u=8}$, including fMRI event-related task, duration of device use, participant cohort (e.g., retinitis pigmentosa), demographic characteristics (e.g., gender), clinical assessments (e.g., visual acuity), etc. \n \nAs shown in [Chapter 9 (Linear Modeling)](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/09_RegressionForecasting.html), the matrix form of the linear inference model, $Y=X\\beta+\\epsilon$, can also be explicated to:\n \n$$Y=\\beta_o+\\beta_1 X_1+\\beta_2 X_2+...+\\beta_u X_u+ \\epsilon.$$\n\n\nFor a simpler balanced 5-group ANOVA test, we can plot the sample-size against power. This graph indicates the *optimal* sample-size selection that achieves the lower-bound of the power ($0.8$) for the minimal sample-size ($n=20$).\n\n\n\n## Example: Clustered-Design Power Analysis\n\nSuppose we are interested in estimating the relation between sample-size and statistical-power  (power analyses) for a clustered study design within 12 units (sites). Assume the Intra-class correlation of $ICC=0.01$, and the proposed study design involves 2 steps and 40 participants per cluster (site). At each step, conditioning on enrolling four sites (clusters), we want to ensure 76% statistical power (based on a two-sided test, alpha=0.05) for detecting between-site effects, we get 80% statistical power.\n\nUsing the [WebPower package](https://cran.r-project.org/web/packages/WebPower/index.html) and [WebPower manual/tutorial](https://webpower.psychstat.org/wiki/_media/grant/webpower_manual_book.pdf), we can estimate the sample-size corresponding with the desired power using the R function `wp.crt2arm`. The interface of this function involves: \n\n - *n*: sample size\n - *f*: effect size\n - *J*: number of clusters/sites\n - *icc*: Intra-class correlation\n - *alpha*: significance level\n - *power*: statistical power\n - *alternative*: two-sided or one-sided analysis\n\n`wp.crt2arm(n=NULL, f=NULL, J=NULL, icc=NULL, power=NULL, alternative = c(\"two.sided\", \"one.sided\"), alpha=0.05)`\n\nThe R inputs and outputs for several examples are given below:\n\n* *Case 1* (power=0.72): Total number of participants $n=2\\times 4\\times 40$, $icc=0.01$, and $f=0.5$.\n* *Case 2*: (power=0.8): Total number of participants $n=2\\times 4\\times 40$, $icc=0.01$, and $f=0.65$.\n\n\nNote the strong effects of the parameters ICC, n, and effect-size, and number of sites. \n\n## Summary\n\nPower analysis is an important statistical computing technique to design effective research studies based on the collection and interrogation of observational data. Power is the sensitivity or the probability of detecting a *true* effect when it exists. There is no perfect analytical expression (formula) determining the relation between sample size, effect size, and power for all possible research situations. In practice, assumptions and empirical evidence may be used to approximate this unknown relation even when for complex study designs.\n\nIn practice, all sample size calculations are based on many assumptions. For instance, calculating the sample-size/power relation for a one-way ANOVA test requires normality assumptions for each group as well as variance [homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity), equal variances, for all the groups. However, reasonable violations of core assumptions may still generate rough approximations of the expectation of the sensitivity for a test. Exact knowledge of the magnitude of effect size is also rarely tractable, however it can often be estimated from analogous processes, prior observations, or by other techniques. Practicing statisticians tend to use more conservative assumptions when estimating sample-size/power relations.\n\nThere are no closed-form expressions to estimate the *power-size-effect relationships* for non-parametric models and most model-free machine learning techniques. In such situations, two complementary approaches may be utilized to ensure reliable analytics and reproducible inference. The first approach is based on identifying an approximate *power-size-effect relationship* for another analogous statistical test, which is based on a parametric model, compute the power-size-effect relationship and use it cautiously as a rough guide of the relation for the corresponding machine learning technique. An alternative approach is to employ resampling methods to confirm these affect relationships via [internal statistical cross-validation](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/20_PredictionCrossValidation.html).\n\n\n## References\n\n - Cohen, J. 1988. [Statistical Power Analysis for the Behavioral Sciences](https://www.elsevier.com/books/statistical-power-analysis-for-the-behavioral-sciences/cohen/978-0-12-179060-8), Mahwah, NJ: Lawrence Erlbaum Associates.\n - [SMHS EBook Bayesian Inference Chapter](https://wiki.socr.umich.edu/index.php/SMHS_BayesianInference)\n - Che, Annie, Cui, Jenny, and Dinov, Ivo (2009). [SOCR Analyses: Implementation and Demonstration of a New Graphical Statistics Educational Toolkit](https://www.jstatsoft.org/v30/i03), JSS, Vol. 30, Issue 3, Apr 2009.\n - Che, A, Cui, J, and Dinov, ID (2009) [SOCR Analyses - an Instructional Java Web-based Statistical Analysis Toolkit](https://jolt.merlot.org/vol5no1/dinov_0309.htm), JOLT, 5(1), 1-19, March 2009.\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 2353
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "      smooth_scroll: true\n---",
      "row_count": 2
    },
    {
      "section": "Power Analysis in Experimental Design",
      "content": "function \t| Corresponding Statistical Inference\n----------|------------------------------------\n*cohen.ES* | Conventional effects size\n*ES.h* |\tEffect size calculation for proportions\n*ES.w1*\t| Effect size calculation in the chi-squared test for goodness of fit\n*ES.w2* |\tEffect size calculation in the chi-squared test for association\n*pwr.2p.test* |\tTwo proportions test (equal sample sizes, n)\n*pwr.2p2n.test* |\tTwo proportions (unequal n)\n*pwr.anova.test* |\tBalanced one way ANOVA\n*pwr.chisq.test* |\tChi-square test\n*pwr.f2.test* |\tGeneral linear model (GLM)\n*pwr.norm.test* |\tPower calculations for the mean of a normal distribution (known variance)\n*pwr.p.test* |\tSingle sample proportion \n*pwr.r.test* |\tCorrelation\n*pwr.t.test* \t| T-tests (one sample, 2 sample, paired)\n*pwr.t2n.test* |\tT-test (two samples with unequal n), t-tests of means",
      "row_count": 16
    }
  ],
  "r_code": [
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# install.packages(\"pwr\")\nlibrary(pwr)\n\npwr.anova.test(k=5, f=0.25, sig.level=0.025, power=0.8)",
      "line_count": 4
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# install.packages(\"pwr\")\n# library(pwr)\n\npwr.anova.test(k=5, f=0.1, sig.level=0.025, power=0.8) # small effect-size\npwr.anova.test(k=5, f=0.4, sig.level=0.025, power=0.8) # large effect-size",
      "line_count": 5
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# install.packages(\"pwr\")\n# library(pwr)\n\npwr.t.test(n=30, d=0.8, sig.level=0.001, alternative=\"greater\") # large effect-size",
      "line_count": 4
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# install.packages(\"pwr\")\n# library(pwr)\n\nr <- seq(0.1, 0.8, 0.01) # define a range of correlations and sampling rate within this range\nnr <- length(r)\n\np <- seq(0.3, 0.85, 0.1) # define a range for the power values, and their sampling rate\nnp <- length(p)\n\n# Compute the corresponding sample sizes for all combinations of correlations and power values\nsampleSize <- array(numeric(nr*np), dim=c(nr, np))\nfor (i in 1:np) {\n  for (j in 1:nr) {\n    # solve for sample size (n)\n    testResult <- pwr.r.test(n = NULL, r = r[j], sig.level = 0.05, power = p[i], alternative = \"two.sided\")\n    sampleSize[j, i] <- ceiling(testResult$n) # round sample sizes up to nearest integer\n    # print(sprintf(\"sampleSize[%d,%d]=%s\", j,i, round(sampleSize[j, i], 2)))\n  }\n}\n\n# Graph the power plot\nxRange <- range(r)\nyRange <- round(range(sampleSize))\ncolors <- rainbow(length(p))\nplot(xRange, yRange, type=\"n\", xlab=\"Correlation Coefficient (r)\", ylab=\"Sample Size (n)\")\n# Add power curves\nfor (i in 1:np) lines(r, sampleSize[ , i], type=\"l\", lwd=2, col=colors[i])\n# add annotations (grid lines, title, legend)\nabline(v=0, h=seq(0, yRange[2], 100), lty=2, col=\"light gray\")\nabline(h=0, v=seq(xRange[1], xRange[2], 0.1), lty=2, col=\"light gray\")\ntitle(\"Effect-size (X) vs. Sample-size (Y) for \\n different Power values in \n      (0.3, 0.85), Significance=0.05 (Two-tailed Correlation Test)\")\nlegend(\"topright\", title=\"Power\", as.character(p), fill=colors)",
      "line_count": 33
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "p.out <- pwr.r.test(n = NULL, r = r[50], sig.level = 0.05, power = p[6], alternative = \"two.sided\")\nplot(p.out, lwd=2)",
      "line_count": 2
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# install.packages(\"pwr\")\n# library(pwr)\n# install.packages(\"tidyverse\")\n# install.packages(\"pwr\")\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(plotly)\n\n# Load the data\n# data <- read.csv(\"seizure_burden_pilot.csv\")\ndata <- read.csv(\"https://umich.instructure.com/files/36068854/download?download_frd=1\")\n\n# Replace non-numeric values with NA\ndata[data == \".\"] <- NA\n\n# Separate the data into control and treatment groups\ncontrol_data <- data %>% filter(Condition == \"control\")\ncontrol_data <- control_data[,-2] %>% mutate_if(is.character, as.numeric)\n\ntreatment_data <- data %>% filter(Condition == \"treatment\")\ntreatment_data <- treatment_data[,-2] %>% mutate_if(is.character, as.numeric)\n\n# Calculate the mean and standard deviation of differences\ndifferences <- treatment_data[,-c(1,2)] - control_data[,-c(1,2)]\nmean_diff <- colMeans(differences, na.rm = TRUE)\nsd_diff <- apply(differences, 2, sd, na.rm = TRUE)\n\n# Compute the overall mean difference and standard deviation\noverall_mean_diff <- mean(mean_diff)\noverall_sd_diff <- mean(sd_diff)\n\n# Calculate effect size (Cohen's d)\neffect_size <- overall_mean_diff / overall_sd_diff\n\n# Perform power analysis to determine sample size\nsample_size <- pwr.t.test(d = effect_size, sig.level = 0.05, power = 0.80, type = \"paired\")\n\n# Print the required sample size\nsample_size\n\n# Run the gammut of effect-sizes\neffect_size <- c(0.2, 0.5, 0.7, 0.9, 1.1)  # Placeholder, adjust based on your data or literature\nsample_size <- rep(x=0, times=length(effect_size))\n\nfor(i in 1:length(effect_size))  {\n   sample_size[i] = ceiling(pwr.t.test(d = effect_size[i], sig.level = 0.05, power = 0.80, type = \"paired\")$n)\n}\n\nplot_ly(x=effect_size, y=sample_size, type=\"scatter\", mode=\"markers+lines\") %>%\n  layout(title = 'Parametric T-Statistic \\n Power=0.8, Effect-size (x-axis) vs. Number of Cases (y-axis)',\n         xaxis = list(title = '(assumed) effect-size'), yaxis = list(title = '(estim.) N'))",
      "line_count": 51
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# Protocol\n# 1. **Load Data and Preprocess**:\n#    - Read the CSV file.\n#    - Replace non-numeric values with NA.\n#    - Separate the data into control and treatment groups.\n# \n# 2. **Perform Kruskal-Wallis Test**:\n#    - Use the `kruskal.test` function to perform the test.\n# \n# 3. **Power Analysis**:\n#    - Use the `pwr` package or a suitable method for non-parametric tests to determine the required sample size.\n\n# Install and load necessary packages\n# install.packages(\"tidyverse\")\n# install.packages(\"coin\")  # For non-parametric tests\n# install.packages(\"Hmisc\")  # For power analysis of non-parametric tests\n\nlibrary(tidyverse)\nlibrary(coin)\nlibrary(Hmisc)\nlibrary(foreach)\nlibrary(doParallel)\nlibrary(plotly)\n\n# Load the data\n# data <- read.csv(\"seizure_burden_pilot.csv\")\ndata <- read.csv(\"https://umich.instructure.com/files/36068854/download?download_frd=1\")\n\n# Replace non-numeric values with NA\ndata[data == \".\"] <- NA\ndata1 <- data[, -2] %>% mutate_if(is.character, as.numeric)\ndata1$Condition <- data$Condition\n\n# Reshape the data for Kruskal-Wallis test\ndata_long <- data1 %>%\n  pivot_longer(cols = starts_with(\"Mouse\"), names_to = \"Mouse\", values_to = \"Seizures\") %>%\n  filter(!is.na(Seizures))\n\n# Perform Kruskal-Wallis test\nkruskal_test <- kruskal_test(Seizures ~ as.factor(Condition) | as.factor(Mouse), data = data_long)\nprint(kruskal_test)\n\n# Power analysis using simulation for Kruskal-Wallis test\n# Define a function to simulate data and perform the Kruskal-Wallis test\nsimulate_power <- function(n, effect_size, n_sim = 1000, alpha = 0.05) {\n  p_values <- replicate(n_sim, {\n    # Simulate control and treatment groups\n    control <- rnorm(n, mean = 0, sd = 1)\n    treatment <- rnorm(n, mean = effect_size, sd = 1)\n    \n    # Combine data into a data frame\n    sim_data <- data.frame(\n      Condition = rep(c(\"control\", \"treatment\"), each = n),\n      Seizures = c(control, treatment)\n    )\n    \n    # Perform Kruskal-Wallis test\n    test_result <- kruskal.test(Seizures ~ Condition, data = sim_data)\n    return(test_result$p.value)\n  })\n  \n  # Calculate the proportion of p-values less than alpha\n  power <- mean(p_values < alpha)\n  return(power)\n}\n\n# Define effect size (you may need to estimate this from the data or literature)\neffect_size <- c(0.2, 0.5, 0.7, 0.9, 1.1)  # Placeholder, adjust based on your data or literature\nsample_size <- rep(x=0, times=length(effect_size))\n\n# Set up parallel processing\ncl <- makeCluster(detectCores() - 3)  # Use one less core than available\nclusterExport(cl, c(\"simulate_power\"))\n\n# Estimate the sample size needed for desired power (e.g., 0.80)\n# for (it in 1:length(effect_size)) {\n#   desired_power <- 0.80\n#   # sample_size[it] <- NULL\n#   for (n in seq(10, 100, by = 5)) {\n#     power <- simulate_power(n, effect_size[it])\n#     if (power >= desired_power) {\n#       sample_size[it] <- n\n#       break\n#     }\n#   }\n# }\n\n# Function to find sample size for a given effect size\nfind_sample_size <- function(effect_size, desired_power=0.80, upperLimit=100, step=5) {\n  for (n in seq(10, upperLimit, by = step)) {\n    power <- simulate_power(n, effect_size)\n    if (power >= desired_power) {\n      return(n)\n    }\n  }\n  return(NA)  # If no suitable sample size found\n}\n\n# Pilot Test: find_sample_size(0.5) # find_sample_size(0.2, upperLimit=600, step=10, desired_power=0.8)\n\n# For speed, parallelize the outer loop\n# setup parallel backend to use many processors\ncl <- makeCluster(detectCores() - 3)  # Use one less core than available\nregisterDoParallel(cl)\n\nsample_size <- foreach(i=1:length(effect_size), .combine=cbind) %dopar% {\n   t = find_sample_size(effect_size[i], upperLimit=(1+length(effect_size)-i)*200, \n                        step=(1+length(effect_size)-i)*10, desired_power=0.8)\n   t \n}\n#stop cluster\nstopCluster(cl)\n\n# Print the estimated sample size\nprint(as.numeric(sample_size[1,]))\n\nplot_ly(x=effect_size, y=as.numeric(sample_size[1,]), type=\"scatter\", mode=\"markers+lines\") %>%\n  layout(title=\"Nonparametric Kruskal-Wallis Statistic\\n Power=0.8: Effect-size (x-axis) vs. Number of Cases (y-axis)\",\n         xaxis=list(title=\"(assumed) effect-size\"), yaxis=list(title=\"(estim. N)\"))",
      "line_count": 119
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "# install.packages(\"pwr\")\n# library(pwr)\n\nf2 <- seq(0.2, 4, 0.02) # define a range of effect-sizes and sampling rate within this range\nnf2 <- length(f2)\n\np <- seq(0.3, 0.85, 0.1) # define a range for the power values, and their sampling rate\nnp <- length(p)\n\n#`pwr.f2.test(u = u, v = v, f2 = f2, sig.level = a, power = b)`: Multivariate linear Models, including multiple linear regression, with $u$, $v$, and $f2 representing the ANOVA numerator and denominator degrees of freedom, and the effect size measure.\n# Cohen's f2 values of 0.02, 0.15, and 0.35 approximately represent small, medium, and large effect sizes, respectively.\n\n# Compute the corresponding sample sizes for all combinations of correlations and power values\nsampleSize <- array(numeric(nf2*np), dim=c(nf2, np))\nfor (i in 1:np) {\n  for (j in 1:nf2) {\n    # solve for sample size (v), assuming we use u=8 predictors (X) explaining the outcome (Y)\n    testResult <- pwr.f2.test(u = 8, v = NULL, f2 = f2[j], sig.level = 0.05, power = p[i])  # num-covariates=8\n    sampleSize[j, i] <- ceiling(testResult$v) # extract and round sample sizes up to nearest integer\n    # print(sprintf(\"sampleSize[%d,%d]=%s\", j,i, round(sampleSize[j, i], 2)))\n  }\n}\n\n# Graph the power plot\nxRange <- range(f2)\nyRange <- round(range(sampleSize))\ncolors <- rainbow(length(p))\nplot(xRange, yRange, type=\"n\", xlab=\"Effect-size\", ylab=\"Sample Size (u)\")\n# Add power curves\nfor (i in 1:np) lines(f2, sampleSize[ , i], type=\"l\", lwd=2, col=colors[i])\n# add annotations (grid lines, title, legend)\nabline(v=0, h=seq(0, yRange[2], 10), lty=2, col=\"light gray\")\nabline(h=0, v=seq(xRange[1], xRange[2], 0.5), lty=2, col=\"light gray\")\ntitle(\"Effect-size vs. Sample-size for \\n different Power values in \n      (0.3, 0.8), Significance=0.05 (MLR)\")\nlegend(\"topright\", title=\"Power\", as.character(p), fill=colors)",
      "line_count": 36
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "p.out <- pwr.anova.test(k=5, f=0.4, sig.level=0.025, power=0.8) # large effect-size\nplot(p.out, lwd=2)",
      "line_count": 2
    },
    {
      "section": "Power Analysis in Experimental Design",
      "code": "## calculate power given sample size and effect size\n## install.packages(\"WebPower\", lib=\"C:/Users/Dinov/Documents/R/R-4.0.2/library\")\nlibrary(WebPower)\n# wp.crt2arm(f=0.6,n=20,J=10,icc=.1)\nwp.crt2arm(f=0.5, n=320, J=4, icc= 0.03)\n\nwp.crt2arm(f=0.65, n=320, J=4, icc= 0.01)",
      "line_count": 7
    }
  ]
}