{
  "metadata": {
    "created_at": "2024-11-30T13:46:16.875576",
    "total_sections": 3,
    "total_code_chunks": 2,
    "total_tables": 1,
    "r_libraries": []
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Appendix: Information Theory and Statistical Learning</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: ../SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false",
      "word_count": 52
    },
    {
      "title": "Information Theory and Statistical Learning",
      "content": "## Summary \nMachine learning relies heavily on entropy-based (e.g., Renyi-entropy) information theory and kernel-based methods. For instance, Parzen-kernel windows may be used for estimation of various probability density functions, which facilitates the expression of information theoretic concepts as kernel matrices or statistics, e.g., mean vectors, in a Mercer kernel feature space. The parallels between machine learning and information theory allows the interpretation and understanding of computational methods from one field in terms of their dual representations in the other.\n\n\nMachine learning (ML) is the process of data-driven estimation (quantitative evidence-based learning) of *optimal* parameters of a model, network or system, that lead to output prediction, classification, regression or forecasting based on a specific input (prospective, validation or testing data, which may or may not be related to the original training data). Parameter optimality is tracked and assessed iteratively by a learning criterion depending on the specific type of ML problem. Classical learning assessment criteria, including mean squared error (MSE), accuracy, $R^2$, see [Chapter 9](https://socr.umich.edu/DSPA2/DSPA2_notes/09_ModelEvalImprovement.html), may only capture low-order statistics of the data, e.g., first or second order.  Higher-order learning criteria enable solving problems where sensitivity to higher-moments is important (e.g., matching skewness or kurtosis for non-linear clustering, classification, dimensionality reduction). \n\nThis Figure provides a schematic workflow description of machine learning.\n![](https://wiki.socr.umich.edu/images/5/5c/DSPA_Appendix2_Fig1.png)\n\nFigure: Prior knowledge (e.g., initialization parameters) are fed into the machine learning system, along with batches of data (input samples) to generate outputs (e.g., regression, classification, clustering, etc.). At each iteration, the outputs, which depend on the current parameters and input data, are compared against known or expected results and the discrepancies are evaluated leading to a feedback loop which recursively attempts to improve these assessment quality measures. When more (training) data is available, multiple batches of data may be provided to fine-tune the system and reduce the in-bag error rate, i.e., fit the model better to the data. Learning the optimal parameters is equivalent to minimizing the error rate of the model at each iteration. The correction feedback guides the optimization of the parameters. \n\nInformation-theoretic learning (ITL) utilizes assessment criteria that are defined in terms of information theoretic functions defined on the probability densities (pdf's) of the input or output datasets, which describe completely the statistical properties of the data.\n\nAn example of an information-theoretic criteria is the entropy of a pdf, $H(p)$, where  $p(x)$ is a specific pdf. If the expectation of a process is $E$, then the entropy is the expected value of the logarithm of the probability \n\n$$H(p)=E(-\\ln (p(x)).$$\n\nWidely varying pdf's, like the Gaussian distribution, have large entropy, and tight pdfs, like Cauchy distribution, tend to have lower entropy measures.\n\nFor example, the Gaussian density is:\n\n$$p(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}.$$\n\nThen, \n$$H(p)= E(-\\ln (p(x)))=-\\int_{-\\infty}^{\\infty}{p(x)\\ln(p(x))dx}=$$\n$$=\\frac{1}{2}\\ln(2\\pi\\sigma^2)\n\\int_{-\\infty}^{\\infty}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}dx} +\n\\frac{1}{2 \\sigma^2}\\int_{-\\infty}^{\\infty}{(x-\\mu)^2\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}dx} =$$\n$$=\\frac{1}{2}\\ln(2\\pi\\sigma^2) + \\frac{1}{2} =\n\\frac{1}{2}\\ln(2 e \\pi\\sigma^2).$$\n\nWhereas the entropy of the Cauchy distribution,\n$q(x) = \\frac{1}{1+x^2}$, is constant:\n\n$$H(q)= \\int_{-\\infty}^{\\infty}{\\frac{\\ln(1+x^2)}{1+x^2}dx}.$$\n\nMaking the substitution $x=\\tan(\\theta)$ yields $dx=\\sec^2(\\theta)d\\theta$, and\n\n$$H(q) = -2 \\int_{-\\pi/2 }^{\\pi/2}{\\ln(\\cos(\\theta))d\\theta} = \n-4 \\int_{0}^{\\pi/2}{\\ln(\\cos(\\theta))d\\theta} = -4A,$$\nwhere $A=\\int_{0}^{\\pi/2}{\\ln(\\cos(\\theta))d\\theta}$.\n\nHowever, if\n$A'=\\int_{0}^{\\pi/2}{\\ln(\\sin(\\theta))d\\theta}$, then $A==A'$.\n\nThus, $2A = -\\frac{\\pi}{2}\\ln(2)+ \\int_{0}^{\\pi/2}{\\ln(\\sin(2\\theta))d\\theta}$. \n\nA change of variables in the last integral, $\\rho = 2\\theta$, leads to:\n\n$$2A = -\\frac{\\pi}{2}\\ln(2)+{\\frac{1}{2}(A+A)} = -\\frac{\\pi}{2}\\ln(2)+A.$$\n\nTherefore, $A= -\\frac{\\pi}{2}\\ln(2)$, and thus,\n\n$$H(q)=-4A = 2\\pi\\ln(2).$$\n\n[See more details here](https://stats.stackexchange.com/questions/99986/entropy-of-cauchy-lorentz-distribution).\n\nSimilarly, for an n-dimensional Gaussian\n$$ P(X) = \\prod_{i=1}^n{\\sqrt{2 e \\pi\\sigma_i^2} \\left (e^{\\frac{-(x_i-\\mu_i)^2}{2\\sigma_i^2}} \\right ) }, $$\n\nthe corresponding entropy is\n\n$$H(P) = \\frac{n}{2}\\ln\\left (2 e \\pi \\prod_{i=1}^n{\\sigma_i^2}\\right ).$$\n\nThe [Renyi entropy](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy) is a \ngeneralized form of the entropy, which is not unique, but\nrepresents a one-parameter operator (indexed by $\\alpha$)\n\n$$H_{\\alpha}(p) = \\frac{1}{1-\\alpha}\\ln \\left (\\int{p^{\\alpha}(x)dx}\\right ).$$\n\n * For $\\alpha=0$, $H_o$ is the Hartley entropy representing the logarithm of the cardinality of $X$.\n\n$$H_o(X) = \\ln(n) = \\ln(| X |).$$\n\n * For $\\alpha=1$, this reduces to the classical Shannon's entropy, \n\n * For $\\alpha=2$, the quadratic Renyi entropy corresponds to information-theoretic learning (ITL) and can be used as an objective function in supervised neural network training. For samples from the set D = $\\{x_1,..., x_N\\}$, generated from a pdf p(x), the pdf may be approximated using Parzen window density estimation.\n\nParzen-window density estimates are defined by: \n\n$$P(x)=\\frac{1}{n}\\sum_{i=1}^n{\\frac{1}{h_{d,n}\\times K\\left ( \\frac{x-x_i}{h_n}\\right )}}, $$\n\nWhere $K(y)\\ge 0$ is an n-dimensional smoothing kernel (window) function or kernel, i.e., $\\int_{R^d}{K(y)dy}$, $h_n$ is the bandwidth (window width) parameter that corresponds to the full-width at half-maximum of the kernel, which typically depends on the number of observations, $n$. Note that the kernel function is also itself a pdf, guaranteeing that the estimated function $P(x)$ remains a pdf, as well. \n\nThe quadratic Renyi entropy may be used as an objective function in supervised neural networks learning, see [Chapter 14](https://socr.umich.edu/DSPA2/DSPA2_notes/14_DeepLearning.html). Briefly, in supervised learning, the input training data is paired with labels, $\\{x_t, d_t\\},\\ t = 1,...,n$, where $d_t$ is the class label associated with the input $x_t$ . The output label generated by the neural network, $y_t$  for each input $x_t$ is then compared to $d_t$, based on the residuals $e_t  = d_t - y_t$ , using some assessment criterion (e.g., MSE). \n\nRather than minimizing the mean residual square error, like in MSE assessment, the Renyi quadratic entropy minimizes the error pdf, $p(e)$\nwith respect to the (current) neural network weights. A gradient descent  optimization may be used to approximate $p(e)$ by a Parzen window estimation. \n\n## Comparing PDFs\nThere are many statistical approaches to compare two pdfs. e.g., [quantile-quantile plots]( https://en.wikipedia.org/wiki/Q-Q_plot), [Kolmogorov-Smirnov test (KS test)]( https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test).\n\n The *divergence measure* provides an information theoretic approach to quantity is the similarity between two or more pdf's. Let $\\{p_1(x),..., p_k (x)\\}$ represent $k$ pdf functions. Then their divergence measure is:\n\n$$ D(p_1,..., p_k )$$\nAnd it measures the *distance between the pdf's*. $D=0$ only when $p_1(x)= ??? = p_k (x)$ and $D$ increases with the increase of the similarity of the pdf functions. There are many alternative divergence measures.\n\nThe most well-known is [Kullback-Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback-Leibler_divergence), which is based on the Shannon entropy. There are several divergence measures based on Renyi's quadratic entropy that utilize the Parzen window estimation. \n\nThe *Cauchy-Schwarz (CS)* divergence is an example of a Renyi's quadratic entropy and is successfully applied for non-linear clustering, or partitioning of data into groups, or clusters, which include objects that are more similar within a group and the differences between objects in different groups are amplified. The CS clustering partitions the input data to maximize the between-cluster divergence relative to within-cluster variability. That is, CS divergence aims to increase $D(p_1(x),..., p_k (x))$, where $p_i(x)$ represents the pdf associated with the $k$ clusters $D_i$. The cluster partitions represent the group weights, the cluster pdfs encode the output labels, and the divergence, $D$, represents the *learning criterion*. \n\nThe Cauchy-Schwarz Inequality for symmetric kernels $K(x,z)=\\langle \\Phi(x), \\Phi(z)\\rangle =\\langle\\Phi(z), \\Phi(x)\\rangle=K(z,x)$:\n\n$$K^2(x,z) =\\langle\\Phi(x), \\Phi(z)\\rangle^2 \\leq ||\\Phi(x)||^2 ||\\Phi(z)||^2=$$\n$$\t= \\langle \\Phi(x), \\Phi(x)\\rangle \\times \\langle \\Phi(z), \\Phi(z)\\rangle\n= K(x,x)\\times K(z,z).$$\n\nThe output pdfs are approximated by Parzen window estimates $p_1(x),..., p_k(x)$ to enable evaluation of the CS divergence measure by $D_{CS}( p_1(x),..., p_k(x))$. The process maximizes the dissimilarity between the clusters by increasing the dissimilarity between the corresponding cluster pdfs:\n\n$$\\arg \\max_{D_1,...,D_k } D_{CS}( p_1,..., p_k).$$\n\nKernel methods essentially allow non-linear mapping of the input\ndata $x_t \\in D$ into a higher (could be infinite) dimensional space, where the kernel-filtered data are represented by $\\Phi(x_t ),\\  t = 1,..., n$.  \n\nNext, the machine learning algorithm is trained on the kernel-mapped data. The [Cover's theorem]( https://en.wikipedia.org/wiki/Cover%27s_theorem) suggests that the mapped data set is more likely to be *linearly separable* compared to the original data, which may only be non-linearly separable.\n\n\nSuppose we are trying to optimize an objective function:\n$$W(\\alpha) = \\sum_{i=1}^m{\\alpha_i} -\\frac{1}{2}\\sum_{i,j=1}^m{\\alpha_i \\alpha_j y_i y_j \\langle x_i,x_j\\rangle},$$\n\nSubject to $\\alpha_i\\ge 0$ and $\\sum_{i=1}^m{\\alpha_i y_i} =0$.\n\nWhen the input data is *not-linearly separable*, we can $\\Phi$-transform the data into a new space where the data becomes linearly separable. \n\n$$\\Phi:\\mathbb{R}^2 \\longrightarrow \\mathbb{R}^3$$\n$$(x_1,x_2) \\longrightarrow \\Phi(x_1,x_2) = (\\phi_1, \\phi_2, \\phi_3)=\n(x_1^2, \\sqrt{2}x_1x_2, x_2^2)$$\n\nThe following hyper-plane linearly separates the clusters in the data.\n$$\\langle w,\\Phi\\rangle (x_1,x_2)=(w_1\\phi_1+w_2\\phi_2+w_3\\phi_3)(x_1,x_2)=0$$\n\nThis we can complete the data clustering challenge by solving the corresponding maximization problem based on the $\\Phi$-mapped data:\n\n$$W(\\alpha) = \\sum_{i=1}^m{\\alpha_i} -\\frac{1}{2}\\sum_{i,j=1}^m{\\alpha_i \\alpha_j y_i y_j \\langle\\Phi(x_i), \\Phi(x_j)\\rangle },$$\n\nAnd the Lagrangian dual representation of the hyper-plane is:\n\n$$f(x)=\\langle w,x\\rangle +b=\\sum{\\alpha_iy_i\\langle x_i,x_j \\rangle},\\ \\text{with} \\ w=\\sum_{i=1}^m{\\alpha_i y_i x_i}.$$\n\n When $x=(x_1, x_2)$ and $z=(z_1,z_2)$, *the kernel-trick* refers to $K(x,z)=\\langle \\Phi(x),\\Phi(z)\\rangle$:\n\n$$K(x,z)=\\langle x,z\\rangle ^2=(x_1z_1 + x_2z_2)^2=(x_1^2 z_1^2 + 2x_1 z_1 x_2z_2+x_2^2 z_2^2)=$$\n\n$$=\\langle (x_1^2,\\sqrt{2}x_1x_2, x_2^2), (z_1^2,\\sqrt{2}z_1z_2, z_2^2)\\rangle\n=\\langle \\Phi(x),\\Phi(z)\\rangle .$$\n \n\nNote that: (1) the weight vector ($w$) representation includes only data points, (2) the coordinates of the data points are not used, only the inner-products are necessary, (3) the kernel function $K(x_1, x_2)=\\langle \\Phi(x_i),\\Phi(x_j)\\rangle$.\n\nMachine learning classification algorithms based on kernel estimation methods that rely on  $\\Phi$-transformed data are linear and can be expressed as inner-products. Inner-product representation of computational algorithms is beneficial since in high-dimensions they may be efficiently computed using a positive semi-definite [Mercer kernel function]( https://en.wikipedia.org/wiki/Mercer%27s_theorem):\n\n$$k(x_t, x_{t'} )= \\langle \\Phi(x_t ),\\Phi(_{t'}  )) \\rangle.$$\n\nExamples of  Mercer kernels are:\n\n* *Radial basis* function: $K(x,z)=e^{-\\frac{||x-z||^2}{2\\sigma^2}}, \\ ||x||=\\sqrt{\\langle x,x \\rangle},$$  \n\n \nwhere $\\sigma$ is a scale parameter. \n\n* *Polynomial kernel*: $$K(x,z)=(\\langle x,z\\rangle +\\theta)^2,\\ d\\ge 0.$$\n\n* *Sigmoid kernel*: $$K(x,z)=\\tanh(\\eta\\times \\langle x,z\\rangle +\\theta).$$\n\n\nNote that the kernel-trick facilitates the learning algorithm without operating directly on the $\\Phi$-transformed data. It only computes inner-products via the kernel function. The learning algorithm operates implicitly on the kernel feature space, avoiding the $\\Phi$ mapping. \n\nAs the kernel feature space is non-linearly related to the original input data, kernel methods are considered non-linear operations. \n\n\nSupport vector machines (SVM), see [Chapter 6](https://socr.umich.edu/DSPA2/DSPA2_notes/06_ML_NN_SVM_RF_Class.html), represents an example of kernel-based machine learning methods. \n \n![]( https://wiki.socr.umich.edu/images/0/00/DSPA_Appendix2_Fig2.png)\n\n\nInformation theoretic machine learning utilizes the second-order (quadratic) Renyi entropy, $\\alpha = 2$: \n\n\n$$H_{\\alpha=2}=-\\ln\\left ( \\int_{\\mathbb{R}} {p^2(x)dx} \\right ) .$$\n\nAs the logarithm function, $\\ln()$, is monotonic, optimizing $H_2(p)$ is equivalent to optimizing $V (p)= \\int{ p^2(x)dx}$. \n\nThe usefulness of the Renyi entropy, i.e., $V (p)$ as a machine learning assessment criterion, relies on its effective estimation using real data samples $D = \\{x_1,..., x_N\\}$ generated from the distribution  $p(x)$. \n\nFor example, to *estimate* $V(p)$, we can replace $p(x)$ by a smooth sample-driven kernel density $\\hat{p}(x)$. This ensures that we can compute the derivatives of $V (p)$  with respect to system weights and thus find the extrema. \n\nWe already saw that the Parzen window provides kernel density estimation: \n\n\n$$\\hat{p}(x)=\\frac{1}{n}\\sum_{x_t\\in D}{W_{\\sigma}(x,x_t)},$$\n\n \nwhere $W_{\\sigma}(x, \\ .)$ is the Parzen window centered at $x$ with a width parameter $\\sigma$. The smoothness of the density estimator requires smoothness of the window function. In addition, the estimate is assumed to be a density (pdf) which requires $\\hat{p}(x)\\ge0$ and $\\int{ \\hat{p}(x)dx}=1$.\n\nMany alternative Parzen windows may be used, e.g., any density function works, and we already saw the Gaussian windowing function \n\nWhen using kernels with fixed kernel sizes,\n\n$$E(\\hat{p}(x)) = \\lim_{n\\longrightarrow \\infty}{\\hat{p}(x)} = p(x) \\star W_{\\sigma}(x),$$\n\nwhere $\\star$ is the [convolution operator](https://en.wikipedia.org/wiki/Convolution). Assuming a suitable annealing rate for the kernel size as \n$n\\longrightarrow \\infty$, the Parzen density estimator is asymptotically unbiased and consistent.\n\nIn practice we always deal with a limited number of samples. Thus, the choice of a kernel size trades-off with estimation bias and precision. Smaller kernel sizes yield low bias higher variance, whereas larger kernel sizes may increase the bias but reduce the variance. \n\n\n\n\n\nIn the above plot, gray lines indicate the sample data points, the color curves represent the individual (Gaussian) kernel functions at the  sampling points, and the thick black line represents the kernel-density estimate. \n\n\n## Divergence Measures based on Renyi Entropy\n\nLet's start with $k$ groups, each of size $n_i$ representing the data set $D$ composed of subgroups $D_1,..., D_k$  including the data points corresponding to the pdf's $p_i(x)$. \n\n\nThe Cauchy-Schwarz Divergence is:\n\n$$ D_{CS}(p_i,p_j)=-\\ln\n\\frac{\\int{p_i(x)p_j(x)dx}}{\\sqrt{\\int{p_i^2(x)dx}\\times \\int{p_j^2(x)dx}}} \\ \\ \\in [0, \\infty].$$\n\n\nNote that $D_{CS}(p_i,p_j)=0 \\iff p_i(x)= p_ j (x)$, it's symmetric, and its magnitude indicates the discrepancy between the two pdfs. \n\nIt is based on the Renyi entropy:\n\n$$D_{CS}(p_i,p_j)=-\\ln\\int{p_i(x)p_j(x)dx} -\\frac{1}{2}H_2(p_i) -\\frac{1}{2}H_2(p_j).$$\n\nThe monotonicity of the logarithm function allows us to use Parzen window estimators on the simpler  $V_{CS}(p_i, p_ j )$. \n\n$$\\hat{p}_i(x)=\\frac{1}{n_i}\\sum_{x_n\\in D_i}{W_{\\sigma}(x,x_n)}$$\n\n$$\\hat{p}_j(x)=\\frac{1}{n_j}\\sum_{x_l\\in D_j}{W_{\\sigma}(x,x_l)}$$\n\nThese yield the following estimator:\n\n$$V_{CS}(\\hat{p}_i,\\hat{ p}_ j )=\n\\frac{\\frac{1}{n_in_j}\\sum_{x_n\\in D_i}{\n\\sum_{x_l\\in D_j}{W_{\\sigma}(x_n,x_l)}}}\n{\\sqrt{\\frac{1}{n_i^2}\\sum_{x_n\\in D_i}{\n\\sum_{x_{n'} \\in D_j}{W_{\\sigma}(x_n,x_{n'})}} \\times\n\\frac{1}{n_j^2}\\sum_{x_l\\in D_j}{\n\\sum_{x_{l'} \\in D_j}{W_{\\sigma}(x_l,x_{l'})}\n}}}. $$\n\nFinally, the CS-divergence estimator is:\n\n$$ D_{CS}(\\hat{p}_i,\\hat{ p}_ j )= -\\ln\\left (V_{CS}(\\hat{p}_i,\\hat{ p}_ j )\\right ).$$\n\n \nThe Integrated Squared Error (ISE) Divergence is an alternative measure that is based on Renyi entropy capturing the aggregate square error between the pdfs:\n\n$$D_{ISE}(p_i,p_j)=\\int{[p_i(x)-p_j(x)]^2dx}.$$\n\n## Parzen windows\n\nA common approach for estimating non-parametric density functions based on using Parzen windows. Suppose we are trying to estimate a process density function $f(x)$, which generates the observed random sample $\\{x_1, \\cdots, x_N\\}$. Then, the Parzen window estimator of this distribution is defined by\n$$\\hat{f}(x) \\equiv \\frac{1}{N} \\sum_{l=1}^N {W_{\\sigma^2} (x, x_l)}.$$\nThe Parzen window, $W_{\\sigma^2}$, is a kernel whose extent (width) is controlled by the parameter $\\sigma^2$. Typically, the Parzen window is normalized to integrate to one, similarly to the unknown pdf function $f$. For instance, for a $d$-dimensional process, we can use a Gaussian kernel\n$$W_{\\sigma^2} (x, x_l ) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{d}{2}}}\ne^{−\\frac{||x − x_l||^2}{2\\sigma^2}}.$$\n\n\n## Relation between Information Theoretic Learning and Kernel Methods\n\nBased on the Parzen window kernel trick, the information theoretic learning criteria may be expressed in terms of mean vectors in a Mercer kernel feature space.\n\nWe can express every Renyi entropy-based information-theoretic measure in terms of sums of Parzen windows, $W_{\\sigma} (\\cdot, \\cdot)$, by positive semi-definite functions -- Parzen window estimates. \n\nParzen window estimates satisfy the Mercer's conditions, and therefore compute inner products in some kernel induced feature space\n\n$$W_{\\sigma} (\\cdot,\\cdot)= k(\\cdot,\\cdot)= (\\Phi(\\cdot),\\Phi(\\cdot)).$$\n\nMachine learning clustering may be achieved by maximizing the divergence measure.\n\nVarious objective functions may be estimated using Parzen windowing to obtain non-linear clustering of classification.\nTo optimize with respect to the data partitioning, i.e., optimize the clusters, we maximize the Cauchy-Schwarz (CS) divergence:\n\n\n$$\\max_{D_1, D_k}{D_{CS}(\\hat{p}_1, \\hat{p}_k)}.$$\n\nUsing gradient descent and Lagrange multipliers we can obtain an optimal solution corresponding to a partition of the data.\n\n## Some notes\n\n**Question**:  Given a specific probability distribution, is there an analytical \nexpression of the entropy as an inner product of a function multiplied by the \nimaginary unit $i$?\n\n*Approach*: The Dirac notation used in quantum mechanics conveniently represents \n*kets* (*state vectors*), $|\\psi\\rangle$, *bras* (*dual vectors*) $\\langle \\psi |$,\nand operators. The *inner product* of two state vectors $|\\psi\\rangle$) and $|\\varphi\\rangle$\nis $\\langle \\psi| \\varphi \\rangle$. Then the states are normalized, the inner product\ncorresponds to the probability of finding the system in state $| \\varphi \\rangle$ \nif it was initially in state $|\\psi\\rangle$. The entropy of a discrete \nprobability distribution can be expressed using Dirac notation as follows.\n\nCOnsider a discrete process with a probability distribution with probabilities \n$\\{p_1, p_2, \\cdots, p_n\\}$. The entropy $S$ of this distribution is\n\n$$S = - \\sum_{i} {(p_i \\times \\log(p_i))}$$\n\nUsing Dirac notation, express $p_i$ as $|\\psi\\rangle$ and the normalization \ncondition as $\\sum_i |\\psi_i\\rangle \\langle \\psi_i|=1$. Then, the entropy is\n\n$$S = - \\sum_i {\\left [\\langle \\psi_i | \\psi_i \\rangle \\times \\log(\\langle \n\\psi_i | \\psi_i \\rangle)\\right ]}\\equiv -Tr(\\rho \\log(\\rho))\\ ,$$\n\nwhere $\\rho=\\sum_i p_i |\\psi_i\\rangle \\langle \\psi_i|$ is the \n[*density matrix*](https://en.wikipedia.org/wiki/Density_matrix) \nrepresenting the probability distribution, and $Tr$ is the *trace* of a matrix.\n\nThe Dirac notation provides a compact way to express the entropy of a probability distribution. \nThe entropy is still the *negative expectation of the logarithm of the probabilities*.",
      "word_count": 2553
    },
    {
      "title": "References",
      "content": "* [Information Theory and Statistical Learning](https://link.springer.com/book/10.1007/978-0-387-84816-7), ed. Frank Emmert-Streib and Matthias Dehmer, Springer, 2009, ISBN: 978-0-387-84815-0.\n* Jose C. Principe, 2010, [Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives](https://books.google.com/books?id=oJSkBXWctsgC). Springer, Information Science and Statistics, ISBN\t9781441915702.\n* Renyi A. (1976). On measures of entropy and information. Selected Papers of Alfred Renyi, Akademiai Kiado, Budapest, 2:565-580.\n* Olaf Sporns (2007) Complexity. Scholarpedia, 2(10):1623.\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 157
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "      smooth_scroll: true\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Information Theory and Statistical Learning",
      "code": "##### Illustration of Gaussian Kernels with Different Bandwidths\n\nkde <- function(data, n, MIN, MAX) {\n  # Gaussian kernel density estimator for one-dimensional data;\n  # The estimator does not use the commonly employed 'gaussian rule of thumb'.\n  # As a result it outperforms many plug-in methods on multimodal densities\n  # with widely separated modes (see example).\n  # INPUTS:\n  #     data    - a vector of data from which the density estimate is constructed;\n  #          n  - the number of mesh points used in the uniform discretization of the\n  #               interval [MIN, MAX]; n has to be a power of two; if n is not a power of two, then\n  #               n is rounded up to the next power of two; the default value of n is n=2^12;\n  #   MIN, MAX  - defines the interval [MIN,MAX] on which the density estimate is constructed;\n  #               the default values of MIN and MAX are:\n  #               MIN=min(data)-Range/10 and MAX=max(data)+Range/10, where Range=max(data)-min(data);\n  # OUTPUT:\n  #       matrix 'out' of with two rows of length 'n', where out[2,]\n  #       are the density values on the mesh out[1,];\n  # EXAMPLE:\n  ##Save this file in your directory as kde.R and copy and paste the commands:\n  # rm(list=ls())\n  # source(file='kde.r')\n  # data=c(rnorm(10^3),rnorm(10^3)*2+30);\n  # d=kde(data)\n  # plot(d[1,],d[2,],type='l',xlab='x',ylab='density f(x)')\n  # REFERENCE:\n  # Z. I. Botev, J. F. Grotowski and D. P. Kroese\n  # \"Kernel Density Estimation Via Diffusion\"\n  # Annals of Statistics, 2010, Volume 38, Number 5, Pages 2916-2957\n  \n  nargin = length(as.list(match.call())) - 1\n  \n  if (nargin < 2)\n    n = 2 ^ 14\n  n = 2 ^ ceiling(log2(n))\n  # round up n to the next power of 2;\n  if (nargin < 4)\n  {\n    # define the default  interval [MIN,MAX]\n    minimum = min(data)\n    maximum = max(data)\n    \n    Range = maximum - minimum\n    \n    MIN = minimum - Range / 2\n    MAX = maximum + Range / 2\n    \n  }\n  # set up the grid over which the density estimate is computed;\n  R = MAX - MIN\n  dx = R / n\n  xmesh = MIN + seq(0, R, dx)\n  N = length(data)\n  \n  # if data has repeated observations use the N below\n  # N=length(as.numeric(names(table(data))));\n  # bin the data uniformly using the grid defined above;\n  w = hist(data, xmesh, plot = FALSE)\n  initial_data = (w$counts) / N\n  \n  initial_data = initial_data / sum(initial_data)\n  \n  dct1d <- function(data) {\n    # computes the discrete cosine transform of the column vector data\n    n = length(data)\n    \n    # Compute weights to multiply DFT coefficients\n    weight = c(1, 2 * exp(-1i * (1:(n - 1)) * pi / (2 * n)))\n    \n    # Re-order the elements of the columns of x\n    data = c(data[seq(1, n - 1, 2)], data[seq(n, 2, -2)])\n    \n    # Multiply FFT by weights:\n    data = Re(weight * fft(data))\n    \n    data\n  }\n  a = dct1d(initial_data)\n  # discrete cosine transform of initial data\n  # now compute the optimal bandwidth^2 using the referenced method\n  I = (1:(n - 1)) ^ 2\n  a2 = (a[2:n] / 2) ^ 2\n  \n  # solve the equation t=zeta*gamma^[5](t)\n  fixed_point <-  function(t, N, I, a2) {\n    # this implements the function t-zeta*gamma^[l](t)\n    l = 7\n    \n    f = 2 * (pi ^ (2 * l)) * sum((I ^ l) * a2 * exp(-I * (pi ^ 2) * t))\n    \n    for (s in (l - 1):2) {\n      K0 = prod(seq(1, 2 * s - 1, 2)) / sqrt(2 * pi)\n      const = (1 + (1 / 2) ^ (s + 1 / 2)) / 3\n      \n      time = (2 * const * K0 / N / f) ^ (2 / (3 + 2 * s))\n      \n      f = 2 * pi ^ (2 * s) * sum(I ^ s * a2 * exp(-I * pi ^ 2 * time))\n      \n    }\n    out = t - (2 * N * sqrt(pi) * f) ^ (-2 / 5)\n    \n    out <- abs(out)\n  }\n  soln <- optimize(\n    fixed_point ,\n    c(0, .1),\n    tol = 10 ^ -22,\n    N = N,\n    I = I,\n    a2 = a2\n  )\n  t_star <- soln$minimum\n  # smooth the discrete cosine transform of initial data using t_star\n  a_t = a * exp(-(0:(n - 1)) ^ 2 * pi ^ 2 * t_star / 2)\n  \n  # now apply the inverse discrete cosine transform\n  idct1d <-  function(data) {\n    # computes the inverse discrete cosine transform\n    n = length(data)\n    \n    # Compute weights\n    weights = n * exp(1i * (0:(n - 1)) * pi / (2 * n))\n    \n    # Compute x tilde using equation (5.93) in Jain\n    data = Re(fft(weights * data, inverse = TRUE)) / n\n    \n    # Reorder elements of each column according to equations (5.93) and\n    # (5.94) in Jain\n    out = rep(0, n)\n    \n    out[seq(1, n, 2)] = data[1:(n / 2)]\n    \n    out[seq(2, n, 2)] = data[n:(n / 2 + 1)]\n    \n    out\n    \n  }\n  density = idct1d(a_t) / R\n  \n  # take the rescaling of the data into account\n  bandwidth = sqrt(t_star) * R\n  \n  xmesh = seq(MIN, MAX, R / (n - 1))\n  \n  out = matrix(c(xmesh, density), nrow = 2, byrow = TRUE)\n  \n}",
      "line_count": 147
    },
    {
      "section": "Information Theory and Statistical Learning",
      "code": "# define support set of X\nx = seq(-5, 10, by = 0.01)\n\n# get normal kernel function values\nnorm1 = dnorm(x, -1, 1)\nnorm2 = dnorm(x, 2, 1)\nnorm3 = dnorm(x, 4, 1)\nnorm4 = dnorm(x, 6,1)\n\nplot(x, norm1, type = 'l', ylim=c(0, 1), ylab = 'f(x)', xlab = 'x', main = 'Gaussian Kernels KDE Simulation', col = 'red')\n\n# add plots of additional kernel functions\nlines(x, norm2, col = 'green')\nlines(x, norm3, col = 'blue')\nlines(x, norm4, col = 'pink')\n\ndata_1=c(rnorm(1000, -1, 1), rnorm(1000, 2, 1), rnorm(1000, 4, 1), rnorm(1000, 6, 1))\ndata_2=c(-1, 2, 4, 6)\ndata_kde=kde(data_1)\nlines(data_kde[1,],4*data_kde[2,],type='l',xlab='x',ylab='density f(x)', col = 'black', lwd=4)\n\ndata2_kde=kde(data_2)\nlines(data2_kde[1,],data2_kde[2,], type='l',xlab='x', col = 'gray', lwd=2)\n\n# legend; use 'lty' to discriminate between kernels\nlegend(\"top\", c('Norm(-1,1)', 'Norm(-2,1)', 'Norm(4,1)', 'Norm(6,1)', 'KDE Estimate'), lty = c(1,2,3,4,5), lwd=c(1,1,1,1,4), col = c('red', 'green', 'blue', 'pink', 'black'), cex=0.5, box.lwd = 0)",
      "line_count": 26
    }
  ]
}