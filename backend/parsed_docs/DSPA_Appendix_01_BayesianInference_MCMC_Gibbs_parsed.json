{
  "metadata": {
    "created_at": "2024-11-30T13:46:16.835602",
    "total_sections": 2,
    "total_code_chunks": 24,
    "total_tables": 1,
    "r_libraries": [
      "C50",
      "RTextTools",
      "caret",
      "data.table",
      "glmnet",
      "igraph",
      "knitr",
      "mcmc",
      "neuralnet",
      "plotly",
      "randomForest",
      "text2vec",
      "tidyr",
      "tm"
    ]
  },
  "sections": [
    {
      "title": "Main",
      "content": "---\ntitle: \"DSPA2: Data Science and Predictive Analytics (UMich HS650)\"\nsubtitle: \"<h2><u>Appendix: Applied Bayesian Modeling, Simulation and Inference</u></h2>\"\nauthor: \"<h3>SOCR/MIDAS (Ivo Dinov)</h3>\"\ndate: \"`r format(Sys.time(), '%B %Y')`\"\ntags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] \noutput:\n  html_document:\n    theme: spacelab\n    highlight: tango\n    includes:\n      before_body: ../SOCR_header.html\n    toc: true\n    number_sections: true\n    toc_depth: 2\n    toc_float:\n      collapsed: false",
      "word_count": 53
    },
    {
      "title": "Applied Bayesian Modeling, Simulation and Inference",
      "content": "## Background\n\nAfter [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) died in 1761 his unpublished work about the flat-prior posterior binomial distribution was rediscovered and published in the Philosophical Transactions of the Royal Society of London in 1764.\n\nThe [Monte Carlo (MC) Method](https://en.wikipedia.org/wiki/Monte_Carlo_method)\nis a simple technique used to estimate analytic quantities that may have closed-form expressions, but are difficult to compute exactly, e.g., integration,  differentiation, optimization of non-trivial functions. MC simulation may be used to approximate probability distributions, estimate likelihoods, or calculate expectations. The intuition of MC sampling is rooted in the fact that we can estimate any desired (posterior) expectation using *ergodic averages*. In other words, we can compute any population characteristic, associated with a corresponding sample-driven statistic, of a (posterior) distribution provided we can generate $N$ simulated samples from that distribution:\n\n$$E(f(s))_P = \\int {f(s)p(s)ds} \\approx \\frac{1}{N}\\sum_{i=1}^N {f(s^{(i)})},$$\nwhere $P$ and $p$ are the probability (posterior) distribution and density functions of interest, $E$ represents the expectation, $f(s)$ is the population characteristic of interest (corresponding to a statistic), and $s^{(i)}$ is the $i^{th}$ simulated sample from the distribution $P$. For instance, we can estimate the *population mean* (linked to the sample arithmetic average statistic) by:\n\n$$E(x)_P =\\int{xp(x)dx} \\approx \\frac{1}{N}\\sum_{i=1}^N {x^{(i)}},$$\nwhere $p(x)$ is the density of the distribution function $P$. Similarly, assuming a trivial mean, to estimate the *population variance* (corresponding to the sample-variance statistic) we compute:\n\n$$E(x^2)_P =\\int{x^2p(x)dx} \\approx \\frac{1}{N-1}\\sum_{i=1}^N {\\left ( x^{(i)}\\right ) ^2}.$$\n\nMany integrals, differentials, or sums that cannot be exactly computed in closed analytic form, can still be expressed as expectations of some random variables with respect to some probability distribution, which are estimable by MC methods.\n\nThe [Markov Chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain) method was proposed by [Andrey Markov](https://en.wikipedia.org/wiki/Andrey_Markov) in the early $20^{th}$ century. *Markov Chains* refer to *one-step dependent* sequences of random variables obeying the *Markov property* (past and future are conditionally independent given the present). For example:\n\n* Sampling *with replacement* vs. *without replacement* from a space (e.g., an urn) containing a set of identifiable objects represent processes *obeying* vs. *not obeying* the Markov property, respectively. \n\n* A simple Brownian motion (Wiener process) random walk on an integer lattice\n$Z^d \\subset R^d$ represents a Markov chain whose transition probabilities are defined by $p(x,x\\pm e_i)=\\frac{1}{2d}$, for each $x\\in Z^d$, where $e_i$ are the normalized vectors in $Z^d$. At each step, this random walk process, currently at position $x$, moves to a randomly chosen nearest neighbor, $x\\pm e_i$.\n\n* Consider [cancer staging](https://www.cancer.gov/about-cancer/diagnosis-staging/staging) and the corresponding transitions between stages. The Cancer Tumor-Node-Metastasis (TNM) staging system describes the size of the primary tumor (T), whether the cancer has spread to the lymph nodes (N), and whether it has spread to a different part of the body (M). $T=1(small), 2, 3, 4(large)$ refers to the size of the cancer and how far it has spread into nearby tissue, $N=0$ (no lymph nodes containing cancer cells), $1, 2, 3$ (lots of lymph nodes containing cancer cells), and $M=$ $0$ (no spread), $1$ (cancer has spread), refers to whether the cancer has spread to another part of the body. Most types of cancer have four stages, $CS=1(early), 2, 3,\\ to\\ 4\\ (advanced)$. *Stage 1* implies that a cancer is relatively small and contained within the organ it started in. *Stage 2* means the cancer has not started to spread into surrounding tissue but tumor is larger than in *stage 1*, or that cancer cells have spread into lymph nodes close to the tumor. *Stage 3* usually means the cancer is larger and may have started to spread into surrounding tissues possibly with cancer cells in the lymph nodes in the area. And *stage 4* means the cancer has spread from its point of origination to another body organ indicating metastatic progression.\n\nLet's use cancer staging to illustrate the fundamentals of MC representation, its properties, and computational characteristics. For each stage, let $X_t$ denote the cancer stage of a patient at time $t$. Cancer onset, progression, prognosis and outcomes have stochastic behavior. We have 5 states, $X_n=0(no\\ cancer), 1,2, 3, 4$, and initially $X_o=0$ (cancer *occurs*), with $P(X_o=0)=1$. Next we can define the marginal probabilities for transition from one cancer stage to another (these depend upon the type of cancer), say $P(X_1=1)=0.6$, $P(X_1=2)=0.4$, $P(X_1=3)=0.2$, $P(X_1=4)=0.1$. Computing the distribution of $X_t$ for $t\\geq 2$ requires conditional probabilities. \n\nSuppose that at time $t$, the cancer patient is at stage $s_o$. Then, we can compute the conditional probabilities: $P(X_{t+1}=s_k|X_t=s_o)$ of patient being in a specific stage $s_k$ at the next time point $t+1$. For instance, if at time $t$ the patient is in stage $1$  we may have $P(X_{t+1}=0|X_t=1)=0.15$, $P(X_{t+1}=1|X_t=1)=0.5$, $P(X_{t+1}=2|X_t=1)=0.2$, $P(X_{t+1}=3|X_t=1)=0.1$, and $P(X_{t+1}=4|X_t=1)=0.05$.\n\nWould these probabilities change if we condition further on the oncological history of the same patient up to time $t$? Because of the stochastic coin-toss like mechanism of many biomedical, physiologic and natural processes, these conditional likelihoods may not be affected by knowing a longer staging history of the disease provenance. To predict the likelihood of being in a certain state at the next time period, only the current state of the disease matters. Thus, for any cancer provenance record ($s_o, s_1, \\cdots, s_{t-1}$), we may have $P(X_{t+1}=0|X_o=S_o, X_1=s_1, \\cdots, X_{t-1}=s_{t-1}, X_t=1)=P(X_{t+1}=0|X_t=1)=0.2$, $P(X_{t+1}=1|X_o=S_o, X_1=s_1, \\cdots, X_{t-1}=s_{t-1}, X_t=1)=P(X_{t+1}=1|X_t=1)=0.2$, $P(X_{t+1}=2|X_o=S_o, X_1=s_1, \\cdots, X_{t-1}=s_{t-1}, X_t=1)=P(X_{t+1}=2|X_t=1)=0.2$, $P(X_{t+1}=3|X_o=S_o, X_1=s_1, \\cdots, X_{t-1}=s_{t-1}, X_t=1)=P(X_{t+1}=3|X_t=1)=0.2$, and $P(X_{t+1}=4|X_o=S_o, X_1=s_1, \\cdots, X_{t-1}=s_{t-1}, X_t=1)=P(X_{t+1}=4|X_t=1)=0.2$.\n\nThis is because the coin-toss like cancer staging outcome (not necessarily a fair coin) at time $t+1$ is independent of the prior coin flips and therefore it does not depend on $X_o, X_1, \\cdots, X_{t-1}$. It only depends on the current cancer stage, $X_t$, at time $t$. This rule, that the conditional distribution of $X_{t+1}$ depends only on the state at the prior time, $X_t$, is referred to as the *Markov property*, or *memoryless* property.\n\nAnother characteristic of Markov processes, *time homogeneity*, is that the conditional distribution of $X_{t+1}$ given (say) $X_t=s_o$ is the same for all time points, i.e., $P(X_{t'+1} | X_{t'}=s_o) = P(X_{t''+1} | X_{t''}=s_o)$, for all $t'$ and $t''$. This follows from the fact that albeit dependent on human intervention, the biophysiological mechanism of cancer stage transitioning obeys the same rules independently of the time.\n\nCollectively, the *transition probabilities* comprise the *Markov transition matrix*, $P$, where each element $P_{i,j}$ represents the conditional probability of observing cancer stage $s_j$ at time $t+1$ given that the current, time $t$, stage is $s_i$, i.e., $P_{i,j} = P(X_{t'+1} =s_j| X_{t'}=s_i)$.\n\nThis a *Markov chain model* for cancer progression, using the above 5-cancer stages (model states) may be represented mathematically as a transition matrix or a graph (here $k=5$):\n$$P=\n\\begin{pmatrix}\n  P_{1,1} & P_{1,2} & \\cdots & P_{1,k} \\\\\n  P_{2,1} & P_{2,2} & \\cdots & P_{2,k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  P_{k,1} & P_{k,2} & \\cdots & P_{k,k} \n \\end{pmatrix} .$$\nThe axioms of probability theory dictate that:\n\n* the conditional probabilities are non-negative, $P_{i,j} = P(X_{t+1} =s_j| X_{t}=s_i) \\geq 0$, for all $i,j \\in \\{1, 2, 3, \\cdots, k\\}$,\n* the law of total probability is satisfied, $\\sum_{j=1}^k{P_{i,j}} = 1$, for all all $i \\in \\{1, 2, 3, \\cdots, k\\}$, and\n* The *initial distribution* of the Markov Chain $\\{X_o, X_1, X_2, \\cdots \\}$ defining the initial conditions, $t=0$, of starting the chain over its states $\\{s_1, s_2, s_3, \\cdots, s_k\\}$, can be expressed as $\\mu^{(o)}=(\\mu_1^{(o)}=P(X_o=s_1), \\mu_2^{(o)} =P(X_o=s_2), \\cdots, \\mu_k^{(o)} =P(X_o=s_k))$, where $\\sum_{i=1}^k{\\mu_i^{(o)}}=1$.\n\nBelow we show the mathematical matrix representation of the MC process above as a (directed, cyclic, complete) graphical object illustrating all states and their corresponding transition probabilities.\n\n\nIn quantum physics, the [Heisenberg uncertainty\nprinciple](https://en.wikipedia.org/wiki/Uncertainty_principle) suggests that there is a limit to the process of simultaneously identifying the position and momentum of particles in the universe, which implies that the future is difficult to predict exactly, albeit may be probabilistically quantified. Similarly, in logic, the [G?del's incompleteness theorem](https://en.wikipedia.org/wiki/G?del%27s_incompleteness_theorems) suggests that no system (closed system supported by quantitative arithmetic) can be *complete* and *consistent* at the same time. This also imposes some theoretical bounds on the precision of our forecasting and inference. \n\nMonte Carlo experiments represent computational algorithms utilizing repeated random sampling of data and calculation of specific statistics that estimate concrete population characteristics. Two examples of using randomness to solve challenging problems involve the [estimation of the area of a 2D region in the plane with complex boundary](https://wiki.socr.umich.edu/index.php/AP_Statistics_Curriculum_2007_Prob_Simul) and [compressed sensing](https://en.wikipedia.org/wiki/Compressed_sensing) for rapid estimation of complex signals using (sub-Nyquist frequency) stochastic sampling.\nThe [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)\nand [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) represent a couple of specific Markov Chain Monte Carlo (MCMC) methods of simulation and estimation.  \n\n## Bayesian Modeling using MCMC\n\nEarlier in [Chapter 7](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/07_NaiveBayesianClass.html#3_bayes_formula), we presented the Bayes conditional probability formula in terms of events. Below, we will expand that formulation of the relation between the posterior, prior, conditional and marginal probabilities. \n\nGibbs sampling can be applied to estimate Bayesian models facilitating subsequent Bayesian inference. The fundamental idea is to obtain MCMC computationally accurate estimates of complicated or intractable statistical estimations, mathematical operations.\n\n*Conditional probability* reflects partial knowledge about some observed data. Let's consider the bivariate density and distribution situations to avoid the limitations of univariate cases and reduce the complexities of more general multivariate processes. For discrete-state processes, $X,Y\\in \\Omega$, the *probability mass function (PMF)*, $f(.,.)\\geq 0$, consists of the complete set of probability values assigned to all possible $(x, y)$ pair states, where \n$\\sum_{(x, y) \\in \\Omega} f(x, y) = 1$. The PMF values, $f(x, y)$ correspond to the probability\nof observing the event $X = x$ and $Y = y$. For continuous-state processes, the *probability density function (PDF)*, is a non-negative function $f(x,y)\\geq 0$ that integrates to one\n$\\int_{-\\infty}^\\infty { \\int_{-\\infty}^{\\infty} { f(x, y)  \\ d x  \\ d y }} =  1$.\n\nThe PDF assigns probabilities, $f(x, y) \\, d x \\, d y$ , to outcome \"areas\" corresponding to observing the event $x < X < x + d x$ and $y < Y < y + d y$, where the infinitesimal $d x$ and $d y$ represent the small increments in the two planar directions.  In mixed situations where one of the variable may be continuous and the other discrete, probability mass-density functions may be defined in the same spirit yielding probabilities of outcomes, $f(x, y) \\, d x$, defined for mixed continuous-discrete events like $x < X < x + d x$ and $Y = y$.\n\nOften we may be able to observe $X$ but not $Y$. The *joint distribution* represents the probability of the outcome $(x,y)\\in A$, $P((X,Y)\\in A)$, without any other knowledge and before observing either of the random processes $X$ or $Y$. Suppose we see $X = x$ first, what should be the distribution of $Y$? Given $X$, the probability function has one unknown variable and has a *conditional probability function* analytic representation. For instance, when $X$ and $Y$ are discrete random variables and $(x,y)\\in \\Omega$, then the conditional probability mass function of $Y$ given $X$ is:\n\n$$f(y | x)\n   =\n   \\frac{h(x, y)}{\\sum_{y \\in \\Omega_x} h(x, y)},\n   \\qquad y \\in \\Omega _x,$$\n\nwhere\n$$\\Omega_x = \\{\\ y : \\text{such that } (x, y) \\in \\Omega \\}.$$\n\nSimilarly, when $X$ and $Y$ are continuous random variables, then\n$$    f(y | x)\n   =  \\frac{h(x, y)}{\\int_{- \\infty}^\\infty h(x, y) \\ d y},\n   \\qquad - \\infty < y < \\infty,$$\nrepresents the  conditional probability density function of $Y$ given $X$.\n\nThe *joint* multivariate distribution is the product of the *marginal* and the *conditional* distributions, $f(x, y)= f_X(x)\\times f(y | x)$. In the discrete case, the marginal distribution of $X$ (ignoring $Y$) is $f_X(x) = \\sum_{y \\in \\Omega_x} f(x, y)$, where $\\Omega_x$ is the support of $X$, represents the probability of the event $X = x$. And when $X$ and $Y$ are both continuous, the marginal is $f_X(x) = \\int_{- \\infty}^\\infty f(x, y) \\ d y$ and for any event $E\\subset \\Re$, $P(X \\in E) = \\int_E f_X(x) \\ d x  =  \\int_E d x \\int_{- \\infty}^\\infty f(x, y) \\ d y$ represents the probability of the event $X \\in E$, which is equivalent to $(X, Y) \\in E \\times \\Re$.\n\nThe conditional distributions are expressed as:\n$$\\begin{align*}\n   f(y | x) & = \\frac{f(x, y)}{f_X(x)}\n   \\\\\n   f(x | y) & = \\frac{f(x, y)}{f_Y(y)}\n\\end{align*}.$$\n\nNotice that there are separate marginal distributions for each variable. In our bivariate case, one marginal for $X$ and another for $Y$.\n\nThis bivariate situation generalizes to $X$ and $Y$ variable vectors where multiple variables can be conditioned upon in the above relation between joint, marginal and conditional probabilities. \n\nReview the many hands-on examples of constructing and validating Bayesian confidence intervals that are included in the [SOCR General Confidence Intervals Activity](https://wiki.socr.umich.edu/index.php/SOCR_EduMaterials_Activities_General_CI_Experiment).\n\n## Bayesian Inference\n\nBayesian inference is based on probability theory as a consistent model for describing uncertainty using probability distributions. For instance, statistical inference frequently involves probability distribution models, whose true parameters are *unknown* but may be estimated from observations.  A *prior* distribution of the parameter (or parameters, if the model distribution has multiple parameters like in $Normal(\\mu, \\sigma^2)$) is the initial belief about the parameter distribution before any data are observed.  Of course, if there is no reason to believe in a certain distribution without any prior evidence, we can use a *uniform prior*. In both Bayesian and frequentist inference approaches, the distribution of the data given the parameters ($\\theta$) are identically interpreted based on some underlying statistical model. However, *frequentist representation of this distribution* is expressed as $f^{freq}_{\\theta}(x)$ emphasizing $x$ as random (described by a probability\ndistribution) and the parameter $\\theta$ as fixed. The corresponding *Bayesian distribution* representation is written as $f^{Bayes}(x | \\theta)$, emphasizing as random both $X$ and $\\theta$, and\nconditioning on the $\\theta$ parameter as being given (i.e., known).  This conditional distribution describes the probability of observing $X$ for a given fixed value of $\\theta$. \n\nThus, in the Bayesian framework, assuming a *prior distribution*, $g()$, for the parameter $\\theta$, the\njoint = marginal *times* conditional, $f(X, \\theta) = g(\\theta) \\times f(x | \\theta)$, describes the uncertainty about $X$ and $\\theta$ before observing the data.\n\nTaking a data sample, $X = x$, the conditional distribution of the parameter $\\theta$ given $X$ can be computed, $ f(\\theta | x) = \\frac{f(X, \\theta)}{g(x)} $, and is called the *posterior distribution* representing the probability distribution describing the post-evidence uncertainty in the parameter $\\theta$ after taking a data sample $x$.\n\nBoth the (marginal) prior and the (conditional) posterior distributions capture the probability of the parameter of interest.  The prior is the initial distribution before seeing any data and the posterior the probability of the same parameter after observing the data.\nThe Bayesian rule or Bayesian theorem provides a mechanism of inverting the conditioning and computing one conditional posterior probability from the other, e.g., having $f(x | \\theta)$, we can compute the conditional posterior probability of $\\theta$ given $x$:\n\n$$f(\\theta |x) = \\frac{g(\\theta)\\times f(x | \\theta)}{f_X(x)}.$$\n\nBayesian inference employs the Bayesian rule to compute the conditional posterior probability from the joint distribution model and the prior belief.\n\nThe numerator in the right hand size above represents an unnormalized posterior distribution, $f(\\theta |x) \\sim g(\\theta)\\times f(x | \\theta)$. In other words, $\\text{un-normalized posterior}=\\text{unnormalized prior} \\times \\text{likelihood}$.\n\nBayesian inference suggests unique answers for each problem, each problem yields a single posterior distribution.  On the other hand, frequentist inference always provides ranges of answers, e.g., confidence intervals of certain confidence levels. Bayesian inference is *subjective* as it heavily depends on prior distributions, which may be difficult to justify objectively. The frequentist inference methods (e.g., parametric hypothesis tests) tend to be simple (e.g., using sampling distributions) to compute using samples of data. Bayesian inference is much more difficult due to general challenges of analytically estimating the likelihoods, $f(x | \\theta)$. Hence, MCMC are developed to provide stochastic approaches to compute analytically intractable likelihoods. MCMC transforms  analytic challenges into computational problems that are tractable using programming, storage, and compute infrastructure.\n\nFrequentist inference treats parameters as fixed and data as random, even after it is observed (think of the sampling distribution of the mean), whereas Bayesian inference treats both data and parameters as random until data is observed, and then treats the data as fixed to compute the conditional posterior probability. Yet, the two inference strategies tend to agree for large sample sizes.\n\nSuppose $\\hat{\\theta}_n$ and $\\theta_o$ denote the maximum likelihood estimate (MLE) and the true unknown parameter value.  Frequentist inference treats $\\hat{\\theta}_n$ as random and\n$\\theta_o$ as fixed and computes the asymptotic distribution:\n$$\\hat{\\theta}_n - \\theta_o \\sim   Normal\\left(0, I_n(\\theta)^{- 1} \\right),$$\nwhere the reciprocal of the Fisher's information, $I(\\theta)$, is related to the \nthe lower bound of the Cramer-Rao inequality, see [this paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0019178).\n \nThis inference may not be useful as frequentist inference has no access to $\\theta_o$. However, plugging in a specific value for $\\theta_o$ does yield a useful result:\n$$\\hat{\\theta}_n - \\theta_o \\sim Normal\\left(0, I_n(\\hat{\\theta}_n)^{- 1} \\right).$$\n\nA corresponding Bayesian inference is based on a random $\\theta_o$ and a fixed $\\hat{\\theta}_n$, not random once the data are observed. Both inference strategies agree that\n$\\hat{\\theta}_n - \\theta_o$ is random, as a function of a random variable is itself a random variable, and its distribution is independent of the prior distribution, under regularity conditions (assuming smooth prior and locally strictly positive function around the true unknown parameter value).\n\nIn practice, having large amounts of data suggests that small perturbations of the prior don't substantially alter the posterior probability or final inference. \n\nThe main downside of Bayesian inference is that two Bayesian scientists that look at the same data could arrive at different conclusions based on their prior values or prior beliefs.  \n\nTo mediate the Bayesian subjectivity, we can employ change-of-variables for priors. Suppose the prior distribution for the parameter $\\theta$ is $g$.  Then, the distribution of another parameter $\\phi = h(\\theta)$, where $h$ is an invertible function with inverse $h^{-1}$. The change-of-variable theorem for integration implies that:\n$$d \\theta = h^{-1}(\\phi) d \\phi .$$\n\nThus,\n$$P(\\phi \\in A) = \\int_{h(\\theta) \\in A} {g(\\theta) \\, d \\theta } = \\int_A {g(h^{-1}(\\phi)) h^{-1}(\\phi) \\ d \\phi} .$$\n\nTherefore, we can employ the following prior for $\\phi$: $g(h^{-1} (\\phi)) h^{-1}(\\phi)$.\n\nThis suggests that a flat uninformative (diffuse) prior for one parameter may transform to a non-flat, or informative, prior for another parameter.\n\n\n## Monte Carlo \n\nMonte Carlo simulations may be used to estimate probabilities (likelihoods) or expectations (integrals) involving functions that may be difficult to analytically process (i.e., compute a closed form analytical expression of the desired solutions). Likelihoods are special cases of integrals, and each probability can be expressed as an expectation (e.g., for a characteristic function $g$, $E(g(X)) = P( g(X) = 1)$). Let's assume we want to compute $\\theta = E(g(X)) = \\int_{\\mathbb{R}} {g(x)P_X(x)dx}$, but the integral can't be exactly solved using simple analytical terms.\n\nInstead, we can employ Monte Carlo simulation to estimate $\\theta$.  Let's generate a random sample of observations $\\{ X_1, X_2, \\cdots \\}$, independent and identically distributed (IID). Then, for each iteration $n$, the average of the random variables is:\n$$\\hat{\\theta}_n = \\frac{1}{n} \\sum_{i = 1}^n g(X_i).$$\n\nIf $Y_i = g(X_i)$, then $\\hat{\\theta}_n$ can be written explicitly as a sample arithmetic average:\n$$\\overline{Y}_n = \\frac{1}{n} \\sum_{i = 1}^n {Y_i}$$\n\nThe [law of large numbers (LLN)](https://wiki.socr.umich.edu/index.php/SMHS_CLT_LLN) implies that\n$\\hat{\\theta}_n \\longrightarrow \\theta$ as $n \\longrightarrow \\infty$.\n\nHaving a sufficiently large simulation sample would allow us to calculate a good estimate of $\\theta$.\n\nBy the [central limit theorem (CLT)](https://wiki.socr.umich.edu/index.php/SMHS_CLT_LLN):\n$$\\hat{\\theta}_n \\sim Normal\\left( \\theta, \\frac{\\sigma^2}{n} \\right),$$\nas $n\\longrightarrow \\infty$ and $\\sigma^2 = \\mathop{\\rm var}( g(X))$ represents the population variance.\n\nNeither $\\theta$ nor $\\sigma$  are known. However, we can use the sample variance as an estimate\n$$\\hat{\\sigma}^2 = \\frac{1}{n-1} \\sum_{i = 1}^n  \\left ( g(X_i) - \\hat{\\theta}_n \\right )^2. $$\n\nThus,\n$$\\hat{\\theta}_n \\sim Normal\\left( \\theta, \\frac{\\hat{\\sigma}^2}{n} \\right). $$\n\nLet's construct a 95% confidence interval for $\\theta$:\n$$\\hat{\\theta}_n \\pm 1.96 \\times \\frac{\\hat{\\sigma}_n}{\\sqrt{n}}. $$\n\nLet's now go back to *Markov chains*, sequences $\\{X_1$, $X_2$, $\\ldots \\}$ of random variables obeying the *Markov property* that the conditional distribution of $X_{n + 1}$ given the entire transition history back to the initial time actually only depends on the current state $X_n$:\n$$f(x_{n + 1} | x_1, x_2, \\cdots, x_n) = f(x_{n + 1} | x_n).$$\n\nReferring back to our 5-state cancer staging Markov process, as the conditional distribution does not depend on $n$, we have stationary transition probabilities. If $g$ represents the chain distribution at the initial state, the Bayesian inference,\n$\\text{joint} = \\text{marginal} \\times \\text{conditional} $, implies that\n\n$$\\begin{align*}\n   f(x_1, x_2) &= g(x_1)   f(x_2 | x_1)  \\\\\n   f(x_1, x_2, x_3) &=  g(x_1)  f(x_3 | x_2, x_1) f(x_2 | x_1)    \\\\\n   f(x_1, x_2, x_3, x_4) &=  g(x_1)  f(x_4 | x_3 , x_2, x_1) f(x_3 | x_2, x_1) f(x_2 |x_1) \\\\\n   f(x_1, x_2, x_3, x_4, x_5) &= g(x_1)   f(x_5, x_4, x_3, x_2 | x_1) \\\\\n    & = g(x_1)   f(x_5 | x_4, x_3, x_2, x_1) f(x_4 | x_3, x_2, x_1)  f(x_3 | x_2, x_1) f(x_2 | x_1).\n\\end{align*} $$\n\nAll of the Markov chain probabilities are computed by multiplying the initial state distribution $g$ and the conditional transition probability distribution $f(\\,\\cdot\\, |  \\,\\cdot\\,)$.\n\nThe marginal distributions of the different variables can also be estimated by:\n\\begin{align*}\n   f(x_2) & = \\int f(x_1, x_2) \\, d x_1    \\\\\n          & = \\int f(x_2 | x_1) g(x_1) \\, d x_1    \\\\\n   f(x_3) & = \\iint f(x_1, x_2, x_3) \\, d x_1 \\, d x_2    \\\\\n          & = \\iint f(x_3 | x_2) f(x_2 | x_1) g(x_1) \\, d x_1 \\, d x_2    \\\\\n   f(x_4) & = \\iiint f(x_1, x_2, x_3, x_4) \\, d x_1 \\, d x_2 \\, d x_3    \\\\\n          & = \\iiint f(x_4 | x_3) f(x_3 \\mid x_2) f(x_2 \\mid x_1) g(x_1)\n              \\, d x_1 \\, d x_2 \\, d x_3 \\\\\n   f(x_5) & = \\iiiint f(x_1, x_2, x_3, x_4, x_5) \\, d x_1 \\, d x_2 \\, d x_3 \\, d x_4    \\\\\n          & = \\iiiint f(x_5 | x_4)  f(x_4 | x_3) f(x_3 | x_2) f(x_2 | x_1) g(x_1)\n              \\, d x_1 \\, d x_2 \\, d x_3\\, d x_4\n\\end{align*}\n\n## Ergodic Averaging\n\nAn ergodic law, ergodicity, or ergodic theory represents the idea is that for certain dynamical systems, like Markov chains, the *time average of their properties is equal to the average over the entire state space*. In physics, ergodic methods are used to study geodesic flows on Riemannian manifolds and in mathematics, there are ergodic applications in harmonic analysis, Lie group representation, the theory of diophantine approximations, etc.\n\nErgodic theory characterizes the behavior of a dynamical system that runs indefinitely. The [PoincarÃ© recurrence theorem](https://en.wikipedia.org/wiki/Poincar%C3%A9_recurrence_theorem) suggests that on the long run, almost all points in any subset of the phase-space are eventually revisited. In general, ergodic theorems determine the conditions required to guarantee the almost certain existence of the time average of a function along the trajectories and its relation to the space average. For example, one special class of ergodic systems, this time average is statistically the same for almost all initial points, i.e., the evolving dynamical system *forgets* its initial state.\n\nOne special case of ergodic law for Markov chains suggests that:\n$\\hat{\\theta}_n \\longrightarrow \\theta$ as $n \\longrightarrow \\infty$. \n\n### Batch Means\n\nThe CLT suggests that for sufficiently large $b<<n$, we have\n$$\\hat{\\theta}_b \\approx Normal\\left( \\theta, \\frac{\\sigma^2}{b} \\right) .$$\nWhen $n$ is a multiple of $b$, i.e., $m\\times b = n$, for an integer $m$, we can divide the Markov chain into $m$ *batches*:\n$$ X_{m, 1},  X_{m, 2}, \\ldots, X_{m, b}, \\ \\forall\\ m=1, 2, 3, \\cdots, \\frac{n}{b}.$$\nComputing the *batch means* yields:\n$$\\hat{\\theta}_{b, m} = \\frac{1}{b} \\sum_{i = 1}^b {X_{m, i}}. $$\nAgain, assume that $b$ is sufficiently large, then we have\n$$\\hat{\\theta}_{b, m} \\sim Normal\\left( \\theta, \\frac{\\sigma^2}{b} \\right) . $$\nTherefore, we can use the batches and their batch means, $\\hat{\\theta}_{b, m}$, to obtain Bayesian simulation based estimates of the unknown process variance, $\\frac{\\sigma^2}{b}$, using the sample variance statistic:\n\n$$\\frac{1}{m} \\sum_{j = 1}^m ( \\hat{\\theta}_{b, m} - \\hat{\\theta}_n )^2. $$\nAs the last expression estimates $\\sigma^2 / b$, we can solve for the numerator and estimate the variance by:\n$$\\hat{\\sigma}^2_{b, m} = \\frac{b}{m} \\sum_{j = 1}^m ( \\hat{\\theta}_{b, m} - \\hat{\\theta}_n )^2.$$\nTherefore, for sufficiently large $b$ and $m$, the MCMC simulation yields a good estimator of $\\sigma^2$.\n\nSimilarly, we can perform *Bayesian inference* on the parameter $\\theta$ as well as compute confidence intervals for $\\theta$. For instance, when we run the MCMC simulation long enough (large sample), we get large $b$ and $m$ such that $n = b m$, the expression below represents a 95% confidence interval for $\\theta$: \n$$\\hat{\\theta}_n \\pm 1.96 \\times \\frac{\\hat{\\sigma}_{b, m}}{\\sqrt{n}}. $$\n\nSome additional details and demonstrations are provided in the [SOCR Bayesian Estimator]().\n\n* Christou N, Dinov ID. (2011) Confidence Interval Based Parameter Estimation-A New SOCR Applet and Activity. PLoS ONE 6(5): e19178. [doi:10.1371/journal.pone.0019178](https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0019178).\n* [SOCR General Confidence Interval Activity](https://wiki.socr.umich.edu/index.php/SOCR_EduMaterials_Activities_General_CI_Experiment).\n\n### Random-Walk Metropolis Algorithm\n\nRemember that in [Chapter 21](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/21_FunctionOptimization.html), we discussed the importance of randomly sampling from diverse arrays of probability distributions. The Metropolis random walk algorithm provides a protocol for simulating data from the distribution of a continuous random vector for a given PDF $h$. The pseudo-code implementation of the Metropolis algorithm is provided below.\n\n* Initiate the process starting at any point $x$ where $h(x) > 0$.\n* Repeat the following\n     - Generate a random vector $y_n \\sim MultivariateNormal(\\mu=0, \\Sigma)$ that is independent\n        of $x$.  These normal random vectors will be IID.\n     - Calculate $r = \\frac{h(x_n + y_n) }{h(x_n)}$.\n     - Simulate $u_n \\sim {Uniform}(0, 1)$ random variable.  \n     - If $u_n < r$, set   $x_{n+1} := x_n + y_n$, otherwise $x_{n+1} := x_n$.\n     - Return $x_n$.\n\nIn the Metropolis algorithm, we can only adjust the proposal variance, $\\Sigma$, which in the scalar version corresponds to $\\sigma_2$. Too small variance yields MCMC chains taking tiny steps forward and requiring a very long time to equilibrate. On the flip side, large variance would force the algorithm to take very large steps, $y_n$, most of which may be discarded (not accepted or used) in the last step because the calculation of $r$ in step 3 may be too small.\n\nThis process generates a positively recurrent Markov chain that obeys the ergodic LLN law and has $h$ as its equilibrium distribution. This algorithm works for any distribution function $h\\geq 0$. \n\n### MCMC R implementation\n\nThe R function ` mcmc::metrop` implements the Random-Walk Metropolis algorithm. It requires a predefined R function to compute $\\log (h(x))$.  In the degenerate case of $h(x) = 0$ for some $x$, would yield $\\log h(x) = - \\infty$.\n\nThe `metrop` function parameters include a `scale` parameter vector and an output function, $g$, coded as an R function.\n\nThe `scale` argument to the `metrop` function specifies the multivariate normal random vector $y$ in the Metropolis algorithm.  If `z` is a standard multivariate normal random vector, then $y$ in the algorithm is $scale \\times z$ (when `scale` is scalar or vector) and $scale\\ \\%*\\%\\ z$ (when `scale` is a matrix of appropriate dimensions.\n\nThe *acceptance rate* represents the proportion of the time that $u < r$ in step 4 of the algorithm, e.g., acceptance=20%.  When $h$ is a continuous distribution function the acceptance rate may be close to one, which reduces the iterative steps and may substantially increase the execution time to transition from one part of the sample space to another. For `scale` parameters, the algorithm will take large steps.  When $h(x)$ is moderately large and $x$ resembles a sample from the equilibrium distribution, $x + y$ will be extremely large, in the tails of the distribution, corresponding to a very small $r = \\frac{h(x + y)}{ h(x)}$.\n\nThe Metropolis algorithm only requires un-normalized prior distribution, so $h$ can be defined as\n$$\\text{un-normalized posterior}  \\sim  \\text{likelihood} \\times \\text{un-normalized prior}. $$\n\nMCMC-based Bayesian inference is always possible when we can provide specific functions that evaluate the *likelihood* and *unnormalized posterior*.\n\n## Hospital Admissions Case-Study\n\n### Data\n\nWe will demonstrate frequentist and Bayesian inference using a [clinical case-study of hospital admissions](https://umich.instructure.com/courses/38100/files/folder/Case_Studies/18_HospitalAdmissions); N=58,863 (cases) and k=9 (features) including:\n\n* ID: \t\t\tCoded Case Identifier\n* AdmissionLengthDays: \tDuration of hospital stay (in days)\n* Death_1: \t\tIndicator of Death (1) survival (0)\n* Admission_Type: \tType of admission (categorical)\n* Insurance_Type: \tType of Health insurance (string)\n* EnglishLanguage_1: \tPrimary Language English Indicator (1), 0 otherwise\n* Religion_Type: \t\tType of Religious Affiliation (string)\n* Married_1: \t\tIndicator of marital status (Married=1)\n* Race: \t\t\tRace, categorical (string)\n* Dx: \t\t\tDiagnosis\n\nFirst, we will load and inspect the data.\n\n\nEach row of this data frame represents one hospital admission record.  The response variable, $y$ (random outcome), is a binary survival outcome (1 for death, 0 for survival).  The frequentist inference will be based on [logistic regression modeling](https://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/notes/17_RegularizedLinModel_KnockoffFilter.html).\n\n\nLet's fit a logistic prediction model for the clinical outcome $y$ representing the binary variable *Death_1*.\n\n\nStratify the population, say on *Race=white*, refit the logistic model, and report the results.\n\n\nReport the variance-covariance matrix of the estimates of the model coefficients:\n\n\n### Bayesian Inference\n\nTo perform the analogous *MCMC Bayesian inference*, we will use the R function `mcmc::metrop`, which takes as input the log unnormalized posterior probability distribution (that we need to generate MCMC samples from). Let's rewrite the Bayesian rule ($unnormalized\\ posterior\\ \\sim \\ likelihood \\times \\ unnormalized\\ prior$) in terms of `logs`:\n$$\\text{log unnormalized posterior} \\sim \\text{log likelihood} + \\text{log unnormalized prior}. $$\nLet's examine the two terms on the right-hand side.\n\n### Log Likelihood\n\nThe frequentist and Bayesian analyses share the same (log) likelihood function. Thus, the frequentist model matrix can be reused in the Bayesian analysis.\n\n\nWe need to define a function that computes the log likelihood, $\\log(f(x | \\beta))$.\n\n\nThe reason for computing `log likelihoods`, $\\log(f(x | \\beta))$, instead of the raw likelihoods, $f(x | \\beta)$, is that it makes the asymptotic calculations more stable. For instance, let $\\eta = X\\times \\beta$, $p=\\frac{\\exp{(\\eta)}}{1+\\exp{(\\eta)}}$, and $q=1-p=\\frac{1}{1+\\exp{(\\eta)}}$. Then, $\\log(p)=\\eta-\\log(1+\\exp{(\\eta)})$ and $\\log(q)=-\\log(1+\\exp{(\\eta)})$ are computable for large $|\\eta|$, either positive or negative. \n\nThe `mcmc::demo` package (*vignette(\"demo\", \"mcmc\")*) includes additional vignettes and examples.\n\nIdeally, we would choose *informative Bayesian priors*, but we also don't want to introduce strong subjectivity. [Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)\nassume the prior and posterior distributions are of the same probability distribution family.\n\n$$\\prod_{i = 1}^{k} p(\\beta_i) q(\\beta_i),$$\nwhere $p(\\beta) = {\\rm logit}^{- 1}(\\beta)$ is the mean-value\nparameter corresponding to the linear predictor $\\beta$ and $q(\\beta) = 1 - p(\\beta)$; the $p$ and $q$ functions are used to calculate for various $\\beta$ values the log likelihood, see `log_likelihod`.\n\n\nWe are ready to run the MCMC Bayesian inference.\n\n\nThe main output, `mcmc_out`, of the MCMC call, `metrop`, includes:\n\n* accept: fraction of Metropolis proposals accepted.\n* batch: $nbatch \\times k$ batch means matrix, where $k$ is the dimension of the result of `outfun` if `outfun` is a function, or the dimension of state when `outfun` is missing.\n* initial: value of argument initial.\n* final: final state of Markov chain.\n* time: running time of Markov chain from `system.time()`.\n\nSometimes, we need to adjust the *proposal* to get higher acceptance rates (`mcmc_out$accept`). In general, an acceptance rate of $\\sim 20\\%$ is good. Forcing the acceptance rates below 70% may keep the sampler from ever visiting part of the state space. The *acceptance rate* reflects the proportion of the time that $u < r$ in [step 4 of the algorithm](https://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/notes/DSPA_Appendix_01_BayesianInference_MCMC_Gibbs.html#33_random-walk_metropolis_algorithm). \n\nThe `metrop` function accepts as first argument the output of a previous run, and it uses all the same arguments as the previous run except for any modified in the call for this new run. Of course the initial state of the next run is the final\nstate of the previous run and the random number generation seed at the start of the next are the same as at the end of the previous run. This yields a continuous feed-forward iteration if no modification of other arguments is made.\n\n\nIt may be appropriate to check that the batch length is long enough for the validity of this confidence interval for the true unknown acceptance rate.\n\nSimilar to diagnostic plots for linear models, analogous MCMC diagnostic plots may be drawn to confirm there are no clear problems with the simulation estimates. For instance, we can display the time series of any component of the Markov function, e.g., acceptance indicators or their batch means. Ideally, the time series should behave stationarily.\n\n\nAutocorrelation (ACF) plots may also be used to report the timeseries autocorrelation of the time series of *batch means*. These depict the internal timeseries correlation that should not be significantly nonzero, except for lag zero. Otherwise, it may suggest that the batch length is not long enough.\n\n\nIt appears as if some of the low-lag autocorrelations may be significantly non-trivial, i.e., the blue dashed error lines indicate the cutoff points for $0.05$ level tests of the null hypothesis that the true unknown autocorrelation is zero. To mediate this problem, we can increase the batch length and re-plot the resulting autocorrelations.\n\nAre there examples of questions that may be *answerable* using Bayesian inference that may not be within the reach of a frequentist inference? Question of about\nprobability of parameters only make sense in a Bayesian setting, as frequentist inference considers all parameters as static, *not* random.\n\nOnce we applied the Bayesian rule and sampled the posterior probability distribution, we can examine the function of those parameters to answer specific parameter probabilistic questions.  In order to calculate the probabilities of specific events, we need to output the indicators of these events.\n\n\nLet's call the MCMC with a reference to the *output indicator function*.\n\n\nNotice that the increase of `blen` drives the computing time higher. However, longer run times yield more precise answers.\n\n\nLet's report only the coefficient estimates corresponding to the larger probabilities ($p>0.1$).\n\n\nThe Monte Carlo standard errors (MCSE) are computed for the corresponding Monte Carlo estimates of the posterior probabilities via this command:\n`apply(mcmc_out_ts_highProb, 2, sd) / sqrt(mcmc_out$batch)`, where the second parameter represents `margin=2` indicates the function is computed for each column separately.\n\nThe extra term in the denominator, `sqrt(mcmc_out$batch)` ensures that the batch means have variance $\\frac{\\sigma^2}{b}$, where $b$ is the batch length ($out\\$blen$), whereas the CLT dictates that the overall means, $\\mu$, have variance $\\frac{\\sigma^2}{n}$, where $n$ is the total number of iterations, which\nis $out\\$blen \\times out\\$nbatch$.\n\nThe square root law implies that t guarantee one more significant\ndigit of precision ($10 \\times \\ accuracy$) we need to run $100$ times as many iterations. Let's display some diagnostic plots on the `mcmc_out_ts_highProb` results.\n\n\nMost of these autocorrelation plots appear fine and we can generate the final overall summary of the MCMC Bayesian inference.\n\n\nWe can also display histograms of the marginal posteriors for the parameters.\n\n\nOf course, we can also try to forecast one of the clinical outcomes (\"Death_1\", \"AdmissionLengthDays\", or \"Dx\") using the machine learning methods we saw in the [previous chapters](https://dspa.predictive.space/).\n\n\n\nLet's also try *decision tree classification* for the same hospitalization case-study.\n\n\nLastly, we can employ *RandomForest classification* of the clinical outcome variable, `death`.\n\n\n## Summary\n\nData-driven Bayesian inference treats all unknowns (parameters and data prior to being observed) as random variables. The inference is probabilistic in nature, not deterministic (the way the frequentist inference is), which quantifies the measures of intrinsic process uncertainty.\n\nBayesian modeling propagates uncertainty from the initial prior to derived posterior distributions using the Bayesian inversion of conditioning theorem. When the posterior is not in simple standard form, e.g., complex or non-conjugate priors, MCMC sample-based simulation provides a computationally-expensive, but scientifically valid, protocol for modeling and inference. MCMC provides a general recipe for obtaining representative samples from any posterior density.\n\n## References\n\n* [SMHS EBook Bayesian Inference Chapter](https://wiki.socr.umich.edu/index.php/SMHS_BayesianInference)\n* Christou N, Dinov ID. (2011) Confidence Interval Based Parameter Estimation-A New SOCR Applet and Activity. PLoS ONE 6(5): e19178. [doi:10.1371/journal.pone.0019178](https://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0019178).\n* [SOCR General Confidence Interval Activity](https://wiki.socr.umich.edu/index.php/SOCR_EduMaterials_Activities_General_CI_Experiment).\n\n<!--html_preserve-->\n<div>\n    \t<footer><center>\n\t\t\t<a href=\"https://www.socr.umich.edu/\">SOCR Resource</a>\n\t\t\t\tVisitor number \n\t\t\t\t<img class=\"statcounter\" src=\"https://c.statcounter.com/5714596/0/038e9ac4/0/\" alt=\"Web Analytics\" align=\"middle\" border=\"0\">\n\t\t\t\t<script type=\"text/javascript\">\n\t\t\t\t\tvar d = new Date();\n\t\t\t\t\tdocument.write(\" | \" + d.getFullYear() + \" | \");\n\t\t\t\t</script> \n\t\t\t\t<a href=\"https://socr.umich.edu/img/SOCR_Email.png\"><img alt=\"SOCR Email\"\n\t \t\t\ttitle=\"SOCR Email\" src=\"https://socr.umich.edu/img/SOCR_Email.png\"\n\t \t\t\tstyle=\"border: 0px solid ;\"></a>\n\t \t\t </center>\n\t \t</footer>\n\n\t<!-- Start of StatCounter Code -->\n\t\t<script type=\"text/javascript\">\n\t\t\tvar sc_project=5714596; \n\t\t\tvar sc_invisible=1; \n\t\t\tvar sc_partition=71; \n\t\t\tvar sc_click_stat=1; \n\t\t\tvar sc_security=\"038e9ac4\"; \n\t\t</script>\n\t\t\n\t\t<script type=\"text/javascript\" src=\"https://www.statcounter.com/counter/counter.js\"></script>\n\t<!-- End of StatCounter Code -->\n\t\n\t<!-- GoogleAnalytics -->\n\t\t<script src=\"https://www.google-analytics.com/urchin.js\" type=\"text/javascript\"> </script>\n\t\t<script type=\"text/javascript\"> _uacct = \"UA-676559-1\"; urchinTracker(); </script>\n\t<!-- End of GoogleAnalytics Code -->\n</div>\n<!--/html_preserve-->",
      "word_count": 5994
    }
  ],
  "tables": [
    {
      "section": "Main",
      "content": "      smooth_scroll: true\n---",
      "row_count": 2
    }
  ],
  "r_code": [
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# install.packages(\"network\")\nlibrary('igraph')\n\nnodes <- data.frame(c('Stage_0','Stage_1','Stage_2', 'Stage_3', 'Stage_4'))\nnode_names <- c('Stage_0','Stage_1',' Stage_2', ' Stage_3', ' Stage_4' )\nedges <- data.frame(from=c(\t'Stage_0', 'Stage_0', 'Stage_0', 'Stage_0', 'Stage_0',\n\t\t             \t'Stage_1', 'Stage_1', 'Stage_1', 'Stage_1', 'Stage_1',\n\t\t\t'Stage_2', 'Stage_2', 'Stage_2', 'Stage_2', 'Stage_2', \n\t\t\t'Stage_3', 'Stage_3', 'Stage_3', 'Stage_3', 'Stage_3', \n\t\t\t'Stage_4', 'Stage_4', 'Stage_4', 'Stage_4', 'Stage_4'),\n                        to=c('Stage_0', 'Stage_1', 'Stage_2', 'Stage_3', 'Stage_4',\n\t\t\t'Stage_0', 'Stage_1', 'Stage_2', 'Stage_3', 'Stage_4',\n\t\t\t'Stage_0', 'Stage_1', 'Stage_2', 'Stage_3', 'Stage_4',\n\t\t\t'Stage_0', 'Stage_1', 'Stage_2', 'Stage_3', 'Stage_4',\n\t\t\t'Stage_0', 'Stage_1', 'Stage_2', 'Stage_3', 'Stage_4'),\n                        \tsame.dept=c(T,T,T,F, T)\n                        )\nedge_names <- rep('P{i,j}', 25)\nvertex_size <- rep(20, 5)\nfor (i in 1:5) {\n\tvertex_size[i] <- 70-5*(i-1)\n\tfor (j in 1:5) {\n\t\tedge_names[j + (i-1)*5] <- paste0('P{', i-1, ',', j-1, '}')\n\t\t# edge_names[j + (i-1)*5]\n\t}\n}\n\ng <- graph_from_data_frame(edges, directed=TRUE, vertices=nodes)\nplot(g, layout=layout_with_fr, vertex.size=vertex_size, vertex.label=node_names, edge.label=edge_names, edge.label.cex = 0.7, edge.curved=0.3, margin=c(0.1, 0.1, 0.1, 0.1), main='Cancer Staging Transition Graph', ylim=c(-1,1),xlim=c(-1,1), asp = 1)",
      "line_count": 29
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "hosAdmissions <- read.csv(\"https://umich.instructure.com/files/5005926/download?download_frd=1\")\nhead(hosAdmissions)\n\n# remove the case ID:\nhosAdmissions <- hosAdmissions[, -1]\n\n# Clean the diagnostic variable (free text feature!)\npreproc_fun = function(x)                    # text data\n{ require(\"tm\")\n  x  =  gsub(\"<.*?>\", \" \", x)               # regex removing HTML tags\n  x  =  iconv(x, \"latin1\", \"ASCII\", sub=\"\") # remove non-ASCII characters\n  x  =  gsub(\"[^[:alnum:]]\", \" \", x)        # remove non-alpha-numeric values\n  x  =  tolower(x)                          # convert to lowercase characters\n  # x  =  removeNumbers(x)                  # removing numbers\n  x  =  stripWhitespace(x)                  # removing white space\n  x  =  gsub(\"^\\\\s+|\\\\s+$\", \"\", x)          # remove leading and trailing white space\n  return(x)\n}\n\n# Clean the DX feature by itself\nDx11 <- preproc_fun(hosAdmissions$Dx); hosAdmissions$Dx <- Dx11",
      "line_count": 21
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "#Check sizes\ncases <- unique(hosAdmissions$ID); length(cases); dim(hosAdmissions)\n\n# define the  binary outcome variable\ny <- hosAdmissions$Death_1 ",
      "line_count": 5
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# model_glm <- glm(y ~ ., data = hosAdmissions, family = binomial)\nmodel_glm <- glm(y ~ AdmissionLengthDays + Married_1 + Insurance_Type + Admission_Type, data = hosAdmissions, family = binomial)\nsummary(model_glm)",
      "line_count": 3
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "model_glm_white <- glm(y ~ AdmissionLengthDays + as.factor(Married_1) + as.factor(Insurance_Type) + as.factor(Admission_Type), data = hosAdmissions, family = binomial, subset = hosAdmissions$Race == \"white\")\nsummary(model_glm_white)",
      "line_count": 2
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "summ_out <- summary(model_glm_white)\nsumm_out$coefficients\nvar_cov <- vcov(model_glm_white)\nvar_cov\n\n# It may be easier to display the matrix in Rstudio as a table\nView(var_cov)",
      "line_count": 7
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "hosAdmissions$Married_1 <- as.factor(hosAdmissions$Married_1)\nhosAdmissions$Insurance_Type <- as.factor(hosAdmissions$Insurance_Type)\nhosAdmissions$Admission_Type <- as.factor(hosAdmissions$Admission_Type)\n\nmodel_glm <- glm(y ~ AdmissionLengthDays + Married_1 + Insurance_Type + Admission_Type, \n                 data = hosAdmissions, family = binomial, x=TRUE) # return the design matrix X\ndesign_mat <- model_glm$x\nresponse <- model_glm$y",
      "line_count": 8
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "log_likelihod <- function(beta) {\n    # housekeeping\n    stopifnot(is.numeric(beta))\n    stopifnot(is.finite(beta))\n    stopifnot(length(beta) == ncol(design_mat))\n    # Compute the estimates y_hat = X*b\n    eta <- drop(design_mat %*% beta)\n    # use log1p to calculate the function x --> log(1+x)\n    # correctly for small x, avoiding singularity problems\n    logp <- ifelse(eta < 0, eta - log1p(exp(eta)), - log1p(exp(-eta)))\n    logq <- ifelse(eta < 0, -log1p(exp(eta)), -eta -log1p(exp(-eta)))\n    sum(ifelse(response == 1, logp, logq))\n}",
      "line_count": 13
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "log_unnormalized_prior <- function(beta) {\n    stopifnot(is.numeric(beta))\n    stopifnot(is.finite(beta))\n    stopifnot(length(beta) == ncol(design_mat))\n    logp <- ifelse(beta < 0, beta - log1p(exp(beta)), - log1p(exp(- beta)))\n    logq <- ifelse(beta < 0, - log1p(exp(beta)), - beta - log1p(exp(- beta)))\n    sum(logp) + sum(logq)\n}\nlog_unnormalized_posterior <- function(beta) {\n  log_likelihod(beta) + log_unnormalized_prior(beta)\n}",
      "line_count": 11
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# install.packages(\"mcmc\")\nlibrary(mcmc)\nlogUnnormalizedPDF <- function(x) {\n  if (all(x >= 0) && sum(x) <= 1) return(1) \n  else return(-Inf)\n}\nset.seed(1234) \nmcmc_out <- metrop(log_unnormalized_prior, rep(0, ncol(design_mat)),nbatch = 100, blen = 100)\n# Runs a \"random-walk\" Metropolis algorithm with multivariate normal proposal\n# Generates a Markov chain (MC) with equilibrium distribution having the\n# specified unnormalized density\nmcmc_out$accept",
      "line_count": 12
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "mcmc_out <- metrop(mcmc_out, scale = 0.1)\nmcmc_out$accept\nmcmc_out <- metrop(mcmc_out, scale = 0.3)\nmcmc_out$accept\nmcmc_out <- metrop(mcmc_out, scale = 0.5)\nmcmc_out$accept\nmcmc_out <- metrop(mcmc_out, scale = 0.4)\nmcmc_out$accept\nt.test(mcmc_out$batch)$conf.int\nmcmc_out$blen  # check the length of batches",
      "line_count": 10
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# Recall the actual model\n# model_glm <- glm(y ~ AdmissionLengthDays + as.factor(Married_1) + as.factor(Insurance_Type) + as.factor(Admission_Type), data = hosAdmissions, family = binomial, x=TRUE)\n\nlibrary(plotly)\n# Batch Means for AdmissionLengthDays\n#plot(ts(mcmc_out$batch[ , 1]), main=\"Batch Means for AdmissionLengthDays Coefficient\")\nplot_ly(x=c(1:length(ts(mcmc_out$batch[ , 1]))), y=ts(mcmc_out$batch[ , 1]), \n        type=\"scatter\", mode=\"markers+lines\") %>%\n  layout(title=\"Batch Means for AdmissionLengthDays Coefficient\", \n         xaxis=list(title=\"time\"), yaxis=list(title=\"mcmc_out\"))\n\n# Married_1\n# plot(ts(mcmc_out$batch[ , 2]), main=\"Batch Means for Married_1 Coefficient\")\nplot_ly(x=c(1:length(ts(mcmc_out$batch[ , 2]))), y=ts(mcmc_out$batch[ , 2]), \n        type=\"scatter\", mode=\"markers+lines\") %>%\n  layout(title=\"Batch Means for Married_1 Coefficient\", \n         xaxis=list(title=\"time\"), yaxis=list(title=\"mcmc_out\"))",
      "line_count": 17
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "acf(ts(mcmc_out$batch[ , 1]), main=\"Autocorrelation of AdmissionLengthDays Coefficient\")\nacf(ts(mcmc_out$batch[ , 2]), main=\"Autocorrelation of Married_1 Coefficient\")\nacf(ts(mcmc_out$batch), main=\"Autocorrelation of Acceptance Indicators\")",
      "line_count": 3
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "outputIndicatorFun <- function(beta) {\n    stopifnot(is.numeric(beta))\n    stopifnot(is.finite(beta))\n    stopifnot(length(beta) == ncol(design_mat))\n    as.numeric(beta == max(beta))\n}",
      "line_count": 6
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "mcmc_out <- metrop(mcmc_out, blen = 1000, outfun = outputIndicatorFun)\nmcmc_out$time\nmcmc_out$accept",
      "line_count": 3
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "colnames(mcmc_out$batch)\ncolnames(mcmc_out$batch) <- names(model_glm_white$coefficients)\n# skip over the intercept column and deal with the remaining 10-1=9 coefficient estimates\nmcmc_out_ts <- as.ts(mcmc_out$batch[ , 2:10])\n# report the overall means for each coefficient by batch averaging\ncolMeans(mcmc_out_ts)",
      "line_count": 6
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "mcmc_out_ts_highProb <- mcmc_out_ts[ , colMeans(mcmc_out_ts) > 0.1]\ncolMeans(mcmc_out_ts_highProb)\n# Compute the grand variance (variance of batch means)\napply(mcmc_out_ts_highProb, 2, sd) / sqrt(mcmc_out$batch)",
      "line_count": 4
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# plot(mcmc_out_ts_highProb)\ndf <- as.data.frame(mcmc_out_ts_highProb)\n\nlibrary(tidyr)\ndf_long <- gather(df, factors, value, colnames(df), factor_key=TRUE)\ndf_long$index <- rep(c(1:(dim(df)[1])), times=dim(df)[2])\n\nplot_ly(data=df_long, x=~index, y=~value, color=~factors,\n        type=\"scatter\", mode=\"markers+lines\") %>%\n  layout(title=\"MCMC Output (Time-Serties High-Probility)\", \n         legend = list(orientation = 'h', y=-0.5))\n       \nacf(mcmc_out_ts_highProb)",
      "line_count": 13
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "library(knitr)\nbar <- rbind(colMeans(mcmc_out_ts_highProb), apply(mcmc_out_ts_highProb, 2, sd) / sqrt(mcmc_out$nbatch))\nrownames(bar) <- c(\"probability of death\", \"MCSE\")\nkable(bar, digits = 2, caption = \"Estimated (higherst) Posterior Probability of dying following an admission to the hospital. These probability values may not add to one due to rounding error and the fact that we only chose the high-probability predictors.\")",
      "line_count": 4
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# mcmc_out <- metrop(out, blen = 1, nbatch = 1e5)\ntheta <- mcmc_out$batch[ , 1]\n# hist(theta, freq = FALSE)\nplot_ly(x = theta, type = \"histogram\") %>%\n  layout(title=\"Histogram of Theta\", xaxis=list(title=\"Theta\"), yaxis=list(title=\"Density\"))\n\n# Other parameters\nbeta <- mcmc_out$batch[ , 2]\n# hist(beta, freq = FALSE)\nplot_ly(x = beta, type = \"histogram\") %>%\n  layout(title=\"Histogram of Beta\", xaxis=list(title=\"Beta\"), yaxis=list(title=\"Density\"))\n\n# display smooth parameter density plots\nout <- density(beta)\n# plot(out)\nplot_ly(x = out$x, y=out$y, type = \"scatter\", mode=\"lines\") %>%\n  layout(title=\"Beta Density\", xaxis=list(title=\"Beta\"), yaxis=list(title=\"Density\"))",
      "line_count": 17
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "# Define Outcome and features\ndim(hosAdmissions)\nhosAdmissions$Married_1 <- as.numeric(hosAdmissions$Married_1)\nhosAdmissions$Insurance_Type <- as.numeric(hosAdmissions$Insurance_Type)\nhosAdmissions$Admission_Type <- as.numeric(hosAdmissions$Admission_Type)\n\ny <- hosAdmissions[ , \"Death_1\"]   # outcome\nx <- hosAdmissions[ , c(\"AdmissionLengthDays\", \"Admission_Type\", \"Insurance_Type\", \"Married_1\", \"Race\", \"Dx\")]\n\n# split training/testing data (80:20)\nsub <- sample(nrow(x), floor(nrow(x)*0.8))\nx <- x[sub, ]; y <- y[sub]\ndata_train <- cbind(x[ , c(\"AdmissionLengthDays\",\"Married_1\")], y); dim(data_train)  # training data\n\nx_test <- hosAdmissions[-sub, c(\"AdmissionLengthDays\", \"Admission_Type\", \"Insurance_Type\", \"Married_1\", \"Race\", \"Dx\")]\ny_test <- hosAdmissions[-sub, \"Death_1\"]\ndata_test <- cbind(x_test[ , c(\"AdmissionLengthDays\",\"Married_1\")], y_test)  # testing data\ndim(data_test)\n\nlibrary(neuralnet)\n# hosAdmissions_NNmodel <- neuralnet(y.train ~ AdmissionLengthDays + Admission_Type + Insurance_Type + Married_1 + Race + Dx, data=data_train. hidden=3) # can;t have categorical/unstructured features\nset.seed(1234)\nhosAdmissions_NNmodel <- neuralnet(y ~ AdmissionLengthDays + Married_1, data=data_train, hidden=1)\nplot(hosAdmissions_NNmodel)\n\n# evaluate model on testing data\n\n# Compute the posterior probabilities\nhosAdmissions_pred<-compute(hosAdmissions_NNmodel, x_test[, c(\"AdmissionLengthDays\",\"Married_1\")])\n# dichotomize the outcome\nplot(hosAdmissions_pred$net.result)\n# plot(data_test$y_test, hosAdmissions_pred$net.result)\n\n# hist(hosAdmissions_pred$net.result) \npred_results_bin <- as.factor(ifelse (hosAdmissions_pred$net.result>0.107, 1, 0))\ntable(pred_results_bin, data_test$y_test)",
      "line_count": 36
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "library(caret)\ncaret::confusionMatrix(pred_results_bin, as.factor(data_test$y_test))",
      "line_count": 2
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "library(C50)\n\n# The Dx free text is an important feature, but needs to be cleaned prior to processing .... here is an instance of a \"cleaning\" function\npreproc_fun = function(x)                    # text data\n{ require(\"tm\")\n  x  =  gsub(\"<.*?>\", \" \", x)               # regex removing HTML tags\n  x  =  iconv(x, \"latin1\", \"ASCII\", sub=\"\") # remove non-ASCII characters\n  x  =  gsub(\"[^[:alnum:]]\", \" \", x)        # remove non-alpha-numeric values\n  x  =  tolower(x)                          # convert to lowercase characters\n  # x  =  removeNumbers(x)                  # removing numbers\n  x  =  stripWhitespace(x)                  # removing white space\n  x  =  gsub(\"^\\\\s+|\\\\s+$\", \"\", x)          # remove leading and trailing white space\n  return(x)\n}\n\n# Clean the DX feature by itself, if not already pre-cleaned upon loading the hosAdmissions dataset\n# Dx11 <- preproc_fun(x[ , \"Dx\"]); x$Dx <- Dx11\n\nhosAdmissions_DTmodel <- C5.0(x=x[ , c(\"AdmissionLengthDays\", \"Admission_Type\", \"Dx\")], y=as.factor(y), control = C5.0Control(winnow = TRUE, minCases= 100))\n# hosAdmissions_DTmodel\nsummary(hosAdmissions_DTmodel)\nplot(hosAdmissions_DTmodel, type=\"simple\")\n\n# Predict death (survival) based on C50 model using testing data\n# Clean Dx, if not previously cleaned!\n# Dx11 <- preproc_fun(hosAdmissions[-sub, \"Dx\"]); hosAdmissions[-sub, \"Dx\"] <- Dx11\n\n# Extract ONLY the hospital-admission TESTING cases that have the common Dx with the training cases (otherwise, prediction on test cases may fail)\ncommon <- intersect(hosAdmissions[sub, ]$Dx, hosAdmissions[-sub, ]$Dx)\nhosAdmission_test <- hosAdmissions[which(hosAdmissions$Dx %in% common), ]\nhosAdmission_test <- hosAdmission_test[-sub, c(\"AdmissionLengthDays\", \"Admission_Type\", \"Dx\")]\n# View(hosAdmission_test)\n\n# Same preprocessing for the TESTING data outcome variable (death)\ny_test <- hosAdmissions[which(hosAdmissions$Dx %in% common), ]\ny_test <- y_test[-sub, \"Death_1\"]\n\nhosAdmissions_DTpred <- predict(hosAdmissions_DTmodel, hosAdmission_test)\ncaret::confusionMatrix(hosAdmissions_DTpred, as.factor(y_test))",
      "line_count": 39
    },
    {
      "section": "Applied Bayesian Modeling, Simulation and Inference",
      "code": "require(randomForest)\ny <- hosAdmissions[sub, \"Death_1\"]\ndata_train <- cbind(hosAdmissions[sub, c(\"AdmissionLengthDays\",\"Married_1\")], y); dim(data_train)  # training data\nhosAdmissions_RFmodel_sm <- randomForest(as.factor(y) ~ AdmissionLengthDays + Married_1, data=data_train, importance=TRUE, ntree=3000, mtry=2)\n# hosAdmissions_RFmodel <- randomForest(as.factor(y) ~ . , data=data_train, importance=TRUE, ntree=2000, mtry=2)\nvarImpPlot(hosAdmissions_RFmodel_sm, cex=0.8)\nprint(hosAdmissions_RFmodel_sm)\nplot(hosAdmissions_RFmodel_sm,log=\"x\",main=\"hosAdmissions_RF_model\")\nhosAdmissions_RFmodel_pred<-predict(hosAdmissions_RFmodel_sm, hosAdmissions[-sub, ], type = 'class')\ncaret::confusionMatrix(hosAdmissions_RFmodel_pred, as.factor(data_test$y_test))\n\n# Use caret::train to fine-tune RF\n# control <- trainControl(method=\"repeatedcv\", number=10, repeats=3, search=\"grid\")\n# metric <- \"Accuracy\"\n# set.seed(1234)\n# tunegrid <- expand.grid(.mtry=c(1:3))\n# rf_gridsearch <- train(as.factor(y) ~ AdmissionLengthDays + Race + Admission_Type, data=data_train, method=\"rf\", metric=metric, tuneGrid=tunegrid, trControl=control)\n# print(rf_gridsearch)\n# plot(rf_gridsearch)\n\n# Alternative parameter tuning using randomForest::tuneRF()\nset.seed(234)\nbest_mtry <- tuneRF(hosAdmissions[sub, c(\"AdmissionLengthDays\",\"Admission_Type\", \"Race\")], y, stepFactor=1.5, improve=1e-5, ntree=500)\nprint(best_mtry)\n\n# Try NLP on the clinical diagnosis \"Dx\"\nlibrary(text2vec)\nlibrary(data.table)\n\n# split data into train:test (80:20)\nset.seed(1234)\nsub <- sample(nrow(hosAdmissions), floor(nrow(hosAdmissions)*0.8))\ny_train <- hosAdmissions[sub , ] # Training Dx outcome\ny_test <- hosAdmissions[-sub , ] # Testing Dx outcome\n\n# coerce Dx as string/character\ny_train$Dx <- as.character(y_train$Dx)\ny_test$Dx <- as.character(y_test$Dx)\n\n# either a simple (tolower case) function\npreproc_fun = tolower\n\n# or a more elaborate \"cleaning\" function\npreproc_fun = function(x)                    # text data\n{ require(\"tm\")\n  x  =  gsub(\"<.*?>\", \" \", x)               # regex removing HTML tags\n  x  =  iconv(x, \"latin1\", \"ASCII\", sub=\"\") # remove non-ASCII characters\n  x  =  gsub(\"[^[:alnum:]]\", \" \", x)        # remove non-alpha-numeric values\n  x  =  tolower(x)                          # convert to lowercase characters\n  # x  =  removeNumbers(x)                  # removing numbers\n  x  =  stripWhitespace(x)                  # removing white space\n  x  =  gsub(\"^\\\\s+|\\\\s+$\", \"\", x)          # remove leading and trailing white space\n  return(x)\n}\n\n# define the tokenization function\ntoken_fun = word_tokenizer\n\n# iterator for both training and testing sets\niter_train = itoken(y_train$Dx, \n             preprocessor = preproc_fun, \n             tokenizer = token_fun, \n             ids = sub, \n             progressbar = TRUE)\n\niter_test = itoken(y_test$Dx, \n             preprocessor = preproc_fun, \n             tokenizer = token_fun, \n             # ids = -sub, \n             ids = 1:11773,\n             progressbar = TRUE)\nreviewVocab = create_vocabulary(iter_train)\nreviewVocab\n\n# Next, compute the document term matrix (DTM)\nreviewVectorizer = vocab_vectorizer(reviewVocab)\nt0 = Sys.time()\ndtm_train = create_dtm(iter_train, reviewVectorizer)\ndtm_test = create_dtm(iter_test, reviewVectorizer)\ndim(dtm_train); dim(dtm_test)\n\n# confirm that the training data review DTM dimensions are consistent with training review IDs, i.e., #rows = number of documents, and #columns = number of unique terms (n-grams), dim(dtm_train)[[2]]\nidentical(dim(dtm_train)[1], length(sub))\n\n# TM/NLP analytics\nlibrary(glmnet)\nnFolds = 10\nt0 = Sys.time()\nglmnet_classifier = cv.glmnet(x = dtm_train, \n                              y = y_train$Death_1, \n        family = \"binomial\", \n        # LASSO L1 penalty\n        alpha = 1,\n        # interested in the area under ROC curve or MSE\n        type.measure = \"auc\",\n        # n-fold internal (training data) stats cross-validation\n        nfolds = nFolds,\n        # threshold: high value is less accurate / faster training\n        thresh = 1e-2,\n        # again lower number of iterations for faster training\n        maxit = 1e3\n      )\n\nlambda.best <- glmnet_classifier$lambda.min\nlambda.best\n\nt1 = Sys.time()\nprint(difftime(t1, t0, units = 'sec'))\n\n# some prediction plots\nplot(glmnet_classifier)\n# plot(glmnet_classifier, xvar=\"lambda\", label=\"TRUE\")\nmtext(\"CV LASSO: Number of Nonzero (Active) Coefficients\", side=3, line=2.5)\n\n# report the mean internal cross-validated error\nprint(paste(\"max AUC =\", round(max(glmnet_classifier$cvm), 4)))\n\n# report TESTING data prediction accuracy\nxTest = dtm_test\nyTest = y_test$Death_1\npredLASSO <- predict(glmnet_classifier, \n       s = glmnet_classifier$lambda.1se, newx = xTest)\ntestMSE_LASSO <- mean((predLASSO - yTest)^2); testMSE_LASSO\n\n# Binarize the LASSO probability prediction \nbinPredLASSO <- ifelse(predLASSO < -2, 0, 1)\ntable(binPredLASSO, yTest)\n\n# and testing data AUC\nglmnet:::auc(yTest, predLASSO)\n\n# predict\npredTest <- predict(glmnet_classifier, \n                    s = glmnet_classifier$lambda.1se, \n                    newx = dtm_test, type=\"response\") \npredVSrealDeath <- cbind(predTest, y_test$Death_1)\ncor(predTest, y_test$Death_1)\n\ntable (ifelse(predTest < 0.2, 0, 1), y_test$Death_1)\n\n# install.packages(\"RTextTools\")\n# library(\"RTextTools\")\n# \n# # DxCorpus <- create_corpus(iter_train, reviewVectorizer)\n# # DxDTM <- get_dtm(DxCorpus)\n# DxDTM_train <- create_dtm(iter_train, reviewVectorizer)\n# dim(DxDTM_train)\n# image(DxDTM_train[1:dim(DxDTM_train)[2], ], xlab=\"Terms\", ylab=\"Training records/documents\")\n# \n# DxDTM_test <- create_dtm(iter_test, reviewVectorizer)\n# dim(DxDTM_test)\n# image(DxDTM_test[1:dim(DxDTM_test)[2], ], xlab=\"Terms\", ylab=\"Testing records/documents\")",
      "line_count": 152
    }
  ]
}